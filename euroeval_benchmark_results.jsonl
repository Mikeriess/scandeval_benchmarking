
{"dataset": "allocine", "task": "sentiment-classification", "dataset_languages": ["fr"], "model": "meta-llama/Llama-3.1-70B-Instruct", "results": {"raw": [{"mcc": 0.94936677108302, "macro_f1": 0.9745705727733969}, {"mcc": 0.9443675291125764, "macro_f1": 0.9721556939788263}, {"mcc": 0.9556923540744713, "macro_f1": 0.9778345542847784}, {"mcc": 0.9442990974735204, "macro_f1": 0.9721493166519344}, {"mcc": 0.9423754456722386, "macro_f1": 0.9711484760377291}, {"mcc": 0.9452456725014704, "macro_f1": 0.9726144629867352}, {"mcc": 0.9478549059696882, "macro_f1": 0.9736263736263736}, {"mcc": 0.9385070951398887, "macro_f1": 0.9692144340744753}, {"mcc": 0.9490034649118624, "macro_f1": 0.9744867206178998}, {"mcc": 0.9549771199849664, "macro_f1": 0.977442492819085}], "total": {"test_mcc": 94.71689455923702, "test_mcc_se": 0.33318126789712005, "test_macro_f1": 97.35243097851234, "test_macro_f1_se": 0.16627665129449537}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "sentipolc16", "task": "sentiment-classification", "dataset_languages": ["it"], "model": "meta-llama/Llama-3.1-70B-Instruct", "results": {"raw": [{"mcc": 0.4812559406716472, "macro_f1": 0.49456678130181664}, {"mcc": 0.5144384867690885, "macro_f1": 0.5056392633712221}, {"mcc": 0.5067504203338918, "macro_f1": 0.5028084164137739}, {"mcc": 0.4798547952032152, "macro_f1": 0.4877808402238369}, {"mcc": 0.5121947006017273, "macro_f1": 0.5029872117710956}, {"mcc": 0.49928012019694584, "macro_f1": 0.49890041183094436}, {"mcc": 0.5002922068861326, "macro_f1": 0.5004062266419681}, {"mcc": 0.5076948302775661, "macro_f1": 0.5010434766362125}, {"mcc": 0.4998425854355079, "macro_f1": 0.49801906219816666}, {"mcc": 0.4876131521422453, "macro_f1": 0.4891078054159121}], "total": {"test_mcc": 49.89217238517968, "test_mcc_se": 0.7623489767557768, "test_macro_f1": 49.812594958049495, "test_macro_f1_se": 0.36804533763633424}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "eltec", "task": "named-entity-recognition", "dataset_languages": ["fr"], "model": "meta-llama/Llama-3.1-70B-Instruct", "results": {"raw": [{"micro_f1_no_misc": 0.6100320170757737, "micro_f1": 0.5622188905547226}, {"micro_f1_no_misc": 0.6686541737649063, "micro_f1": 0.6350379156134551}, {"micro_f1_no_misc": 0.6969342382401124, "micro_f1": 0.6325994748535648}, {"micro_f1_no_misc": 0.7097353073126964, "micro_f1": 0.6413086322428064}, {"micro_f1_no_misc": 0.7436578883788355, "micro_f1": 0.6931239388794568}, {"micro_f1_no_misc": 0.6867770469494217, "micro_f1": 0.6236472945891784}, {"micro_f1_no_misc": 0.6518046709129511, "micro_f1": 0.5985401459854015}, {"micro_f1_no_misc": 0.6781818181818181, "micro_f1": 0.6234753049390122}, {"micro_f1_no_misc": 0.6511116002641426, "micro_f1": 0.5777347531461762}, {"micro_f1_no_misc": 0.6626452189454871, "micro_f1": 0.5603416264389157}], "total": {"test_micro_f1_no_misc": 67.59533980026144, "test_micro_f1_no_misc_se": 2.2692138368164154, "test_micro_f1": 61.480279772426904, "test_micro_f1_se": 2.5375907272425153}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "multinerd-it", "task": "named-entity-recognition", "dataset_languages": ["it"], "model": "meta-llama/Llama-3.1-70B-Instruct", "results": {"raw": [{"micro_f1_no_misc": 0.8505246992577424, "micro_f1": 0.5521435543208949}, {"micro_f1_no_misc": 0.8441952814762907, "micro_f1": 0.6279683377308708}, {"micro_f1_no_misc": 0.8152941176470588, "micro_f1": 0.5586786438713416}, {"micro_f1_no_misc": 0.8573518653986832, "micro_f1": 0.6373732496378561}, {"micro_f1_no_misc": 0.862142767216417, "micro_f1": 0.6510478786846141}, {"micro_f1_no_misc": 0.83056640625, "micro_f1": 0.6138582677165354}, {"micro_f1_no_misc": 0.8049921996879876, "micro_f1": 0.5504666188083274}, {"micro_f1_no_misc": 0.846957590657652, "micro_f1": 0.6626881162428646}, {"micro_f1_no_misc": 0.8444916755224937, "micro_f1": 0.5589130596183829}, {"micro_f1_no_misc": 0.8372588472008522, "micro_f1": 0.5814432989690722}], "total": {"test_micro_f1_no_misc": 83.93775450315178, "test_micro_f1_no_misc_se": 1.1158239210216594, "test_micro_f1": 59.945810256007604, "test_micro_f1_se": 2.7242923730873496}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-fr", "task": "linguistic-acceptability", "dataset_languages": ["fr"], "model": "meta-llama/Llama-3.1-70B-Instruct", "results": {"raw": [{"mcc": 0.396198137014601, "macro_f1": 0.6086954654942187}, {"mcc": 0.39976841669547497, "macro_f1": 0.6288184097306977}, {"mcc": 0.4262643507588093, "macro_f1": 0.651973126920687}, {"mcc": 0.39762836275933755, "macro_f1": 0.6292814628459226}, {"mcc": 0.3719547850663212, "macro_f1": 0.5979984876031728}, {"mcc": 0.4109698654816255, "macro_f1": 0.6398536144864625}, {"mcc": 0.4374666072993036, "macro_f1": 0.6690656811255917}, {"mcc": 0.3796108889901981, "macro_f1": 0.6261107234850671}, {"mcc": 0.43027837595157326, "macro_f1": 0.6464454649034975}, {"mcc": 0.3805788240783069, "macro_f1": 0.5954004551080098}], "total": {"test_mcc": 40.307186140955515, "test_mcc_se": 1.4066984185995675, "test_macro_f1": 62.93642891703327, "test_macro_f1_se": 1.4703046999539742}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-it", "task": "linguistic-acceptability", "dataset_languages": ["it"], "model": "meta-llama/Llama-3.1-70B-Instruct", "results": {"raw": [{"mcc": 0.17671386484649523, "macro_f1": 0.5466035856165367}, {"mcc": 0.26971023315311093, "macro_f1": 0.595633104404576}, {"mcc": 0.20169796521193578, "macro_f1": 0.5552440180811953}, {"mcc": 0.24093691786143048, "macro_f1": 0.5437498941477034}, {"mcc": 0.16522648294634282, "macro_f1": 0.5261146323228634}, {"mcc": 0.20276122334598115, "macro_f1": 0.5392612859097128}, {"mcc": 0.1588680409035216, "macro_f1": 0.5271014133399454}, {"mcc": 0.3019736041917631, "macro_f1": 0.6469560611532443}, {"mcc": 0.21033679967049274, "macro_f1": 0.5446394738902371}, {"mcc": 0.21391040032333963, "macro_f1": 0.5469516492764914}], "total": {"test_mcc": 21.421355324544137, "test_mcc_se": 2.8228119772144487, "test_macro_f1": 55.72255118142506, "test_macro_f1_se": 2.2901249173560863}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "squad-it", "task": "reading-comprehension", "dataset_languages": ["it"], "model": "meta-llama/Llama-3.1-70B-Instruct", "results": {"raw": [{"f1": 72.75495764568358, "em": 48.78787878787879}, {"f1": 73.2212782683279, "em": 47.99090219863533}, {"f1": 72.93485859551353, "em": 53.69075369075369}, {"f1": 72.8517278618178, "em": 49.54337899543379}, {"f1": 68.9669491384747, "em": 41.094834232845024}, {"f1": 71.20904323935275, "em": 46.56488549618321}, {"f1": 75.29108121490476, "em": 54.13416536661467}, {"f1": 69.81872128080968, "em": 41.90251572327044}, {"f1": 71.96990883133977, "em": 49.845201238390096}, {"f1": 70.23393998817876, "em": 44.21296296296296}], "total": {"test_f1": 71.92524660644034, "test_f1_se": 1.1712551747864024, "test_em": 47.7767478692968, "test_em_se": 2.750088698622679}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fquad", "task": "reading-comprehension", "dataset_languages": ["fr"], "model": "meta-llama/Llama-3.1-70B-Instruct", "results": {"raw": [{"f1": 71.8956063511102, "em": 41.89393939393939}, {"f1": 69.90132682423464, "em": 40.10614101592115}, {"f1": 71.34055734161349, "em": 40.94794094794095}, {"f1": 71.43118559133897, "em": 40.41095890410959}, {"f1": 67.789945481697, "em": 38.16499614494988}, {"f1": 72.33787091648126, "em": 42.900763358778626}, {"f1": 70.53326543798424, "em": 41.65366614664587}, {"f1": 69.9501754072553, "em": 40.56603773584906}, {"f1": 68.524207898654, "em": 38.85448916408669}, {"f1": 74.13166693197653, "em": 43.82716049382716}], "total": {"test_f1": 70.78358081823457, "test_f1_se": 1.153904432804268, "test_em": 40.93260933060484, "test_em_se": 1.0688377615404479}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "orange-sum", "task": "summarization", "dataset_languages": ["fr"], "model": "meta-llama/Llama-3.1-70B-Instruct", "results": {"raw": [{"bertscore": 0.6786719934316352, "rouge_l": 0.22523338644458835}, {"bertscore": 0.6802215885254554, "rouge_l": 0.2270797192734706}, {"bertscore": 0.6798073055688292, "rouge_l": 0.2266841071848561}, {"bertscore": 0.6738334292895161, "rouge_l": 0.21715972335146633}, {"bertscore": 0.6764012625208125, "rouge_l": 0.2307764514442902}, {"bertscore": 0.6784973667818122, "rouge_l": 0.22001265431986672}, {"bertscore": 0.6797272520780098, "rouge_l": 0.22068743123193657}, {"bertscore": 0.6759132213483099, "rouge_l": 0.2217698195556339}, {"bertscore": 0.6803429461142514, "rouge_l": 0.22817528814376148}, {"bertscore": 0.6775528457947075, "rouge_l": 0.21562423877776948}], "total": {"test_bertscore": 67.80969211453339, "test_bertscore_se": 0.13349517341545847, "test_rouge_l": 22.3320281972764, "test_rouge_l_se": 0.31040691505225626}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "ilpost-sum", "task": "summarization", "dataset_languages": ["it"], "model": "meta-llama/Llama-3.1-70B-Instruct", "results": {"raw": [{"bertscore": 0.6833176146756159, "rouge_l": 0.252916881703454}, {"bertscore": 0.6722295910294633, "rouge_l": 0.22542460872584696}, {"bertscore": 0.6790720994176809, "rouge_l": 0.24430764649316802}, {"bertscore": 0.680148717132397, "rouge_l": 0.25016068055746915}, {"bertscore": 0.6715286978287622, "rouge_l": 0.21913953948595793}, {"bertscore": 0.6721191934484523, "rouge_l": 0.22261466498392213}, {"bertscore": 0.6782850751042133, "rouge_l": 0.23868844754004026}, {"bertscore": 0.6812655951216584, "rouge_l": 0.2519841120889551}, {"bertscore": 0.6782630771631375, "rouge_l": 0.24021686736564066}, {"bertscore": 0.6800111327320337, "rouge_l": 0.24005596055894257}], "total": {"test_bertscore": 67.76240793653415, "test_bertscore_se": 0.25907673310787144, "test_rouge_l": 23.85509409503397, "test_rouge_l_se": 0.7627182882686983}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-fr", "task": "knowledge", "dataset_languages": ["fr"], "model": "meta-llama/Llama-3.1-70B-Instruct", "results": {"raw": [{"mcc": 0.6978137974746945, "accuracy": 0.7724609375}, {"mcc": 0.7014775085058984, "accuracy": 0.775390625}, {"mcc": 0.6930886836404639, "accuracy": 0.76904296875}, {"mcc": 0.6928346962244241, "accuracy": 0.7685546875}, {"mcc": 0.6792269409992017, "accuracy": 0.7587890625}, {"mcc": 0.6853559758627419, "accuracy": 0.7626953125}, {"mcc": 0.7031073822511391, "accuracy": 0.77685546875}, {"mcc": 0.6923835772272289, "accuracy": 0.76904296875}, {"mcc": 0.7077412138032424, "accuracy": 0.78076171875}, {"mcc": 0.6893128425398907, "accuracy": 0.7646484375}], "total": {"test_mcc": 69.42342618528924, "test_mcc_se": 0.5313363186177646, "test_accuracy": 76.982421875, "test_accuracy_se": 0.4179401721473716}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-it", "task": "knowledge", "dataset_languages": ["it"], "model": "meta-llama/Llama-3.1-70B-Instruct", "results": {"raw": [{"mcc": 0.7174186374832023, "accuracy": 0.787109375}, {"mcc": 0.7170438887490672, "accuracy": 0.78662109375}, {"mcc": 0.6997523443151092, "accuracy": 0.7744140625}, {"mcc": 0.6877001556413215, "accuracy": 0.765625}, {"mcc": 0.6904979593687663, "accuracy": 0.767578125}, {"mcc": 0.6946015167418456, "accuracy": 0.77001953125}, {"mcc": 0.6892006496292836, "accuracy": 0.76611328125}, {"mcc": 0.6911500621067949, "accuracy": 0.76806640625}, {"mcc": 0.6827944445163988, "accuracy": 0.76171875}, {"mcc": 0.7007663092469499, "accuracy": 0.775390625}], "total": {"test_mcc": 69.70925967798739, "test_mcc_se": 0.7367673287522584, "test_accuracy": 77.2265625, "test_accuracy_se": 0.5382113222599968}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-fr", "task": "common-sense-reasoning", "dataset_languages": ["fr"], "model": "meta-llama/Llama-3.1-70B-Instruct", "results": {"raw": [{"mcc": 0.7110308282858266, "accuracy": 0.77978515625}, {"mcc": 0.7427197693462066, "accuracy": 0.80322265625}, {"mcc": 0.6825570418669009, "accuracy": 0.759765625}, {"mcc": 0.6496925569547255, "accuracy": 0.72900390625}, {"mcc": 0.7119767924040227, "accuracy": 0.7802734375}, {"mcc": 0.7382170942701173, "accuracy": 0.80029296875}, {"mcc": 0.6920508325978055, "accuracy": 0.763671875}, {"mcc": 0.7083258813860898, "accuracy": 0.77734375}, {"mcc": 0.695384518841369, "accuracy": 0.76806640625}, {"mcc": 0.7274124526194234, "accuracy": 0.7900390625}], "total": {"test_mcc": 70.59367768572487, "test_mcc_se": 1.7209530363847936, "test_accuracy": 77.5146484375, "test_accuracy_se": 1.3456984713196622}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-it", "task": "common-sense-reasoning", "dataset_languages": ["it"], "model": "meta-llama/Llama-3.1-70B-Instruct", "results": {"raw": [{"mcc": 0.687619041702407, "accuracy": 0.75732421875}, {"mcc": 0.6705478973052977, "accuracy": 0.744140625}, {"mcc": 0.6959412175477057, "accuracy": 0.76708984375}, {"mcc": 0.6833995540209219, "accuracy": 0.75439453125}, {"mcc": 0.6668924563520735, "accuracy": 0.740234375}, {"mcc": 0.7152798023996924, "accuracy": 0.783203125}, {"mcc": 0.7275526593317296, "accuracy": 0.79150390625}, {"mcc": 0.7022494639863731, "accuracy": 0.7705078125}, {"mcc": 0.7009799406332958, "accuracy": 0.77099609375}, {"mcc": 0.6797225069605626, "accuracy": 0.7548828125}], "total": {"test_mcc": 69.3018454024006, "test_mcc_se": 1.1924489768387507, "test_accuracy": 76.3427734375, "test_accuracy_se": 1.0125503192637242}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "meta-llama/Llama-3.1-70B-Instruct", "results": {"raw": [{"test_speed": 254.89, "test_speed_short": 28.88}, {"test_speed": 501.96, "test_speed_short": 54.0}, {"test_speed": 746.94, "test_speed_short": 104.4}, {"test_speed": 994.74, "test_speed_short": 128.88}, {"test_speed": 1242.54, "test_speed_short": 154.8}, {"test_speed": 1498.76, "test_speed_short": 207.48000000000002}, {"test_speed": 1733.2299999999998, "test_speed_short": 232.96}, {"test_speed": 1935.45, "test_speed_short": 257.73}, {"test_speed": 2158.02, "test_speed_short": 281.58}, {"test_speed": 2327.3199999999997, "test_speed_short": 305.15}], "total": {"test_speed": 1339.3849999999998, "test_speed_se": 440.14242016167736, "test_speed_short": 175.586, "test_speed_short_se": 59.544786609317484}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "allocine", "task": "sentiment-classification", "dataset_languages": ["fr"], "model": "Qwen/Qwen2.5-72B-Instruct", "results": {"raw": [{"mcc": 0.9384326031517211, "macro_f1": 0.9687266174966024}, {"mcc": 0.9332436309032404, "macro_f1": 0.9663062721521025}, {"mcc": 0.9529132703100698, "macro_f1": 0.9763800414425705}, {"mcc": 0.9458436826219949, "macro_f1": 0.9726536420480774}, {"mcc": 0.9485293410014741, "macro_f1": 0.9740949991635002}, {"mcc": 0.9599409849602625, "macro_f1": 0.9799590196703196}, {"mcc": 0.9418278760913863, "macro_f1": 0.970213643576089}, {"mcc": 0.9368866586929161, "macro_f1": 0.9682489935298261}, {"mcc": 0.9455444083523088, "macro_f1": 0.972562049271945}, {"mcc": 0.9639357800998487, "macro_f1": 0.9818638520406637}], "total": {"test_mcc": 94.67098236185223, "test_mcc_se": 0.6149243650924974, "test_macro_f1": 97.31009130391698, "test_macro_f1_se": 0.3155221885848264}}, "num_model_parameters": 72706203648, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "sentipolc16", "task": "sentiment-classification", "dataset_languages": ["it"], "model": "Qwen/Qwen2.5-72B-Instruct", "results": {"raw": [{"mcc": 0.48574745023638505, "macro_f1": 0.49913972384346134}, {"mcc": 0.47177568062223346, "macro_f1": 0.4858687951967791}, {"mcc": 0.4858251338893662, "macro_f1": 0.49401514120472517}, {"mcc": 0.48274683515084704, "macro_f1": 0.488292419949822}, {"mcc": 0.48542882158816764, "macro_f1": 0.4911603030753231}, {"mcc": 0.5114542543597171, "macro_f1": 0.5051480051480052}, {"mcc": 0.5161370174956572, "macro_f1": 0.5073366424151765}, {"mcc": 0.4653854738025716, "macro_f1": 0.48011630678297346}, {"mcc": 0.48206989667616246, "macro_f1": 0.49033377154542795}, {"mcc": 0.4726373716069476, "macro_f1": 0.4790851802025065}], "total": {"test_mcc": 48.59207935428056, "test_mcc_se": 1.009689695815816, "test_macro_f1": 49.204962893642005, "test_macro_f1_se": 0.5937532820398816}}, "num_model_parameters": 72706203648, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "eltec", "task": "named-entity-recognition", "dataset_languages": ["fr"], "model": "Qwen/Qwen2.5-72B-Instruct", "results": {"raw": [{"micro_f1_no_misc": 0.5980206359233523, "micro_f1": 0.5161059413027917}, {"micro_f1_no_misc": 0.5760341813944455, "micro_f1": 0.5197391005835907}, {"micro_f1_no_misc": 0.6515641855447681, "micro_f1": 0.5833965125094769}, {"micro_f1_no_misc": 0.6769883351007424, "micro_f1": 0.5930447650758417}, {"micro_f1_no_misc": 0.6881525192918747, "micro_f1": 0.6404380450212939}, {"micro_f1_no_misc": 0.6079039933788537, "micro_f1": 0.5326633165829145}, {"micro_f1_no_misc": 0.6545529630394384, "micro_f1": 0.5626319493314567}, {"micro_f1_no_misc": 0.6611606172571179, "micro_f1": 0.572352158204982}, {"micro_f1_no_misc": 0.5742171614477429, "micro_f1": 0.48106519107729556}, {"micro_f1_no_misc": 0.5881868697262811, "micro_f1": 0.47887323943661964}], "total": {"test_micro_f1_no_misc": 62.76781462104617, "test_micro_f1_no_misc_se": 2.6824756803454806, "test_micro_f1": 54.80310219126263, "test_micro_f1_se": 3.1969135489108647}}, "num_model_parameters": 72706203648, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "multinerd-it", "task": "named-entity-recognition", "dataset_languages": ["it"], "model": "Qwen/Qwen2.5-72B-Instruct", "results": {"raw": [{"micro_f1_no_misc": 0.7310834813499112, "micro_f1": 0.43396603396603395}, {"micro_f1_no_misc": 0.7246876346402413, "micro_f1": 0.5051104972375692}, {"micro_f1_no_misc": 0.7347167694896242, "micro_f1": 0.5145790261638269}, {"micro_f1_no_misc": 0.6021560574948666, "micro_f1": 0.49833835690547956}, {"micro_f1_no_misc": 0.7783430232558141, "micro_f1": 0.5527669045974365}, {"micro_f1_no_misc": 0.7365380171019181, "micro_f1": 0.5232896652110626}, {"micro_f1_no_misc": 0.690083578043822, "micro_f1": 0.4723341391351061}, {"micro_f1_no_misc": 0.7246446063791662, "micro_f1": 0.5483364720652857}, {"micro_f1_no_misc": 0.7351977277692812, "micro_f1": 0.4918214582755752}, {"micro_f1_no_misc": 0.7375585806739566, "micro_f1": 0.477963317820969}], "total": {"test_micro_f1_no_misc": 71.95009476198602, "test_micro_f1_no_misc_se": 2.876103548975509, "test_micro_f1": 50.18505871378345, "test_micro_f1_se": 2.2202509582490584}}, "num_model_parameters": 72706203648, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-fr", "task": "linguistic-acceptability", "dataset_languages": ["fr"], "model": "Qwen/Qwen2.5-72B-Instruct", "results": {"raw": [{"mcc": 0.4318119667487807, "macro_f1": 0.6553552492046659}, {"mcc": 0.4650704329977707, "macro_f1": 0.6818654035446122}, {"mcc": 0.4822308238156501, "macro_f1": 0.699305684541973}, {"mcc": 0.4756412648905439, "macro_f1": 0.6822098755016692}, {"mcc": 0.46475406614707065, "macro_f1": 0.6758397904930464}, {"mcc": 0.4755483610299895, "macro_f1": 0.6923174739907807}, {"mcc": 0.4756108346387524, "macro_f1": 0.6867016793101213}, {"mcc": 0.422651273730738, "macro_f1": 0.6670878119897037}, {"mcc": 0.47436833886044155, "macro_f1": 0.685904565154022}, {"mcc": 0.4432561586105979, "macro_f1": 0.6621068186290999}], "total": {"test_mcc": 46.10943521470335, "test_mcc_se": 1.2963260666010823, "test_macro_f1": 67.88694352359694, "test_macro_f1_se": 0.8546811561072948}}, "num_model_parameters": 72706203648, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-it", "task": "linguistic-acceptability", "dataset_languages": ["it"], "model": "Qwen/Qwen2.5-72B-Instruct", "results": {"raw": [{"mcc": 0.35230565768949634, "macro_f1": 0.6490232973461695}, {"mcc": 0.4649073514856071, "macro_f1": 0.7275101118091447}, {"mcc": 0.39687559118219984, "macro_f1": 0.6784296353174545}, {"mcc": 0.3607271978267065, "macro_f1": 0.6334725088384444}, {"mcc": 0.3186832239252427, "macro_f1": 0.6210628488231011}, {"mcc": 0.3434769824809389, "macro_f1": 0.6344554314068468}, {"mcc": 0.31110041709945674, "macro_f1": 0.5943316990421812}, {"mcc": 0.37840353637757457, "macro_f1": 0.6529870487208045}, {"mcc": 0.29860013433136995, "macro_f1": 0.5610425946566554}, {"mcc": 0.3172839149240636, "macro_f1": 0.5919280265727496}], "total": {"test_mcc": 35.423640073226565, "test_mcc_se": 3.0919620682094853, "test_macro_f1": 63.44243202533551, "test_macro_f1_se": 2.931771300525812}}, "num_model_parameters": 72706203648, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "squad-it", "task": "reading-comprehension", "dataset_languages": ["it"], "model": "Qwen/Qwen2.5-72B-Instruct", "results": {"raw": [{"f1": 72.74960532491124, "em": 49.92424242424242}, {"f1": 70.26752954884758, "em": 43.669446550416986}, {"f1": 72.98404347149706, "em": 51.981351981351985}, {"f1": 72.13283438528453, "em": 47.71689497716895}, {"f1": 69.39248458619633, "em": 43.25366229760987}, {"f1": 70.30681952068976, "em": 46.79389312977099}, {"f1": 73.71294807316394, "em": 51.014040561622465}, {"f1": 69.16425589975896, "em": 41.588050314465406}, {"f1": 71.91961626754426, "em": 49.76780185758514}, {"f1": 69.76737610140795, "em": 43.28703703703704}], "total": {"test_f1": 71.23975131793017, "test_f1_se": 1.0205867917641005, "test_em": 46.89964211312713, "test_em_se": 2.3172337778191996}}, "num_model_parameters": 72706203648, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fquad", "task": "reading-comprehension", "dataset_languages": ["fr"], "model": "Qwen/Qwen2.5-72B-Instruct", "results": {"raw": [{"f1": 71.6413727919635, "em": 38.40909090909091}, {"f1": 70.68360882940324, "em": 37.6800606520091}, {"f1": 72.0585667903638, "em": 38.77233877233877}, {"f1": 71.34660904093484, "em": 37.51902587519026}, {"f1": 68.29675908019617, "em": 35.92906707787201}, {"f1": 71.1915041302204, "em": 36.56488549618321}, {"f1": 71.6332426329535, "em": 40.5616224648986}, {"f1": 71.18157648476829, "em": 40.25157232704402}, {"f1": 70.20253271434957, "em": 40.01547987616099}, {"f1": 71.63325967000938, "em": 37.114197530864196}], "total": {"test_f1": 70.98690321651625, "test_f1_se": 0.6715545955061207, "test_em": 38.28173409816521, "test_em_se": 0.9936344825627262}}, "num_model_parameters": 72706203648, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "orange-sum", "task": "summarization", "dataset_languages": ["fr"], "model": "Qwen/Qwen2.5-72B-Instruct", "results": {"raw": [{"bertscore": 0.6632946155732498, "rouge_l": 0.1660163778517221}, {"bertscore": 0.6615190334850922, "rouge_l": 0.1576043356587636}, {"bertscore": 0.6641684235655703, "rouge_l": 0.1641749837431774}, {"bertscore": 0.6613074163615238, "rouge_l": 0.15909774591885695}, {"bertscore": 0.6632818244979717, "rouge_l": 0.16327318933864737}, {"bertscore": 0.6633794842637144, "rouge_l": 0.16120285734201636}, {"bertscore": 0.6616456420742907, "rouge_l": 0.1598133618988858}, {"bertscore": 0.6607642391463742, "rouge_l": 0.15860870196131932}, {"bertscore": 0.6657400036638137, "rouge_l": 0.16590559181413744}, {"bertscore": 0.6627792607469019, "rouge_l": 0.15779361985236495}], "total": {"test_bertscore": 66.27879943378503, "test_bertscore_se": 0.09393774795457596, "test_rouge_l": 16.134907653798916, "test_rouge_l_se": 0.20227583322814788}}, "num_model_parameters": 72706203648, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "ilpost-sum", "task": "summarization", "dataset_languages": ["it"], "model": "Qwen/Qwen2.5-72B-Instruct", "results": {"raw": [{"bertscore": 0.6482558840507409, "rouge_l": 0.15249573353193036}, {"bertscore": 0.6454841164959362, "rouge_l": 0.1474273790993777}, {"bertscore": 0.6460280791798141, "rouge_l": 0.14810136762936826}, {"bertscore": 0.6474101626808988, "rouge_l": 0.15140323305107067}, {"bertscore": 0.64641465623572, "rouge_l": 0.14674840067217793}, {"bertscore": 0.647441564273322, "rouge_l": 0.15172008193907577}, {"bertscore": 0.6493601826659869, "rouge_l": 0.15420507870374456}, {"bertscore": 0.649682809074875, "rouge_l": 0.15505545536809584}, {"bertscore": 0.6528959429560928, "rouge_l": 0.16447141960927375}, {"bertscore": 0.6511710307677276, "rouge_l": 0.15860410030799105}], "total": {"test_bertscore": 64.84144428381114, "test_bertscore_se": 0.14674090606914375, "test_rouge_l": 15.302322499121058, "test_rouge_l_se": 0.33774645469120174}}, "num_model_parameters": 72706203648, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-fr", "task": "knowledge", "dataset_languages": ["fr"], "model": "Qwen/Qwen2.5-72B-Instruct", "results": {"raw": [{"mcc": 0.7383701752956613, "accuracy": 0.802734375}, {"mcc": 0.7359766432308731, "accuracy": 0.80126953125}, {"mcc": 0.7324718231584136, "accuracy": 0.798828125}, {"mcc": 0.7187754131912278, "accuracy": 0.78759765625}, {"mcc": 0.7257423877741621, "accuracy": 0.7939453125}, {"mcc": 0.74552498851486, "accuracy": 0.80859375}, {"mcc": 0.7509586844989641, "accuracy": 0.8125}, {"mcc": 0.7429175819099259, "accuracy": 0.806640625}, {"mcc": 0.7473480684656059, "accuracy": 0.810546875}, {"mcc": 0.7369079005880059, "accuracy": 0.8017578125}], "total": {"test_mcc": 73.74993666627701, "test_mcc_se": 0.6161573583071688, "test_accuracy": 80.244140625, "test_accuracy_se": 0.47557167319043064}}, "num_model_parameters": 72706203648, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-it", "task": "knowledge", "dataset_languages": ["it"], "model": "Qwen/Qwen2.5-72B-Instruct", "results": {"raw": [{"mcc": 0.7579018708911776, "accuracy": 0.81787109375}, {"mcc": 0.766632669170291, "accuracy": 0.82421875}, {"mcc": 0.7639683646816806, "accuracy": 0.82275390625}, {"mcc": 0.749412004559809, "accuracy": 0.8115234375}, {"mcc": 0.7449167354561523, "accuracy": 0.80810546875}, {"mcc": 0.7312842346319485, "accuracy": 0.79736328125}, {"mcc": 0.7361212359979511, "accuracy": 0.80126953125}, {"mcc": 0.7481828232570205, "accuracy": 0.81103515625}, {"mcc": 0.7289245855192162, "accuracy": 0.79638671875}, {"mcc": 0.7329786117157993, "accuracy": 0.7998046875}], "total": {"test_mcc": 74.60323135881046, "test_mcc_se": 0.8475884655478746, "test_accuracy": 80.9033203125, "test_accuracy_se": 0.63665560314958}}, "num_model_parameters": 72706203648, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-fr", "task": "common-sense-reasoning", "dataset_languages": ["fr"], "model": "Qwen/Qwen2.5-72B-Instruct", "results": {"raw": [{"mcc": 0.7579635533739547, "accuracy": 0.81689453125}, {"mcc": 0.7847105828181367, "accuracy": 0.837890625}, {"mcc": 0.7577110602735674, "accuracy": 0.81787109375}, {"mcc": 0.7410806732467947, "accuracy": 0.80322265625}, {"mcc": 0.7865862759094822, "accuracy": 0.83837890625}, {"mcc": 0.8149935895888939, "accuracy": 0.8603515625}, {"mcc": 0.7705081345866968, "accuracy": 0.82666015625}, {"mcc": 0.7999167800715671, "accuracy": 0.84912109375}, {"mcc": 0.7705441340876575, "accuracy": 0.82666015625}, {"mcc": 0.7962253878499229, "accuracy": 0.84619140625}], "total": {"test_mcc": 77.80240171806673, "test_mcc_se": 1.399360223585755, "test_accuracy": 83.232421875, "test_accuracy_se": 1.0706780426193794}}, "num_model_parameters": 72706203648, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-it", "task": "common-sense-reasoning", "dataset_languages": ["it"], "model": "Qwen/Qwen2.5-72B-Instruct", "results": {"raw": [{"mcc": 0.7867890179715157, "accuracy": 0.83935546875}, {"mcc": 0.7656515486040225, "accuracy": 0.82373046875}, {"mcc": 0.7971680048056207, "accuracy": 0.84765625}, {"mcc": 0.7642588087262807, "accuracy": 0.822265625}, {"mcc": 0.7767931320153318, "accuracy": 0.83203125}, {"mcc": 0.7655892835760174, "accuracy": 0.82373046875}, {"mcc": 0.7840492991984319, "accuracy": 0.83740234375}, {"mcc": 0.7828888719928933, "accuracy": 0.8369140625}, {"mcc": 0.7507113380763106, "accuracy": 0.81201171875}, {"mcc": 0.7886917019216051, "accuracy": 0.84130859375}], "total": {"test_mcc": 77.6259100688803, "test_mcc_se": 0.8839620746144102, "test_accuracy": 83.1640625, "test_accuracy_se": 0.6754890208914095}}, "num_model_parameters": 72706203648, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "Qwen/Qwen2.5-72B-Instruct", "results": {"raw": [{"test_speed": 237.85, "test_speed_short": 27.2}, {"test_speed": 463.89, "test_speed_short": 50.699999999999996}, {"test_speed": 700.52, "test_speed_short": 98.02}, {"test_speed": 924.49, "test_speed_short": 120.24}, {"test_speed": 1126.71, "test_speed_short": 143.19}, {"test_speed": 1385.09, "test_speed_short": 193.79999999999998}, {"test_speed": 1635.03, "test_speed_short": 218.88}, {"test_speed": 1834.47, "test_speed_short": 240.69}, {"test_speed": 1987.6499999999999, "test_speed_short": 264.42}, {"test_speed": 2187.12, "test_speed_short": 288.15000000000003}], "total": {"test_speed": 1248.282, "test_speed_se": 412.99227861116265, "test_speed_short": 164.529, "test_speed_short_se": 56.02244574154025}}, "num_model_parameters": 72706203648, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "allocine", "task": "sentiment-classification", "dataset_languages": ["fr"], "model": "Qwen/QwQ-32B-Preview", "results": {"raw": [{"mcc": 0.9530084371041373, "macro_f1": 0.9765042185520687}, {"mcc": 0.938510869779241, "macro_f1": 0.9692033236083267}, {"mcc": 0.9615596204606185, "macro_f1": 0.9807604803617045}, {"mcc": 0.9415885342489505, "macro_f1": 0.9706605670189246}, {"mcc": 0.9424854744078595, "macro_f1": 0.9710970754302897}, {"mcc": 0.9310937372696825, "macro_f1": 0.9652298921349034}, {"mcc": 0.9501492856313287, "macro_f1": 0.9750725463648776}, {"mcc": 0.9424129643215295, "macro_f1": 0.9711393245572026}, {"mcc": 0.9529149011180333, "macro_f1": 0.9764112369127285}, {"mcc": 0.9569002341205874, "macro_f1": 0.9783735421484572}], "total": {"test_mcc": 94.70624058461968, "test_mcc_se": 0.5794974357830457, "test_macro_f1": 97.34452207089483, "test_macro_f1_se": 0.29412689262775443}}, "num_model_parameters": 32763876352, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "sentipolc16", "task": "sentiment-classification", "dataset_languages": ["it"], "model": "Qwen/QwQ-32B-Preview", "results": {"raw": [{"mcc": 0.4806768377491414, "macro_f1": 0.49566120442009104}, {"mcc": 0.48636156344696196, "macro_f1": 0.4934655715687794}, {"mcc": 0.4831254225370413, "macro_f1": 0.491297530434208}, {"mcc": 0.4775948593829106, "macro_f1": 0.4862234613471557}, {"mcc": 0.4868363417724969, "macro_f1": 0.4909442394115137}, {"mcc": 0.47522846754775233, "macro_f1": 0.48793338391032953}, {"mcc": 0.49063327130133205, "macro_f1": 0.49476806772362814}, {"mcc": 0.4710528335430336, "macro_f1": 0.483354242397995}, {"mcc": 0.48289528809897425, "macro_f1": 0.4902398093332678}, {"mcc": 0.4800886757960338, "macro_f1": 0.482557974511084}], "total": {"test_mcc": 48.14493561175678, "test_mcc_se": 0.3616931035137291, "test_macro_f1": 48.96445485058052, "test_macro_f1_se": 0.28195409710321434}}, "num_model_parameters": 32763876352, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "eltec", "task": "named-entity-recognition", "dataset_languages": ["fr"], "model": "Qwen/QwQ-32B-Preview", "results": {"raw": [{"micro_f1_no_misc": 0.6961556276053729, "micro_f1": 0.6573780873970868}, {"micro_f1_no_misc": 0.6743008147985027, "micro_f1": 0.652648938363224}, {"micro_f1_no_misc": 0.6913925822253324, "micro_f1": 0.6335046248715314}, {"micro_f1_no_misc": 0.703905540417802, "micro_f1": 0.6635847867198991}, {"micro_f1_no_misc": 0.7070321811680572, "micro_f1": 0.685425368212794}, {"micro_f1_no_misc": 0.6733200266134398, "micro_f1": 0.5985970381917382}, {"micro_f1_no_misc": 0.6530078465562337, "micro_f1": 0.6196899536943828}, {"micro_f1_no_misc": 0.7038948850305021, "micro_f1": 0.6637781629116117}, {"micro_f1_no_misc": 0.6583463338533542, "micro_f1": 0.6219715956558062}, {"micro_f1_no_misc": 0.6796296296296296, "micro_f1": 0.6247697031729785}], "total": {"test_micro_f1_no_misc": 68.40985467898226, "test_micro_f1_no_misc_se": 1.1994835470990652, "test_micro_f1": 64.21348259191052, "test_micro_f1_se": 1.643416391884909}}, "num_model_parameters": 32763876352, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "multinerd-it", "task": "named-entity-recognition", "dataset_languages": ["it"], "model": "Qwen/QwQ-32B-Preview", "results": {"raw": [{"micro_f1_no_misc": 0.8334972946384653, "micro_f1": 0.6106437911218456}, {"micro_f1_no_misc": 0.8130280124499777, "micro_f1": 0.6303570008746123}, {"micro_f1_no_misc": 0.8058973763640584, "micro_f1": 0.5879293199724622}, {"micro_f1_no_misc": 0.8502835082639644, "micro_f1": 0.69847533632287}, {"micro_f1_no_misc": 0.8376152427781192, "micro_f1": 0.6391644908616189}, {"micro_f1_no_misc": 0.8147888147888147, "micro_f1": 0.678481452523673}, {"micro_f1_no_misc": 0.8303249097472923, "micro_f1": 0.6013261184360947}, {"micro_f1_no_misc": 0.8231038225009043, "micro_f1": 0.6745325830459249}, {"micro_f1_no_misc": 0.8314167433302668, "micro_f1": 0.6613328644276419}, {"micro_f1_no_misc": 0.8270799347471451, "micro_f1": 0.6633749133749134}], "total": {"test_micro_f1_no_misc": 82.67035659609009, "test_micro_f1_no_misc_se": 0.808535485743633, "test_micro_f1": 64.45617870961658, "test_micro_f1_se": 2.267787896353144}}, "num_model_parameters": 32763876352, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-fr", "task": "linguistic-acceptability", "dataset_languages": ["fr"], "model": "Qwen/QwQ-32B-Preview", "results": {"raw": [{"mcc": 0.6114522353914031, "macro_f1": 0.8056638771666309}, {"mcc": 0.5209631063017867, "macro_f1": 0.7531416277684935}, {"mcc": 0.5684034962577635, "macro_f1": 0.7817092907923218}, {"mcc": 0.4387021652527085, "macro_f1": 0.6860377358490566}, {"mcc": 0.540933584227519, "macro_f1": 0.7527527527527527}, {"mcc": 0.5918857438745824, "macro_f1": 0.7941379187085256}, {"mcc": 0.5866201382814398, "macro_f1": 0.7917053437388983}, {"mcc": 0.5861578112522261, "macro_f1": 0.7919278952007723}, {"mcc": 0.6240330278554352, "macro_f1": 0.8120116739300904}, {"mcc": 0.5253708271471978, "macro_f1": 0.7334507573577915}], "total": {"test_mcc": 55.94522135842062, "test_mcc_se": 3.396359121007067, "test_macro_f1": 77.02538873265333, "test_macro_f1_se": 2.4160201853892733}}, "num_model_parameters": 32763876352, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-it", "task": "linguistic-acceptability", "dataset_languages": ["it"], "model": "Qwen/QwQ-32B-Preview", "results": {"raw": [{"mcc": 0.5189663302563168, "macro_f1": 0.7573645188412443}, {"mcc": 0.4833578225678222, "macro_f1": 0.7280115057059442}, {"mcc": 0.4388432097779861, "macro_f1": 0.7114680888854521}, {"mcc": 0.44462640542709875, "macro_f1": 0.7065123931850751}, {"mcc": 0.4613260753207008, "macro_f1": 0.7152391546162402}, {"mcc": 0.4463500627570273, "macro_f1": 0.7044534412955465}, {"mcc": 0.3944309460292508, "macro_f1": 0.6628638670111757}, {"mcc": 0.47062966961236746, "macro_f1": 0.7343017704121104}, {"mcc": 0.4627803442891086, "macro_f1": 0.709126232168285}, {"mcc": 0.46808507961893003, "macro_f1": 0.7286862560442819}], "total": {"test_mcc": 45.893959456566094, "test_mcc_se": 1.9997547619572655, "test_macro_f1": 71.58027228165355, "test_macro_f1_se": 1.5274534448714623}}, "num_model_parameters": 32763876352, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "squad-it", "task": "reading-comprehension", "dataset_languages": ["it"], "model": "Qwen/QwQ-32B-Preview", "results": {"raw": [{"f1": 73.18657240149622, "em": 53.40909090909091}, {"f1": 73.39656438204914, "em": 51.70583775587566}, {"f1": 68.53921435970015, "em": 48.64024864024864}, {"f1": 72.7219815053455, "em": 52.28310502283105}, {"f1": 73.6920903776561, "em": 55.05011565150347}, {"f1": 72.07945377740694, "em": 52.13740458015267}, {"f1": 74.40426516314112, "em": 55.22620904836194}, {"f1": 72.2705463433228, "em": 51.41509433962264}, {"f1": 69.71305175117985, "em": 53.48297213622291}, {"f1": 72.95991797379708, "em": 51.92901234567901}], "total": {"test_f1": 72.29636580350947, "test_f1_se": 1.1288317768296718, "test_em": 52.52790904295889, "test_em_se": 1.1855819728498578}}, "num_model_parameters": 32763876352, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fquad", "task": "reading-comprehension", "dataset_languages": ["fr"], "model": "Qwen/QwQ-32B-Preview", "results": {"raw": [{"f1": 73.27909568154266, "em": 40.833333333333336}, {"f1": 72.51839860317727, "em": 40.71266110689917}, {"f1": 74.89212724998362, "em": 40.63714063714064}, {"f1": 74.25683539157318, "em": 40.41095890410959}, {"f1": 68.22868290982066, "em": 38.319198149575946}, {"f1": 72.76970166413955, "em": 39.083969465648856}, {"f1": 72.1971536112248, "em": 40.09360374414977}, {"f1": 70.98466138913741, "em": 40.801886792452834}, {"f1": 68.72502920357132, "em": 40.092879256965944}, {"f1": 74.15647768289067, "em": 41.589506172839506}], "total": {"test_f1": 72.20081633870612, "test_f1_se": 1.404592851309649, "test_em": 40.25751375631155, "test_em_se": 0.5830859881978652}}, "num_model_parameters": 32763876352, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "orange-sum", "task": "summarization", "dataset_languages": ["fr"], "model": "Qwen/QwQ-32B-Preview", "results": {"raw": [{"bertscore": 0.6606685960432515, "rouge_l": 0.16244070710143169}, {"bertscore": 0.664805612701457, "rouge_l": 0.16875796357438505}, {"bertscore": 0.6644678291049786, "rouge_l": 0.1704943171769101}, {"bertscore": 0.644360936479643, "rouge_l": 0.15442452929627581}, {"bertscore": 0.6591782003815752, "rouge_l": 0.17029350041044752}, {"bertscore": 0.664642866293434, "rouge_l": 0.1687799095073539}, {"bertscore": 0.6622150595649146, "rouge_l": 0.16318982429816442}, {"bertscore": 0.656541735981591, "rouge_l": 0.15909274021623823}, {"bertscore": 0.6667182258097455, "rouge_l": 0.17547725452538154}, {"bertscore": 0.6557516889297403, "rouge_l": 0.16272250872234462}], "total": {"test_bertscore": 65.9935075129033, "test_bertscore_se": 0.40817437635872045, "test_rouge_l": 16.556732548289325, "test_rouge_l_se": 0.38922600548790753}}, "num_model_parameters": 32763876352, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "ilpost-sum", "task": "summarization", "dataset_languages": ["it"], "model": "Qwen/QwQ-32B-Preview", "results": {"raw": [{"bertscore": 0.6514114381716354, "rouge_l": 0.15628472875628863}, {"bertscore": 0.6479049881163519, "rouge_l": 0.15620523012956414}, {"bertscore": 0.6531618768058252, "rouge_l": 0.1687135267289689}, {"bertscore": 0.6528248513641302, "rouge_l": 0.1695855180739317}, {"bertscore": 0.641592187792412, "rouge_l": 0.13905658713237165}, {"bertscore": 0.6467353194311727, "rouge_l": 0.1567235234023283}, {"bertscore": 0.652589791076025, "rouge_l": 0.16369856439406705}, {"bertscore": 0.6518426612310577, "rouge_l": 0.15971380271339866}, {"bertscore": 0.6547289823647588, "rouge_l": 0.16748105300874339}, {"bertscore": 0.6509494734345935, "rouge_l": 0.1567824986376801}], "total": {"test_bertscore": 65.03741569787962, "test_bertscore_se": 0.2424167654982685, "test_rouge_l": 15.942450329773425, "test_rouge_l_se": 0.5551952926239432}}, "num_model_parameters": 32763876352, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-fr", "task": "knowledge", "dataset_languages": ["fr"], "model": "Qwen/QwQ-32B-Preview", "results": {"raw": [{"mcc": 0.7051699795834354, "accuracy": 0.7783203125}, {"mcc": 0.7140103555852597, "accuracy": 0.78466796875}, {"mcc": 0.7063022821417541, "accuracy": 0.7783203125}, {"mcc": 0.6927774761174693, "accuracy": 0.76806640625}, {"mcc": 0.6988174666164186, "accuracy": 0.77392578125}, {"mcc": 0.7037588293777007, "accuracy": 0.77734375}, {"mcc": 0.7336194371432857, "accuracy": 0.798828125}, {"mcc": 0.7304705444044002, "accuracy": 0.79736328125}, {"mcc": 0.7203104587756454, "accuracy": 0.79052734375}, {"mcc": 0.6860709325002283, "accuracy": 0.76318359375}], "total": {"test_mcc": 70.91307762245597, "test_mcc_se": 0.9606384348540976, "test_accuracy": 78.10546875, "test_accuracy_se": 0.7311117449097104}}, "num_model_parameters": 32763876352, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-it", "task": "knowledge", "dataset_languages": ["it"], "model": "Qwen/QwQ-32B-Preview", "results": {"raw": [{"mcc": 0.690321630905072, "accuracy": 0.76708984375}, {"mcc": 0.7205367246291757, "accuracy": 0.7900390625}, {"mcc": 0.6932617880754732, "accuracy": 0.76953125}, {"mcc": 0.6727065285677194, "accuracy": 0.75439453125}, {"mcc": 0.6845325273452917, "accuracy": 0.763671875}, {"mcc": 0.6913605623147737, "accuracy": 0.76806640625}, {"mcc": 0.6883093882748903, "accuracy": 0.76513671875}, {"mcc": 0.7029534419459744, "accuracy": 0.77734375}, {"mcc": 0.6702117064428048, "accuracy": 0.751953125}, {"mcc": 0.6980297936338573, "accuracy": 0.77392578125}], "total": {"test_mcc": 69.12224092135033, "test_mcc_se": 0.8971872331822981, "test_accuracy": 76.8115234375, "test_accuracy_se": 0.6791925811504185}}, "num_model_parameters": 32763876352, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-fr", "task": "common-sense-reasoning", "dataset_languages": ["fr"], "model": "Qwen/QwQ-32B-Preview", "results": {"raw": [{"mcc": 0.7886078523366064, "accuracy": 0.837890625}, {"mcc": 0.8078200036371108, "accuracy": 0.8544921875}, {"mcc": 0.7719516785350322, "accuracy": 0.826171875}, {"mcc": 0.7813032304849482, "accuracy": 0.8330078125}, {"mcc": 0.8098581180595544, "accuracy": 0.85546875}, {"mcc": 0.8202458113834502, "accuracy": 0.86328125}, {"mcc": 0.8065505392682868, "accuracy": 0.8525390625}, {"mcc": 0.8338994552191397, "accuracy": 0.8740234375}, {"mcc": 0.7845739232064244, "accuracy": 0.83642578125}, {"mcc": 0.8048879574935851, "accuracy": 0.8515625}], "total": {"test_mcc": 80.09698569624139, "test_mcc_se": 1.1830239101941926, "test_accuracy": 84.8486328125, "test_accuracy_se": 0.9186226085364617}}, "num_model_parameters": 32763876352, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-it", "task": "common-sense-reasoning", "dataset_languages": ["it"], "model": "Qwen/QwQ-32B-Preview", "results": {"raw": [{"mcc": 0.7963971241606956, "accuracy": 0.845703125}, {"mcc": 0.8167434584932604, "accuracy": 0.86181640625}, {"mcc": 0.8045542551680385, "accuracy": 0.85205078125}, {"mcc": 0.8032844351016809, "accuracy": 0.85107421875}, {"mcc": 0.8112544047654485, "accuracy": 0.85693359375}, {"mcc": 0.8217959896568708, "accuracy": 0.86572265625}, {"mcc": 0.7949928598138478, "accuracy": 0.84423828125}, {"mcc": 0.8101770652190832, "accuracy": 0.8564453125}, {"mcc": 0.7918524704329143, "accuracy": 0.8427734375}, {"mcc": 0.8002873119476402, "accuracy": 0.849609375}], "total": {"test_mcc": 80.5133937475948, "test_mcc_se": 0.6052121932198807, "test_accuracy": 85.263671875, "test_accuracy_se": 0.4694550181344846}}, "num_model_parameters": 32763876352, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "Qwen/QwQ-32B-Preview", "results": {"raw": [{"test_speed": 300.33000000000004, "test_speed_short": 34.08}, {"test_speed": 586.5600000000001, "test_speed_short": 63.6}, {"test_speed": 877.76, "test_speed_short": 123.25}, {"test_speed": 1166.15, "test_speed_short": 154.08}, {"test_speed": 1446.1200000000001, "test_speed_short": 181.89000000000001}, {"test_speed": 1730.3100000000002, "test_speed_short": 242.25}, {"test_speed": 1988.55, "test_speed_short": 270.72}, {"test_speed": 5475.36, "test_speed_short": 302.46}, {"test_speed": 6152.25, "test_speed_short": 331.5}, {"test_speed": 6785.679999999999, "test_speed_short": 360.40000000000003}], "total": {"test_speed": 2650.907, "test_speed_se": 1535.073466267853, "test_speed_short": 206.423, "test_speed_short_se": 69.86509760892184}}, "num_model_parameters": 32763876352, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "allocine", "task": "sentiment-classification", "dataset_languages": ["fr"], "model": "ThatsGroes/gemma-2-27b-it-FP8-Dynamic", "results": {"raw": [{"mcc": 0.9533259263588385, "macro_f1": 0.9765284536151396}, {"mcc": 0.9423925286316577, "macro_f1": 0.971177490811084}, {"mcc": 0.9595797769112395, "macro_f1": 0.9797896504274795}, {"mcc": 0.9511422391157586, "macro_f1": 0.9755701879950377}, {"mcc": 0.9433397365659492, "macro_f1": 0.9716364080934043}, {"mcc": 0.9481731473621545, "macro_f1": 0.9740863408248758}, {"mcc": 0.9459709984216846, "macro_f1": 0.9726503814085168}, {"mcc": 0.9394272308169881, "macro_f1": 0.9696987920180601}, {"mcc": 0.9538533263338099, "macro_f1": 0.9769151164784617}, {"mcc": 0.9597483499239028, "macro_f1": 0.9798739388744717}], "total": {"test_mcc": 94.96953260441984, "test_mcc_se": 0.43664727326453523, "test_macro_f1": 97.47926760546531, "test_macro_f1_se": 0.21960045493361646}}, "num_model_parameters": 28410968576, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "sentipolc16", "task": "sentiment-classification", "dataset_languages": ["it"], "model": "ThatsGroes/gemma-2-27b-it-FP8-Dynamic", "results": {"raw": [{"mcc": 0.4688566003778971, "macro_f1": 0.4891743267596525}, {"mcc": 0.47026356709740413, "macro_f1": 0.48420101637492946}, {"mcc": 0.4600510937240484, "macro_f1": 0.4814511261124063}, {"mcc": 0.45389921282702694, "macro_f1": 0.47351670253419725}, {"mcc": 0.4690785062696305, "macro_f1": 0.4806317799961204}, {"mcc": 0.46640311251844446, "macro_f1": 0.48205714201187816}, {"mcc": 0.48282701100404224, "macro_f1": 0.4912803586836736}, {"mcc": 0.4618402995943875, "macro_f1": 0.47764145097478433}, {"mcc": 0.4491462152753162, "macro_f1": 0.4742604194159292}, {"mcc": 0.4623871323815712, "macro_f1": 0.47452054535305077}], "total": {"test_mcc": 46.44752751069768, "test_mcc_se": 0.5814677037510134, "test_macro_f1": 48.08734868216622, "test_macro_f1_se": 0.3802452558673126}}, "num_model_parameters": 28410968576, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "eltec", "task": "named-entity-recognition", "dataset_languages": ["fr"], "model": "ThatsGroes/gemma-2-27b-it-FP8-Dynamic", "results": {"raw": [{"micro_f1_no_misc": 0.40326975476839244, "micro_f1": 0.3341082696778479}, {"micro_f1_no_misc": 0.4085192697768762, "micro_f1": 0.3443072702331962}, {"micro_f1_no_misc": 0.47850055126791624, "micro_f1": 0.39504424778761066}, {"micro_f1_no_misc": 0.4112837285363859, "micro_f1": 0.3458898376025448}, {"micro_f1_no_misc": 0.44223985890652556, "micro_f1": 0.40263901979264843}, {"micro_f1_no_misc": 0.4067368421052632, "micro_f1": 0.3355172413793103}, {"micro_f1_no_misc": 0.3827009383924928, "micro_f1": 0.3289363833695108}, {"micro_f1_no_misc": 0.4389388292041219, "micro_f1": 0.38294744579198825}, {"micro_f1_no_misc": 0.38909013424248884, "micro_f1": 0.30916225900017064}, {"micro_f1_no_misc": 0.40932420872540626, "micro_f1": 0.3386988598256204}], "total": {"test_micro_f1_no_misc": 41.7060411592587, "test_micro_f1_no_misc_se": 1.7668286748736526, "test_micro_f1": 35.17250834460449, "test_micro_f1_se": 1.916655141336009}}, "num_model_parameters": 28410968576, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "multinerd-it", "task": "named-entity-recognition", "dataset_languages": ["it"], "model": "ThatsGroes/gemma-2-27b-it-FP8-Dynamic", "results": {"raw": [{"micro_f1_no_misc": 0.5674060300172666, "micro_f1": 0.32884469304606034}, {"micro_f1_no_misc": 0.6011854360711262, "micro_f1": 0.41426692004092963}, {"micro_f1_no_misc": 0.5842696629213483, "micro_f1": 0.3845936794582393}, {"micro_f1_no_misc": 0.5916216910819841, "micro_f1": 0.4443740597038562}, {"micro_f1_no_misc": 0.6083969465648854, "micro_f1": 0.4038663269893601}, {"micro_f1_no_misc": 0.6170132325141776, "micro_f1": 0.4168313208927918}, {"micro_f1_no_misc": 0.5378192534381139, "micro_f1": 0.32028422040487997}, {"micro_f1_no_misc": 0.6346777331085462, "micro_f1": 0.4507420271550363}, {"micro_f1_no_misc": 0.578481945467944, "micro_f1": 0.33548903260217344}, {"micro_f1_no_misc": 0.556726907630522, "micro_f1": 0.3401029350396439}], "total": {"test_micro_f1_no_misc": 58.77598838815914, "test_micro_f1_no_misc_se": 1.8067642076999197, "test_micro_f1": 38.39395215332971, "test_micro_f1_se": 3.0553773636403574}}, "num_model_parameters": 28410968576, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-fr", "task": "linguistic-acceptability", "dataset_languages": ["fr"], "model": "ThatsGroes/gemma-2-27b-it-FP8-Dynamic", "results": {"raw": [{"mcc": 0.3737264645214941, "macro_f1": 0.6454558749987009}, {"mcc": 0.40983247100606707, "macro_f1": 0.6660879847813239}, {"mcc": 0.4291745752466439, "macro_f1": 0.6771895898799452}, {"mcc": 0.42044266634188604, "macro_f1": 0.671370874525749}, {"mcc": 0.4189673653431798, "macro_f1": 0.6619687313740773}, {"mcc": 0.4243032395947041, "macro_f1": 0.6775266893284202}, {"mcc": 0.438263349670444, "macro_f1": 0.6969202784921282}, {"mcc": 0.35852062015830005, "macro_f1": 0.6596059956390772}, {"mcc": 0.4052355814404919, "macro_f1": 0.6781990115132989}, {"mcc": 0.4055283076314652, "macro_f1": 0.6687038179450271}], "total": {"test_mcc": 40.83994640954677, "test_mcc_se": 1.5382537929935691, "test_macro_f1": 67.03028848477747, "test_macro_f1_se": 0.850127759112561}}, "num_model_parameters": 28410968576, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-it", "task": "linguistic-acceptability", "dataset_languages": ["it"], "model": "ThatsGroes/gemma-2-27b-it-FP8-Dynamic", "results": {"raw": [{"mcc": 0.30459687212408226, "macro_f1": 0.6179780654974005}, {"mcc": 0.2634476149014612, "macro_f1": 0.5703728037558442}, {"mcc": 0.25697014185955913, "macro_f1": 0.5620812158725048}, {"mcc": 0.26188416796156183, "macro_f1": 0.5407396176897783}, {"mcc": 0.21768952512927392, "macro_f1": 0.5400059682662399}, {"mcc": 0.2584717582364517, "macro_f1": 0.560008464680904}, {"mcc": 0.3018446458011024, "macro_f1": 0.5986931167609925}, {"mcc": 0.32470571044659763, "macro_f1": 0.6298212023621204}, {"mcc": 0.2648956391110225, "macro_f1": 0.5502051983584131}, {"mcc": 0.24881720268924845, "macro_f1": 0.5535403566255406}], "total": {"test_mcc": 27.033232782603605, "test_mcc_se": 1.940879996465408, "test_macro_f1": 57.23446009869739, "test_macro_f1_se": 1.9848904591751366}}, "num_model_parameters": 28410968576, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "squad-it", "task": "reading-comprehension", "dataset_languages": ["it"], "model": "ThatsGroes/gemma-2-27b-it-FP8-Dynamic", "results": {"raw": [{"f1": 70.83992941608557, "em": 50.75757575757576}, {"f1": 72.25519578069924, "em": 48.29416224412434}, {"f1": 72.59567264329445, "em": 54.85625485625486}, {"f1": 71.39528387533659, "em": 50.83713850837138}, {"f1": 71.35655797263533, "em": 49.113338473400155}, {"f1": 70.95255285984472, "em": 50.0763358778626}, {"f1": 73.47317495798467, "em": 52.730109204368176}, {"f1": 70.57778195381472, "em": 46.776729559748425}, {"f1": 72.44554038124721, "em": 53.1733746130031}, {"f1": 70.06365530000981, "em": 47.608024691358025}], "total": {"test_f1": 71.59553451409523, "test_f1_se": 0.6581347310709088, "test_em": 50.42230437860668, "test_em_se": 1.6058080421815346}}, "num_model_parameters": 28410968576, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fquad", "task": "reading-comprehension", "dataset_languages": ["fr"], "model": "ThatsGroes/gemma-2-27b-it-FP8-Dynamic", "results": {"raw": [{"f1": 69.67828607125571, "em": 38.18181818181818}, {"f1": 68.89315152329625, "em": 38.51402577710387}, {"f1": 69.93270312355982, "em": 37.995337995338}, {"f1": 70.22983575057806, "em": 39.117199391171994}, {"f1": 65.6742124937578, "em": 34.46414803392444}, {"f1": 70.57428540345153, "em": 37.93893129770992}, {"f1": 69.4465176781517, "em": 39.46957878315133}, {"f1": 68.43917314031462, "em": 37.57861635220126}, {"f1": 66.18234365852867, "em": 37.461300309597526}, {"f1": 70.41302628486561, "em": 36.882716049382715}], "total": {"test_f1": 68.94635351277599, "test_f1_se": 1.0698343335336384, "test_em": 37.760367217139915, "test_em_se": 0.8606392295164772}}, "num_model_parameters": 28410968576, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "orange-sum", "task": "summarization", "dataset_languages": ["fr"], "model": "ThatsGroes/gemma-2-27b-it-FP8-Dynamic", "results": {"raw": [{"bertscore": 0.665451243519783, "rouge_l": 0.18187079205658513}, {"bertscore": 0.6659451677114703, "rouge_l": 0.18119122997415982}, {"bertscore": 0.6676151237334125, "rouge_l": 0.1874313737342349}, {"bertscore": 0.6650439903314691, "rouge_l": 0.18223124088918358}, {"bertscore": 0.6666269602137618, "rouge_l": 0.18516015822172907}, {"bertscore": 0.6675751530565321, "rouge_l": 0.18569791765251914}, {"bertscore": 0.665341705083847, "rouge_l": 0.17941050283588766}, {"bertscore": 0.6646085072425194, "rouge_l": 0.1809750458465187}, {"bertscore": 0.6685305880382657, "rouge_l": 0.18521113687993523}, {"bertscore": 0.6660251223365776, "rouge_l": 0.18033966679251773}], "total": {"test_bertscore": 66.62763561267639, "test_bertscore_se": 0.07924286929808738, "test_rouge_l": 18.29519064883271, "test_rouge_l_se": 0.1673963993131067}}, "num_model_parameters": 28410968576, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "ilpost-sum", "task": "summarization", "dataset_languages": ["it"], "model": "ThatsGroes/gemma-2-27b-it-FP8-Dynamic", "results": {"raw": [{"bertscore": 0.6611570219683927, "rouge_l": 0.18824347852190654}, {"bertscore": 0.6573479755898006, "rouge_l": 0.18237373306002164}, {"bertscore": 0.6598520419793203, "rouge_l": 0.1861332995176633}, {"bertscore": 0.6603934661543462, "rouge_l": 0.18788125468820832}, {"bertscore": 0.6583917787647806, "rouge_l": 0.1815059762292425}, {"bertscore": 0.6605956534622237, "rouge_l": 0.1903911761404117}, {"bertscore": 0.660556194401579, "rouge_l": 0.1872153353573809}, {"bertscore": 0.6630107221426442, "rouge_l": 0.19389917609437116}, {"bertscore": 0.6613633965607733, "rouge_l": 0.18930159045967426}, {"bertscore": 0.661284712550696, "rouge_l": 0.18828351745228983}], "total": {"test_bertscore": 66.03952963574557, "test_bertscore_se": 0.09867358791213306, "test_rouge_l": 18.752285375211702, "test_rouge_l_se": 0.2244432623354008}}, "num_model_parameters": 28410968576, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-fr", "task": "knowledge", "dataset_languages": ["fr"], "model": "ThatsGroes/gemma-2-27b-it-FP8-Dynamic", "results": {"raw": [{"mcc": 0.6023746777593956, "accuracy": 0.7001953125}, {"mcc": 0.6099132641274628, "accuracy": 0.70654296875}, {"mcc": 0.611207372870713, "accuracy": 0.70703125}, {"mcc": 0.60376181913372, "accuracy": 0.70166015625}, {"mcc": 0.5938813099439194, "accuracy": 0.69384765625}, {"mcc": 0.6045935239041059, "accuracy": 0.7021484375}, {"mcc": 0.6132163343659301, "accuracy": 0.70947265625}, {"mcc": 0.6116503110756366, "accuracy": 0.7080078125}, {"mcc": 0.6195876771903358, "accuracy": 0.712890625}, {"mcc": 0.6172341023140073, "accuracy": 0.71044921875}], "total": {"test_mcc": 60.87420392685227, "test_mcc_se": 0.4752151653486322, "test_accuracy": 70.5224609375, "test_accuracy_se": 0.35410156249999997}}, "num_model_parameters": 28410968576, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-it", "task": "knowledge", "dataset_languages": ["it"], "model": "ThatsGroes/gemma-2-27b-it-FP8-Dynamic", "results": {"raw": [{"mcc": 0.6050986980559713, "accuracy": 0.703125}, {"mcc": 0.613539473988179, "accuracy": 0.70947265625}, {"mcc": 0.5902755439238021, "accuracy": 0.69140625}, {"mcc": 0.5817451748182597, "accuracy": 0.68603515625}, {"mcc": 0.6001041067513411, "accuracy": 0.69970703125}, {"mcc": 0.580873482002496, "accuracy": 0.6845703125}, {"mcc": 0.5998100683796117, "accuracy": 0.697265625}, {"mcc": 0.574400312263471, "accuracy": 0.6796875}, {"mcc": 0.5871743621676606, "accuracy": 0.689453125}, {"mcc": 0.5819112104274425, "accuracy": 0.68603515625}], "total": {"test_mcc": 59.14932432778235, "test_mcc_se": 0.7802847761578715, "test_accuracy": 69.267578125, "test_accuracy_se": 0.5830477206062323}}, "num_model_parameters": 28410968576, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-fr", "task": "common-sense-reasoning", "dataset_languages": ["fr"], "model": "ThatsGroes/gemma-2-27b-it-FP8-Dynamic", "results": {"raw": [{"mcc": 0.6709412376099132, "accuracy": 0.7509765625}, {"mcc": 0.6781061279296514, "accuracy": 0.75439453125}, {"mcc": 0.6654936112758494, "accuracy": 0.748046875}, {"mcc": 0.6569184348508965, "accuracy": 0.7373046875}, {"mcc": 0.698778547109312, "accuracy": 0.77001953125}, {"mcc": 0.7027447627388601, "accuracy": 0.77490234375}, {"mcc": 0.6657469882091788, "accuracy": 0.74658203125}, {"mcc": 0.6978360448012578, "accuracy": 0.77099609375}, {"mcc": 0.6648166174586841, "accuracy": 0.74609375}, {"mcc": 0.6960922284012637, "accuracy": 0.77001953125}], "total": {"test_mcc": 67.97474600384867, "test_mcc_se": 1.0753100480702096, "test_accuracy": 75.693359375, "test_accuracy_se": 0.8248501522754103}}, "num_model_parameters": 28410968576, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-it", "task": "common-sense-reasoning", "dataset_languages": ["it"], "model": "ThatsGroes/gemma-2-27b-it-FP8-Dynamic", "results": {"raw": [{"mcc": 0.6833893317511477, "accuracy": 0.7607421875}, {"mcc": 0.7099833271831728, "accuracy": 0.7822265625}, {"mcc": 0.6995468588116524, "accuracy": 0.77392578125}, {"mcc": 0.7126943659962383, "accuracy": 0.7841796875}, {"mcc": 0.6969788688350911, "accuracy": 0.77294921875}, {"mcc": 0.7139062537362992, "accuracy": 0.78515625}, {"mcc": 0.6904614954399575, "accuracy": 0.7666015625}, {"mcc": 0.7174228291627829, "accuracy": 0.787109375}, {"mcc": 0.7114766290328273, "accuracy": 0.783203125}, {"mcc": 0.6909426350490231, "accuracy": 0.7666015625}], "total": {"test_mcc": 70.26802594998192, "test_mcc_se": 0.7379756510739758, "test_accuracy": 77.626953125, "test_accuracy_se": 0.5796516711280835}}, "num_model_parameters": 28410968576, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "ThatsGroes/gemma-2-27b-it-FP8-Dynamic", "results": {"raw": [{"test_speed": 673.79, "test_speed_short": 46.96}, {"test_speed": 1321.1699999999998, "test_speed_short": 87.75}, {"test_speed": 1968.63, "test_speed_short": 170.23}, {"test_speed": 2613.3, "test_speed_short": 210.24}, {"test_speed": 3260.7899999999995, "test_speed_short": 249.82999999999998}, {"test_speed": 3847.94, "test_speed_short": 327.75}, {"test_speed": 4531.93, "test_speed_short": 609.92}, {"test_speed": 5222.91, "test_speed_short": 672.37}, {"test_speed": 5817.820000000001, "test_speed_short": 733.98}, {"test_speed": 4871.95, "test_speed_short": 804.1}], "total": {"test_speed": 3413.0229999999997, "test_speed_se": 1078.624693332799, "test_speed_short": 391.313, "test_speed_short_se": 176.64762585008404}}, "num_model_parameters": 28410968576, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "allocine", "task": "sentiment-classification", "dataset_languages": ["fr"], "model": "mistralai/Mixtral-8x7B-v0.1", "results": {"raw": [{"mcc": 0.9343516099395951, "macro_f1": 0.9667683520477706}, {"mcc": 0.9438275134619517, "macro_f1": 0.9716764191119314}, {"mcc": 0.9455768162131247, "macro_f1": 0.9724840813232055}, {"mcc": 0.9487428173667201, "macro_f1": 0.9741183724885625}, {"mcc": 0.9561308145133125, "macro_f1": 0.9779978365494228}, {"mcc": 0.9409101350163378, "macro_f1": 0.9702029016202983}, {"mcc": 0.9206124637819958, "macro_f1": 0.95898374914168}, {"mcc": 0.9257234874406645, "macro_f1": 0.962396740427677}, {"mcc": 0.9346721087580994, "macro_f1": 0.9671667254900788}, {"mcc": 0.9579713044698153, "macro_f1": 0.9789176477798343}], "total": {"test_mcc": 94.08519070961617, "test_mcc_se": 0.7559819206692395, "test_macro_f1": 97.00712825980462, "test_macro_f1_se": 0.3958254135390497}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "sentipolc16", "task": "sentiment-classification", "dataset_languages": ["it"], "model": "mistralai/Mixtral-8x7B-v0.1", "results": {"raw": [{"mcc": 0.4973193975122564, "macro_f1": 0.6331892979688653}, {"mcc": 0.4714559138613655, "macro_f1": 0.5864590774176557}, {"mcc": 0.5392636150052179, "macro_f1": 0.629651286679452}, {"mcc": 0.4656654753348443, "macro_f1": 0.5635525340507125}, {"mcc": 0.5319108575334511, "macro_f1": 0.6584727855358273}, {"mcc": 0.454890475010658, "macro_f1": 0.573493733945959}, {"mcc": 0.47508542024013944, "macro_f1": 0.6174549628780407}, {"mcc": 0.5196649209216159, "macro_f1": 0.6526687268645155}, {"mcc": 0.489818551102142, "macro_f1": 0.5999005175306747}, {"mcc": 0.48763822257271683, "macro_f1": 0.6185560391182946}], "total": {"test_mcc": 49.32712849094408, "test_mcc_se": 1.77807260875817, "test_macro_f1": 61.333989619899974, "test_macro_f1_se": 1.987855124581046}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "eltec", "task": "named-entity-recognition", "dataset_languages": ["fr"], "model": "mistralai/Mixtral-8x7B-v0.1", "results": {"raw": [{"micro_f1_no_misc": 0.5899538698103537, "micro_f1": 0.5333035315154224}, {"micro_f1_no_misc": 0.598302362927277, "micro_f1": 0.5822454308093995}, {"micro_f1_no_misc": 0.5814006888633755, "micro_f1": 0.5630815840727115}, {"micro_f1_no_misc": 0.5760943524608755, "micro_f1": 0.5623250807319697}, {"micro_f1_no_misc": 0.5863273453093812, "micro_f1": 0.564437194127243}, {"micro_f1_no_misc": 0.5856712265244609, "micro_f1": 0.5667610542365497}, {"micro_f1_no_misc": 0.5863752615670774, "micro_f1": 0.5731330092797172}, {"micro_f1_no_misc": 0.5737586951307269, "micro_f1": 0.5454136690647482}, {"micro_f1_no_misc": 0.5098585015077708, "micro_f1": 0.4858071505958829}, {"micro_f1_no_misc": 0.6070577014783023, "micro_f1": 0.5636441038739889}], "total": {"test_micro_f1_no_misc": 57.948000055796015, "test_micro_f1_no_misc_se": 1.6336355381883298, "test_micro_f1": 55.401518083076326, "test_micro_f1_se": 1.7077861028276453}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "multinerd-it", "task": "named-entity-recognition", "dataset_languages": ["it"], "model": "mistralai/Mixtral-8x7B-v0.1", "results": {"raw": [{"micro_f1_no_misc": 0.7456061577934574, "micro_f1": 0.5801500682128239}, {"micro_f1_no_misc": 0.6898231222763394, "micro_f1": 0.6107896210565615}, {"micro_f1_no_misc": 0.6688508064516129, "micro_f1": 0.57735104091888}, {"micro_f1_no_misc": 0.6170241108298921, "micro_f1": 0.5182405306336185}, {"micro_f1_no_misc": 0.7233930453108535, "micro_f1": 0.6251843657817109}, {"micro_f1_no_misc": 0.6925738621240352, "micro_f1": 0.5682761055697766}, {"micro_f1_no_misc": 0.7170552147239263, "micro_f1": 0.5581092094539528}, {"micro_f1_no_misc": 0.7277360515021459, "micro_f1": 0.6103932849193624}, {"micro_f1_no_misc": 0.7118434093161546, "micro_f1": 0.5628602455935178}, {"micro_f1_no_misc": 0.7361686112156568, "micro_f1": 0.6075072886297376}], "total": {"test_micro_f1_no_misc": 70.30074391544073, "test_micro_f1_no_misc_se": 2.360993342114025, "test_micro_f1": 58.18861760769942, "test_micro_f1_se": 1.9994725157404216}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-fr", "task": "linguistic-acceptability", "dataset_languages": ["fr"], "model": "mistralai/Mixtral-8x7B-v0.1", "results": {"raw": [{"mcc": 0.45114048114476246, "macro_f1": 0.7250079001940168}, {"mcc": 0.4124936343669984, "macro_f1": 0.7060521645254905}, {"mcc": 0.3899975570559855, "macro_f1": 0.6948183251105151}, {"mcc": 0.39509953669174463, "macro_f1": 0.693998587004258}, {"mcc": 0.45770116282742535, "macro_f1": 0.7238206293678557}, {"mcc": 0.4696063477789877, "macro_f1": 0.7301118837775763}, {"mcc": 0.48867796677530045, "macro_f1": 0.7438582421569189}, {"mcc": 0.348454781539491, "macro_f1": 0.6730591705708833}, {"mcc": 0.39847393258633335, "macro_f1": 0.6991702649829694}, {"mcc": 0.4353842246673321, "macro_f1": 0.7036803983659075}], "total": {"test_mcc": 42.47029625434361, "test_mcc_se": 2.6732968559789603, "test_macro_f1": 70.9357756605639, "test_macro_f1_se": 1.3045333940935777}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-it", "task": "linguistic-acceptability", "dataset_languages": ["it"], "model": "mistralai/Mixtral-8x7B-v0.1", "results": {"raw": [{"mcc": 0.14002955413219567, "macro_f1": 0.553270858691499}, {"mcc": 0.2917612617561677, "macro_f1": 0.6454136979496897}, {"mcc": 0.15823718063460784, "macro_f1": 0.5790915272599997}, {"mcc": 0.10491688500471895, "macro_f1": 0.3522021806003133}, {"mcc": 0.17319395781319935, "macro_f1": 0.5430889362133657}, {"mcc": 0.2704143639351132, "macro_f1": 0.6239055062584474}, {"mcc": 0.08681583933638609, "macro_f1": 0.54240317775571}, {"mcc": 0.2661593908197124, "macro_f1": 0.5826461502996174}, {"mcc": 0.11995806866655116, "macro_f1": 0.558640515230316}, {"mcc": 0.23902906534424076, "macro_f1": 0.5687313816127375}], "total": {"test_mcc": 18.505155674428934, "test_mcc_se": 4.682235403637561, "test_macro_f1": 55.49393931871697, "test_macro_f1_se": 4.8847135570110005}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "squad-it", "task": "reading-comprehension", "dataset_languages": ["it"], "model": "mistralai/Mixtral-8x7B-v0.1", "results": {"raw": [{"f1": 75.41983619240457, "em": 60.984848484848484}, {"f1": 75.6403061604644, "em": 56.785443517816525}, {"f1": 69.46134427680151, "em": 54.77855477855478}, {"f1": 75.78721632116942, "em": 58.98021308980213}, {"f1": 72.26868363085607, "em": 54.43330763299923}, {"f1": 73.73651835027245, "em": 57.786259541984734}, {"f1": 75.7086967435218, "em": 60.9204368174727}, {"f1": 73.3052674826382, "em": 54.009433962264154}, {"f1": 64.85110859559457, "em": 51.1609907120743}, {"f1": 72.32776365158, "em": 50.23148148148148}], "total": {"test_f1": 72.8506741405303, "test_f1_se": 2.1496388584751474, "test_em": 56.00709700192986, "test_em_se": 2.3223114245730856}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fquad", "task": "reading-comprehension", "dataset_languages": ["fr"], "model": "mistralai/Mixtral-8x7B-v0.1", "results": {"raw": [{"f1": 76.14486389525983, "em": 50.15151515151515}, {"f1": 74.65226319126725, "em": 50.03790750568613}, {"f1": 78.52705483256712, "em": 51.67055167055167}, {"f1": 75.86101161225888, "em": 48.55403348554034}, {"f1": 68.15658208289257, "em": 44.71858134155744}, {"f1": 78.2804175121847, "em": 51.98473282442748}, {"f1": 71.71700036951225, "em": 45.94383775351014}, {"f1": 74.91037131316156, "em": 50.0}, {"f1": 68.3588527423156, "em": 44.969040247678016}, {"f1": 78.21166220804747, "em": 50.23148148148148}], "total": {"test_f1": 74.48200797594673, "test_f1_se": 2.398090881170108, "test_em": 48.82616814619478, "test_em_se": 1.6628107683535824}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "orange-sum", "task": "summarization", "dataset_languages": ["fr"], "model": "mistralai/Mixtral-8x7B-v0.1", "results": {"raw": [{"bertscore": 0.6833288996131159, "rouge_l": 0.2485630570360659}, {"bertscore": 0.6800649159995373, "rouge_l": 0.2386611027032613}, {"bertscore": 0.685911480395589, "rouge_l": 0.254448283058216}, {"bertscore": 0.657989228493534, "rouge_l": 0.21715252780824845}, {"bertscore": 0.6565442374849226, "rouge_l": 0.2085637825024657}, {"bertscore": 0.6814837857964449, "rouge_l": 0.24266282529733507}, {"bertscore": 0.6792436297400855, "rouge_l": 0.24069086793969408}, {"bertscore": 0.6647059981478378, "rouge_l": 0.22368353849760547}, {"bertscore": 0.6847547920770012, "rouge_l": 0.2518067925234738}, {"bertscore": 0.6770602953620255, "rouge_l": 0.22750445239308298}], "total": {"test_bertscore": 67.51087263110094, "test_bertscore_se": 0.6881317753232921, "test_rouge_l": 23.537372297594487, "test_rouge_l_se": 0.9578463278010109}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "ilpost-sum", "task": "summarization", "dataset_languages": ["it"], "model": "mistralai/Mixtral-8x7B-v0.1", "results": {"raw": [{"bertscore": 0.6862033743673237, "rouge_l": 0.2668016960308076}, {"bertscore": 0.6748660411394667, "rouge_l": 0.2467076613010068}, {"bertscore": 0.6895044963166583, "rouge_l": 0.2734709553228889}, {"bertscore": 0.6877960723213619, "rouge_l": 0.26825612091623413}, {"bertscore": 0.6682277271320345, "rouge_l": 0.2309090339798998}, {"bertscore": 0.6884320274402853, "rouge_l": 0.27512553438829856}, {"bertscore": 0.6833193673810456, "rouge_l": 0.2604620393884814}, {"bertscore": 0.6848636295762844, "rouge_l": 0.2652949001747843}, {"bertscore": 0.6881995736330282, "rouge_l": 0.2690476507478209}, {"bertscore": 0.6896517913555726, "rouge_l": 0.2704297203870604}], "total": {"test_bertscore": 68.41064100663061, "test_bertscore_se": 0.43927920674577914, "test_rouge_l": 26.265053126372827, "test_rouge_l_se": 0.8509101574154243}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-fr", "task": "knowledge", "dataset_languages": ["fr"], "model": "mistralai/Mixtral-8x7B-v0.1", "results": {"raw": [{"mcc": 0.5006859420321625, "accuracy": 0.62451171875}, {"mcc": 0.48717164216668196, "accuracy": 0.61376953125}, {"mcc": 0.4676514692116121, "accuracy": 0.59814453125}, {"mcc": 0.4611930659871347, "accuracy": 0.59375}, {"mcc": 0.4884915174641235, "accuracy": 0.6162109375}, {"mcc": 0.4952237949995888, "accuracy": 0.62158203125}, {"mcc": 0.5027110960460932, "accuracy": 0.62451171875}, {"mcc": 0.46858027486964765, "accuracy": 0.60107421875}, {"mcc": 0.49853595006116735, "accuracy": 0.62255859375}, {"mcc": 0.4918200155627594, "accuracy": 0.61572265625}], "total": {"test_mcc": 48.62064768400972, "test_mcc_se": 0.9317028365255045, "test_accuracy": 61.318359375, "test_accuracy_se": 0.7101547340047686}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-it", "task": "knowledge", "dataset_languages": ["it"], "model": "mistralai/Mixtral-8x7B-v0.1", "results": {"raw": [{"mcc": 0.5197492297201288, "accuracy": 0.63818359375}, {"mcc": 0.5293050767124098, "accuracy": 0.6455078125}, {"mcc": 0.5088287654395456, "accuracy": 0.630859375}, {"mcc": 0.4871003241734459, "accuracy": 0.615234375}, {"mcc": 0.5069446654095616, "accuracy": 0.62939453125}, {"mcc": 0.5179588667559633, "accuracy": 0.63671875}, {"mcc": 0.5078471115322906, "accuracy": 0.63037109375}, {"mcc": 0.47404575134590715, "accuracy": 0.60595703125}, {"mcc": 0.492981272281797, "accuracy": 0.6171875}, {"mcc": 0.47116942959008407, "accuracy": 0.6044921875}], "total": {"test_mcc": 50.15930492961134, "test_mcc_se": 1.2142519552655608, "test_accuracy": 62.5390625, "test_accuracy_se": 0.8648418876385147}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-fr", "task": "common-sense-reasoning", "dataset_languages": ["fr"], "model": "mistralai/Mixtral-8x7B-v0.1", "results": {"raw": [{"mcc": 0.3654267286505184, "accuracy": 0.513671875}, {"mcc": 0.3137197292209792, "accuracy": 0.48046875}, {"mcc": 0.26721321202380754, "accuracy": 0.439453125}, {"mcc": 0.2686255642992917, "accuracy": 0.4384765625}, {"mcc": 0.31708277105408583, "accuracy": 0.4658203125}, {"mcc": 0.23703570289644293, "accuracy": 0.38232421875}, {"mcc": 0.26629673750735533, "accuracy": 0.447265625}, {"mcc": 0.2561128035594591, "accuracy": 0.42529296875}, {"mcc": 0.46862701084455965, "accuracy": 0.59912109375}, {"mcc": 0.31119355848624175, "accuracy": 0.47265625}], "total": {"test_mcc": 30.713338185427414, "test_mcc_se": 4.22894189977854, "test_accuracy": 46.6455078125, "test_accuracy_se": 3.6209874543581906}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-it", "task": "common-sense-reasoning", "dataset_languages": ["it"], "model": "mistralai/Mixtral-8x7B-v0.1", "results": {"raw": [{"mcc": 0.193471879351847, "accuracy": 0.38037109375}, {"mcc": 0.3117517347006753, "accuracy": 0.4736328125}, {"mcc": 0.22827430397200119, "accuracy": 0.40185546875}, {"mcc": 0.28118716044128333, "accuracy": 0.45361328125}, {"mcc": 0.250404474584061, "accuracy": 0.41650390625}, {"mcc": 0.40131360491498047, "accuracy": 0.54931640625}, {"mcc": 0.2692846388983576, "accuracy": 0.43408203125}, {"mcc": 0.2774718822613239, "accuracy": 0.44482421875}, {"mcc": 0.3255541109775055, "accuracy": 0.4736328125}, {"mcc": 0.2854320722272773, "accuracy": 0.44873046875}], "total": {"test_mcc": 28.24145862329312, "test_mcc_se": 3.5142242902733454, "test_accuracy": 44.765625, "test_accuracy_se": 2.8856885988925836}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "mistralai/Mixtral-8x7B-v0.1", "results": {"raw": [{"test_speed": 425.29, "test_speed_short": 111.68}, {"test_speed": 822.03, "test_speed_short": 91.2}, {"test_speed": 1247.01, "test_speed_short": 174.87}, {"test_speed": 1677.57, "test_speed_short": 219.6}, {"test_speed": 2035.8, "test_speed_short": 255.85}, {"test_speed": 2454.43, "test_speed_short": 340.86}, {"test_speed": 2901.81, "test_speed_short": 382.72}, {"test_speed": 3236.97, "test_speed_short": 428.13}, {"test_speed": 3640.87, "test_speed_short": 460.98}, {"test_speed": 4065.7999999999997, "test_speed_short": 504.04999999999995}], "total": {"test_speed": 2250.758, "test_speed_se": 756.8694667214673, "test_speed_short": 296.99399999999997, "test_speed_short_se": 91.35525196605855}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "allocine", "task": "sentiment-classification", "dataset_languages": ["fr"], "model": "mistralai/Mixtral-8x7B-Instruct-v0.1", "results": {"raw": [{"mcc": 0.9198165849405936, "macro_f1": 0.9589702494240209}, {"mcc": 0.9276055054310063, "macro_f1": 0.9633778497504759}, {"mcc": 0.9394646711018596, "macro_f1": 0.9695214686227821}, {"mcc": 0.9449670156613538, "macro_f1": 0.9721660509082586}, {"mcc": 0.9177910224635979, "macro_f1": 0.9584723215207985}, {"mcc": 0.922442511600236, "macro_f1": 0.9609240470704961}, {"mcc": 0.9316935996636108, "macro_f1": 0.9648437164723553}, {"mcc": 0.9320891031000796, "macro_f1": 0.9658085411866841}, {"mcc": 0.9340643374116406, "macro_f1": 0.9666972757881849}, {"mcc": 0.9366993192806362, "macro_f1": 0.9681541995094427}], "total": {"test_mcc": 93.06633670654617, "test_mcc_se": 0.5440480715394123, "test_macro_f1": 96.489357202535, "test_macro_f1_se": 0.2797943918138441}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "sentipolc16", "task": "sentiment-classification", "dataset_languages": ["it"], "model": "mistralai/Mixtral-8x7B-Instruct-v0.1", "results": {"raw": [{"mcc": 0.5918776913599149, "macro_f1": 0.7062239501036499}, {"mcc": 0.5780200844610323, "macro_f1": 0.6825901679241869}, {"mcc": 0.5763602114563356, "macro_f1": 0.6712785944664095}, {"mcc": 0.5772674319483121, "macro_f1": 0.6763341687552215}, {"mcc": 0.5743736653274051, "macro_f1": 0.6715974979976836}, {"mcc": 0.5856542159723728, "macro_f1": 0.6568949948617114}, {"mcc": 0.5652363057731246, "macro_f1": 0.6613011773422349}, {"mcc": 0.5435555660617, "macro_f1": 0.6408358888797537}, {"mcc": 0.5376438894137935, "macro_f1": 0.6372863166373545}, {"mcc": 0.5908771162413272, "macro_f1": 0.686499684131789}], "total": {"test_mcc": 57.20866178015318, "test_mcc_se": 1.1431726808811735, "test_macro_f1": 66.90842441099994, "test_macro_f1_se": 1.2979794900833215}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "eltec", "task": "named-entity-recognition", "dataset_languages": ["fr"], "model": "mistralai/Mixtral-8x7B-Instruct-v0.1", "results": {"raw": [{"micro_f1_no_misc": 0.5518920115069705, "micro_f1": 0.48762048762048765}, {"micro_f1_no_misc": 0.521431543903454, "micro_f1": 0.4776556776556777}, {"micro_f1_no_misc": 0.5786330457863305, "micro_f1": 0.48757309941520466}, {"micro_f1_no_misc": 0.5866896403187595, "micro_f1": 0.5046388939421502}, {"micro_f1_no_misc": 0.5773052923503582, "micro_f1": 0.5115925503610795}, {"micro_f1_no_misc": 0.5027411167512691, "micro_f1": 0.43420150788211104}, {"micro_f1_no_misc": 0.5349915682967961, "micro_f1": 0.44196194798394134}, {"micro_f1_no_misc": 0.5582835655828355, "micro_f1": 0.46949749675505287}, {"micro_f1_no_misc": 0.5276507276507276, "micro_f1": 0.4321989980998446}, {"micro_f1_no_misc": 0.5588697017268446, "micro_f1": 0.48254086181277855}], "total": {"test_micro_f1_no_misc": 54.98488213874345, "test_micro_f1_no_misc_se": 1.7082514103472928, "test_micro_f1": 47.29481521528328, "test_micro_f1_se": 1.7500174377653726}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "multinerd-it", "task": "named-entity-recognition", "dataset_languages": ["it"], "model": "mistralai/Mixtral-8x7B-Instruct-v0.1", "results": {"raw": [{"micro_f1_no_misc": 0.6323657655412981, "micro_f1": 0.4205515889682206}, {"micro_f1_no_misc": 0.5564371640644996, "micro_f1": 0.4818863210493441}, {"micro_f1_no_misc": 0.49412415318678277, "micro_f1": 0.4272469542563788}, {"micro_f1_no_misc": 0.5346148488647444, "micro_f1": 0.47203297165447056}, {"micro_f1_no_misc": 0.6105430885672553, "micro_f1": 0.477076923076923}, {"micro_f1_no_misc": 0.5422487454224875, "micro_f1": 0.45014028967922953}, {"micro_f1_no_misc": 0.5383067221253186, "micro_f1": 0.3763635146891521}, {"micro_f1_no_misc": 0.6102758530526867, "micro_f1": 0.46088474087563547}, {"micro_f1_no_misc": 0.5347005126922596, "micro_f1": 0.3672522373218429}, {"micro_f1_no_misc": 0.6368662510292907, "micro_f1": 0.45326430831176306}], "total": {"test_micro_f1_no_misc": 56.90483104546623, "test_micro_f1_no_misc_se": 3.052630827289838, "test_micro_f1": 43.866998498829595, "test_micro_f1_se": 2.507918852960048}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-fr", "task": "linguistic-acceptability", "dataset_languages": ["fr"], "model": "mistralai/Mixtral-8x7B-Instruct-v0.1", "results": {"raw": [{"mcc": 0.3717296721516209, "macro_f1": 0.6531773604114832}, {"mcc": 0.42993500052541556, "macro_f1": 0.6784347742565275}, {"mcc": 0.44134417491433175, "macro_f1": 0.6845616825400416}, {"mcc": 0.40484549971594874, "macro_f1": 0.6620411804058575}, {"mcc": 0.4468503475665241, "macro_f1": 0.6733593131767108}, {"mcc": 0.42279099731708714, "macro_f1": 0.6579338577850165}, {"mcc": 0.5143942355009601, "macro_f1": 0.743583322899712}, {"mcc": 0.3940681838466933, "macro_f1": 0.6708854893478029}, {"mcc": 0.4527488999989021, "macro_f1": 0.7040025971227982}, {"mcc": 0.44547221569650486, "macro_f1": 0.6908556447127989}], "total": {"test_mcc": 43.24179227233989, "test_mcc_se": 2.409790508947492, "test_macro_f1": 68.18835222658748, "test_macro_f1_se": 1.650316872167443}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-it", "task": "linguistic-acceptability", "dataset_languages": ["it"], "model": "mistralai/Mixtral-8x7B-Instruct-v0.1", "results": {"raw": [{"mcc": 0.2968827036741089, "macro_f1": 0.6167418562460867}, {"mcc": 0.2534886680390129, "macro_f1": 0.555488449343198}, {"mcc": 0.30528187308541455, "macro_f1": 0.5959767977349799}, {"mcc": 0.26115737287507, "macro_f1": 0.5377024162780839}, {"mcc": 0.27583415612982204, "macro_f1": 0.5698441715253121}, {"mcc": 0.24756151184486896, "macro_f1": 0.5413512755817214}, {"mcc": 0.25808674302914103, "macro_f1": 0.5715348375898834}, {"mcc": 0.3009587416275648, "macro_f1": 0.6207536052724479}, {"mcc": 0.23372722162822096, "macro_f1": 0.5451463985997346}, {"mcc": 0.27705026144766665, "macro_f1": 0.545921672806444}], "total": {"test_mcc": 27.10029253380891, "test_mcc_se": 1.5057258412113783, "test_macro_f1": 57.00461480977892, "test_macro_f1_se": 1.9281435263776818}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "squad-it", "task": "reading-comprehension", "dataset_languages": ["it"], "model": "mistralai/Mixtral-8x7B-Instruct-v0.1", "results": {"raw": [{"f1": 67.57387786119232, "em": 42.803030303030305}, {"f1": 66.4680842696858, "em": 38.81728582259287}, {"f1": 67.58186370312008, "em": 44.21134421134421}, {"f1": 67.60610521734455, "em": 40.791476407914764}, {"f1": 65.22569155443044, "em": 37.39398612181958}, {"f1": 65.84429286159818, "em": 38.62595419847328}, {"f1": 68.62424436910985, "em": 42.979719188767554}, {"f1": 64.32277950184064, "em": 35.92767295597484}, {"f1": 67.42533662402509, "em": 44.6594427244582}, {"f1": 67.08898270740707, "em": 39.120370370370374}], "total": {"test_f1": 66.77612586697542, "test_f1_se": 0.8080056452231204, "test_em": 40.5330282304746, "test_em_se": 1.8640333321237836}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fquad", "task": "reading-comprehension", "dataset_languages": ["fr"], "model": "mistralai/Mixtral-8x7B-Instruct-v0.1", "results": {"raw": [{"f1": 71.53478051768761, "em": 37.42424242424242}, {"f1": 69.86562602219848, "em": 37.90750568612585}, {"f1": 71.20308733763437, "em": 37.60683760683761}, {"f1": 71.3993368256039, "em": 36.68188736681888}, {"f1": 65.14142814948207, "em": 35.31225905936777}, {"f1": 74.3835815447374, "em": 40.0}, {"f1": 68.86357886212467, "em": 36.349453978159126}, {"f1": 68.1646865940102, "em": 36.79245283018868}, {"f1": 65.14626787584396, "em": 34.98452012383901}, {"f1": 71.87451571700038, "em": 37.5}], "total": {"test_f1": 69.75768894463229, "test_f1_se": 1.8449685018891822, "test_em": 37.05591590755793, "test_em_se": 0.8782084312325723}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "orange-sum", "task": "summarization", "dataset_languages": ["fr"], "model": "mistralai/Mixtral-8x7B-Instruct-v0.1", "results": {"raw": [{"bertscore": 0.6664245546562597, "rouge_l": 0.17264837747188844}, {"bertscore": 0.6630417229025625, "rouge_l": 0.17073932396882788}, {"bertscore": 0.6642167239042465, "rouge_l": 0.1749362498547416}, {"bertscore": 0.6648351939511485, "rouge_l": 0.1759828979810677}, {"bertscore": 0.6641257353767287, "rouge_l": 0.17401495875746176}, {"bertscore": 0.6651173714490142, "rouge_l": 0.17230676854797122}, {"bertscore": 0.6647482554835733, "rouge_l": 0.17494760512865956}, {"bertscore": 0.6624582103104331, "rouge_l": 0.1702781372001379}, {"bertscore": 0.668338658491848, "rouge_l": 0.18047248429016988}, {"bertscore": 0.6656269554514438, "rouge_l": 0.17055523800046923}], "total": {"test_bertscore": 66.48933381977258, "test_bertscore_se": 0.10378137527750425, "test_rouge_l": 17.368820412013953, "test_rouge_l_se": 0.19344855341083428}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "ilpost-sum", "task": "summarization", "dataset_languages": ["it"], "model": "mistralai/Mixtral-8x7B-Instruct-v0.1", "results": {"raw": [{"bertscore": 0.6556652686558664, "rouge_l": 0.17286451525220056}, {"bertscore": 0.6527383733919123, "rouge_l": 0.1680039661225873}, {"bertscore": 0.6537106012256118, "rouge_l": 0.17264553293774382}, {"bertscore": 0.6546510687912814, "rouge_l": 0.17205999002096073}, {"bertscore": 0.6499761949380627, "rouge_l": 0.16392090425192024}, {"bertscore": 0.6551072656438919, "rouge_l": 0.17607972428410607}, {"bertscore": 0.6528925608145073, "rouge_l": 0.16378710595056512}, {"bertscore": 0.6545998970250366, "rouge_l": 0.16801748992186216}, {"bertscore": 0.6557892111450201, "rouge_l": 0.17502173207783842}, {"bertscore": 0.6607258957519662, "rouge_l": 0.18412511689579403}], "total": {"test_bertscore": 65.45856337383157, "test_bertscore_se": 0.17147366232515257, "test_rouge_l": 17.165260777155783, "test_rouge_l_se": 0.37944351883375577}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-fr", "task": "knowledge", "dataset_languages": ["fr"], "model": "mistralai/Mixtral-8x7B-Instruct-v0.1", "results": {"raw": [{"mcc": 0.48790926685537633, "accuracy": 0.61376953125}, {"mcc": 0.5070321558260423, "accuracy": 0.6298828125}, {"mcc": 0.47730114003034757, "accuracy": 0.607421875}, {"mcc": 0.4709491318263019, "accuracy": 0.60107421875}, {"mcc": 0.5052481516728093, "accuracy": 0.62841796875}, {"mcc": 0.5001433445994252, "accuracy": 0.625}, {"mcc": 0.49591345303828493, "accuracy": 0.62158203125}, {"mcc": 0.4861050093639055, "accuracy": 0.6142578125}, {"mcc": 0.50546477462812, "accuracy": 0.6279296875}, {"mcc": 0.4938714619731698, "accuracy": 0.61962890625}], "total": {"test_mcc": 49.299378898137824, "test_mcc_se": 0.7643484338754354, "test_accuracy": 61.8896484375, "test_accuracy_se": 0.5970269507411666}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-it", "task": "knowledge", "dataset_languages": ["it"], "model": "mistralai/Mixtral-8x7B-Instruct-v0.1", "results": {"raw": [{"mcc": 0.498042043995566, "accuracy": 0.6220703125}, {"mcc": 0.5393095907479433, "accuracy": 0.6533203125}, {"mcc": 0.5148048169663574, "accuracy": 0.63525390625}, {"mcc": 0.45156232525136225, "accuracy": 0.587890625}, {"mcc": 0.4929365599350935, "accuracy": 0.6171875}, {"mcc": 0.4879447877840731, "accuracy": 0.61474609375}, {"mcc": 0.4964757032507167, "accuracy": 0.61962890625}, {"mcc": 0.4690957617438959, "accuracy": 0.6015625}, {"mcc": 0.4829127710247168, "accuracy": 0.611328125}, {"mcc": 0.48081674227040305, "accuracy": 0.6103515625}], "total": {"test_mcc": 49.13901102970129, "test_mcc_se": 1.4892346269553722, "test_accuracy": 61.7333984375, "test_accuracy_se": 1.1031535192062436}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-fr", "task": "common-sense-reasoning", "dataset_languages": ["fr"], "model": "mistralai/Mixtral-8x7B-Instruct-v0.1", "results": {"raw": [{"mcc": 0.4975560860258938, "accuracy": 0.619140625}, {"mcc": 0.4950092209590244, "accuracy": 0.61669921875}, {"mcc": 0.4646598750319235, "accuracy": 0.59521484375}, {"mcc": 0.4722588349583453, "accuracy": 0.59716796875}, {"mcc": 0.45435034990100454, "accuracy": 0.58349609375}, {"mcc": 0.48704137516480417, "accuracy": 0.609375}, {"mcc": 0.4421395802076741, "accuracy": 0.57373046875}, {"mcc": 0.5224165773606652, "accuracy": 0.6376953125}, {"mcc": 0.5182507534512586, "accuracy": 0.6337890625}, {"mcc": 0.5086620038899519, "accuracy": 0.62890625}], "total": {"test_mcc": 48.62344656950545, "test_mcc_se": 1.6866563797310619, "test_accuracy": 60.9521484375, "test_accuracy_se": 1.3418209153115896}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-it", "task": "common-sense-reasoning", "dataset_languages": ["it"], "model": "mistralai/Mixtral-8x7B-Instruct-v0.1", "results": {"raw": [{"mcc": 0.39477351675646505, "accuracy": 0.54150390625}, {"mcc": 0.47311298372505606, "accuracy": 0.60009765625}, {"mcc": 0.429886606323744, "accuracy": 0.56298828125}, {"mcc": 0.4136774127034324, "accuracy": 0.55615234375}, {"mcc": 0.4807776829418131, "accuracy": 0.6025390625}, {"mcc": 0.47119751408000016, "accuracy": 0.60107421875}, {"mcc": 0.48861261231975045, "accuracy": 0.611328125}, {"mcc": 0.5002016155963279, "accuracy": 0.6201171875}, {"mcc": 0.4831979506632357, "accuracy": 0.609375}, {"mcc": 0.5052320896818246, "accuracy": 0.62939453125}], "total": {"test_mcc": 46.4066998479165, "test_mcc_se": 2.345609797534557, "test_accuracy": 59.34570312500001, "test_accuracy_se": 1.8221723089644557}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "mistralai/Mixtral-8x7B-Instruct-v0.1", "results": {"raw": [{"test_speed": 445.88, "test_speed_short": 50.96}, {"test_speed": 878.4300000000001, "test_speed_short": 96.3}, {"test_speed": 1322.9699999999998, "test_speed_short": 185.31}, {"test_speed": 1756.25, "test_speed_short": 232.92}, {"test_speed": 2204.28, "test_speed_short": 276.06}, {"test_speed": 2618.62, "test_speed_short": 361.38}, {"test_speed": 3044.2000000000003, "test_speed_short": 402.56}, {"test_speed": 3472.59, "test_speed_short": 457.95}, {"test_speed": 3931.13, "test_speed_short": 500.76}, {"test_speed": 4353.21, "test_speed_short": 542.3}], "total": {"test_speed": 2402.7560000000003, "test_speed_se": 813.8924568925443, "test_speed_short": 310.65, "test_speed_short_se": 105.11464565095262}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hotter-and-colder-sentiment", "task": "sentiment-classification", "dataset_languages": ["is"], "model": "google/gemma-2-27b-it", "results": {"raw": [{"mcc": 0.5296418885679082, "macro_f1": 0.6800405843001157}, {"mcc": 0.5115532671897405, "macro_f1": 0.6738836093473508}, {"mcc": 0.4965337694820813, "macro_f1": 0.6593330762880919}, {"mcc": 0.4905099606283613, "macro_f1": 0.6542378945577932}, {"mcc": 0.4712814771633221, "macro_f1": 0.6263121612543571}, {"mcc": 0.5182182080537155, "macro_f1": 0.6724578247372511}, {"mcc": 0.5185839973358988, "macro_f1": 0.677001756729349}, {"mcc": 0.48828545460071926, "macro_f1": 0.650238153700885}, {"mcc": 0.5033677056077046, "macro_f1": 0.6553082493897957}, {"mcc": 0.5018559074472281, "macro_f1": 0.664554487734074}], "total": {"test_mcc": 50.298316360766805, "test_mcc_se": 1.0694012009790095, "test_macro_f1": 66.13367798039064, "test_macro_f1_se": 0.9966992624869969}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fosent", "task": "sentiment-classification", "dataset_languages": ["fo"], "model": "google/gemma-2-27b-it", "results": {"raw": [{"mcc": 0.642344623933186, "macro_f1": 0.7527966706048899}, {"mcc": 0.6182760743413565, "macro_f1": 0.7185492801771872}, {"mcc": 0.5711069335175734, "macro_f1": 0.690628332374998}, {"mcc": 0.6537762759105618, "macro_f1": 0.7557544951065928}, {"mcc": 0.5807789268103354, "macro_f1": 0.7024978466838933}, {"mcc": 0.6422700976221607, "macro_f1": 0.7606209870064872}, {"mcc": 0.5781039132818693, "macro_f1": 0.724158799322198}, {"mcc": 0.6168832005581921, "macro_f1": 0.7231884342393616}, {"mcc": 0.577753777596556, "macro_f1": 0.721037896083323}, {"mcc": 0.6214996401227534, "macro_f1": 0.7120377140698677}], "total": {"test_mcc": 61.027934636945446, "test_mcc_se": 1.924280226258606, "test_macro_f1": 72.61270455668797, "test_macro_f1_se": 1.4449933014184226}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "allocine", "task": "sentiment-classification", "dataset_languages": ["fr"], "model": "google/gemma-2-27b-it", "results": {"raw": [{"mcc": 0.9495177096302705, "macro_f1": 0.9745761821683774}, {"mcc": 0.9423446630874471, "macro_f1": 0.9711720996923532}, {"mcc": 0.9595797769112395, "macro_f1": 0.9797896504274795}, {"mcc": 0.9472307358307027, "macro_f1": 0.9736116479835073}, {"mcc": 0.9423487500297776, "macro_f1": 0.9711462713924899}, {"mcc": 0.9471947660917065, "macro_f1": 0.9735964520232405}, {"mcc": 0.9459709984216846, "macro_f1": 0.9726503814085168}, {"mcc": 0.9413527549652895, "macro_f1": 0.9706726670593526}, {"mcc": 0.9509191009619803, "macro_f1": 0.9754360108081552}, {"mcc": 0.9617101438043867, "macro_f1": 0.9808529443704104}], "total": {"test_mcc": 94.88169399734487, "test_mcc_se": 0.43321089639806276, "test_macro_f1": 97.43504307333882, "test_macro_f1_se": 0.2176650031672505}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "sentipolc16", "task": "sentiment-classification", "dataset_languages": ["it"], "model": "google/gemma-2-27b-it", "results": {"raw": [{"mcc": 0.4622656584934561, "macro_f1": 0.4857823291259824}, {"mcc": 0.463722753884752, "macro_f1": 0.48099482130780347}, {"mcc": 0.4598340118747765, "macro_f1": 0.48105047317884386}, {"mcc": 0.46066816445333264, "macro_f1": 0.4768659236744343}, {"mcc": 0.46490685853132047, "macro_f1": 0.4787344079348306}, {"mcc": 0.47257220575042874, "macro_f1": 0.48546796044983137}, {"mcc": 0.48282701100404224, "macro_f1": 0.4912803586836736}, {"mcc": 0.468004163545104, "macro_f1": 0.480946069722227}, {"mcc": 0.461067062573838, "macro_f1": 0.47980025218439454}, {"mcc": 0.4607039349702313, "macro_f1": 0.4743653925020859}], "total": {"test_mcc": 46.56571825081283, "test_mcc_se": 0.44711099100307566, "test_macro_f1": 48.15287988764106, "test_macro_f1_se": 0.3018154094674805}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mim-gold-ner", "task": "named-entity-recognition", "dataset_languages": ["is"], "model": "google/gemma-2-27b-it", "results": {"raw": [{"micro_f1_no_misc": 0.5217179111761835, "micro_f1": 0.29666344294003866}, {"micro_f1_no_misc": 0.4665432425284316, "micro_f1": 0.1929841800137565}, {"micro_f1_no_misc": 0.5574168487496965, "micro_f1": 0.31503347534996956}, {"micro_f1_no_misc": 0.566420664206642, "micro_f1": 0.27492447129909364}, {"micro_f1_no_misc": 0.5400904116107542, "micro_f1": 0.307031793919703}, {"micro_f1_no_misc": 0.47119130613081817, "micro_f1": 0.27258353536418256}, {"micro_f1_no_misc": 0.5147347740667977, "micro_f1": 0.29311345952851003}, {"micro_f1_no_misc": 0.5385987261146498, "micro_f1": 0.29482758620689653}, {"micro_f1_no_misc": 0.5160997732426305, "micro_f1": 0.229264475743349}, {"micro_f1_no_misc": 0.5424309122034727, "micro_f1": 0.2690987124463519}], "total": {"test_micro_f1_no_misc": 52.35244570030076, "test_micro_f1_no_misc_se": 2.0628334340801064, "test_micro_f1": 27.455251328118514, "test_micro_f1_se": 2.3261099091558233}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fone", "task": "named-entity-recognition", "dataset_languages": ["fo"], "model": "google/gemma-2-27b-it", "results": {"raw": [{"micro_f1_no_misc": 0.6015549076773566, "micro_f1": 0.3664464823959469}, {"micro_f1_no_misc": 0.6073522568636575, "micro_f1": 0.48375611927013795}, {"micro_f1_no_misc": 0.5683855507233514, "micro_f1": 0.4018571060098014}, {"micro_f1_no_misc": 0.5593123209169054, "micro_f1": 0.4491126403477001}, {"micro_f1_no_misc": 0.6352267303102624, "micro_f1": 0.4919583727530748}, {"micro_f1_no_misc": 0.5435917262710226, "micro_f1": 0.4369538077403246}, {"micro_f1_no_misc": 0.5950107191580588, "micro_f1": 0.4706482899438489}, {"micro_f1_no_misc": 0.6468678380443087, "micro_f1": 0.4784445091756482}, {"micro_f1_no_misc": 0.574006488240065, "micro_f1": 0.4886649874055416}, {"micro_f1_no_misc": 0.6202097235462346, "micro_f1": 0.5208139173656414}], "total": {"test_micro_f1_no_misc": 59.51518261751223, "test_micro_f1_no_misc_se": 2.087068683185799, "test_micro_f1": 45.88656232407666, "test_micro_f1_se": 2.8723598520872446}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "eltec", "task": "named-entity-recognition", "dataset_languages": ["fr"], "model": "google/gemma-2-27b-it", "results": {"raw": [{"micro_f1_no_misc": 0.40312698077329395, "micro_f1": 0.334390121808777}, {"micro_f1_no_misc": 0.420319541171651, "micro_f1": 0.3510928961748634}, {"micro_f1_no_misc": 0.48963210702341137, "micro_f1": 0.40078375489846807}, {"micro_f1_no_misc": 0.403157255616272, "micro_f1": 0.33449593090848695}, {"micro_f1_no_misc": 0.43816725978647686, "micro_f1": 0.3991715307851629}, {"micro_f1_no_misc": 0.4196371398078976, "micro_f1": 0.33836650652024713}, {"micro_f1_no_misc": 0.4018961253091508, "micro_f1": 0.33912324234904884}, {"micro_f1_no_misc": 0.4641031340297844, "micro_f1": 0.3955482576172232}, {"micro_f1_no_misc": 0.38412563667232597, "micro_f1": 0.30387020449552143}, {"micro_f1_no_misc": 0.4144568006843456, "micro_f1": 0.3287581699346405}], "total": {"test_micro_f1_no_misc": 42.3862198087461, "test_micro_f1_no_misc_se": 1.9804362430414189, "test_micro_f1": 35.2560061549244, "test_micro_f1_se": 2.0992229436347674}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "multinerd-it", "task": "named-entity-recognition", "dataset_languages": ["it"], "model": "google/gemma-2-27b-it", "results": {"raw": [{"micro_f1_no_misc": 0.5695715612150153, "micro_f1": 0.328084702049681}, {"micro_f1_no_misc": 0.6135621902574641, "micro_f1": 0.42487121816730755}, {"micro_f1_no_misc": 0.586124702044913, "micro_f1": 0.3828655421012109}, {"micro_f1_no_misc": 0.5862642281621692, "micro_f1": 0.43746573733260236}, {"micro_f1_no_misc": 0.6218722215165757, "micro_f1": 0.40730837789661317}, {"micro_f1_no_misc": 0.6161041587662749, "micro_f1": 0.409443116807879}, {"micro_f1_no_misc": 0.5745667561630461, "micro_f1": 0.3320605661619486}, {"micro_f1_no_misc": 0.6442795782897306, "micro_f1": 0.4505520319473808}, {"micro_f1_no_misc": 0.57895375652385, "micro_f1": 0.3336172443163205}, {"micro_f1_no_misc": 0.5520485208565417, "micro_f1": 0.33174775754281055}], "total": {"test_micro_f1_no_misc": 59.433476737955814, "test_micro_f1_no_misc_se": 1.760078941678038, "test_micro_f1": 38.38016294323754, "test_micro_f1_se": 3.0111994347419566}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-is", "task": "linguistic-acceptability", "dataset_languages": ["is"], "model": "google/gemma-2-27b-it", "results": {"raw": [{"mcc": 0.1825232810271985, "macro_f1": 0.5912339609523919}, {"mcc": 0.2192233440621069, "macro_f1": 0.6066262978054664}, {"mcc": 0.18884841262349827, "macro_f1": 0.5924424901359724}, {"mcc": 0.19846203320749034, "macro_f1": 0.5931063138400063}, {"mcc": 0.18864652240577712, "macro_f1": 0.5872211547602173}, {"mcc": 0.24402804048072532, "macro_f1": 0.615686274509804}, {"mcc": 0.19153646079070244, "macro_f1": 0.5889417099747112}, {"mcc": 0.20808479479076789, "macro_f1": 0.603845134656753}, {"mcc": 0.23398608078739466, "macro_f1": 0.615140676701796}, {"mcc": 0.21750744643059633, "macro_f1": 0.6053881983271607}], "total": {"test_mcc": 20.728464166062576, "test_mcc_se": 1.296330137293017, "test_macro_f1": 59.99632211664279, "test_macro_f1_se": 0.6630861195506094}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-fo", "task": "linguistic-acceptability", "dataset_languages": ["fo"], "model": "google/gemma-2-27b-it", "results": {"raw": [{"mcc": 0.10053694324079931, "macro_f1": 0.485617278330861}, {"mcc": 0.11631592955680266, "macro_f1": 0.4990067991214477}, {"mcc": 0.12399247595053746, "macro_f1": 0.5124804461674488}, {"mcc": 0.12843065169658358, "macro_f1": 0.5221994903269804}, {"mcc": 0.19120976972394607, "macro_f1": 0.5837989123456397}, {"mcc": 0.2134889090840803, "macro_f1": 0.5738274464389115}, {"mcc": 0.24672950911861122, "macro_f1": 0.5569178631405358}, {"mcc": 0.08267972847076845, "macro_f1": 0.45}, {"mcc": 0.15482766911609325, "macro_f1": 0.5092167723167585}, {"mcc": 0.06732781934155299, "macro_f1": 0.47756871407798757}], "total": {"test_mcc": 14.255394052997753, "test_mcc_se": 3.6166647455140346, "test_macro_f1": 51.70633722266571, "test_macro_f1_se": 2.673510369955275}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-fr", "task": "linguistic-acceptability", "dataset_languages": ["fr"], "model": "google/gemma-2-27b-it", "results": {"raw": [{"mcc": 0.36975695102002365, "macro_f1": 0.6441678013356675}, {"mcc": 0.41029840224348924, "macro_f1": 0.6659425204235969}, {"mcc": 0.42567435283436655, "macro_f1": 0.6757195724667542}, {"mcc": 0.42739377279799734, "macro_f1": 0.6742840620403301}, {"mcc": 0.4155079873714341, "macro_f1": 0.6595213446818059}, {"mcc": 0.4251122125515364, "macro_f1": 0.6772024427945165}, {"mcc": 0.43496958041449735, "macro_f1": 0.693847779581948}, {"mcc": 0.3539452983876945, "macro_f1": 0.6556473685943938}, {"mcc": 0.40369197958531006, "macro_f1": 0.6778681291361182}, {"mcc": 0.41801233337469745, "macro_f1": 0.6741101669973891}], "total": {"test_mcc": 40.84362870581047, "test_mcc_se": 1.6361642223829367, "test_macro_f1": 66.98311188052519, "test_macro_f1_se": 0.8642145715001915}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-it", "task": "linguistic-acceptability", "dataset_languages": ["it"], "model": "google/gemma-2-27b-it", "results": {"raw": [{"mcc": 0.3142943322893227, "macro_f1": 0.6271651987776761}, {"mcc": 0.2725243310477935, "macro_f1": 0.5737009544008483}, {"mcc": 0.2555011827897916, "macro_f1": 0.5616883116883117}, {"mcc": 0.26710040565605303, "macro_f1": 0.5429033886311693}, {"mcc": 0.2291000967720246, "macro_f1": 0.5477001895073998}, {"mcc": 0.2619509519787617, "macro_f1": 0.5661480011099016}, {"mcc": 0.29816867718323203, "macro_f1": 0.5972313542861454}, {"mcc": 0.34303411059552774, "macro_f1": 0.640905733936888}, {"mcc": 0.2585148103334326, "macro_f1": 0.5486627820110563}, {"mcc": 0.2572886333388937, "macro_f1": 0.5574669027742643}], "total": {"test_mcc": 27.57477531984833, "test_mcc_se": 2.0675858407602985, "test_macro_f1": 57.6357281712366, "test_macro_f1_se": 2.124525263748282}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "nqii", "task": "reading-comprehension", "dataset_languages": ["is"], "model": "google/gemma-2-27b-it", "results": {"raw": [{"f1": 50.39785890338492, "em": 20.46153846153846}, {"f1": 51.96593137138411, "em": 19.626168224299064}, {"f1": 48.09058742074159, "em": 21.65109034267913}, {"f1": 48.39403772618975, "em": 21.524663677130047}, {"f1": 50.415537198547, "em": 21.786833855799372}, {"f1": 51.38550985569753, "em": 25.731895223420647}, {"f1": 49.04201210469516, "em": 19.53846153846154}, {"f1": 49.13542358162343, "em": 24.539877300613497}, {"f1": 47.83807266701112, "em": 15.963855421686747}, {"f1": 50.50910755691582, "em": 23.343373493975903}], "total": {"test_f1": 49.71740783861904, "test_f1_se": 0.8799463360686949, "test_em": 21.416775753960444, "test_em_se": 1.7244837272279947}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "foqa", "task": "reading-comprehension", "dataset_languages": ["fo"], "model": "google/gemma-2-27b-it", "results": {"raw": [{"f1": 69.23686511921807, "em": 45.34313725490196}, {"f1": 67.22901706906721, "em": 45.7002457002457}, {"f1": 71.73668498761516, "em": 50.123456790123456}, {"f1": 69.69671653596951, "em": 47.57281553398058}, {"f1": 71.89290452756306, "em": 50.24752475247525}, {"f1": 68.92219247482402, "em": 47.160493827160494}, {"f1": 72.14420622231391, "em": 51.65094339622642}, {"f1": 73.32038911352599, "em": 46.81933842239186}, {"f1": 73.41616213721943, "em": 52.61845386533666}, {"f1": 72.6926886960661, "em": 50.122850122850124}], "total": {"test_f1": 71.02878268833824, "test_f1_se": 1.307857405908687, "test_em": 48.735925966569255, "test_em_se": 1.5726151443627037}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "squad-it", "task": "reading-comprehension", "dataset_languages": ["it"], "model": "google/gemma-2-27b-it", "results": {"raw": [{"f1": 71.441077167896, "em": 51.515151515151516}, {"f1": 72.08472134179856, "em": 47.83927217589083}, {"f1": 72.97159280027462, "em": 54.77855477855478}, {"f1": 71.32495606167784, "em": 50.91324200913242}, {"f1": 71.51441723340413, "em": 49.113338473400155}, {"f1": 71.25820080247183, "em": 50.458015267175576}, {"f1": 73.1140873590205, "em": 52.18408736349454}, {"f1": 70.78826087384041, "em": 47.09119496855346}, {"f1": 72.5521748028971, "em": 53.40557275541796}, {"f1": 70.32444315005044, "em": 48.07098765432099}], "total": {"test_f1": 71.73739315933314, "test_f1_se": 0.5709689511010848, "test_em": 50.53694169610922, "test_em_se": 1.5638609395777643}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fquad", "task": "reading-comprehension", "dataset_languages": ["fr"], "model": "google/gemma-2-27b-it", "results": {"raw": [{"f1": 69.93123667178699, "em": 38.56060606060606}, {"f1": 69.46626438214713, "em": 38.89310083396513}, {"f1": 70.28106711048684, "em": 38.61693861693862}, {"f1": 70.97303838925164, "em": 39.80213089802131}, {"f1": 66.55914784629938, "em": 35.31225905936777}, {"f1": 70.73563947211645, "em": 38.396946564885496}, {"f1": 69.49818231859375, "em": 39.39157566302652}, {"f1": 68.51772733929118, "em": 37.893081761006286}, {"f1": 66.79071053222624, "em": 37.693498452012385}, {"f1": 70.76419764883146, "em": 36.882716049382715}], "total": {"test_f1": 69.35172117110312, "test_f1_se": 0.9870711974016373, "test_em": 38.14428539592123, "test_em_se": 0.8055818153957817}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "rrn", "task": "summarization", "dataset_languages": ["is"], "model": "google/gemma-2-27b-it", "results": {"raw": [{"bertscore": 0.6810641674674116, "rouge_l": 0.19456942810595523}, {"bertscore": 0.6795429697376676, "rouge_l": 0.19273154486022087}, {"bertscore": 0.6824548945878632, "rouge_l": 0.20069627007128385}, {"bertscore": 0.678387980820844, "rouge_l": 0.18988627713355294}, {"bertscore": 0.6802410261007026, "rouge_l": 0.19246771000029747}, {"bertscore": 0.6785540980054066, "rouge_l": 0.19250689268170035}, {"bertscore": 0.6778045689570718, "rouge_l": 0.18925130869522816}, {"bertscore": 0.6764641523477621, "rouge_l": 0.1835097208272929}, {"bertscore": 0.6774426776973996, "rouge_l": 0.18766236488276747}, {"bertscore": 0.6790697348187678, "rouge_l": 0.18887896760649953}], "total": {"test_bertscore": 67.91026270540897, "test_bertscore_se": 0.11085473296704042, "test_rouge_l": 19.121604848647987, "test_rouge_l_se": 0.2845984906588884}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "orange-sum", "task": "summarization", "dataset_languages": ["fr"], "model": "google/gemma-2-27b-it", "results": {"raw": [{"bertscore": 0.6663798555964604, "rouge_l": 0.18295179863007205}, {"bertscore": 0.6658922226633877, "rouge_l": 0.1814628836806638}, {"bertscore": 0.6674255357938819, "rouge_l": 0.18570984313801953}, {"bertscore": 0.6645783431595191, "rouge_l": 0.18188384698764984}, {"bertscore": 0.6674308131623548, "rouge_l": 0.18889804878020908}, {"bertscore": 0.6675225311773829, "rouge_l": 0.18740526118849377}, {"bertscore": 0.6658571760635823, "rouge_l": 0.1817613347694145}, {"bertscore": 0.6656030387384817, "rouge_l": 0.18388918170101437}, {"bertscore": 0.668602712568827, "rouge_l": 0.186589378396107}, {"bertscore": 0.6649729025666602, "rouge_l": 0.17895876487572876}], "total": {"test_bertscore": 66.64265131490538, "test_bertscore_se": 0.0793803449542764, "test_rouge_l": 18.395103421473728, "test_rouge_l_se": 0.19341634730706805}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "ilpost-sum", "task": "summarization", "dataset_languages": ["it"], "model": "google/gemma-2-27b-it", "results": {"raw": [{"bertscore": 0.6614408652239945, "rouge_l": 0.18873125208419408}, {"bertscore": 0.6580503574514296, "rouge_l": 0.18379961580602017}, {"bertscore": 0.6597225074656308, "rouge_l": 0.18636288356193498}, {"bertscore": 0.6602725537959486, "rouge_l": 0.18792174385445576}, {"bertscore": 0.6585559037921485, "rouge_l": 0.1803164704809693}, {"bertscore": 0.6604433770698961, "rouge_l": 0.18937329725131752}, {"bertscore": 0.6600934312737081, "rouge_l": 0.18681414853999512}, {"bertscore": 0.6623647137603257, "rouge_l": 0.1935083388287003}, {"bertscore": 0.6611730706645176, "rouge_l": 0.18937346540784994}, {"bertscore": 0.6611928019265179, "rouge_l": 0.18821078137079322}], "total": {"test_bertscore": 66.03309582424117, "test_bertscore_se": 0.08163954002562186, "test_rouge_l": 18.744119971862304, "test_rouge_l_se": 0.21898179980113786}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "icelandic-knowledge", "task": "knowledge", "dataset_languages": ["is"], "model": "google/gemma-2-27b-it", "results": {"raw": [{"mcc": 0.2699789903854144, "accuracy": 0.4541015625}, {"mcc": 0.2448903922870643, "accuracy": 0.4296875}, {"mcc": 0.21872704417785663, "accuracy": 0.4140625}, {"mcc": 0.2976819848275505, "accuracy": 0.462890625}, {"mcc": 0.2265857241664451, "accuracy": 0.4150390625}, {"mcc": 0.2148962552619551, "accuracy": 0.40625}, {"mcc": 0.2176939721077193, "accuracy": 0.40625}, {"mcc": 0.25156937784681327, "accuracy": 0.4248046875}, {"mcc": 0.23611140331168465, "accuracy": 0.421875}, {"mcc": 0.2620136883695016, "accuracy": 0.4443359375}], "total": {"test_mcc": 24.401488327420047, "test_mcc_se": 1.6634342401481084, "test_accuracy": 42.79296875, "test_accuracy_se": 1.226527639056968}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-fr", "task": "knowledge", "dataset_languages": ["fr"], "model": "google/gemma-2-27b-it", "results": {"raw": [{"mcc": 0.614969568717549, "accuracy": 0.70947265625}, {"mcc": 0.6015204347761882, "accuracy": 0.7001953125}, {"mcc": 0.6055978295203517, "accuracy": 0.70263671875}, {"mcc": 0.5982598204015253, "accuracy": 0.697265625}, {"mcc": 0.5969652442157903, "accuracy": 0.6962890625}, {"mcc": 0.6033650809680383, "accuracy": 0.701171875}, {"mcc": 0.6157393042290962, "accuracy": 0.71142578125}, {"mcc": 0.6096029012375477, "accuracy": 0.70654296875}, {"mcc": 0.6120480382364, "accuracy": 0.70751953125}, {"mcc": 0.613230473738382, "accuracy": 0.70751953125}], "total": {"test_mcc": 60.71298696040868, "test_mcc_se": 0.42995528990601717, "test_accuracy": 70.400390625, "test_accuracy_se": 0.323697340523086}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-it", "task": "knowledge", "dataset_languages": ["it"], "model": "google/gemma-2-27b-it", "results": {"raw": [{"mcc": 0.6011264512535901, "accuracy": 0.7001953125}, {"mcc": 0.6128926899176095, "accuracy": 0.708984375}, {"mcc": 0.5961102818011058, "accuracy": 0.69580078125}, {"mcc": 0.5821636563539893, "accuracy": 0.6865234375}, {"mcc": 0.6052489812579012, "accuracy": 0.70361328125}, {"mcc": 0.5871384386928261, "accuracy": 0.68896484375}, {"mcc": 0.582914426916136, "accuracy": 0.6845703125}, {"mcc": 0.5783036722931618, "accuracy": 0.6826171875}, {"mcc": 0.5879194704379607, "accuracy": 0.68994140625}, {"mcc": 0.5849885973681382, "accuracy": 0.6884765625}], "total": {"test_mcc": 59.188066662924186, "test_mcc_se": 0.7062107459436463, "test_accuracy": 69.296875, "test_accuracy_se": 0.544527513736584}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "winogrande-is", "task": "common-sense-reasoning", "dataset_languages": ["is"], "model": "google/gemma-2-27b-it", "results": {"raw": [{"mcc": 0.38475728593693836, "accuracy": 0.6941964285714286}, {"mcc": 0.36772162758397037, "accuracy": 0.6852678571428571}, {"mcc": 0.3830599930714779, "accuracy": 0.6941964285714286}, {"mcc": 0.3094785172800032, "accuracy": 0.6607142857142857}, {"mcc": 0.34017671772617913, "accuracy": 0.6763392857142857}, {"mcc": 0.30542946933514176, "accuracy": 0.6506696428571429}, {"mcc": 0.2760699661504071, "accuracy": 0.6428571428571429}, {"mcc": 0.2911426435834336, "accuracy": 0.6462053571428571}, {"mcc": 0.35464628420741195, "accuracy": 0.6863839285714286}, {"mcc": 0.2813506929007269, "accuracy": 0.6428571428571429}], "total": {"test_mcc": 32.9383319777569, "test_mcc_se": 2.593147932408415, "test_accuracy": 66.79687500000001, "test_accuracy_se": 1.334295307274176}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-fr", "task": "common-sense-reasoning", "dataset_languages": ["fr"], "model": "google/gemma-2-27b-it", "results": {"raw": [{"mcc": 0.6759307371040449, "accuracy": 0.75439453125}, {"mcc": 0.679204295823618, "accuracy": 0.75537109375}, {"mcc": 0.6472182880461436, "accuracy": 0.734375}, {"mcc": 0.6495937871867964, "accuracy": 0.73193359375}, {"mcc": 0.704991030892868, "accuracy": 0.77392578125}, {"mcc": 0.6891991669943004, "accuracy": 0.7646484375}, {"mcc": 0.6693748123078546, "accuracy": 0.7490234375}, {"mcc": 0.6922641022386983, "accuracy": 0.76708984375}, {"mcc": 0.6739865698574696, "accuracy": 0.7529296875}, {"mcc": 0.6852859215226008, "accuracy": 0.76123046875}], "total": {"test_mcc": 67.67048711974395, "test_mcc_se": 1.121066153490109, "test_accuracy": 75.44921875, "test_accuracy_se": 0.8345400300570236}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-it", "task": "common-sense-reasoning", "dataset_languages": ["it"], "model": "google/gemma-2-27b-it", "results": {"raw": [{"mcc": 0.6866186183656724, "accuracy": 0.763671875}, {"mcc": 0.7068051724412849, "accuracy": 0.77978515625}, {"mcc": 0.6958890999109296, "accuracy": 0.77099609375}, {"mcc": 0.7134376993662509, "accuracy": 0.78466796875}, {"mcc": 0.6956973805433561, "accuracy": 0.77197265625}, {"mcc": 0.7132294740609367, "accuracy": 0.78466796875}, {"mcc": 0.6855835739491356, "accuracy": 0.7626953125}, {"mcc": 0.7110240207837872, "accuracy": 0.7822265625}, {"mcc": 0.713739967248178, "accuracy": 0.78515625}, {"mcc": 0.682031901724461, "accuracy": 0.759765625}], "total": {"test_mcc": 70.04056908393991, "test_mcc_se": 0.7882705084205565, "test_accuracy": 77.4560546875, "test_accuracy_se": 0.61990709613698}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "google/gemma-2-27b-it", "results": {"raw": [{"test_speed": 493.45, "test_speed_short": 34.64}, {"test_speed": 978.5400000000001, "test_speed_short": 63.45}, {"test_speed": 1443.24, "test_speed_short": 124.69999999999999}, {"test_speed": 1927.66, "test_speed_short": 153.0}, {"test_speed": 2383.29, "test_speed_short": 184.04000000000002}, {"test_speed": 2854.38, "test_speed_short": 241.68}, {"test_speed": 3348.6200000000003, "test_speed_short": 444.8}, {"test_speed": 3826.02, "test_speed_short": 494.16}, {"test_speed": 4265.5599999999995, "test_speed_short": 543.66}, {"test_speed": 4703.71, "test_speed_short": 586.5}], "total": {"test_speed": 2622.447, "test_speed_se": 882.0410564519767, "test_speed_short": 287.063, "test_speed_short_se": 129.6834242512887}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "norec", "task": "sentiment-classification", "dataset_languages": ["nb", "nn", "no"], "model": "meta-llama/Llama-3.1-70B-Instruct", "results": {"raw": [{"mcc": 0.4045660382981765, "macro_f1": 0.404605997677742}, {"mcc": 0.419891993458429, "macro_f1": 0.4110463994617867}, {"mcc": 0.4108794600141888, "macro_f1": 0.4127191355735232}, {"mcc": 0.3969959717361812, "macro_f1": 0.4058760654765184}, {"mcc": 0.404089860163871, "macro_f1": 0.4053272897953069}, {"mcc": 0.413486656990253, "macro_f1": 0.40755225893459207}, {"mcc": 0.402307704962605, "macro_f1": 0.39733518759674963}, {"mcc": 0.40497552493424704, "macro_f1": 0.3998510282682945}, {"mcc": 0.40513142491814524, "macro_f1": 0.4035237706661645}, {"mcc": 0.39682002577654935, "macro_f1": 0.4056485598591089}], "total": {"test_mcc": 40.59144661252646, "test_mcc_se": 0.442755584424942, "test_macro_f1": 40.53485693309787, "test_macro_f1_se": 0.2850316027147299}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hotter-and-colder-sentiment", "task": "sentiment-classification", "dataset_languages": ["is"], "model": "meta-llama/Llama-3.1-70B-Instruct", "results": {"raw": [{"mcc": 0.5497607237217628, "macro_f1": 0.6539447601414703}, {"mcc": 0.49348908145065873, "macro_f1": 0.6203794322303667}, {"mcc": 0.5175898802847683, "macro_f1": 0.6467820450988767}, {"mcc": 0.48286737208205055, "macro_f1": 0.6108360978472963}, {"mcc": 0.5062551612956362, "macro_f1": 0.618501970418138}, {"mcc": 0.515028085963063, "macro_f1": 0.6382947291555351}, {"mcc": 0.5197124154202954, "macro_f1": 0.6518181531658831}, {"mcc": 0.5006339249805455, "macro_f1": 0.6441381309485559}, {"mcc": 0.49714791811791526, "macro_f1": 0.6147138870752005}, {"mcc": 0.5230642424484612, "macro_f1": 0.6434570180474086}], "total": {"test_mcc": 51.055488057651566, "test_mcc_se": 1.1689816244029825, "test_macro_f1": 63.42866224128733, "test_macro_f1_se": 1.0169212732335484}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "norne-nb", "task": "named-entity-recognition", "dataset_languages": ["nb", "no"], "model": "meta-llama/Llama-3.1-70B-Instruct", "results": {"raw": [{"micro_f1_no_misc": 0.7923033389926429, "micro_f1": 0.6990805672432602}, {"micro_f1_no_misc": 0.8110019646365422, "micro_f1": 0.6738578680203046}, {"micro_f1_no_misc": 0.7954133977066988, "micro_f1": 0.6436358053124519}, {"micro_f1_no_misc": 0.8187836160529582, "micro_f1": 0.713034691815426}, {"micro_f1_no_misc": 0.8084150326797386, "micro_f1": 0.6654985664224276}, {"micro_f1_no_misc": 0.8148897430710095, "micro_f1": 0.7369858337600272}, {"micro_f1_no_misc": 0.7789563003818414, "micro_f1": 0.6390766694146743}, {"micro_f1_no_misc": 0.7822004490712391, "micro_f1": 0.6562703053931124}, {"micro_f1_no_misc": 0.7879818594104308, "micro_f1": 0.6627074767778286}, {"micro_f1_no_misc": 0.7961204283693675, "micro_f1": 0.6791653081186828}], "total": {"test_micro_f1_no_misc": 79.86066130372468, "test_micro_f1_no_misc_se": 0.863387602843976, "test_micro_f1": 67.69313092278195, "test_micro_f1_se": 1.9288822190817463}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "norne-nn", "task": "named-entity-recognition", "dataset_languages": ["nn"], "model": "meta-llama/Llama-3.1-70B-Instruct", "results": {"raw": [{"micro_f1_no_misc": 0.8291886004719551, "micro_f1": 0.7713556523044588}, {"micro_f1_no_misc": 0.7888268156424582, "micro_f1": 0.6612326612326612}, {"micro_f1_no_misc": 0.7849322694377435, "micro_f1": 0.687490791218506}, {"micro_f1_no_misc": 0.8135534915720674, "micro_f1": 0.7024029574861368}, {"micro_f1_no_misc": 0.8011736278909216, "micro_f1": 0.6968947513053037}, {"micro_f1_no_misc": 0.810480976310122, "micro_f1": 0.6188503366131539}, {"micro_f1_no_misc": 0.7950266429840143, "micro_f1": 0.7101024890190337}, {"micro_f1_no_misc": 0.8167619047619047, "micro_f1": 0.7559656379255489}, {"micro_f1_no_misc": 0.7899535523825908, "micro_f1": 0.6442217768871076}, {"micro_f1_no_misc": 0.7876461988304094, "micro_f1": 0.6189692982456141}], "total": {"test_micro_f1_no_misc": 80.17544080284186, "test_micro_f1_no_misc_se": 0.9302237273049632, "test_micro_f1": 68.67486352237525, "test_micro_f1_se": 3.234304323267265}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mim-gold-ner", "task": "named-entity-recognition", "dataset_languages": ["is"], "model": "meta-llama/Llama-3.1-70B-Instruct", "results": {"raw": [{"micro_f1_no_misc": 0.7117039586919105, "micro_f1": 0.5188811188811189}, {"micro_f1_no_misc": 0.6395494367959951, "micro_f1": 0.2995966607260107}, {"micro_f1_no_misc": 0.7285006753714544, "micro_f1": 0.4856022711910234}, {"micro_f1_no_misc": 0.6802151823072324, "micro_f1": 0.4095641230980437}, {"micro_f1_no_misc": 0.6766270978305362, "micro_f1": 0.4326934401130609}, {"micro_f1_no_misc": 0.637051406401552, "micro_f1": 0.4446618564265623}, {"micro_f1_no_misc": 0.7417130144605119, "micro_f1": 0.5495114481551698}, {"micro_f1_no_misc": 0.6988390621443206, "micro_f1": 0.36971350613915416}, {"micro_f1_no_misc": 0.7048868203934843, "micro_f1": 0.4064014425786092}, {"micro_f1_no_misc": 0.7262196480285141, "micro_f1": 0.3997731140102099}], "total": {"test_micro_f1_no_misc": 69.45306302425512, "test_micro_f1_no_misc_se": 2.2306653324131824, "test_micro_f1": 43.16398981318964, "test_micro_f1_se": 4.526306250020194}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-nb", "task": "linguistic-acceptability", "dataset_languages": ["nb", "no"], "model": "meta-llama/Llama-3.1-70B-Instruct", "results": {"raw": [{"mcc": 0.5499239658599979, "macro_f1": 0.7616642631311477}, {"mcc": 0.5578538884554337, "macro_f1": 0.7546661884789588}, {"mcc": 0.6135062060132437, "macro_f1": 0.7938595906151202}, {"mcc": 0.5576914591015278, "macro_f1": 0.7602290885503189}, {"mcc": 0.5992427863088867, "macro_f1": 0.7877101648636421}, {"mcc": 0.5904059720578015, "macro_f1": 0.7863990502693947}, {"mcc": 0.5434418101700474, "macro_f1": 0.7364791844706697}, {"mcc": 0.5344630645327181, "macro_f1": 0.7276855954894335}, {"mcc": 0.5372273342905001, "macro_f1": 0.7597172598930069}, {"mcc": 0.5890800490886983, "macro_f1": 0.7789524042893315}], "total": {"test_mcc": 56.72836535878856, "test_mcc_se": 1.7529471846528188, "test_macro_f1": 76.47362790051024, "test_macro_f1_se": 1.3665300998318757}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-nn", "task": "linguistic-acceptability", "dataset_languages": ["nn"], "model": "meta-llama/Llama-3.1-70B-Instruct", "results": {"raw": [{"mcc": 0.2833262182944416, "macro_f1": 0.5547709459308008}, {"mcc": 0.3665530394680042, "macro_f1": 0.6509218095651326}, {"mcc": 0.41246732017988175, "macro_f1": 0.6848121531096714}, {"mcc": 0.3610466962231734, "macro_f1": 0.6313020313020313}, {"mcc": 0.3549507914284846, "macro_f1": 0.6437429729099047}, {"mcc": 0.40244409282852683, "macro_f1": 0.6727811364041805}, {"mcc": 0.4645285292493063, "macro_f1": 0.6999326020492884}, {"mcc": 0.39093669466506037, "macro_f1": 0.6578034565891151}, {"mcc": 0.3136732560829185, "macro_f1": 0.5573487031700288}, {"mcc": 0.41392343965587863, "macro_f1": 0.686377458379821}], "total": {"test_mcc": 37.638500780756765, "test_mcc_se": 3.2506359637272277, "test_macro_f1": 64.39793269409975, "test_macro_f1_se": 3.152853519570934}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-is", "task": "linguistic-acceptability", "dataset_languages": ["is"], "model": "meta-llama/Llama-3.1-70B-Instruct", "results": {"raw": [{"mcc": 0.20138597516297138, "macro_f1": 0.5633202644555069}, {"mcc": 0.16383030684652589, "macro_f1": 0.5405889038208629}, {"mcc": 0.17410231573742246, "macro_f1": 0.5762890322981704}, {"mcc": 0.21558276794980402, "macro_f1": 0.5589689063539631}, {"mcc": 0.19669005841096943, "macro_f1": 0.562293749103257}, {"mcc": 0.23417911631068306, "macro_f1": 0.5893908970226831}, {"mcc": 0.18789194470978082, "macro_f1": 0.5611293180589556}, {"mcc": 0.16788083388207764, "macro_f1": 0.538143548520805}, {"mcc": 0.2207902530035791, "macro_f1": 0.5777248534136592}, {"mcc": 0.17694545214112392, "macro_f1": 0.5635110481251134}], "total": {"test_mcc": 19.392790241549378, "test_mcc_se": 1.486406847081408, "test_macro_f1": 56.313605211729765, "test_macro_f1_se": 0.9757279368149279}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "norquad", "task": "reading-comprehension", "dataset_languages": ["nb", "nn", "no"], "model": "meta-llama/Llama-3.1-70B-Instruct", "results": {"raw": [{"f1": 77.22561556120431, "em": 53.10457516339869}, {"f1": 73.40477850928238, "em": 47.39837398373984}, {"f1": 76.87990689723205, "em": 53.3721898417985}, {"f1": 73.79040776408755, "em": 47.17906786590352}, {"f1": 79.11299156816474, "em": 55.33498759305211}, {"f1": 73.04612185171845, "em": 46.41975308641975}, {"f1": 73.06761849472578, "em": 46.554621848739494}, {"f1": 75.62961074717519, "em": 49.66216216216216}, {"f1": 72.75233803272734, "em": 44.6843853820598}, {"f1": 74.32876940219589, "em": 48.50993377483444}], "total": {"test_f1": 74.92381588285137, "test_f1_se": 1.3516991676567687, "test_em": 49.222005070210834, "test_em_se": 2.2003320778747315}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "nqii", "task": "reading-comprehension", "dataset_languages": ["is"], "model": "meta-llama/Llama-3.1-70B-Instruct", "results": {"raw": [{"f1": 58.36230525973138, "em": 31.53846153846154}, {"f1": 62.60396232449341, "em": 34.11214953271028}, {"f1": 53.62826944044094, "em": 28.97196261682243}, {"f1": 52.603218858180405, "em": 27.3542600896861}, {"f1": 57.28620420255365, "em": 30.877742946708462}, {"f1": 55.604071790156304, "em": 31.124807395993837}, {"f1": 58.7568428371152, "em": 33.07692307692308}, {"f1": 53.61987244929458, "em": 29.141104294478527}, {"f1": 55.68994858216095, "em": 30.572289156626507}, {"f1": 54.71816339253493, "em": 31.174698795180724}], "total": {"test_f1": 56.287285913666175, "test_f1_se": 1.8748072217548502, "test_em": 30.79443994435915, "test_em_se": 1.2217561110118067}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "rrn", "task": "summarization", "dataset_languages": ["is"], "model": "meta-llama/Llama-3.1-70B-Instruct", "results": {"raw": [{"bertscore": 0.6971632335335016, "rouge_l": 0.24308334808378756}, {"bertscore": 0.697089564811904, "rouge_l": 0.24200468690953986}, {"bertscore": 0.6995407947106287, "rouge_l": 0.2481742219844787}, {"bertscore": 0.695979735639412, "rouge_l": 0.23060695108997023}, {"bertscore": 0.6963417801889591, "rouge_l": 0.23247449464015085}, {"bertscore": 0.6972221614560112, "rouge_l": 0.24301100238707335}, {"bertscore": 0.6932636772398837, "rouge_l": 0.23277527831928924}, {"bertscore": 0.6905347259016708, "rouge_l": 0.22352030848171942}, {"bertscore": 0.6934771224041469, "rouge_l": 0.2286241725975348}, {"bertscore": 0.6927343301940709, "rouge_l": 0.23634443394747973}], "total": {"test_bertscore": 69.53347126080189, "test_bertscore_se": 0.16866697421159124, "test_rouge_l": 23.606188984410235, "test_rouge_l_se": 0.4826425772986681}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "no-sammendrag", "task": "summarization", "dataset_languages": ["nb", "nn", "no"], "model": "meta-llama/Llama-3.1-70B-Instruct", "results": {"raw": [{"bertscore": 0.667784125675098, "rouge_l": 0.2013212016081884}, {"bertscore": 0.6682417261908995, "rouge_l": 0.20399972644533104}, {"bertscore": 0.6751142420835095, "rouge_l": 0.21966022150277287}, {"bertscore": 0.6681697192107094, "rouge_l": 0.206504941960152}, {"bertscore": 0.6704786932969, "rouge_l": 0.20443074660303856}, {"bertscore": 0.6530296324926894, "rouge_l": 0.16834691331617202}, {"bertscore": 0.6625775440770667, "rouge_l": 0.1859999640957361}, {"bertscore": 0.6673653998877853, "rouge_l": 0.2031220308158848}, {"bertscore": 0.6707871063554194, "rouge_l": 0.2099539118434303}, {"bertscore": 0.651874261937337, "rouge_l": 0.1657244926534501}], "total": {"test_bertscore": 66.55422451207414, "test_bertscore_se": 0.47031362823126543, "test_rouge_l": 19.690641508441562, "test_rouge_l_se": 1.1033536095346868}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "nrk-quiz-qa", "task": "knowledge", "dataset_languages": ["nb", "nn", "no"], "model": "meta-llama/Llama-3.1-70B-Instruct", "results": {"raw": [{"mcc": 0.5605333732142921, "accuracy": 0.67138671875}, {"mcc": 0.5596294836241874, "accuracy": 0.67041015625}, {"mcc": 0.5470557168800034, "accuracy": 0.6640625}, {"mcc": 0.5584624685426349, "accuracy": 0.6689453125}, {"mcc": 0.5464579747790048, "accuracy": 0.66357421875}, {"mcc": 0.5448580443679762, "accuracy": 0.66259765625}, {"mcc": 0.5256185187975431, "accuracy": 0.6474609375}, {"mcc": 0.5592574063990058, "accuracy": 0.6708984375}, {"mcc": 0.5608802753609401, "accuracy": 0.6728515625}, {"mcc": 0.5555607371070986, "accuracy": 0.67041015625}], "total": {"test_mcc": 55.18313999072687, "test_mcc_se": 0.6910725372938489, "test_accuracy": 66.6259765625, "test_accuracy_se": 0.46738020070833136}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "icelandic-knowledge", "task": "knowledge", "dataset_languages": ["is"], "model": "meta-llama/Llama-3.1-70B-Instruct", "results": {"raw": [{"mcc": 0.30296827087460876, "accuracy": 0.474609375}, {"mcc": 0.343659869687785, "accuracy": 0.505859375}, {"mcc": 0.34732699282067714, "accuracy": 0.5078125}, {"mcc": 0.3568331533058347, "accuracy": 0.5107421875}, {"mcc": 0.356120425746393, "accuracy": 0.509765625}, {"mcc": 0.3071847601535194, "accuracy": 0.478515625}, {"mcc": 0.3699492455584667, "accuracy": 0.5224609375}, {"mcc": 0.3524072964175422, "accuracy": 0.5068359375}, {"mcc": 0.3494920694844177, "accuracy": 0.5087890625}, {"mcc": 0.3519481352078244, "accuracy": 0.513671875}], "total": {"test_mcc": 34.3789021925707, "test_mcc_se": 1.3387245329029178, "test_accuracy": 50.390625, "test_accuracy_se": 0.9411618300153154}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "nor-common-sense-qa", "task": "common-sense-reasoning", "dataset_languages": ["nb", "nn", "no"], "model": "meta-llama/Llama-3.1-70B-Instruct", "results": {"raw": [{"mcc": 0.7858668406905435, "accuracy": 0.8284371327849589}, {"mcc": 0.7664186767634683, "accuracy": 0.8119858989424207}, {"mcc": 0.7633392915980212, "accuracy": 0.8096357226792009}, {"mcc": 0.7498382745681523, "accuracy": 0.7967097532314924}, {"mcc": 0.7748539447923697, "accuracy": 0.81786133960047}, {"mcc": 0.7813002546134435, "accuracy": 0.8237367802585194}, {"mcc": 0.7776987728351764, "accuracy": 0.8202115158636898}, {"mcc": 0.7854805129237294, "accuracy": 0.827262044653349}, {"mcc": 0.8210065248383723, "accuracy": 0.8554641598119859}, {"mcc": 0.8079265055493656, "accuracy": 0.8460634547591069}], "total": {"test_mcc": 78.13729599172643, "test_mcc_se": 1.2922563165534924, "test_accuracy": 82.37367802585194, "test_accuracy_se": 1.064343833238442}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "winogrande-is", "task": "common-sense-reasoning", "dataset_languages": ["is"], "model": "meta-llama/Llama-3.1-70B-Instruct", "results": {"raw": [{"mcc": 0.5521505022205678, "accuracy": 0.7790178571428571}, {"mcc": 0.4606086103351664, "accuracy": 0.7332589285714286}, {"mcc": 0.4911749643804222, "accuracy": 0.75}, {"mcc": 0.5280862405005741, "accuracy": 0.7689732142857143}, {"mcc": 0.5197514559463441, "accuracy": 0.7633928571428571}, {"mcc": 0.4688181081231403, "accuracy": 0.7377232142857143}, {"mcc": 0.4507786068207424, "accuracy": 0.7287946428571429}, {"mcc": 0.4956146735525133, "accuracy": 0.75}, {"mcc": 0.5061784354831232, "accuracy": 0.7611607142857143}, {"mcc": 0.42788070439792225, "accuracy": 0.7154017857142857}], "total": {"test_mcc": 49.01042301760516, "test_mcc_se": 2.3739102805331163, "test_accuracy": 74.87723214285714, "test_accuracy_se": 1.2341886286871036}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "sentipolc16", "task": "sentiment-classification", "dataset_languages": ["it"], "model": "meta-llama/Meta-Llama-3-70B-Instruct", "results": {"raw": [{"mcc": 0.4883769926339765, "macro_f1": 0.49684550491410756}, {"mcc": 0.4993046641880286, "macro_f1": 0.49842436974789917}, {"mcc": 0.5159328714770718, "macro_f1": 0.5066817463736896}, {"mcc": 0.47165346133938635, "macro_f1": 0.4842543341077179}, {"mcc": 0.4915925362984222, "macro_f1": 0.4945376016260163}, {"mcc": 0.49512249847483164, "macro_f1": 0.4970565949758116}, {"mcc": 0.5032700357277192, "macro_f1": 0.5014245014245015}, {"mcc": 0.48663138823517704, "macro_f1": 0.49136348985435957}, {"mcc": 0.48560825374052907, "macro_f1": 0.4910546293865406}, {"mcc": 0.4739341250839789, "macro_f1": 0.48316049711692904}], "total": {"test_mcc": 49.114268271991214, "test_mcc_se": 0.8209362302020615, "test_macro_f1": 49.44803269527572, "test_macro_f1_se": 0.45219244494656435}}, "num_model_parameters": 70553706496, "max_sequence_length": 8192, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "multinerd-it", "task": "named-entity-recognition", "dataset_languages": ["it"], "model": "meta-llama/Meta-Llama-3-70B-Instruct", "results": {"raw": [{"micro_f1_no_misc": 0.8235740483116226, "micro_f1": 0.5593155014260388}, {"micro_f1_no_misc": 0.8065472660419879, "micro_f1": 0.6305106248017761}, {"micro_f1_no_misc": 0.7890596635604503, "micro_f1": 0.5936777178103315}, {"micro_f1_no_misc": 0.7961408344818944, "micro_f1": 0.6240354621572813}, {"micro_f1_no_misc": 0.8387583260022621, "micro_f1": 0.6561824937354187}, {"micro_f1_no_misc": 0.794617737003058, "micro_f1": 0.6382663150955833}, {"micro_f1_no_misc": 0.8022303950646578, "micro_f1": 0.5965152552689644}, {"micro_f1_no_misc": 0.8107572189862436, "micro_f1": 0.640793297999658}, {"micro_f1_no_misc": 0.7916912545733507, "micro_f1": 0.5938163795138068}, {"micro_f1_no_misc": 0.8181064107121696, "micro_f1": 0.6001245717844907}], "total": {"test_micro_f1_no_misc": 80.71483154737697, "test_micro_f1_no_misc_se": 0.9845516123779785, "test_micro_f1": 61.332376195933506, "test_micro_f1_se": 1.8204272276400837}}, "num_model_parameters": 70553706496, "max_sequence_length": 8192, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-it", "task": "linguistic-acceptability", "dataset_languages": ["it"], "model": "meta-llama/Meta-Llama-3-70B-Instruct", "results": {"raw": [{"mcc": 0.17116717920181806, "macro_f1": 0.5381948323300028}, {"mcc": 0.22583358936634232, "macro_f1": 0.5504729546319422}, {"mcc": 0.2420033677818982, "macro_f1": 0.547306538371588}, {"mcc": 0.2168934673208548, "macro_f1": 0.5362506712702563}, {"mcc": 0.20266143080839955, "macro_f1": 0.5286951989576025}, {"mcc": 0.242753930266177, "macro_f1": 0.5357235611312865}, {"mcc": 0.19672230424301063, "macro_f1": 0.5336281036110626}, {"mcc": 0.28257592422511396, "macro_f1": 0.597771042811536}, {"mcc": 0.22515938371824473, "macro_f1": 0.5412000724703804}, {"mcc": 0.22238691175236283, "macro_f1": 0.5298654287147732}], "total": {"test_mcc": 22.281574886842222, "test_mcc_se": 1.8641731625720845, "test_macro_f1": 54.3910840430043, "test_macro_f1_se": 1.2491774790964014}}, "num_model_parameters": 70553706496, "max_sequence_length": 8192, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "squad-it", "task": "reading-comprehension", "dataset_languages": ["it"], "model": "meta-llama/Meta-Llama-3-70B-Instruct", "results": {"raw": [{"f1": 69.63069326939618, "em": 40.90909090909091}, {"f1": 69.81883285180858, "em": 40.40940106141016}, {"f1": 71.04463712309803, "em": 46.62004662004662}, {"f1": 69.89502530115358, "em": 41.55251141552512}, {"f1": 68.08677971047265, "em": 39.321511179645334}, {"f1": 68.17506210561508, "em": 39.31297709923664}, {"f1": 72.29156190958885, "em": 47.50390015600624}, {"f1": 68.16271486548976, "em": 37.42138364779874}, {"f1": 70.09757222183754, "em": 45.743034055727556}, {"f1": 67.58859888436437, "em": 37.73148148148148}], "total": {"test_f1": 69.47914782428246, "test_f1_se": 0.9239686610930857, "test_em": 41.65253376259688, "test_em_se": 2.2814286799667864}}, "num_model_parameters": 70553706496, "max_sequence_length": 8192, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "ilpost-sum", "task": "summarization", "dataset_languages": ["it"], "model": "meta-llama/Meta-Llama-3-70B-Instruct", "results": {"raw": [{"bertscore": 0.674356411662302, "rouge_l": 0.23026648561985763}, {"bertscore": 0.6655581990198698, "rouge_l": 0.20488956100606665}, {"bertscore": 0.6703292610764038, "rouge_l": 0.21581263926670202}, {"bertscore": 0.6715254266455304, "rouge_l": 0.22241013618042058}, {"bertscore": 0.6672134843975073, "rouge_l": 0.2050373814548398}, {"bertscore": 0.6657818015810335, "rouge_l": 0.20769321445348526}, {"bertscore": 0.6706702857336495, "rouge_l": 0.21802618022658216}, {"bertscore": 0.6745145510649309, "rouge_l": 0.23281874071745856}, {"bertscore": 0.67017081764061, "rouge_l": 0.21670147635417208}, {"bertscore": 0.6708058380609145, "rouge_l": 0.21638596348297578}], "total": {"test_bertscore": 67.00926076882752, "test_bertscore_se": 0.19394689528405784, "test_rouge_l": 21.700417787625607, "test_rouge_l_se": 0.5969385710479964}}, "num_model_parameters": 70553706496, "max_sequence_length": 8192, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-it", "task": "knowledge", "dataset_languages": ["it"], "model": "meta-llama/Meta-Llama-3-70B-Instruct", "results": {"raw": [{"mcc": 0.6594833401009415, "accuracy": 0.74365234375}, {"mcc": 0.6687350158524538, "accuracy": 0.75048828125}, {"mcc": 0.6474275767190794, "accuracy": 0.73486328125}, {"mcc": 0.6396668250337788, "accuracy": 0.7294921875}, {"mcc": 0.6370478608020909, "accuracy": 0.728515625}, {"mcc": 0.6207900986113931, "accuracy": 0.71435546875}, {"mcc": 0.6451724369410906, "accuracy": 0.7333984375}, {"mcc": 0.6289138520069381, "accuracy": 0.720703125}, {"mcc": 0.6393380443508088, "accuracy": 0.72900390625}, {"mcc": 0.6397772330720439, "accuracy": 0.72998046875}], "total": {"test_mcc": 64.2635228349062, "test_mcc_se": 0.85659383278156, "test_accuracy": 73.14453125, "test_accuracy_se": 0.6384991696494303}}, "num_model_parameters": 70553706496, "max_sequence_length": 8192, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-it", "task": "common-sense-reasoning", "dataset_languages": ["it"], "model": "meta-llama/Meta-Llama-3-70B-Instruct", "results": {"raw": [{"mcc": 0.6331293341433375, "accuracy": 0.71875}, {"mcc": 0.674489865775377, "accuracy": 0.75244140625}, {"mcc": 0.6626241551156411, "accuracy": 0.74365234375}, {"mcc": 0.6111991928384908, "accuracy": 0.7001953125}, {"mcc": 0.6366967635065011, "accuracy": 0.72216796875}, {"mcc": 0.6539362963095291, "accuracy": 0.73876953125}, {"mcc": 0.6471880781587953, "accuracy": 0.72900390625}, {"mcc": 0.6210984421173512, "accuracy": 0.70849609375}, {"mcc": 0.6683652045711433, "accuracy": 0.74853515625}, {"mcc": 0.6241591280976901, "accuracy": 0.71337890625}], "total": {"test_mcc": 64.32886460633857, "test_mcc_se": 1.329261375378926, "test_accuracy": 72.75390625, "test_accuracy_se": 1.1060050199799818}}, "num_model_parameters": 70553706496, "max_sequence_length": 8192, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "meta-llama/Meta-Llama-3-70B-Instruct", "results": {"raw": [{"test_speed": 241.4, "test_speed_short": 27.76}, {"test_speed": 476.58, "test_speed_short": 51.150000000000006}, {"test_speed": 708.9599999999999, "test_speed_short": 99.17999999999999}, {"test_speed": 946.97, "test_speed_short": 123.84}, {"test_speed": 1186.3799999999999, "test_speed_short": 147.92}, {"test_speed": 1414.56, "test_speed_short": 196.65}, {"test_speed": 1639.9399999999998, "test_speed_short": 220.16}, {"test_speed": 1862.52, "test_speed_short": 242.11}, {"test_speed": 2088.61, "test_speed_short": 269.1}, {"test_speed": 2250.21, "test_speed_short": 294.1}], "total": {"test_speed": 1281.613, "test_speed_se": 426.0243508723866, "test_speed_short": 167.19699999999997, "test_speed_short_se": 56.79133084087316}}, "num_model_parameters": 70553706496, "max_sequence_length": 8192, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "allocine", "task": "sentiment-classification", "dataset_languages": ["fr"], "model": "meta-llama/Meta-Llama-3-70B", "results": {"raw": [{"mcc": 0.9470230691122943, "macro_f1": 0.9731206851170828}, {"mcc": 0.9255734530271266, "macro_f1": 0.9624008287763747}, {"mcc": 0.9572311963071729, "macro_f1": 0.9783735421484572}, {"mcc": 0.9532614885694525, "macro_f1": 0.9765552557972701}, {"mcc": 0.9407075998002984, "macro_f1": 0.9701848103579909}, {"mcc": 0.9407904314714937, "macro_f1": 0.9702004566012901}, {"mcc": 0.9332989955293084, "macro_f1": 0.9658197909514}, {"mcc": 0.9413821773732438, "macro_f1": 0.9706762503400581}, {"mcc": 0.9502035895385118, "macro_f1": 0.9749984620588114}, {"mcc": 0.954131414052304, "macro_f1": 0.9769621904300323}], "total": {"test_mcc": 94.43603414781207, "test_mcc_se": 0.6169949531524882, "test_macro_f1": 97.1929227257877, "test_macro_f1_se": 0.3165917472592973}}, "num_model_parameters": 70553706496, "max_sequence_length": 8192, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "sentipolc16", "task": "sentiment-classification", "dataset_languages": ["it"], "model": "meta-llama/Meta-Llama-3-70B", "results": {"raw": [{"mcc": 0.660785598226847, "macro_f1": 0.7732417277850315}, {"mcc": 0.6680502469611584, "macro_f1": 0.7676980394594185}, {"mcc": 0.6515903623956557, "macro_f1": 0.7587331580455808}, {"mcc": 0.6403773728300536, "macro_f1": 0.758604240068771}, {"mcc": 0.6961512426776509, "macro_f1": 0.7949193675179674}, {"mcc": 0.65748036187855, "macro_f1": 0.7587927672145077}, {"mcc": 0.6734487678486923, "macro_f1": 0.7764034203427044}, {"mcc": 0.6875293317197007, "macro_f1": 0.7821238881556218}, {"mcc": 0.6387713159943118, "macro_f1": 0.7388762855477395}, {"mcc": 0.6611739849819261, "macro_f1": 0.7731506546494643}], "total": {"test_mcc": 66.35358585514545, "test_mcc_se": 1.1503491107341157, "test_macro_f1": 76.82543548786806, "test_macro_f1_se": 0.9591595880411199}}, "num_model_parameters": 70553706496, "max_sequence_length": 8192, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "eltec", "task": "named-entity-recognition", "dataset_languages": ["fr"], "model": "meta-llama/Meta-Llama-3-70B", "results": {"raw": [{"micro_f1_no_misc": 0.6786608972899112, "micro_f1": 0.6357260726072608}, {"micro_f1_no_misc": 0.6811308349769887, "micro_f1": 0.6707869798104658}, {"micro_f1_no_misc": 0.696142991533396, "micro_f1": 0.6613937580162462}, {"micro_f1_no_misc": 0.6866285714285714, "micro_f1": 0.6652433817250213}, {"micro_f1_no_misc": 0.6632996632996633, "micro_f1": 0.6580882352941178}, {"micro_f1_no_misc": 0.672127417519909, "micro_f1": 0.6399501661129567}, {"micro_f1_no_misc": 0.7068381855111713, "micro_f1": 0.6663924310983135}, {"micro_f1_no_misc": 0.7281721493940144, "micro_f1": 0.6946882217090069}, {"micro_f1_no_misc": 0.6394153916419273, "micro_f1": 0.6114033242162844}, {"micro_f1_no_misc": 0.714148219441771, "micro_f1": 0.6364764267990074}], "total": {"test_micro_f1_no_misc": 68.66564322037323, "test_micro_f1_no_misc_se": 1.6082433385127806, "test_micro_f1": 65.40148997388681, "test_micro_f1_se": 1.4523389602956567}}, "num_model_parameters": 70553706496, "max_sequence_length": 8192, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "multinerd-it", "task": "named-entity-recognition", "dataset_languages": ["it"], "model": "meta-llama/Meta-Llama-3-70B", "results": {"raw": [{"micro_f1_no_misc": 0.8406621831350233, "micro_f1": 0.6667858739383102}, {"micro_f1_no_misc": 0.8316573997397375, "micro_f1": 0.7306259652948124}, {"micro_f1_no_misc": 0.8234024697278504, "micro_f1": 0.6824727783631893}, {"micro_f1_no_misc": 0.8481075697211156, "micro_f1": 0.6930955324033198}, {"micro_f1_no_misc": 0.8594499116830685, "micro_f1": 0.7096003674781811}, {"micro_f1_no_misc": 0.8436953807740325, "micro_f1": 0.7014882927291687}, {"micro_f1_no_misc": 0.8381353947060202, "micro_f1": 0.6249009979407573}, {"micro_f1_no_misc": 0.8551496702181635, "micro_f1": 0.7304011599806671}, {"micro_f1_no_misc": 0.8454935622317596, "micro_f1": 0.6796133723508025}, {"micro_f1_no_misc": 0.8205676285476785, "micro_f1": 0.7072859744990891}], "total": {"test_micro_f1_no_misc": 84.06321170484449, "test_micro_f1_no_misc_se": 0.7839980656725473, "test_micro_f1": 69.26270314978298, "test_micro_f1_se": 1.957774045272678}}, "num_model_parameters": 70553706496, "max_sequence_length": 8192, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-fr", "task": "linguistic-acceptability", "dataset_languages": ["fr"], "model": "meta-llama/Meta-Llama-3-70B", "results": {"raw": [{"mcc": 0.4192394842020142, "macro_f1": 0.6622222086267096}, {"mcc": 0.4121015656858112, "macro_f1": 0.6498782275309476}, {"mcc": 0.412668341354275, "macro_f1": 0.6777228339085011}, {"mcc": 0.3775988599367876, "macro_f1": 0.6600105776576365}, {"mcc": 0.4105248533137184, "macro_f1": 0.6455972943436004}, {"mcc": 0.46708868051615643, "macro_f1": 0.717290155987365}, {"mcc": 0.5425825571644558, "macro_f1": 0.7643531368792751}, {"mcc": 0.38628620916468903, "macro_f1": 0.6275314539460405}, {"mcc": 0.48825463872632935, "macro_f1": 0.7195041377182123}, {"mcc": 0.36829724590493185, "macro_f1": 0.6034889749901149}], "total": {"test_mcc": 42.846424359691696, "test_mcc_se": 3.394834362149432, "test_macro_f1": 67.27599001588402, "test_macro_f1_se": 2.9911651292170607}}, "num_model_parameters": 70553706496, "max_sequence_length": 8192, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-it", "task": "linguistic-acceptability", "dataset_languages": ["it"], "model": "meta-llama/Meta-Llama-3-70B", "results": {"raw": [{"mcc": 0.3936258333955284, "macro_f1": 0.6967755363913559}, {"mcc": 0.4247793113213664, "macro_f1": 0.6992385010022271}, {"mcc": 0.2745638930681716, "macro_f1": 0.6303873397922581}, {"mcc": 0.27086517361538676, "macro_f1": 0.5911345232424556}, {"mcc": 0.21958785110474566, "macro_f1": 0.5356441181723773}, {"mcc": 0.24472494869636702, "macro_f1": 0.5861195329519373}, {"mcc": 0.24055181583815663, "macro_f1": 0.5596508667566195}, {"mcc": 0.4175601807973233, "macro_f1": 0.7084960242500363}, {"mcc": 0.3003808289414037, "macro_f1": 0.5984440321373952}, {"mcc": 0.20671031303227966, "macro_f1": 0.5567811707308088}], "total": {"test_mcc": 29.933501498107297, "test_mcc_se": 5.119654534682767, "test_macro_f1": 61.62671645427472, "test_macro_f1_se": 3.9833857185945356}}, "num_model_parameters": 70553706496, "max_sequence_length": 8192, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "squad-it", "task": "reading-comprehension", "dataset_languages": ["it"], "model": "meta-llama/Meta-Llama-3-70B", "results": {"raw": [{"f1": 77.87383037248243, "em": 61.515151515151516}, {"f1": 76.66188579510683, "em": 55.799848369977255}, {"f1": 73.02002599479616, "em": 56.64335664335665}, {"f1": 78.12749714636696, "em": 61.03500761035008}, {"f1": 73.07535284422572, "em": 46.106399383191984}, {"f1": 76.36536857120404, "em": 58.47328244274809}, {"f1": 78.06554362454315, "em": 60.9984399375975}, {"f1": 73.67870381514004, "em": 47.9559748427673}, {"f1": 66.54946662671458, "em": 52.476780185758514}, {"f1": 72.6879699218085, "em": 47.0679012345679}], "total": {"test_f1": 74.61056447123885, "test_f1_se": 2.2316338210613926, "test_em": 54.80721421654668, "test_em_se": 3.741047680988497}}, "num_model_parameters": 70553706496, "max_sequence_length": 8192, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fquad", "task": "reading-comprehension", "dataset_languages": ["fr"], "model": "meta-llama/Meta-Llama-3-70B", "results": {"raw": [{"f1": 76.46572660878157, "em": 45.378787878787875}, {"f1": 76.23701929541907, "em": 48.9764973464746}, {"f1": 77.70439177572338, "em": 47.55244755244755}, {"f1": 76.98822204744565, "em": 46.57534246575342}, {"f1": 71.04985463505675, "em": 42.713955281418656}, {"f1": 78.20845812062154, "em": 47.25190839694657}, {"f1": 76.55526882547922, "em": 48.75195007800312}, {"f1": 76.19608548564948, "em": 48.899371069182386}, {"f1": 73.2235030476811, "em": 45.6656346749226}, {"f1": 78.26051066361693, "em": 48.91975308641975}], "total": {"test_f1": 76.08890405054747, "test_f1_se": 1.4106920241204541, "test_em": 47.068564783035654, "test_em_se": 1.266016901891841}}, "num_model_parameters": 70553706496, "max_sequence_length": 8192, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "orange-sum", "task": "summarization", "dataset_languages": ["fr"], "model": "meta-llama/Meta-Llama-3-70B", "results": {"raw": [{"bertscore": 0.6805888873059303, "rouge_l": 0.235402930426448}, {"bertscore": 0.6821291160013061, "rouge_l": 0.2412301382720474}, {"bertscore": 0.6787135395570658, "rouge_l": 0.240174112814772}, {"bertscore": 0.6530235866084695, "rouge_l": 0.19985061342690774}, {"bertscore": 0.6628455453610513, "rouge_l": 0.2154359743241051}, {"bertscore": 0.682567476935219, "rouge_l": 0.23905512941743706}, {"bertscore": 0.6757267358480021, "rouge_l": 0.21761057167452835}, {"bertscore": 0.6591470019193366, "rouge_l": 0.21147655670520615}, {"bertscore": 0.6840513586939778, "rouge_l": 0.24530001544861177}, {"bertscore": 0.6709743713727221, "rouge_l": 0.21163163597298779}], "total": {"test_bertscore": 67.2976761960308, "test_bertscore_se": 0.6829924618501787, "test_rouge_l": 22.57167678483051, "test_rouge_l_se": 1.0008065158461918}}, "num_model_parameters": 70553706496, "max_sequence_length": 8192, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "ilpost-sum", "task": "summarization", "dataset_languages": ["it"], "model": "meta-llama/Meta-Llama-3-70B", "results": {"raw": [{"bertscore": 0.6836091046716319, "rouge_l": 0.26127747552259484}, {"bertscore": 0.6672423540730961, "rouge_l": 0.22436696164269015}, {"bertscore": 0.6870472721348051, "rouge_l": 0.26695775161884605}, {"bertscore": 0.6859823531267466, "rouge_l": 0.2617758928510262}, {"bertscore": 0.6535926105716499, "rouge_l": 0.19112630016136362}, {"bertscore": 0.6787089932477102, "rouge_l": 0.25015900285634246}, {"bertscore": 0.68441892194096, "rouge_l": 0.2624292232655089}, {"bertscore": 0.681106130592525, "rouge_l": 0.2586088436921568}, {"bertscore": 0.6766591935156612, "rouge_l": 0.23319586624785277}, {"bertscore": 0.6837513939390192, "rouge_l": 0.25432985365846006}], "total": {"test_bertscore": 67.82118327813805, "test_bertscore_se": 0.6448706453377386, "test_rouge_l": 24.642271715168423, "test_rouge_l_se": 1.4723013749847103}}, "num_model_parameters": 70553706496, "max_sequence_length": 8192, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-fr", "task": "knowledge", "dataset_languages": ["fr"], "model": "meta-llama/Meta-Llama-3-70B", "results": {"raw": [{"mcc": 0.6390316376479408, "accuracy": 0.728515625}, {"mcc": 0.6377219025977533, "accuracy": 0.72705078125}, {"mcc": 0.6409932628138177, "accuracy": 0.7294921875}, {"mcc": 0.6141313928084725, "accuracy": 0.708984375}, {"mcc": 0.6093561091666703, "accuracy": 0.7060546875}, {"mcc": 0.6453680812383007, "accuracy": 0.732421875}, {"mcc": 0.6420987279006806, "accuracy": 0.73046875}, {"mcc": 0.6235577220400947, "accuracy": 0.71728515625}, {"mcc": 0.65066353579189, "accuracy": 0.73779296875}, {"mcc": 0.640012509003316, "accuracy": 0.7275390625}], "total": {"test_mcc": 63.42934881008937, "test_mcc_se": 0.8535906235082975, "test_accuracy": 72.4560546875, "test_accuracy_se": 0.6417503302391308}}, "num_model_parameters": 70553706496, "max_sequence_length": 8192, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-it", "task": "knowledge", "dataset_languages": ["it"], "model": "meta-llama/Meta-Llama-3-70B", "results": {"raw": [{"mcc": 0.642356305057042, "accuracy": 0.73046875}, {"mcc": 0.6467431471372929, "accuracy": 0.73388671875}, {"mcc": 0.6341722897526717, "accuracy": 0.724609375}, {"mcc": 0.6365510256718089, "accuracy": 0.72607421875}, {"mcc": 0.5994082730441374, "accuracy": 0.69970703125}, {"mcc": 0.626788668929689, "accuracy": 0.71875}, {"mcc": 0.6190710494705784, "accuracy": 0.71337890625}, {"mcc": 0.6263386846485524, "accuracy": 0.71923828125}, {"mcc": 0.6146560868054565, "accuracy": 0.7099609375}, {"mcc": 0.6134240433920127, "accuracy": 0.7099609375}], "total": {"test_mcc": 62.59509573909241, "test_mcc_se": 0.9049549529559442, "test_accuracy": 71.8603515625, "test_accuracy_se": 0.6534731969534451}}, "num_model_parameters": 70553706496, "max_sequence_length": 8192, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-fr", "task": "common-sense-reasoning", "dataset_languages": ["fr"], "model": "meta-llama/Meta-Llama-3-70B", "results": {"raw": [{"mcc": 0.5006736080429384, "accuracy": 0.5986328125}, {"mcc": 0.6152447663475165, "accuracy": 0.7021484375}, {"mcc": 0.6635694009395193, "accuracy": 0.74658203125}, {"mcc": 0.5258817393899925, "accuracy": 0.63623046875}, {"mcc": 0.5900806569395325, "accuracy": 0.67529296875}, {"mcc": 0.5913515032384373, "accuracy": 0.68115234375}, {"mcc": 0.5522137273352147, "accuracy": 0.65771484375}, {"mcc": 0.5237324007078892, "accuracy": 0.6259765625}, {"mcc": 0.6180934468668058, "accuracy": 0.70361328125}, {"mcc": 0.5763424526802278, "accuracy": 0.6708984375}], "total": {"test_mcc": 57.57183702488074, "test_mcc_se": 3.1262707723883993, "test_accuracy": 66.982421875, "test_accuracy_se": 2.654151329583091}}, "num_model_parameters": 70553706496, "max_sequence_length": 8192, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-it", "task": "common-sense-reasoning", "dataset_languages": ["it"], "model": "meta-llama/Meta-Llama-3-70B", "results": {"raw": [{"mcc": 0.5894991711526065, "accuracy": 0.685546875}, {"mcc": 0.6332496573006723, "accuracy": 0.72119140625}, {"mcc": 0.5922174894606865, "accuracy": 0.6884765625}, {"mcc": 0.6227368076553446, "accuracy": 0.71240234375}, {"mcc": 0.647257631980088, "accuracy": 0.72802734375}, {"mcc": 0.5327624153491458, "accuracy": 0.64013671875}, {"mcc": 0.6044014122942064, "accuracy": 0.6962890625}, {"mcc": 0.5617386046399709, "accuracy": 0.66064453125}, {"mcc": 0.6735540282546668, "accuracy": 0.75439453125}, {"mcc": 0.6456630228044934, "accuracy": 0.73095703125}], "total": {"test_mcc": 61.0308024089188, "test_mcc_se": 2.6509215162581388, "test_accuracy": 70.1806640625, "test_accuracy_se": 2.1431731401600786}}, "num_model_parameters": 70553706496, "max_sequence_length": 8192, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "meta-llama/Meta-Llama-3-70B", "results": {"raw": [{"test_speed": 234.29999999999998, "test_speed_short": 26.48}, {"test_speed": 456.84000000000003, "test_speed_short": 50.099999999999994}, {"test_speed": 698.41, "test_speed_short": 97.15}, {"test_speed": 932.92, "test_speed_short": 120.96}, {"test_speed": 1130.22, "test_speed_short": 143.62}, {"test_speed": 1372.4599999999998, "test_speed_short": 190.95000000000002}, {"test_speed": 1571.2, "test_speed_short": 217.6}, {"test_speed": 1817.64, "test_speed_short": 235.01}, {"test_speed": 2019.2, "test_speed_short": 255.83999999999997}, {"test_speed": 2152.0699999999997, "test_speed_short": 276.25}], "total": {"test_speed": 1238.526, "test_speed_se": 409.27833120270145, "test_speed_short": 161.39600000000002, "test_speed_short_se": 53.966358186878686}}, "num_model_parameters": 70553706496, "max_sequence_length": 8192, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "allocine", "task": "sentiment-classification", "dataset_languages": ["fr"], "model": "CohereForAI/c4ai-command-r-v01", "results": {"raw": [{"mcc": 0.9157818925916902, "macro_f1": 0.9565364921826396}, {"mcc": 0.9180897025850727, "macro_f1": 0.9584958463659665}, {"mcc": 0.9218703649893203, "macro_f1": 0.9598039215686274}, {"mcc": 0.9359665479958722, "macro_f1": 0.9672851484501716}, {"mcc": 0.9074271633249587, "macro_f1": 0.9526307443770827}, {"mcc": 0.9222782408615512, "macro_f1": 0.9604458143565614}, {"mcc": 0.921917812664714, "macro_f1": 0.959960899315738}, {"mcc": 0.9189258295856969, "macro_f1": 0.9589787415874372}, {"mcc": 0.934143643329881, "macro_f1": 0.9667008113425362}, {"mcc": 0.9358369513270431, "macro_f1": 0.9672175561530991}], "total": {"test_mcc": 92.32238149255801, "test_mcc_se": 0.5824901355763489, "test_macro_f1": 96.08055975699858, "test_macro_f1_se": 0.30182549816658333}}, "num_model_parameters": 34980831232, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "sentipolc16", "task": "sentiment-classification", "dataset_languages": ["it"], "model": "CohereForAI/c4ai-command-r-v01", "results": {"raw": [{"mcc": 0.5727895878236514, "macro_f1": 0.6968596699747435}, {"mcc": 0.5156634425104549, "macro_f1": 0.6473581638595336}, {"mcc": 0.5526954018300891, "macro_f1": 0.6533218827955669}, {"mcc": 0.5399135418642895, "macro_f1": 0.6696183833249112}, {"mcc": 0.5216465933801391, "macro_f1": 0.6642270151078231}, {"mcc": 0.5681369568318695, "macro_f1": 0.6651361126870711}, {"mcc": 0.5012693851807382, "macro_f1": 0.6247073822651663}, {"mcc": 0.5974670507760569, "macro_f1": 0.7141037547551813}, {"mcc": 0.5214553867851467, "macro_f1": 0.6225008809523622}, {"mcc": 0.5356949536813522, "macro_f1": 0.6708570797506302}], "total": {"test_mcc": 54.26732300663788, "test_mcc_se": 1.8537449602007583, "test_macro_f1": 66.28690325472989, "test_macro_f1_se": 1.7669424974682688}}, "num_model_parameters": 34980831232, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "eltec", "task": "named-entity-recognition", "dataset_languages": ["fr"], "model": "CohereForAI/c4ai-command-r-v01", "results": {"raw": [{"micro_f1_no_misc": 0.5485005170630817, "micro_f1": 0.4832432432432432}, {"micro_f1_no_misc": 0.5355715427657873, "micro_f1": 0.45948232109626125}, {"micro_f1_no_misc": 0.5362076182503138, "micro_f1": 0.4619324001365654}, {"micro_f1_no_misc": 0.5953248981342484, "micro_f1": 0.5007985803016859}, {"micro_f1_no_misc": 0.6034878384580082, "micro_f1": 0.54715108199325}, {"micro_f1_no_misc": 0.5508653440270156, "micro_f1": 0.4590107821324662}, {"micro_f1_no_misc": 0.569919729615547, "micro_f1": 0.4973204715969988}, {"micro_f1_no_misc": 0.5787592008412198, "micro_f1": 0.49809402795425667}, {"micro_f1_no_misc": 0.5273611674098095, "micro_f1": 0.44682661221711756}, {"micro_f1_no_misc": 0.5488689714041827, "micro_f1": 0.432658524343316}], "total": {"test_micro_f1_no_misc": 55.94866827969215, "test_micro_f1_no_misc_se": 1.620795361336183, "test_micro_f1": 47.86518045015161, "test_micro_f1_se": 2.07542449457083}}, "num_model_parameters": 34980831232, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "multinerd-it", "task": "named-entity-recognition", "dataset_languages": ["it"], "model": "CohereForAI/c4ai-command-r-v01", "results": {"raw": [{"micro_f1_no_misc": 0.6988099619678567, "micro_f1": 0.4665720369056068}, {"micro_f1_no_misc": 0.7172719841177156, "micro_f1": 0.5501549625822058}, {"micro_f1_no_misc": 0.6381601756616202, "micro_f1": 0.4672470386756101}, {"micro_f1_no_misc": 0.6644043146285298, "micro_f1": 0.5135626729932781}, {"micro_f1_no_misc": 0.7210493441599001, "micro_f1": 0.571147379644058}, {"micro_f1_no_misc": 0.6841069568237064, "micro_f1": 0.5300022990267452}, {"micro_f1_no_misc": 0.6954403063000348, "micro_f1": 0.4853850477796514}, {"micro_f1_no_misc": 0.7095516569200782, "micro_f1": 0.5694733194733195}, {"micro_f1_no_misc": 0.7009105766985757, "micro_f1": 0.48808747918024475}, {"micro_f1_no_misc": 0.70376271601912, "micro_f1": 0.4919462095463277}], "total": {"test_micro_f1_no_misc": 69.33467993297137, "test_micro_f1_no_misc_se": 1.569493703445452, "test_micro_f1": 51.33578445807048, "test_micro_f1_se": 2.4718398192080304}}, "num_model_parameters": 34980831232, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-fr", "task": "linguistic-acceptability", "dataset_languages": ["fr"], "model": "CohereForAI/c4ai-command-r-v01", "results": {"raw": [{"mcc": 0.28238099744816103, "macro_f1": 0.6349088147693032}, {"mcc": 0.27627935093451284, "macro_f1": 0.630403899238863}, {"mcc": 0.3115755855660189, "macro_f1": 0.6440314050987033}, {"mcc": 0.2352362502515704, "macro_f1": 0.6166991273639506}, {"mcc": 0.33652018131946526, "macro_f1": 0.65439386948663}, {"mcc": 0.29545116307900093, "macro_f1": 0.6417758870471493}, {"mcc": 0.27232606215635885, "macro_f1": 0.6042303503039247}, {"mcc": 0.29348255977392884, "macro_f1": 0.6420511759780853}, {"mcc": 0.32332882253723333, "macro_f1": 0.6616074589873717}, {"mcc": 0.3087440606263901, "macro_f1": 0.6352603661026648}], "total": {"test_mcc": 29.353250336926408, "test_mcc_se": 1.7971515836303846, "test_macro_f1": 63.65362354376647, "test_macro_f1_se": 1.0413588989473925}}, "num_model_parameters": 34980831232, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-it", "task": "linguistic-acceptability", "dataset_languages": ["it"], "model": "CohereForAI/c4ai-command-r-v01", "results": {"raw": [{"mcc": 0.16574795258459035, "macro_f1": 0.5826730995910225}, {"mcc": 0.21752131356905638, "macro_f1": 0.6052811942766234}, {"mcc": 0.19419898609445935, "macro_f1": 0.5784115199115372}, {"mcc": 0.1883162742575789, "macro_f1": 0.581817029039225}, {"mcc": 0.13788840062244528, "macro_f1": 0.5586550435865505}, {"mcc": 0.16656003917109302, "macro_f1": 0.5757899258967178}, {"mcc": 0.2220151838717295, "macro_f1": 0.6087954280031223}, {"mcc": 0.21096867346087242, "macro_f1": 0.6044359080220432}, {"mcc": 0.1927907976606629, "macro_f1": 0.568232677802288}, {"mcc": 0.19743270351649203, "macro_f1": 0.5835340615452593}], "total": {"test_mcc": 18.9344032480898, "test_mcc_se": 1.6208052397789177, "test_macro_f1": 58.476258876743906, "test_macro_f1_se": 1.0280129345549647}}, "num_model_parameters": 34980831232, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "squad-it", "task": "reading-comprehension", "dataset_languages": ["it"], "model": "CohereForAI/c4ai-command-r-v01", "results": {"raw": [{"f1": 64.43562694133357, "em": 42.121212121212125}, {"f1": 63.268064092634276, "em": 36.770280515542076}, {"f1": 66.23481318579444, "em": 46.85314685314685}, {"f1": 65.62445529434146, "em": 42.23744292237443}, {"f1": 63.96926231418031, "em": 39.01310717039321}, {"f1": 64.040413965041, "em": 41.603053435114504}, {"f1": 64.47850903237193, "em": 40.24960998439938}, {"f1": 65.21996608496828, "em": 40.801886792452834}, {"f1": 67.48300797088747, "em": 47.29102167182663}, {"f1": 61.983886565793405, "em": 36.111111111111114}], "total": {"test_f1": 64.67380054473462, "test_f1_se": 0.9631870312819742, "test_em": 41.30518725775731, "test_em_se": 2.285721777665527}}, "num_model_parameters": 34980831232, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fquad", "task": "reading-comprehension", "dataset_languages": ["fr"], "model": "CohereForAI/c4ai-command-r-v01", "results": {"raw": [{"f1": 64.91194932656259, "em": 32.196969696969695}, {"f1": 62.71061729138031, "em": 31.2357846853677}, {"f1": 63.93261019123643, "em": 29.836829836829835}, {"f1": 61.09054504095943, "em": 25.951293759512936}, {"f1": 61.79480841624698, "em": 32.92212798766384}, {"f1": 63.72752548420903, "em": 30.076335877862597}, {"f1": 66.26200518711563, "em": 36.661466458658346}, {"f1": 64.83386840435898, "em": 35.14150943396226}, {"f1": 62.91195996295727, "em": 34.28792569659443}, {"f1": 61.29287870919797, "em": 27.237654320987655}], "total": {"test_f1": 63.34687680142247, "test_f1_se": 1.0526451607166885, "test_em": 31.55478977544093, "test_em_se": 2.106638337469155}}, "num_model_parameters": 34980831232, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "orange-sum", "task": "summarization", "dataset_languages": ["fr"], "model": "CohereForAI/c4ai-command-r-v01", "results": {"raw": [{"bertscore": 0.6692124943365343, "rouge_l": 0.19930239128845195}, {"bertscore": 0.6666338128852658, "rouge_l": 0.18114922352469354}, {"bertscore": 0.666421833448112, "rouge_l": 0.18725050541014093}, {"bertscore": 0.6645257015770767, "rouge_l": 0.18795106541359297}, {"bertscore": 0.665732088527875, "rouge_l": 0.19139199515380004}, {"bertscore": 0.6694167630048469, "rouge_l": 0.1896852785379794}, {"bertscore": 0.6658613640465774, "rouge_l": 0.18541817901178476}, {"bertscore": 0.6635555969551206, "rouge_l": 0.1811492194237564}, {"bertscore": 0.6717010359279811, "rouge_l": 0.19323711658414597}, {"bertscore": 0.6662360595364589, "rouge_l": 0.17462583260619763}], "total": {"test_bertscore": 66.69296750245849, "test_bertscore_se": 0.15284130930908912, "test_rouge_l": 18.711608069545434, "test_rouge_l_se": 0.43419332185247395}}, "num_model_parameters": 34980831232, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "ilpost-sum", "task": "summarization", "dataset_languages": ["it"], "model": "CohereForAI/c4ai-command-r-v01", "results": {"raw": [{"bertscore": 0.6632361740339547, "rouge_l": 0.1956085043538307}, {"bertscore": 0.6576563925482333, "rouge_l": 0.18629371456335486}, {"bertscore": 0.6641441164247226, "rouge_l": 0.20112789738906048}, {"bertscore": 0.6597985242697177, "rouge_l": 0.18872433466317606}, {"bertscore": 0.6557286671741167, "rouge_l": 0.17203683792535793}, {"bertscore": 0.6605606417870149, "rouge_l": 0.1908977299010931}, {"bertscore": 0.6621399373689201, "rouge_l": 0.19341400837846223}, {"bertscore": 0.6645103930495679, "rouge_l": 0.20031414724413454}, {"bertscore": 0.6646798647561809, "rouge_l": 0.1983056712880954}, {"bertscore": 0.6640223776048515, "rouge_l": 0.19563976931552024}], "total": {"test_bertscore": 66.1647708901728, "test_bertscore_se": 0.1933606219521777, "test_rouge_l": 19.223626150220856, "test_rouge_l_se": 0.5319553826069068}}, "num_model_parameters": 34980831232, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-fr", "task": "knowledge", "dataset_languages": ["fr"], "model": "CohereForAI/c4ai-command-r-v01", "results": {"raw": [{"mcc": 0.4422626712169452, "accuracy": 0.578125}, {"mcc": 0.4512123600606908, "accuracy": 0.587890625}, {"mcc": 0.40673734486091273, "accuracy": 0.55029296875}, {"mcc": 0.4471593272323532, "accuracy": 0.58203125}, {"mcc": 0.4311336653083763, "accuracy": 0.568359375}, {"mcc": 0.4441545054031431, "accuracy": 0.5791015625}, {"mcc": 0.4233265296568955, "accuracy": 0.5634765625}, {"mcc": 0.4171155707809235, "accuracy": 0.5595703125}, {"mcc": 0.44977895685628216, "accuracy": 0.5849609375}, {"mcc": 0.4612525197577882, "accuracy": 0.58984375}], "total": {"test_mcc": 43.74133451134311, "test_mcc_se": 1.066847557147891, "test_accuracy": 57.4365234375, "test_accuracy_se": 0.8220385306166698}}, "num_model_parameters": 34980831232, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-it", "task": "knowledge", "dataset_languages": ["it"], "model": "CohereForAI/c4ai-command-r-v01", "results": {"raw": [{"mcc": 0.4709641639127738, "accuracy": 0.60205078125}, {"mcc": 0.4222137667504461, "accuracy": 0.564453125}, {"mcc": 0.41358199596217704, "accuracy": 0.55859375}, {"mcc": 0.44187574429282717, "accuracy": 0.5810546875}, {"mcc": 0.4322820096533369, "accuracy": 0.57080078125}, {"mcc": 0.4517432887502547, "accuracy": 0.58740234375}, {"mcc": 0.41579597858455236, "accuracy": 0.55908203125}, {"mcc": 0.41503465739380463, "accuracy": 0.55810546875}, {"mcc": 0.42730057131819493, "accuracy": 0.56884765625}, {"mcc": 0.4235309589129774, "accuracy": 0.5673828125}], "total": {"test_mcc": 43.143231355313446, "test_mcc_se": 1.1468905897543382, "test_accuracy": 57.177734375, "test_accuracy_se": 0.8888891961838931}}, "num_model_parameters": 34980831232, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-fr", "task": "common-sense-reasoning", "dataset_languages": ["fr"], "model": "CohereForAI/c4ai-command-r-v01", "results": {"raw": [{"mcc": 0.44330066693709724, "accuracy": 0.5791015625}, {"mcc": 0.41185625047637764, "accuracy": 0.5546875}, {"mcc": 0.4549361468429659, "accuracy": 0.591796875}, {"mcc": 0.4632422884157874, "accuracy": 0.59228515625}, {"mcc": 0.46744870718120235, "accuracy": 0.5966796875}, {"mcc": 0.4399914303327398, "accuracy": 0.57763671875}, {"mcc": 0.4547541336235615, "accuracy": 0.5888671875}, {"mcc": 0.45724806396524176, "accuracy": 0.58837890625}, {"mcc": 0.47155452381576785, "accuracy": 0.59619140625}, {"mcc": 0.4141308643621677, "accuracy": 0.55419921875}], "total": {"test_mcc": 44.784630759529094, "test_mcc_se": 1.2876011589130578, "test_accuracy": 58.19824218750001, "test_accuracy_se": 0.9806081326094815}}, "num_model_parameters": 34980831232, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-it", "task": "common-sense-reasoning", "dataset_languages": ["it"], "model": "CohereForAI/c4ai-command-r-v01", "results": {"raw": [{"mcc": 0.4326414099262081, "accuracy": 0.5732421875}, {"mcc": 0.46808006659282264, "accuracy": 0.59814453125}, {"mcc": 0.4461641269647352, "accuracy": 0.58251953125}, {"mcc": 0.45316297741407785, "accuracy": 0.587890625}, {"mcc": 0.47639805327588824, "accuracy": 0.60791015625}, {"mcc": 0.4675338298297135, "accuracy": 0.60107421875}, {"mcc": 0.5093020785648417, "accuracy": 0.63037109375}, {"mcc": 0.501604389963137, "accuracy": 0.6259765625}, {"mcc": 0.4996700169806048, "accuracy": 0.6240234375}, {"mcc": 0.42938261101972985, "accuracy": 0.56640625}], "total": {"test_mcc": 46.839395605317584, "test_mcc_se": 1.7707359416953832, "test_accuracy": 59.9755859375, "test_accuracy_se": 1.3917083079186159}}, "num_model_parameters": 34980831232, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "CohereForAI/c4ai-command-r-v01", "results": {"raw": [{"test_speed": 386.95, "test_speed_short": 43.92}, {"test_speed": 759.99, "test_speed_short": 82.8}, {"test_speed": 1143.62, "test_speed_short": 161.53}, {"test_speed": 1511.78, "test_speed_short": 199.79999999999998}, {"test_speed": 1895.4, "test_speed_short": 239.07999999999998}, {"test_speed": 2264.98, "test_speed_short": 312.93}, {"test_speed": 2661.22, "test_speed_short": 354.56}, {"test_speed": 3001.35, "test_speed_short": 390.5}, {"test_speed": 3319.06, "test_speed_short": 429.78}, {"test_speed": 3694.2699999999995, "test_speed_short": 471.75}], "total": {"test_speed": 2063.862, "test_speed_se": 691.486481873642, "test_speed_short": 268.66499999999996, "test_speed_short_se": 90.88952770780826}}, "num_model_parameters": 34980831232, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hotter-and-colder-sentiment", "task": "sentiment-classification", "dataset_languages": ["is"], "model": "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF", "results": {"raw": [{"mcc": 0.5699978651154732, "macro_f1": 0.6838632184039115}, {"mcc": 0.5093939530673162, "macro_f1": 0.647620904824984}, {"mcc": 0.5249838788761497, "macro_f1": 0.6621473737759168}, {"mcc": 0.5000417482486385, "macro_f1": 0.6393549317207526}, {"mcc": 0.5147913620148575, "macro_f1": 0.6405672863040187}, {"mcc": 0.5217904567277292, "macro_f1": 0.6463993167145773}, {"mcc": 0.5262750543530257, "macro_f1": 0.6641109198449069}, {"mcc": 0.5119829223680648, "macro_f1": 0.6591463479600794}, {"mcc": 0.5144566500135609, "macro_f1": 0.6330527051587734}, {"mcc": 0.5270929422021319, "macro_f1": 0.6681625349113222}], "total": {"test_mcc": 52.20806832986947, "test_mcc_se": 1.1700281108351467, "test_macro_f1": 65.44425539619243, "test_macro_f1_se": 0.9729390754869551}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "allocine", "task": "sentiment-classification", "dataset_languages": ["fr"], "model": "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF", "results": {"raw": [{"mcc": 0.9474572303038945, "macro_f1": 0.973594510317032}, {"mcc": 0.9395507340262337, "macro_f1": 0.9697161364796345}, {"mcc": 0.954743757369809, "macro_f1": 0.977348183199646}, {"mcc": 0.9423512333021778, "macro_f1": 0.9711735301857025}, {"mcc": 0.9454735517935964, "macro_f1": 0.972624268273005}, {"mcc": 0.9423176585641073, "macro_f1": 0.9711567404576891}, {"mcc": 0.9467178456345025, "macro_f1": 0.9731357628963959}, {"mcc": 0.9377348250123663, "macro_f1": 0.9687342266138828}, {"mcc": 0.9491587245382448, "macro_f1": 0.974503457125893}, {"mcc": 0.9513146967499335, "macro_f1": 0.9754989926831953}], "total": {"test_mcc": 94.56820257294866, "test_mcc_se": 0.3288551987318877, "test_macro_f1": 97.27485808232078, "test_macro_f1_se": 0.1639946798383205}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "sentipolc16", "task": "sentiment-classification", "dataset_languages": ["it"], "model": "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF", "results": {"raw": [{"mcc": 0.4812290880404801, "macro_f1": 0.49450748721435955}, {"mcc": 0.49784679289000333, "macro_f1": 0.49824457301092817}, {"mcc": 0.4940292127862506, "macro_f1": 0.4965621301240524}, {"mcc": 0.48327407425832325, "macro_f1": 0.4892566028921417}, {"mcc": 0.5050454062822679, "macro_f1": 0.4992974014890674}, {"mcc": 0.48856116311433656, "macro_f1": 0.49401424002384736}, {"mcc": 0.4805321968681849, "macro_f1": 0.49108774823060536}, {"mcc": 0.48864532329224536, "macro_f1": 0.4920987445814333}, {"mcc": 0.49751910578312636, "macro_f1": 0.4969278033794163}, {"mcc": 0.4745295654781737, "macro_f1": 0.4817396481610461}], "total": {"test_mcc": 48.912119287933926, "test_mcc_se": 0.5872848588973001, "test_macro_f1": 49.33736379106898, "test_macro_f1_se": 0.32170406874842156}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mim-gold-ner", "task": "named-entity-recognition", "dataset_languages": ["is"], "model": "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF", "results": {"raw": [{"micro_f1_no_misc": 0.5760657019945248, "micro_f1": 0.25267379679144386}, {"micro_f1_no_misc": 0.5010274612366897, "micro_f1": 0.20601051840721266}, {"micro_f1_no_misc": 0.5895305359368509, "micro_f1": 0.25414169897779343}, {"micro_f1_no_misc": 0.5708418891170431, "micro_f1": 0.2436118513873648}, {"micro_f1_no_misc": 0.540439106774254, "micro_f1": 0.24841900437814174}, {"micro_f1_no_misc": 0.5021735350373848, "micro_f1": 0.23680124223602486}, {"micro_f1_no_misc": 0.5775666184672589, "micro_f1": 0.2636494252873563}, {"micro_f1_no_misc": 0.5437399678972712, "micro_f1": 0.21578327444051826}, {"micro_f1_no_misc": 0.5831550282046295, "micro_f1": 0.25359625020203647}, {"micro_f1_no_misc": 0.5843409316154609, "micro_f1": 0.23888363292336803}], "total": {"test_micro_f1_no_misc": 55.68880776281369, "test_micro_f1_no_misc_se": 2.071877014290828, "test_micro_f1": 24.135706950312603, "test_micro_f1_se": 1.11708304765513}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "eltec", "task": "named-entity-recognition", "dataset_languages": ["fr"], "model": "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF", "results": {"raw": [{"micro_f1_no_misc": 0.49510763209393344, "micro_f1": 0.36699650756693825}, {"micro_f1_no_misc": 0.5058737151248164, "micro_f1": 0.3978842198060535}, {"micro_f1_no_misc": 0.5461697722567288, "micro_f1": 0.39215089747490117}, {"micro_f1_no_misc": 0.6110540373946991, "micro_f1": 0.41122327790973867}, {"micro_f1_no_misc": 0.5839914621131269, "micro_f1": 0.45079157826016}, {"micro_f1_no_misc": 0.5183055395821353, "micro_f1": 0.3665760480755472}, {"micro_f1_no_misc": 0.5357548240635641, "micro_f1": 0.36640592836558256}, {"micro_f1_no_misc": 0.5325245764484416, "micro_f1": 0.36584642751260005}, {"micro_f1_no_misc": 0.5121218192746945, "micro_f1": 0.3442811869778162}, {"micro_f1_no_misc": 0.5267522211253701, "micro_f1": 0.3569627346595685}], "total": {"test_micro_f1_no_misc": 53.6765559947751, "test_micro_f1_no_misc_se": 2.224650629803322, "test_micro_f1": 38.19118806608906, "test_micro_f1_se": 1.9554477621971031}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "multinerd-it", "task": "named-entity-recognition", "dataset_languages": ["it"], "model": "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF", "results": {"raw": [{"micro_f1_no_misc": 0.7863585814901767, "micro_f1": 0.3384463679193601}, {"micro_f1_no_misc": 0.7940537449971413, "micro_f1": 0.4033487297921478}, {"micro_f1_no_misc": 0.7574812247255922, "micro_f1": 0.34336936461903506}, {"micro_f1_no_misc": 0.7664165103189493, "micro_f1": 0.37977790765634134}, {"micro_f1_no_misc": 0.778102189781022, "micro_f1": 0.3769122146387386}, {"micro_f1_no_misc": 0.7449695835283108, "micro_f1": 0.35658571665652483}, {"micro_f1_no_misc": 0.7014839526055446, "micro_f1": 0.3383663234274129}, {"micro_f1_no_misc": 0.7574970484061394, "micro_f1": 0.38233173076923077}, {"micro_f1_no_misc": 0.7471433420070143, "micro_f1": 0.33097075654958785}, {"micro_f1_no_misc": 0.7288270830958623, "micro_f1": 0.3279155672823219}], "total": {"test_micro_f1_no_misc": 75.62333260955752, "test_micro_f1_no_misc_se": 1.713815445231038, "test_micro_f1": 35.78024679310702, "test_micro_f1_se": 1.6135658804710618}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-is", "task": "linguistic-acceptability", "dataset_languages": ["is"], "model": "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF", "results": {"raw": [{"mcc": 0.19067491782954787, "macro_f1": 0.5364315372603881}, {"mcc": 0.17612510715444515, "macro_f1": 0.5736286921383229}, {"mcc": 0.2011477219754397, "macro_f1": 0.5885429638854296}, {"mcc": 0.22984670060564566, "macro_f1": 0.5636672049061343}, {"mcc": 0.22095258212614782, "macro_f1": 0.5867975162092809}, {"mcc": 0.21664787262190785, "macro_f1": 0.6041989425483862}, {"mcc": 0.20452895919484992, "macro_f1": 0.5749989549795287}, {"mcc": 0.1390411161480463, "macro_f1": 0.52426906049465}, {"mcc": 0.2075645489538488, "macro_f1": 0.5731413666602869}, {"mcc": 0.1870038691043225, "macro_f1": 0.5781772311573636}], "total": {"test_mcc": 19.735333957142014, "test_mcc_se": 1.6212468346392575, "test_macro_f1": 57.03853470239771, "test_macro_f1_se": 1.4851285395484592}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-fr", "task": "linguistic-acceptability", "dataset_languages": ["fr"], "model": "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF", "results": {"raw": [{"mcc": 0.42269292344006215, "macro_f1": 0.6601404403743028}, {"mcc": 0.42023402785400876, "macro_f1": 0.6679809055745027}, {"mcc": 0.41650460508143283, "macro_f1": 0.6717505258101779}, {"mcc": 0.43786526515237384, "macro_f1": 0.6724855325457937}, {"mcc": 0.42681464165181443, "macro_f1": 0.6575592660161528}, {"mcc": 0.4229084579935035, "macro_f1": 0.6665557267591167}, {"mcc": 0.4271262204294957, "macro_f1": 0.6805566732336463}, {"mcc": 0.38538865466294836, "macro_f1": 0.6559237293281784}, {"mcc": 0.45324746016599105, "macro_f1": 0.6867549874780225}, {"mcc": 0.40807568903178204, "macro_f1": 0.6446914606434878}], "total": {"test_mcc": 42.20857945463413, "test_mcc_se": 1.1022560548049445, "test_macro_f1": 66.64399247763382, "test_macro_f1_se": 0.7678431605667956}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-it", "task": "linguistic-acceptability", "dataset_languages": ["it"], "model": "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF", "results": {"raw": [{"mcc": 0.16186056834539653, "macro_f1": 0.5457306674029918}, {"mcc": 0.20123720881726886, "macro_f1": 0.5500007682733824}, {"mcc": 0.2011148489908209, "macro_f1": 0.5508939569258333}, {"mcc": 0.24785155643269965, "macro_f1": 0.5415303571728153}, {"mcc": 0.17065131437375744, "macro_f1": 0.5276503663074306}, {"mcc": 0.20006364830030543, "macro_f1": 0.534708951400056}, {"mcc": 0.19274451266846054, "macro_f1": 0.5337491880899825}, {"mcc": 0.22620346071208167, "macro_f1": 0.5804722836750709}, {"mcc": 0.22366082861819214, "macro_f1": 0.5491298527443106}, {"mcc": 0.21265045485897957, "macro_f1": 0.5450308625458304}], "total": {"test_mcc": 20.380384021179626, "test_mcc_se": 1.5900247490761479, "test_macro_f1": 54.58897254537703, "test_macro_f1_se": 0.8941931530469627}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "nqii", "task": "reading-comprehension", "dataset_languages": ["is"], "model": "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF", "results": {"raw": [{"f1": 50.848727212967944, "em": 19.692307692307693}, {"f1": 52.54147352948579, "em": 19.314641744548286}, {"f1": 47.80515499317135, "em": 21.02803738317757}, {"f1": 49.16530787391608, "em": 21.076233183856502}, {"f1": 50.24009955712985, "em": 21.473354231974923}, {"f1": 52.399571459402836, "em": 22.80431432973806}, {"f1": 51.86825542290159, "em": 20.46153846153846}, {"f1": 49.47415685319469, "em": 22.392638036809817}, {"f1": 51.01893205152103, "em": 21.08433734939759}, {"f1": 49.1937008300771, "em": 21.686746987951807}], "total": {"test_f1": 50.455537978376825, "test_f1_se": 0.9666055922410355, "test_em": 21.101414940130073, "test_em_se": 0.6739781564173261}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "squad-it", "task": "reading-comprehension", "dataset_languages": ["it"], "model": "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF", "results": {"raw": [{"f1": 70.3503181394998, "em": 45.22727272727273}, {"f1": 70.29653368411354, "em": 42.911296436694464}, {"f1": 71.81347039945068, "em": 49.261849261849264}, {"f1": 69.23153178100452, "em": 44.36834094368341}, {"f1": 67.51120565675923, "em": 40.86353122590594}, {"f1": 69.12703826148109, "em": 43.20610687022901}, {"f1": 71.3837056391236, "em": 47.581903276131044}, {"f1": 68.05699655962334, "em": 40.33018867924528}, {"f1": 71.07810530978922, "em": 47.36842105263158}, {"f1": 68.50189578379778, "em": 41.74382716049383}], "total": {"test_f1": 69.73508012146428, "test_f1_se": 0.9109429816436222, "test_em": 44.286273763413654, "test_em_se": 1.8810612061543832}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fquad", "task": "reading-comprehension", "dataset_languages": ["fr"], "model": "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF", "results": {"raw": [{"f1": 69.8238945823161, "em": 34.92424242424242}, {"f1": 70.05277261211583, "em": 37.300985595147836}, {"f1": 70.55827658984359, "em": 38.38383838383838}, {"f1": 69.57017960775191, "em": 35.083713850837135}, {"f1": 68.2499230324857, "em": 35.92906707787201}, {"f1": 71.06582384217901, "em": 37.786259541984734}, {"f1": 70.4381200087748, "em": 39.93759750390016}, {"f1": 69.88537880772164, "em": 39.15094339622642}, {"f1": 69.09367704547086, "em": 37.538699690402474}, {"f1": 71.43877526098206, "em": 38.117283950617285}], "total": {"test_f1": 70.01768213896415, "test_f1_se": 0.5778371337219134, "test_em": 37.415263141506884, "test_em_se": 1.0293213714685794}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "rrn", "task": "summarization", "dataset_languages": ["is"], "model": "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF", "results": {"raw": [{"bertscore": 0.6589496348169632, "rouge_l": 0.17198670966424678}, {"bertscore": 0.6548548516875599, "rouge_l": 0.1681886857617758}, {"bertscore": 0.658488022570964, "rouge_l": 0.1736208978884754}, {"bertscore": 0.6559945656917989, "rouge_l": 0.17137135866793618}, {"bertscore": 0.656390112824738, "rouge_l": 0.1697011368569638}, {"bertscore": 0.6570119956159033, "rouge_l": 0.17236604031718988}, {"bertscore": 0.654540765506681, "rouge_l": 0.17010554430142444}, {"bertscore": 0.6558969622128643, "rouge_l": 0.16597348753509647}, {"bertscore": 0.6562826497247443, "rouge_l": 0.1734125507695568}, {"bertscore": 0.65225265303161, "rouge_l": 0.1714650259186376}], "total": {"test_bertscore": 65.60662213683827, "test_bertscore_se": 0.11975254038973383, "test_rouge_l": 17.081914376813028, "test_rouge_l_se": 0.14785071539295627}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "orange-sum", "task": "summarization", "dataset_languages": ["fr"], "model": "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF", "results": {"raw": [{"bertscore": 0.6475662053562701, "rouge_l": 0.16445509648042833}, {"bertscore": 0.648724660393782, "rouge_l": 0.1633284143582141}, {"bertscore": 0.645814298914047, "rouge_l": 0.16152588222587763}, {"bertscore": 0.6456185362767428, "rouge_l": 0.1640364954625278}, {"bertscore": 0.6486585723469034, "rouge_l": 0.1646281230281955}, {"bertscore": 0.6454581997531932, "rouge_l": 0.1633525955956717}, {"bertscore": 0.6484967456781305, "rouge_l": 0.16570249346317684}, {"bertscore": 0.6434344163862988, "rouge_l": 0.15968636170004136}, {"bertscore": 0.6476119795406703, "rouge_l": 0.16413230260702902}, {"bertscore": 0.6434064308705274, "rouge_l": 0.15932026086097212}], "total": {"test_bertscore": 64.64790045516565, "test_bertscore_se": 0.12612026082438846, "test_rouge_l": 16.301680257821342, "test_rouge_l_se": 0.13285505915343007}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "ilpost-sum", "task": "summarization", "dataset_languages": ["it"], "model": "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF", "results": {"raw": [{"bertscore": 0.6239088740112493, "rouge_l": 0.15626396224415567}, {"bertscore": 0.6218626036716159, "rouge_l": 0.15136268801298963}, {"bertscore": 0.6234251661226153, "rouge_l": 0.1549403672560189}, {"bertscore": 0.6278035118593834, "rouge_l": 0.16147675294318126}, {"bertscore": 0.6266798468714114, "rouge_l": 0.153593326827775}, {"bertscore": 0.6226798245188547, "rouge_l": 0.15125682213205632}, {"bertscore": 0.6232392340316437, "rouge_l": 0.15571310220608714}, {"bertscore": 0.6235013505793177, "rouge_l": 0.1565935680301252}, {"bertscore": 0.6271299210202415, "rouge_l": 0.16007825525940889}, {"bertscore": 0.6224347808602033, "rouge_l": 0.15330142535468783}], "total": {"test_bertscore": 62.42665113546536, "test_bertscore_se": 0.13174316518536666, "test_rouge_l": 15.54580270266486, "test_rouge_l_se": 0.20906008455450065}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "icelandic-knowledge", "task": "knowledge", "dataset_languages": ["is"], "model": "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF", "results": {"raw": [{"mcc": 0.29535224465791327, "accuracy": 0.4697265625}, {"mcc": 0.31266860678589875, "accuracy": 0.484375}, {"mcc": 0.30367403678299665, "accuracy": 0.4775390625}, {"mcc": 0.28426401767156406, "accuracy": 0.4580078125}, {"mcc": 0.3212339273707042, "accuracy": 0.48828125}, {"mcc": 0.2868489855794123, "accuracy": 0.4619140625}, {"mcc": 0.32726017342082236, "accuracy": 0.4892578125}, {"mcc": 0.3048373778175341, "accuracy": 0.4736328125}, {"mcc": 0.31883658991473635, "accuracy": 0.4833984375}, {"mcc": 0.32799391870165184, "accuracy": 0.4931640625}], "total": {"test_mcc": 30.82969878703234, "test_mcc_se": 0.9872983391407923, "test_accuracy": 47.79296875, "test_accuracy_se": 0.7386727357103217}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-fr", "task": "knowledge", "dataset_languages": ["fr"], "model": "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF", "results": {"raw": [{"mcc": 0.7046070571275515, "accuracy": 0.77783203125}, {"mcc": 0.6837247811043327, "accuracy": 0.7626953125}, {"mcc": 0.6905540673867769, "accuracy": 0.767578125}, {"mcc": 0.6814049656604171, "accuracy": 0.7607421875}, {"mcc": 0.6798429486582088, "accuracy": 0.759765625}, {"mcc": 0.6878820429429423, "accuracy": 0.765625}, {"mcc": 0.6913543727370604, "accuracy": 0.7685546875}, {"mcc": 0.6935354506255027, "accuracy": 0.77001953125}, {"mcc": 0.7051690143680378, "accuracy": 0.77880859375}, {"mcc": 0.670024206965027, "accuracy": 0.75146484375}], "total": {"test_mcc": 68.88098907575856, "test_mcc_se": 0.6738774579370604, "test_accuracy": 76.630859375, "test_accuracy_se": 0.5132400751654719}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-it", "task": "knowledge", "dataset_languages": ["it"], "model": "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF", "results": {"raw": [{"mcc": 0.7090128931611094, "accuracy": 0.78125}, {"mcc": 0.7131283391851334, "accuracy": 0.7841796875}, {"mcc": 0.7074336119626476, "accuracy": 0.7802734375}, {"mcc": 0.6804141559116464, "accuracy": 0.76025390625}, {"mcc": 0.7013863871775776, "accuracy": 0.7763671875}, {"mcc": 0.6921361984404086, "accuracy": 0.7685546875}, {"mcc": 0.6780163684812647, "accuracy": 0.7578125}, {"mcc": 0.685225758011168, "accuracy": 0.76416015625}, {"mcc": 0.676688296436608, "accuracy": 0.75732421875}, {"mcc": 0.6910004893375522, "accuracy": 0.7685546875}], "total": {"test_mcc": 69.34442498105116, "test_mcc_se": 0.839836365200718, "test_accuracy": 76.9873046875, "test_accuracy_se": 0.6253011054469234}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "winogrande-is", "task": "common-sense-reasoning", "dataset_languages": ["is"], "model": "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF", "results": {"raw": [{"mcc": 0.5196727632085449, "accuracy": 0.7633928571428571}, {"mcc": 0.44939973708721165, "accuracy": 0.7232142857142857}, {"mcc": 0.4520427646596451, "accuracy": 0.7299107142857143}, {"mcc": 0.5492199370641818, "accuracy": 0.7790178571428571}, {"mcc": 0.4797803104873651, "accuracy": 0.7388392857142857}, {"mcc": 0.4506449080698896, "accuracy": 0.7209821428571429}, {"mcc": 0.4103501080079205, "accuracy": 0.7064732142857143}, {"mcc": 0.42651754095012034, "accuracy": 0.7142857142857143}, {"mcc": 0.4817645578121871, "accuracy": 0.7477678571428571}, {"mcc": 0.4029909542413597, "accuracy": 0.7008928571428571}], "total": {"test_mcc": 46.22383581588426, "test_mcc_se": 2.8802523683248005, "test_accuracy": 73.24776785714285, "test_accuracy_se": 1.5495606596216938}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-fr", "task": "common-sense-reasoning", "dataset_languages": ["fr"], "model": "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF", "results": {"raw": [{"mcc": 0.7504101310814785, "accuracy": 0.8115234375}, {"mcc": 0.765735037359429, "accuracy": 0.82275390625}, {"mcc": 0.7251219847116811, "accuracy": 0.79345703125}, {"mcc": 0.7116791170922923, "accuracy": 0.7822265625}, {"mcc": 0.7522406347983153, "accuracy": 0.8134765625}, {"mcc": 0.7785378738295771, "accuracy": 0.83349609375}, {"mcc": 0.7358033180187515, "accuracy": 0.80126953125}, {"mcc": 0.7432387212562767, "accuracy": 0.80615234375}, {"mcc": 0.7241405201294077, "accuracy": 0.7919921875}, {"mcc": 0.7687425730484098, "accuracy": 0.8251953125}], "total": {"test_mcc": 74.5564991132562, "test_mcc_se": 1.3429394016586875, "test_accuracy": 80.8154296875, "test_accuracy_se": 1.0092483299303916}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-it", "task": "common-sense-reasoning", "dataset_languages": ["it"], "model": "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF", "results": {"raw": [{"mcc": 0.7248927000764096, "accuracy": 0.78955078125}, {"mcc": 0.7264366889310576, "accuracy": 0.79052734375}, {"mcc": 0.7112878546034038, "accuracy": 0.77978515625}, {"mcc": 0.7170497462190243, "accuracy": 0.7841796875}, {"mcc": 0.732112296790996, "accuracy": 0.796875}, {"mcc": 0.750835499197224, "accuracy": 0.8125}, {"mcc": 0.7472062823615281, "accuracy": 0.80908203125}, {"mcc": 0.747786601214214, "accuracy": 0.80908203125}, {"mcc": 0.7378136943042515, "accuracy": 0.80126953125}, {"mcc": 0.6986082151656933, "accuracy": 0.771484375}], "total": {"test_mcc": 72.94029578863802, "test_mcc_se": 1.0636635258900986, "test_accuracy": 79.443359375, "test_accuracy_se": 0.850268990832215}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF", "results": {"raw": [{"test_speed": 252.76, "test_speed_short": 29.12}, {"test_speed": 506.19, "test_speed_short": 54.75}, {"test_speed": 751.16, "test_speed_short": 105.27}, {"test_speed": 997.55, "test_speed_short": 126.72}, {"test_speed": 1246.05, "test_speed_short": 156.09}, {"test_speed": 1494.55, "test_speed_short": 206.91}, {"test_speed": 1747.96, "test_speed_short": 232.96}, {"test_speed": 1952.28, "test_speed_short": 259.86}, {"test_speed": 2170.64, "test_speed_short": 283.92}, {"test_speed": 2341.3399999999997, "test_speed_short": 310.25}], "total": {"test_speed": 1346.048, "test_speed_se": 443.48023008468334, "test_speed_short": 176.585, "test_speed_short_se": 60.234956715589064}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "swerec", "task": "sentiment-classification", "dataset_languages": ["sv"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"mcc": 0.7427745119783682, "macro_f1": 0.6583753327505003}, {"mcc": 0.7716213418523322, "macro_f1": 0.7533876276261124}, {"mcc": 0.7741533865725811, "macro_f1": 0.7377824951065431}, {"mcc": 0.7488848840473707, "macro_f1": 0.727892123940463}, {"mcc": 0.7723522921625077, "macro_f1": 0.7771742561536952}, {"mcc": 0.7905556287096935, "macro_f1": 0.7690918772610962}, {"mcc": 0.755524598475934, "macro_f1": 0.6942901183376793}, {"mcc": 0.758235581779634, "macro_f1": 0.7324482115175224}, {"mcc": 0.782397944517306, "macro_f1": 0.7649627407676975}, {"mcc": 0.7289206997839143, "macro_f1": 0.6848447098786669}], "total": {"test_mcc": 76.2542086987964, "test_mcc_se": 1.1810658327475116, "test_macro_f1": 73.00249493339976, "test_macro_f1_se": 2.4478863300963343}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "angry-tweets", "task": "sentiment-classification", "dataset_languages": ["da"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"mcc": 0.5142026252159755, "macro_f1": 0.6143380411970801}, {"mcc": 0.48619097078814416, "macro_f1": 0.572847951728814}, {"mcc": 0.4837564138319009, "macro_f1": 0.573934154219934}, {"mcc": 0.4876671663011234, "macro_f1": 0.5850651124610932}, {"mcc": 0.49623939223399227, "macro_f1": 0.6101744771376295}, {"mcc": 0.49082410670221427, "macro_f1": 0.5949322191543936}, {"mcc": 0.47011938964310546, "macro_f1": 0.563013835081582}, {"mcc": 0.5426031576474426, "macro_f1": 0.6559361586471422}, {"mcc": 0.5360826384816907, "macro_f1": 0.6314706309486762}, {"mcc": 0.4730062884820826, "macro_f1": 0.5905000812897133}], "total": {"test_mcc": 49.806921493276725, "test_mcc_se": 1.5471608981144087, "test_macro_f1": 59.92212661866058, "test_macro_f1_se": 1.7946948657561808}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "norec", "task": "sentiment-classification", "dataset_languages": ["nb", "nn", "no"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"mcc": 0.35741714945148384, "macro_f1": 0.3769055256666077}, {"mcc": 0.3208978512371556, "macro_f1": 0.3819628715994876}, {"mcc": 0.351167116009351, "macro_f1": 0.3909691025958762}, {"mcc": 0.352828505726251, "macro_f1": 0.38793162721882957}, {"mcc": 0.33553293056305433, "macro_f1": 0.37586657651566374}, {"mcc": 0.3502398543823866, "macro_f1": 0.38722222573468407}, {"mcc": 0.34114779581522, "macro_f1": 0.3651976343164057}, {"mcc": 0.3404094750627394, "macro_f1": 0.3756558910868087}, {"mcc": 0.3479876588807651, "macro_f1": 0.38594717698238185}, {"mcc": 0.3402443679053959, "macro_f1": 0.3803365262667329}], "total": {"test_mcc": 34.37872705033803, "test_mcc_se": 0.6547191793114363, "test_macro_f1": 38.079951579834784, "test_macro_f1_se": 0.4769680185355077}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hotter-and-colder-sentiment", "task": "sentiment-classification", "dataset_languages": ["is"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"mcc": 0.16127567969992432, "macro_f1": 0.2633187479054399}, {"mcc": 0.1607244726873136, "macro_f1": 0.404603869396319}, {"mcc": 0.0, "macro_f1": 0.1924489496106949}, {"mcc": 0.1345363147131022, "macro_f1": 0.30987429368076996}, {"mcc": 0.13971117289017532, "macro_f1": 0.309955254695187}, {"mcc": 0.15031960752226267, "macro_f1": 0.30207165662796714}, {"mcc": 0.10458799049884111, "macro_f1": 0.33746947669182764}, {"mcc": 0.1525045077381781, "macro_f1": 0.3694124092632893}, {"mcc": 0.11714671748256525, "macro_f1": 0.3670101916794219}, {"mcc": 0.2679897547884239, "macro_f1": 0.36797349168958543}], "total": {"test_mcc": 13.887962180207863, "test_mcc_se": 4.075881586426891, "test_macro_f1": 32.24138341240502, "test_macro_f1_se": 3.8299277522911277}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "sb10k", "task": "sentiment-classification", "dataset_languages": ["de"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"mcc": 0.4956633916901444, "macro_f1": 0.5720918458027956}, {"mcc": 0.533620531302188, "macro_f1": 0.6256144049272105}, {"mcc": 0.5701095791340443, "macro_f1": 0.6568347361987855}, {"mcc": 0.5291054628457785, "macro_f1": 0.6296600789869906}, {"mcc": 0.5488593057264972, "macro_f1": 0.6552519585239148}, {"mcc": 0.5529465121500484, "macro_f1": 0.6317948717948718}, {"mcc": 0.5785902454004561, "macro_f1": 0.6850904813007771}, {"mcc": 0.5049617794374192, "macro_f1": 0.5967039953410075}, {"mcc": 0.5498790507247637, "macro_f1": 0.6542028968642103}, {"mcc": 0.5769919469796565, "macro_f1": 0.6746592029185597}], "total": {"test_mcc": 54.40727805390997, "test_mcc_se": 1.7645828148783558, "test_macro_f1": 63.81904472659124, "test_macro_f1_se": 2.1457464738983316}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "dutch-social", "task": "sentiment-classification", "dataset_languages": ["nl"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"mcc": 0.09881343720736901, "macro_f1": 0.21294953053594656}, {"mcc": 0.10788711717259396, "macro_f1": 0.23094747421283768}, {"mcc": 0.04641731435736674, "macro_f1": 0.20953663967902023}, {"mcc": 0.1285993911177882, "macro_f1": 0.2417633722230547}, {"mcc": 0.0930729464700229, "macro_f1": 0.24688878680364032}, {"mcc": 0.06315679498220894, "macro_f1": 0.21353196990915138}, {"mcc": 0.09337848335882855, "macro_f1": 0.21682246220800036}, {"mcc": 0.08995744703828974, "macro_f1": 0.21989362872800985}, {"mcc": 0.08018857005856385, "macro_f1": 0.2596934709945056}, {"mcc": 0.1289504447042552, "macro_f1": 0.23362590982476994}], "total": {"test_mcc": 9.304219464672872, "test_mcc_se": 1.6068889131535962, "test_macro_f1": 22.856532451189366, "test_macro_f1_se": 1.0443573979148078}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "sst5", "task": "sentiment-classification", "dataset_languages": ["en"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"mcc": 0.6613914132545734, "macro_f1": 0.6836451353214059}, {"mcc": 0.6798067663928689, "macro_f1": 0.7206912155667679}, {"mcc": 0.6860056175089253, "macro_f1": 0.6988218848003971}, {"mcc": 0.6720035214888727, "macro_f1": 0.6864821354129859}, {"mcc": 0.6630803496178433, "macro_f1": 0.6936083782438288}, {"mcc": 0.6408939951086123, "macro_f1": 0.6898890779817323}, {"mcc": 0.662643430544776, "macro_f1": 0.6855377384596985}, {"mcc": 0.654485943799245, "macro_f1": 0.6882035287704377}, {"mcc": 0.7000734339171306, "macro_f1": 0.7259803375714432}, {"mcc": 0.6888886613969293, "macro_f1": 0.7052640271515594}], "total": {"test_mcc": 67.09273133029777, "test_mcc_se": 1.1073786684519225, "test_macro_f1": 69.78123459280258, "test_macro_f1_se": 0.9299289608892718}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fosent", "task": "sentiment-classification", "dataset_languages": ["fo"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"mcc": 0.1387362870848555, "macro_f1": 0.2147473073736537}, {"mcc": 0.16255636069343116, "macro_f1": 0.37299673542068557}, {"mcc": 0.3197377629634409, "macro_f1": 0.468402537311424}, {"mcc": 0.0722989024851695, "macro_f1": 0.1498903955044306}, {"mcc": 0.0, "macro_f1": 0.12607449856733524}, {"mcc": 0.3039100252571263, "macro_f1": 0.4720212009296292}, {"mcc": 0.19917766078630844, "macro_f1": 0.3095086911659655}, {"mcc": 0.17708143127171927, "macro_f1": 0.43727255222874745}, {"mcc": 0.3562311021518554, "macro_f1": 0.4357017374032884}, {"mcc": 0.3025161887057362, "macro_f1": 0.5104303183941583}], "total": {"test_mcc": 20.32245721399643, "test_mcc_se": 7.214437549910178, "test_macro_f1": 34.970459742993185, "test_macro_f1_se": 8.771588362650949}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "allocine", "task": "sentiment-classification", "dataset_languages": ["fr"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"mcc": 0.9208955224554514, "macro_f1": 0.9597212028762345}, {"mcc": 0.9082574631577478, "macro_f1": 0.9534863149690822}, {"mcc": 0.9189960800884178, "macro_f1": 0.9592142974541327}, {"mcc": 0.9025029006228306, "macro_f1": 0.95}, {"mcc": 0.918305864246719, "macro_f1": 0.9589467504760814}, {"mcc": 0.8966213051155592, "macro_f1": 0.9472651723733438}, {"mcc": 0.9332048990581616, "macro_f1": 0.9662128866448946}, {"mcc": 0.895346618249723, "macro_f1": 0.94542918274123}, {"mcc": 0.9214255112792128, "macro_f1": 0.9600567769338288}, {"mcc": 0.9338551472456106, "macro_f1": 0.9666900112130362}], "total": {"test_mcc": 91.49411311519432, "test_mcc_se": 0.8557203230639776, "test_macro_f1": 95.67022595681864, "test_macro_f1_se": 0.4588826316550668}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "sentipolc16", "task": "sentiment-classification", "dataset_languages": ["it"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"mcc": 0.5179555109873438, "macro_f1": 0.6008597044487193}, {"mcc": 0.5430881708107437, "macro_f1": 0.587585054156908}, {"mcc": 0.499348398487442, "macro_f1": 0.5614257865697544}, {"mcc": 0.49604618126988337, "macro_f1": 0.5832821151082874}, {"mcc": 0.5057720066780063, "macro_f1": 0.594437265489897}, {"mcc": 0.48354599469262977, "macro_f1": 0.5373349433744771}, {"mcc": 0.4987647385875461, "macro_f1": 0.5589746253324818}, {"mcc": 0.511206348342784, "macro_f1": 0.592950007552179}, {"mcc": 0.46124059535357653, "macro_f1": 0.5245645138865908}, {"mcc": 0.49541954390730103, "macro_f1": 0.5804588725372102}], "total": {"test_mcc": 50.12387489117257, "test_mcc_se": 1.3283945060557967, "test_macro_f1": 57.21872888456504, "test_macro_f1_se": 1.5936330661044236}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "suc3", "task": "named-entity-recognition", "dataset_languages": ["sv"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"micro_f1_no_misc": 0.4189134808853119, "micro_f1": 0.20229405630865485}, {"micro_f1_no_misc": 0.43639655952919876, "micro_f1": 0.24886445135070523}, {"micro_f1_no_misc": 0.4521199586349535, "micro_f1": 0.24063492063492062}, {"micro_f1_no_misc": 0.45848521668691783, "micro_f1": 0.23481368044920875}, {"micro_f1_no_misc": 0.44738500315059865, "micro_f1": 0.23648208469055373}, {"micro_f1_no_misc": 0.42054854157596866, "micro_f1": 0.2420260422888544}, {"micro_f1_no_misc": 0.4312422102201911, "micro_f1": 0.2392564242755604}, {"micro_f1_no_misc": 0.40955329500221144, "micro_f1": 0.21700620017714795}, {"micro_f1_no_misc": 0.3944516688339835, "micro_f1": 0.23462607224839918}, {"micro_f1_no_misc": 0.45335696875692444, "micro_f1": 0.23118747281426705}], "total": {"test_micro_f1_no_misc": 43.22452903276259, "test_micro_f1_no_misc_se": 1.3122936022115914, "test_micro_f1": 23.271914052382723, "test_micro_f1_se": 0.8391701388398114}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "dansk", "task": "named-entity-recognition", "dataset_languages": ["da"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"micro_f1_no_misc": 0.4782608695652174, "micro_f1": 0.28165788871078545}, {"micro_f1_no_misc": 0.47423711855927964, "micro_f1": 0.2521377962374786}, {"micro_f1_no_misc": 0.463519313304721, "micro_f1": 0.2815587266739846}, {"micro_f1_no_misc": 0.5317492830807046, "micro_f1": 0.2987994664295242}, {"micro_f1_no_misc": 0.44547759932375314, "micro_f1": 0.23539518900343642}, {"micro_f1_no_misc": 0.4323661626481406, "micro_f1": 0.2429656096471639}, {"micro_f1_no_misc": 0.4873096446700508, "micro_f1": 0.29621430152756256}, {"micro_f1_no_misc": 0.504355400696864, "micro_f1": 0.3167443000735474}, {"micro_f1_no_misc": 0.5189478377173429, "micro_f1": 0.2817729534147444}, {"micro_f1_no_misc": 0.4911062906724512, "micro_f1": 0.2788311398616997}], "total": {"test_micro_f1_no_misc": 48.27329520238524, "test_micro_f1_no_misc_se": 1.9224909507876708, "test_micro_f1": 27.660773715799277, "test_micro_f1_se": 1.6001356560628683}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "norne-nb", "task": "named-entity-recognition", "dataset_languages": ["nb", "no"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"micro_f1_no_misc": 0.5739542225730071, "micro_f1": 0.3435357729072125}, {"micro_f1_no_misc": 0.5667547918043622, "micro_f1": 0.3163543922417852}, {"micro_f1_no_misc": 0.5977664205943594, "micro_f1": 0.355499527261267}, {"micro_f1_no_misc": 0.5980938680093508, "micro_f1": 0.3823266219239374}, {"micro_f1_no_misc": 0.5579501103003564, "micro_f1": 0.33115380746403783}, {"micro_f1_no_misc": 0.5628600474415453, "micro_f1": 0.3345969672785315}, {"micro_f1_no_misc": 0.562830567207733, "micro_f1": 0.31900569653029515}, {"micro_f1_no_misc": 0.5390489413398126, "micro_f1": 0.3275319773537429}, {"micro_f1_no_misc": 0.5588513726727674, "micro_f1": 0.33935645956131033}, {"micro_f1_no_misc": 0.5659075224292616, "micro_f1": 0.3210557415796731}], "total": {"test_micro_f1_no_misc": 56.84017864372556, "test_micro_f1_no_misc_se": 1.1140150919966614, "test_micro_f1": 33.704169641017934, "test_micro_f1_se": 1.2359641211211596}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "norne-nn", "task": "named-entity-recognition", "dataset_languages": ["nn"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"micro_f1_no_misc": 0.6223162003903708, "micro_f1": 0.39020628494312704}, {"micro_f1_no_misc": 0.5506551613934165, "micro_f1": 0.33970198372794586}, {"micro_f1_no_misc": 0.5570416994492525, "micro_f1": 0.3038793103448276}, {"micro_f1_no_misc": 0.6161888167273879, "micro_f1": 0.380168417068786}, {"micro_f1_no_misc": 0.5773074661963551, "micro_f1": 0.38579529608626945}, {"micro_f1_no_misc": 0.5556889822876014, "micro_f1": 0.33551312232975844}, {"micro_f1_no_misc": 0.572313409724373, "micro_f1": 0.3849644830307814}, {"micro_f1_no_misc": 0.5873861488228218, "micro_f1": 0.3856697819314641}, {"micro_f1_no_misc": 0.6039833256137102, "micro_f1": 0.3614152565378699}, {"micro_f1_no_misc": 0.5876698707325156, "micro_f1": 0.3405623966182687}], "total": {"test_micro_f1_no_misc": 58.305510813378035, "test_micro_f1_no_misc_se": 1.5654251873752565, "test_micro_f1": 36.07876332619099, "test_micro_f1_se": 1.8248376918476226}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mim-gold-ner", "task": "named-entity-recognition", "dataset_languages": ["is"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"micro_f1_no_misc": 0.34562926224426527, "micro_f1": 0.23269790590665793}, {"micro_f1_no_misc": 0.32249027237354083, "micro_f1": 0.16304724901130926}, {"micro_f1_no_misc": 0.45195656627740216, "micro_f1": 0.2645141260478112}, {"micro_f1_no_misc": 0.3499574950410881, "micro_f1": 0.19194139194139193}, {"micro_f1_no_misc": 0.3539251650770359, "micro_f1": 0.224509965409323}, {"micro_f1_no_misc": 0.3557579318448884, "micro_f1": 0.22207324226933714}, {"micro_f1_no_misc": 0.4073201666968654, "micro_f1": 0.268659793814433}, {"micro_f1_no_misc": 0.36627479794268913, "micro_f1": 0.2159339094103124}, {"micro_f1_no_misc": 0.3410379415612734, "micro_f1": 0.20524679052707118}, {"micro_f1_no_misc": 0.3727087576374746, "micro_f1": 0.20625570397411433}], "total": {"test_micro_f1_no_misc": 36.670583566965234, "test_micro_f1_no_misc_se": 2.316821668419849, "test_micro_f1": 21.948800783117612, "test_micro_f1_se": 1.9585331331771623}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fone", "task": "named-entity-recognition", "dataset_languages": ["fo"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"micro_f1_no_misc": 0.5596568057914023, "micro_f1": 0.3933875758749839}, {"micro_f1_no_misc": 0.6002752654345261, "micro_f1": 0.4553036046591771}, {"micro_f1_no_misc": 0.5810421072870602, "micro_f1": 0.38405403775405583}, {"micro_f1_no_misc": 0.5843909831407463, "micro_f1": 0.47936670820200833}, {"micro_f1_no_misc": 0.6136593436896773, "micro_f1": 0.48755854800936776}, {"micro_f1_no_misc": 0.6343257443082312, "micro_f1": 0.4739978331527628}, {"micro_f1_no_misc": 0.6389620157954118, "micro_f1": 0.49424528993475547}, {"micro_f1_no_misc": 0.5439540059347181, "micro_f1": 0.46947549441100594}, {"micro_f1_no_misc": 0.5922859830667921, "micro_f1": 0.4778121775025799}, {"micro_f1_no_misc": 0.6300373134328359, "micro_f1": 0.5552096985583224}], "total": {"test_micro_f1_no_misc": 59.78589567881401, "test_micro_f1_no_misc_se": 1.9784918593056873, "test_micro_f1": 46.7041096805902, "test_micro_f1_se": 3.043892371667294}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "germeval", "task": "named-entity-recognition", "dataset_languages": ["de"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"micro_f1_no_misc": 0.5540266185427475, "micro_f1": 0.36480987792900244}, {"micro_f1_no_misc": 0.595608417200366, "micro_f1": 0.4330397813543881}, {"micro_f1_no_misc": 0.5876010781671158, "micro_f1": 0.3761316872427984}, {"micro_f1_no_misc": 0.5732329366043235, "micro_f1": 0.36013044094711466}, {"micro_f1_no_misc": 0.56228627259404, "micro_f1": 0.36071273359408956}, {"micro_f1_no_misc": 0.5691999108535769, "micro_f1": 0.3752910560197233}, {"micro_f1_no_misc": 0.5793036750483559, "micro_f1": 0.35265972417675207}, {"micro_f1_no_misc": 0.6083438685208596, "micro_f1": 0.3712394705174488}, {"micro_f1_no_misc": 0.5750171271979905, "micro_f1": 0.35035306898424773}, {"micro_f1_no_misc": 0.5598407564070664, "micro_f1": 0.3288571428571429}], "total": {"test_micro_f1_no_misc": 57.64460661136442, "test_micro_f1_no_misc_se": 1.0455387551210091, "test_micro_f1": 36.73224983622708, "test_micro_f1_se": 1.673282670310255}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "conll-nl", "task": "named-entity-recognition", "dataset_languages": ["nl"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"micro_f1_no_misc": 0.5303089558075871, "micro_f1": 0.3283026669423196}, {"micro_f1_no_misc": 0.4924444444444444, "micro_f1": 0.3348560986751941}, {"micro_f1_no_misc": 0.5243328100470958, "micro_f1": 0.2962683754240482}, {"micro_f1_no_misc": 0.49413735343383586, "micro_f1": 0.31238175320580197}, {"micro_f1_no_misc": 0.529650690495532, "micro_f1": 0.3185516680227828}, {"micro_f1_no_misc": 0.5023547880690737, "micro_f1": 0.30146603345034073}, {"micro_f1_no_misc": 0.5157395429064251, "micro_f1": 0.31364124597207305}, {"micro_f1_no_misc": 0.46696832579185515, "micro_f1": 0.312529110386586}, {"micro_f1_no_misc": 0.507758620689655, "micro_f1": 0.2786479802143446}, {"micro_f1_no_misc": 0.5126811594202899, "micro_f1": 0.32286128564523925}], "total": {"test_micro_f1_no_misc": 50.76376691105794, "test_micro_f1_no_misc_se": 1.2158631248467773, "test_micro_f1": 31.1950621793873, "test_micro_f1_se": 1.0169717837396046}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "conll-en", "task": "named-entity-recognition", "dataset_languages": ["en"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"micro_f1_no_misc": 0.6760925449871464, "micro_f1": 0.5478400982122305}, {"micro_f1_no_misc": 0.7145660095300382, "micro_f1": 0.5853972798854687}, {"micro_f1_no_misc": 0.7130606209955643, "micro_f1": 0.5555792921740368}, {"micro_f1_no_misc": 0.701891095695144, "micro_f1": 0.5872270661003055}, {"micro_f1_no_misc": 0.7090353729431472, "micro_f1": 0.5721649484536082}, {"micro_f1_no_misc": 0.7192627101478631, "micro_f1": 0.5640077071290945}, {"micro_f1_no_misc": 0.6904577191621413, "micro_f1": 0.5543974871502}, {"micro_f1_no_misc": 0.7110978875334721, "micro_f1": 0.5460382314314899}, {"micro_f1_no_misc": 0.7136650868878357, "micro_f1": 0.6056936143441334}, {"micro_f1_no_misc": 0.7158091674462114, "micro_f1": 0.5694633517231497}], "total": {"test_micro_f1_no_misc": 70.64938215328564, "test_micro_f1_no_misc_se": 0.83661533470956, "test_micro_f1": 56.87809076603718, "test_micro_f1_se": 1.195890156635078}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "eltec", "task": "named-entity-recognition", "dataset_languages": ["fr"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"micro_f1_no_misc": 0.4798676227247656, "micro_f1": 0.32931726907630526}, {"micro_f1_no_misc": 0.47913012977902486, "micro_f1": 0.389984170384228}, {"micro_f1_no_misc": 0.4852994555353902, "micro_f1": 0.3725898182827021}, {"micro_f1_no_misc": 0.5216106390838566, "micro_f1": 0.35351280367695337}, {"micro_f1_no_misc": 0.5162781737622809, "micro_f1": 0.3689101661503802}, {"micro_f1_no_misc": 0.45911047345767575, "micro_f1": 0.32681638044914135}, {"micro_f1_no_misc": 0.5004533091568449, "micro_f1": 0.3244748165021514}, {"micro_f1_no_misc": 0.4768163563344287, "micro_f1": 0.35266435986159167}, {"micro_f1_no_misc": 0.44799432019879304, "micro_f1": 0.30759330759330755}, {"micro_f1_no_misc": 0.4963935638986499, "micro_f1": 0.33056584493549623}], "total": {"test_micro_f1_no_misc": 48.62954043931711, "test_micro_f1_no_misc_se": 1.4370432652132341, "test_micro_f1": 34.56428936912257, "test_micro_f1_se": 1.6121623884811465}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "multinerd-it", "task": "named-entity-recognition", "dataset_languages": ["it"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"micro_f1_no_misc": 0.5690877965272683, "micro_f1": 0.3377698905649217}, {"micro_f1_no_misc": 0.552142857142857, "micro_f1": 0.39443905593275136}, {"micro_f1_no_misc": 0.4922884012539185, "micro_f1": 0.3656217882836588}, {"micro_f1_no_misc": 0.5620947630922694, "micro_f1": 0.4045035268583831}, {"micro_f1_no_misc": 0.5777889573638536, "micro_f1": 0.3801545554906163}, {"micro_f1_no_misc": 0.5508595988538681, "micro_f1": 0.3694355496954857}, {"micro_f1_no_misc": 0.5751742057399315, "micro_f1": 0.36458968418485554}, {"micro_f1_no_misc": 0.6422957847106454, "micro_f1": 0.4386072918804296}, {"micro_f1_no_misc": 0.5683512841756421, "micro_f1": 0.3715080267981292}, {"micro_f1_no_misc": 0.5950686062106675, "micro_f1": 0.35148680253925824}], "total": {"test_micro_f1_no_misc": 56.85152255070921, "test_micro_f1_no_misc_se": 2.3292848366368597, "test_micro_f1": 37.78116172228489, "test_micro_f1_se": 1.7796797744281452}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-sv", "task": "linguistic-acceptability", "dataset_languages": ["sv"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"mcc": 0.14038292974317748, "macro_f1": 0.40493946731234864}, {"mcc": 0.2531134443759232, "macro_f1": 0.5956122470677938}, {"mcc": 0.2450384823949629, "macro_f1": 0.5869182525584455}, {"mcc": 0.24659515751436864, "macro_f1": 0.5470900616880706}, {"mcc": 0.26741946894558405, "macro_f1": 0.6083010017048758}, {"mcc": 0.2348491637392029, "macro_f1": 0.5487392687134929}, {"mcc": 0.21110622345868327, "macro_f1": 0.5612116749503998}, {"mcc": 0.16761028263717687, "macro_f1": 0.42159213156468417}, {"mcc": 0.30025761736503387, "macro_f1": 0.5994182954001082}, {"mcc": 0.18137571778569905, "macro_f1": 0.5012808899019121}], "total": {"test_mcc": 22.477484879598123, "test_mcc_se": 3.0481777639886958, "test_macro_f1": 53.7510329086213, "test_macro_f1_se": 4.5153388057482395}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-da", "task": "linguistic-acceptability", "dataset_languages": ["da"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"mcc": 0.14502516982470254, "macro_f1": 0.46038349224300695}, {"mcc": 0.21122941726415823, "macro_f1": 0.563687524956221}, {"mcc": 0.19264295137442214, "macro_f1": 0.5220937447602448}, {"mcc": 0.2314332595935192, "macro_f1": 0.5475565272674834}, {"mcc": 0.1495765559244389, "macro_f1": 0.45918884738981913}, {"mcc": 0.2235405468446379, "macro_f1": 0.6067232020649801}, {"mcc": 0.2814171921904867, "macro_f1": 0.5986162935166721}, {"mcc": 0.19462581491767825, "macro_f1": 0.521594684385382}, {"mcc": 0.2338030729916822, "macro_f1": 0.476324881933213}, {"mcc": 0.15956740965377014, "macro_f1": 0.5069093800107807}], "total": {"test_mcc": 20.228613905794962, "test_mcc_se": 2.6674894839980183, "test_macro_f1": 52.63078578527802, "test_macro_f1_se": 3.2868211116761725}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-nb", "task": "linguistic-acceptability", "dataset_languages": ["nb", "no"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"mcc": 0.14766510131796967, "macro_f1": 0.41095631105831715}, {"mcc": 0.2653741411222671, "macro_f1": 0.6022955205876965}, {"mcc": 0.2243183084925776, "macro_f1": 0.5465083401051184}, {"mcc": 0.23409508558088335, "macro_f1": 0.5703496503496504}, {"mcc": 0.16695785238457328, "macro_f1": 0.4763106251752173}, {"mcc": 0.19661535348040998, "macro_f1": 0.5318430610261394}, {"mcc": 0.20564692017430772, "macro_f1": 0.5237984899955521}, {"mcc": 0.17879075862136268, "macro_f1": 0.4749579258707808}, {"mcc": 0.09575920357170235, "macro_f1": 0.36722519310754603}, {"mcc": 0.1647818487916037, "macro_f1": 0.4524674954320546}], "total": {"test_mcc": 18.800045735376575, "test_mcc_se": 2.9952398171117847, "test_macro_f1": 49.567126127080726, "test_macro_f1_se": 4.525247211448824}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-nn", "task": "linguistic-acceptability", "dataset_languages": ["nn"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"mcc": 0.05471080759527953, "macro_f1": 0.3428763649536821}, {"mcc": 0.10835479314631892, "macro_f1": 0.3813653401326564}, {"mcc": 0.003664561772642764, "macro_f1": 0.34522105096688493}, {"mcc": 0.08971095503165895, "macro_f1": 0.38507415106885234}, {"mcc": 0.044994062587007805, "macro_f1": 0.3876000980796718}, {"mcc": 0.06808723995341971, "macro_f1": 0.35444997678357315}, {"mcc": 0.06529947522490231, "macro_f1": 0.3688289545067039}, {"mcc": 0.14395322837466484, "macro_f1": 0.44615679334492486}, {"mcc": 0.01394452481368111, "macro_f1": 0.3427805874145977}, {"mcc": 0.03170778294712018, "macro_f1": 0.3459211768947574}], "total": {"test_mcc": 6.244274314466962, "test_mcc_se": 2.665990772399616, "test_macro_f1": 37.00274494146304, "test_macro_f1_se": 2.0009033516909036}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-is", "task": "linguistic-acceptability", "dataset_languages": ["is"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"mcc": 0.029424864998941796, "macro_f1": 0.46179401993355484}, {"mcc": 0.0019401992673418074, "macro_f1": 0.47165403674914963}, {"mcc": 0.02933711837077134, "macro_f1": 0.47570265300761755}, {"mcc": 0.020014350712641592, "macro_f1": 0.46013391209602306}, {"mcc": 0.013325478654537216, "macro_f1": 0.5050641708155372}, {"mcc": 0.02805608892551997, "macro_f1": 0.4801333664182446}, {"mcc": 0.055617651435923295, "macro_f1": 0.40826622149657865}, {"mcc": 0.03407862619867157, "macro_f1": 0.5014808109965944}, {"mcc": 0.001391371326880572, "macro_f1": 0.49563001350777813}, {"mcc": 0.009753632839256784, "macro_f1": 0.42275517152739883}], "total": {"test_mcc": 2.2293938273048592, "test_mcc_se": 1.028266274191791, "test_macro_f1": 46.826143765484765, "test_macro_f1_se": 1.9808999186039757}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-fo", "task": "linguistic-acceptability", "dataset_languages": ["fo"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"mcc": 0.04820900710139268, "macro_f1": 0.5029061798811212}, {"mcc": 0.02904255663290669, "macro_f1": 0.4812941710432471}, {"mcc": 0.03717464462788104, "macro_f1": 0.42561901410658853}, {"mcc": 0.029637869309137758, "macro_f1": 0.4048138422278725}, {"mcc": 0.07603095894951604, "macro_f1": 0.47382818918090186}, {"mcc": 0.07027645426852024, "macro_f1": 0.5020424042015172}, {"mcc": 0.0, "macro_f1": 0.31505016722408025}, {"mcc": 0.016042149975047403, "macro_f1": 0.5014143287734756}, {"mcc": 0.0024780747140898383, "macro_f1": 0.3966106488792514}, {"mcc": -0.011418063096816125, "macro_f1": 0.47333488446137084}], "total": {"test_mcc": 2.974736524816756, "test_mcc_se": 1.8120113616719675, "test_macro_f1": 44.76913829979427, "test_macro_f1_se": 3.8032383443776583}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-de", "task": "linguistic-acceptability", "dataset_languages": ["de"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"mcc": 0.21458933663186788, "macro_f1": 0.4384217105798844}, {"mcc": 0.24356816952163843, "macro_f1": 0.46290970224945466}, {"mcc": 0.1533686167794497, "macro_f1": 0.3844920857543579}, {"mcc": 0.20630750372246776, "macro_f1": 0.43151660552957466}, {"mcc": 0.3262295964098831, "macro_f1": 0.5545242391368774}, {"mcc": 0.16601658834359703, "macro_f1": 0.40037527790406824}, {"mcc": 0.2504501147757424, "macro_f1": 0.48171602643234535}, {"mcc": 0.21535768835121902, "macro_f1": 0.4494465791133796}, {"mcc": 0.3035484680030492, "macro_f1": 0.5509436590291803}, {"mcc": 0.2500409778347958, "macro_f1": 0.4673561792562061}], "total": {"test_mcc": 23.294770603737103, "test_mcc_se": 3.3674267844911676, "test_macro_f1": 46.21702064985329, "test_macro_f1_se": 3.477888058159763}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-nl", "task": "linguistic-acceptability", "dataset_languages": ["nl"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"mcc": 0.17555870327315662, "macro_f1": 0.46816820291308314}, {"mcc": 0.17954651929716448, "macro_f1": 0.4595312907018595}, {"mcc": 0.1506270005533415, "macro_f1": 0.4151809150764323}, {"mcc": 0.20400042888385947, "macro_f1": 0.4779975450499896}, {"mcc": 0.11092341355135708, "macro_f1": 0.3783652743875743}, {"mcc": 0.19236556849865735, "macro_f1": 0.44608156125769716}, {"mcc": 0.18975400978053533, "macro_f1": 0.4614759336768467}, {"mcc": 0.17594988059848887, "macro_f1": 0.4190963047321693}, {"mcc": 0.2202001696819025, "macro_f1": 0.48566015871415036}, {"mcc": 0.12829792938463364, "macro_f1": 0.39196511291146763}], "total": {"test_mcc": 17.272236235030967, "test_mcc_se": 2.092122720245369, "test_macro_f1": 44.035222994212695, "test_macro_f1_se": 2.296881043538709}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-en", "task": "linguistic-acceptability", "dataset_languages": ["en"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"mcc": 0.41049684996151764, "macro_f1": 0.6762609896832585}, {"mcc": 0.44316005484811694, "macro_f1": 0.6746884741280158}, {"mcc": 0.4420641550271268, "macro_f1": 0.6729242437230338}, {"mcc": 0.3809859682953065, "macro_f1": 0.6149721602703107}, {"mcc": 0.3991098341535224, "macro_f1": 0.6691437802907916}, {"mcc": 0.44490968645106505, "macro_f1": 0.6976069261166996}, {"mcc": 0.39470784459797115, "macro_f1": 0.627497037853119}, {"mcc": 0.40349731592301474, "macro_f1": 0.6688146893465268}, {"mcc": 0.40518798025092684, "macro_f1": 0.6721089296551828}, {"mcc": 0.3647666631882742, "macro_f1": 0.6057424361486924}], "total": {"test_mcc": 40.88886352696842, "test_mcc_se": 1.6835267955651465, "test_macro_f1": 65.7975966721563, "test_macro_f1_se": 1.888606510744313}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-fr", "task": "linguistic-acceptability", "dataset_languages": ["fr"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"mcc": 0.2830618995373342, "macro_f1": 0.548740799248279}, {"mcc": 0.26324905419422195, "macro_f1": 0.5527272919564057}, {"mcc": 0.2063655661240796, "macro_f1": 0.46551347329752857}, {"mcc": 0.277216936785155, "macro_f1": 0.5495217119060056}, {"mcc": 0.27438197385983965, "macro_f1": 0.523266290422521}, {"mcc": 0.255508296132394, "macro_f1": 0.5178895429818993}, {"mcc": 0.27196170879729903, "macro_f1": 0.5364444740320877}, {"mcc": 0.2295460979193482, "macro_f1": 0.4903008478164457}, {"mcc": 0.242479150444662, "macro_f1": 0.48731953319860905}, {"mcc": 0.17820758344380125, "macro_f1": 0.4481319865788147}], "total": {"test_mcc": 24.81978267238135, "test_mcc_se": 2.1288131378522617, "test_macro_f1": 51.19855951438596, "test_macro_f1_se": 2.309486916281498}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-it", "task": "linguistic-acceptability", "dataset_languages": ["it"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"mcc": 0.17152111097507156, "macro_f1": 0.45383956664951675}, {"mcc": 0.14569522459156536, "macro_f1": 0.5105756358768407}, {"mcc": 0.1790763121304492, "macro_f1": 0.45201932570353626}, {"mcc": 0.14050032339141744, "macro_f1": 0.4285239183742271}, {"mcc": 0.12748708509360376, "macro_f1": 0.4094798949001083}, {"mcc": 0.11262230325604086, "macro_f1": 0.4039493073975833}, {"mcc": 0.14780201900222237, "macro_f1": 0.437385415281047}, {"mcc": 0.19455764369638573, "macro_f1": 0.4915631026657503}, {"mcc": 0.15425415618430358, "macro_f1": 0.437566256395815}, {"mcc": 0.13826508037002222, "macro_f1": 0.4404634169021878}], "total": {"test_mcc": 15.117812586910823, "test_mcc_se": 1.5257018691002677, "test_macro_f1": 44.653658401466124, "test_macro_f1_se": 2.058176915462558}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"f1": 62.41411252641184, "em": 50.60606060606061}, {"f1": 61.21888097928314, "em": 50.64442759666414}, {"f1": 61.837868166539394, "em": 49.883449883449885}, {"f1": 61.45004306533978, "em": 51.36986301369863}, {"f1": 61.81655104939597, "em": 48.95913646877409}, {"f1": 61.54527721692063, "em": 50.534351145038165}, {"f1": 59.93888123235643, "em": 47.893915756630264}, {"f1": 59.620094920802316, "em": 47.24842767295598}, {"f1": 62.473617283988744, "em": 52.55417956656347}, {"f1": 61.93203371307933, "em": 48.22530864197531}], "total": {"test_f1": 61.42473601541176, "test_f1_se": 0.5909907146176581, "test_em": 49.79191203518105, "test_em_se": 1.0389588177736855}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "norquad", "task": "reading-comprehension", "dataset_languages": ["nb", "nn", "no"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"f1": 63.71140888386013, "em": 31.61764705882353}, {"f1": 62.1370347330435, "em": 29.51219512195122}, {"f1": 63.228677912727534, "em": 31.307243963363863}, {"f1": 63.460469745218354, "em": 31.316434995911692}, {"f1": 64.04741587076863, "em": 31.265508684863523}, {"f1": 61.71674249022981, "em": 29.54732510288066}, {"f1": 60.334389745945, "em": 29.327731092436974}, {"f1": 64.3004068680641, "em": 33.192567567567565}, {"f1": 62.36340604263607, "em": 29.651162790697676}, {"f1": 62.43634629122787, "em": 29.55298013245033}], "total": {"test_f1": 62.773629858372104, "test_f1_se": 0.7524978207140943, "test_em": 30.629079651094703, "test_em_se": 0.8031373685014849}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scandiqa-sv", "task": "reading-comprehension", "dataset_languages": ["sv"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"f1": 62.717659546466045, "em": 49.92424242424242}, {"f1": 60.58576569881916, "em": 47.460197119029566}, {"f1": 62.521486703344465, "em": 50.194250194250195}, {"f1": 62.23040619723034, "em": 48.858447488584474}, {"f1": 61.291926674823735, "em": 49.26754047802621}, {"f1": 62.31013821886595, "em": 51.52671755725191}, {"f1": 61.79974058155758, "em": 49.53198127925117}, {"f1": 63.180239589985256, "em": 51.179245283018865}, {"f1": 63.23717866860826, "em": 51.0061919504644}, {"f1": 62.03733905047537, "em": 48.99691358024691}], "total": {"test_f1": 62.191188093017615, "test_f1_se": 0.5076374164032011, "test_em": 49.79457273543661, "test_em_se": 0.7694602252192466}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "nqii", "task": "reading-comprehension", "dataset_languages": ["is"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"f1": 47.02255934536219, "em": 16.307692307692307}, {"f1": 49.49599582749127, "em": 21.02803738317757}, {"f1": 45.35320135179953, "em": 22.118380062305295}, {"f1": 43.95362140983676, "em": 16.741405082212257}, {"f1": 49.10997470032677, "em": 17.86833855799373}, {"f1": 46.48716370803352, "em": 17.41140215716487}, {"f1": 47.739412556355205, "em": 20.153846153846153}, {"f1": 42.16550765928229, "em": 18.25153374233129}, {"f1": 46.535909551901184, "em": 17.018072289156628}, {"f1": 48.30159427909999, "em": 23.343373493975903}], "total": {"test_f1": 46.61649403894887, "test_f1_se": 1.4208411365945033, "test_em": 19.024208122985602, "test_em_se": 1.5275980557719095}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "foqa", "task": "reading-comprehension", "dataset_languages": ["fo"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"f1": 37.251952634631444, "em": 17.15686274509804}, {"f1": 25.80593607285097, "em": 8.353808353808354}, {"f1": 44.340877811487665, "em": 25.679012345679013}, {"f1": 40.12541064970606, "em": 23.54368932038835}, {"f1": 37.94722330525455, "em": 21.782178217821784}, {"f1": 38.587001713093265, "em": 20.987654320987655}, {"f1": 38.519418643581204, "em": 20.047169811320753}, {"f1": 40.78431359211756, "em": 21.374045801526716}, {"f1": 46.5952391620812, "em": 31.17206982543641}, {"f1": 46.91542563445166, "em": 28.5012285012285}], "total": {"test_f1": 39.687279921925565, "test_f1_se": 3.7373992648657626, "test_em": 21.859771924329557, "test_em_se": 3.9039959751736713}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "germanquad", "task": "reading-comprehension", "dataset_languages": ["de"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"f1": 51.657489297519284, "em": 23.03030303030303}, {"f1": 50.71485454474761, "em": 22.21379833206975}, {"f1": 53.310189410292814, "em": 23.31002331002331}, {"f1": 52.12580119274473, "em": 23.28767123287671}, {"f1": 53.27440176822748, "em": 23.053199691595992}, {"f1": 52.76032866671094, "em": 23.435114503816795}, {"f1": 49.42279559193476, "em": 22.23088923556942}, {"f1": 52.12426039761262, "em": 21.61949685534591}, {"f1": 50.18677456212451, "em": 19.736842105263158}, {"f1": 49.44453576282207, "em": 23.22530864197531}], "total": {"test_f1": 51.50214311947368, "test_f1_se": 0.91700928810712, "test_em": 22.514264693883938, "test_em_se": 0.7105116764071665}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "squad", "task": "reading-comprehension", "dataset_languages": ["en"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"f1": 78.70347957994531, "em": 57.878787878787875}, {"f1": 77.74861543649449, "em": 56.93707354056103}, {"f1": 78.80403952076593, "em": 56.17715617715618}, {"f1": 77.17554633607737, "em": 55.02283105022831}, {"f1": 78.71066587518322, "em": 57.05474171164225}, {"f1": 77.32481558780779, "em": 56.48854961832061}, {"f1": 78.09501296053172, "em": 56.94227769110764}, {"f1": 76.13006451078049, "em": 52.9874213836478}, {"f1": 76.04025842416027, "em": 52.089783281733745}, {"f1": 77.01533649518382, "em": 55.632716049382715}], "total": {"test_f1": 77.57478347269303, "test_f1_se": 0.631842770575978, "test_em": 55.72113383825681, "test_em_se": 1.156896372885956}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "squad-nl", "task": "reading-comprehension", "dataset_languages": ["nl"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"f1": 69.33592560291233, "em": 47.27272727272727}, {"f1": 70.6504890947453, "em": 48.9764973464746}, {"f1": 69.2045964809771, "em": 49.184149184149184}, {"f1": 68.27232149272416, "em": 46.19482496194825}, {"f1": 70.9804808320554, "em": 51.04086353122591}, {"f1": 70.55038406658966, "em": 47.02290076335878}, {"f1": 69.999740458886, "em": 49.37597503900156}, {"f1": 71.26765937796971, "em": 51.179245283018865}, {"f1": 69.78319442811436, "em": 50.077399380804955}, {"f1": 69.38760085078734, "em": 47.76234567901235}], "total": {"test_f1": 69.94323926857614, "test_f1_se": 0.5748856627525905, "test_em": 48.80869284417217, "test_em_se": 1.0570692531764385}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "squad-it", "task": "reading-comprehension", "dataset_languages": ["it"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"f1": 65.67782355816382, "em": 43.86363636363637}, {"f1": 62.81424003703659, "em": 34.420015163002276}, {"f1": 65.20920827353243, "em": 43.35664335664335}, {"f1": 64.12354550669149, "em": 38.81278538812786}, {"f1": 63.77256118859278, "em": 37.31688511950655}, {"f1": 66.02899692996141, "em": 42.900763358778626}, {"f1": 65.16875337558426, "em": 40.6396255850234}, {"f1": 64.96705275743147, "em": 39.85849056603774}, {"f1": 63.098890754852086, "em": 38.39009287925697}, {"f1": 63.24358939375164, "em": 37.65432098765432}], "total": {"test_f1": 64.41046617755981, "test_f1_se": 0.7117325058466555, "test_em": 39.72132587676675, "test_em_se": 1.8720086163061007}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fquad", "task": "reading-comprehension", "dataset_languages": ["fr"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"f1": 65.2699954677581, "em": 33.93939393939394}, {"f1": 65.17754453998677, "em": 34.495830174374525}, {"f1": 67.06894933961385, "em": 36.36363636363637}, {"f1": 66.77362067526413, "em": 34.32267884322679}, {"f1": 63.53188891143246, "em": 32.38242097147263}, {"f1": 66.16011393079525, "em": 33.282442748091604}, {"f1": 65.51931385214445, "em": 35.80343213728549}, {"f1": 66.67671788997819, "em": 37.106918238993714}, {"f1": 64.54403455885848, "em": 35.06191950464396}, {"f1": 66.75604893377626, "em": 35.108024691358025}], "total": {"test_f1": 65.7478228099608, "test_f1_se": 0.7100686942207461, "test_em": 34.78666976124771, "test_em_se": 0.8800396728735554}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "nordjylland-news", "task": "summarization", "dataset_languages": ["da"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"bertscore": 0.6469972917402629, "rouge_l": 0.13957147596532515}, {"bertscore": 0.6455409311456606, "rouge_l": 0.14094041730314602}, {"bertscore": 0.6407472588180099, "rouge_l": 0.1274216693197092}, {"bertscore": 0.6402195554837817, "rouge_l": 0.12174425472143496}, {"bertscore": 0.6475015889445785, "rouge_l": 0.14250099213661127}, {"bertscore": 0.6435006410320057, "rouge_l": 0.13444213800507804}, {"bertscore": 0.6457468454027548, "rouge_l": 0.13597331850071528}, {"bertscore": 0.6412402091518743, "rouge_l": 0.12420874794254022}, {"bertscore": 0.6439066468155943, "rouge_l": 0.1339311938736134}, {"bertscore": 0.6427943507151213, "rouge_l": 0.12805208431557497}], "total": {"test_bertscore": 64.38195319249644, "test_bertscore_se": 0.1607589418705517, "test_rouge_l": 13.287862920837485, "test_rouge_l_se": 0.4468155839860023}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mlsum", "task": "summarization", "dataset_languages": ["de"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"bertscore": 0.6363143957278226, "rouge_l": 0.11962812072072711}, {"bertscore": 0.6345961944462033, "rouge_l": 0.1208226381643845}, {"bertscore": 0.6333810366340913, "rouge_l": 0.118423807305008}, {"bertscore": 0.6334565614233725, "rouge_l": 0.12058431589207282}, {"bertscore": 0.6355323517345823, "rouge_l": 0.11959806364692371}, {"bertscore": 0.6356112786743324, "rouge_l": 0.12123400070440962}, {"bertscore": 0.6356773633742705, "rouge_l": 0.1221418464119601}, {"bertscore": 0.6359129825723357, "rouge_l": 0.11969800552265962}, {"bertscore": 0.6370793083042372, "rouge_l": 0.12175730250635342}, {"bertscore": 0.6353773578885011, "rouge_l": 0.12047859635741197}], "total": {"test_bertscore": 63.52938830779748, "test_bertscore_se": 0.07288579095777355, "test_rouge_l": 12.043666972319109, "test_rouge_l_se": 0.06980039054660336}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "rrn", "task": "summarization", "dataset_languages": ["is"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"bertscore": 0.6197727134567685, "rouge_l": 0.09176425066986779}, {"bertscore": 0.6271998072043061, "rouge_l": 0.1050147312560644}, {"bertscore": 0.63364828878548, "rouge_l": 0.11373989929180806}, {"bertscore": 0.6287475585704669, "rouge_l": 0.1088105553195526}, {"bertscore": 0.6250462728203274, "rouge_l": 0.10153835409473974}, {"bertscore": 0.6234444465953857, "rouge_l": 0.09803696126595265}, {"bertscore": 0.6203453366179019, "rouge_l": 0.09469168888567431}, {"bertscore": 0.6137494907598011, "rouge_l": 0.08405453089815132}, {"bertscore": 0.6210828918847255, "rouge_l": 0.09238008929623429}, {"bertscore": 0.6032460936985444, "rouge_l": 0.06546065406264914}], "total": {"test_bertscore": 62.162829003937084, "test_bertscore_se": 0.5253362300580858, "test_rouge_l": 9.554917150406943, "test_rouge_l_se": 0.8520861234455048}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "no-sammendrag", "task": "summarization", "dataset_languages": ["nb", "nn", "no"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"bertscore": 0.6241787689505145, "rouge_l": 0.09373576073056758}, {"bertscore": 0.6237632737320382, "rouge_l": 0.08957816420023348}, {"bertscore": 0.622445620392682, "rouge_l": 0.0855219371534504}, {"bertscore": 0.6210180361085804, "rouge_l": 0.08834003757054959}, {"bertscore": 0.6249445419816766, "rouge_l": 0.09300223065993272}, {"bertscore": 0.6248891821451252, "rouge_l": 0.09615239581773163}, {"bertscore": 0.623749050006154, "rouge_l": 0.09265435868869024}, {"bertscore": 0.6246418337104842, "rouge_l": 0.09526818029574895}, {"bertscore": 0.6222457947005751, "rouge_l": 0.08563641255385944}, {"bertscore": 0.6230577167880256, "rouge_l": 0.09486566119225098}], "total": {"test_bertscore": 62.34933818515855, "test_bertscore_se": 0.07988973125359308, "test_rouge_l": 9.147551388630152, "test_rouge_l_se": 0.24406282995033743}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "wiki-lingua-nl", "task": "summarization", "dataset_languages": ["nl"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"bertscore": 0.6524281030433485, "rouge_l": 0.15031228837141267}, {"bertscore": 0.6500992444925942, "rouge_l": 0.14827498197270111}, {"bertscore": 0.6484571431501536, "rouge_l": 0.1444393006842424}, {"bertscore": 0.6503435746853938, "rouge_l": 0.1478957004318051}, {"bertscore": 0.6467908796330448, "rouge_l": 0.14334003051866212}, {"bertscore": 0.6503192005766323, "rouge_l": 0.1477358181431107}, {"bertscore": 0.6505735858809203, "rouge_l": 0.14665011754462576}, {"bertscore": 0.6506669661321212, "rouge_l": 0.1465043947660536}, {"bertscore": 0.6490321978344582, "rouge_l": 0.1451546768638769}, {"bertscore": 0.6523027043149341, "rouge_l": 0.14989927609455478}], "total": {"test_bertscore": 65.01013599743601, "test_bertscore_se": 0.10476688726248458, "test_rouge_l": 14.702065853910455, "test_rouge_l_se": 0.14022775730832912}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "swedn", "task": "summarization", "dataset_languages": ["sv"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"bertscore": 0.6484578329545911, "rouge_l": 0.12705793220429074}, {"bertscore": 0.6483879233128391, "rouge_l": 0.12578726747331045}, {"bertscore": 0.6480792736983858, "rouge_l": 0.13384308430492153}, {"bertscore": 0.647207765665371, "rouge_l": 0.12900534642604977}, {"bertscore": 0.6553146711084992, "rouge_l": 0.15006695422732058}, {"bertscore": 0.6511713692452759, "rouge_l": 0.14405712722568859}, {"bertscore": 0.6558158010302577, "rouge_l": 0.15140691051155808}, {"bertscore": 0.6505881740595214, "rouge_l": 0.13463604840276225}, {"bertscore": 0.6509551115159411, "rouge_l": 0.13671873267048706}, {"bertscore": 0.6500686373910867, "rouge_l": 0.13633716129727624}], "total": {"test_bertscore": 65.06046559981769, "test_bertscore_se": 0.18189323097115825, "test_rouge_l": 13.689165647436653, "test_rouge_l_se": 0.5591813718236159}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "cnn-dailymail", "task": "summarization", "dataset_languages": ["en"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"bertscore": 0.6885177949734498, "rouge_l": 0.22253061969992155}, {"bertscore": 0.6872336850792635, "rouge_l": 0.22006691326371441}, {"bertscore": 0.6869388751510996, "rouge_l": 0.21935083670068561}, {"bertscore": 0.6883319531334564, "rouge_l": 0.21760264526112405}, {"bertscore": 0.6873110717860982, "rouge_l": 0.21971534657942043}, {"bertscore": 0.6888445007498376, "rouge_l": 0.22075390817431398}, {"bertscore": 0.6889792676374782, "rouge_l": 0.22258789420354103}, {"bertscore": 0.6874578548304271, "rouge_l": 0.21715384183568526}, {"bertscore": 0.6885663571883924, "rouge_l": 0.21980771350188952}, {"bertscore": 0.6883579801651649, "rouge_l": 0.22145477456857537}], "total": {"test_bertscore": 68.80539340694668, "test_bertscore_se": 0.04597554775651053, "test_rouge_l": 22.010244937888714, "test_rouge_l_se": 0.11329794346316704}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "orange-sum", "task": "summarization", "dataset_languages": ["fr"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"bertscore": 0.6609689284814522, "rouge_l": 0.16297025293899445}, {"bertscore": 0.6593651455477811, "rouge_l": 0.1620234747807578}, {"bertscore": 0.6622487649437971, "rouge_l": 0.16843319191305733}, {"bertscore": 0.6590052655083127, "rouge_l": 0.16236746447814587}, {"bertscore": 0.6617912334040739, "rouge_l": 0.167662440412923}, {"bertscore": 0.6614647345850244, "rouge_l": 0.16317907971156093}, {"bertscore": 0.6607156118261628, "rouge_l": 0.16539961908186598}, {"bertscore": 0.658115422091214, "rouge_l": 0.16226865623839806}, {"bertscore": 0.6624153620214202, "rouge_l": 0.16353724391761}, {"bertscore": 0.6606513473379891, "rouge_l": 0.1630195754791388}], "total": {"test_bertscore": 66.06741815747228, "test_bertscore_se": 0.08890861130244969, "test_rouge_l": 16.40860998952452, "test_rouge_l_se": 0.14228649354249548}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "ilpost-sum", "task": "summarization", "dataset_languages": ["it"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"bertscore": 0.6531221457116771, "rouge_l": 0.16662260893860104}, {"bertscore": 0.651282125589205, "rouge_l": 0.1641647459726237}, {"bertscore": 0.6506415909389034, "rouge_l": 0.1606619233261743}, {"bertscore": 0.6515396770410007, "rouge_l": 0.16470403988766408}, {"bertscore": 0.6518366813688772, "rouge_l": 0.16140617471656546}, {"bertscore": 0.6522584993363125, "rouge_l": 0.1675275145092034}, {"bertscore": 0.6526844468025956, "rouge_l": 0.16206575685856145}, {"bertscore": 0.6547853728116024, "rouge_l": 0.16833467888994869}, {"bertscore": 0.652595917446888, "rouge_l": 0.16451460532866358}, {"bertscore": 0.6556163645436754, "rouge_l": 0.17115995010889481}], "total": {"test_bertscore": 65.26362821590737, "test_bertscore_se": 0.09587909039319449, "test_rouge_l": 16.511619985369006, "test_rouge_l_se": 0.20589471757166802}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "danske-talemaader", "task": "knowledge", "dataset_languages": ["da"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"mcc": 0.2088976566243679, "accuracy": 0.375}, {"mcc": 0.34277407476844896, "accuracy": 0.46905940594059403}, {"mcc": 0.25595374943068555, "accuracy": 0.40470297029702973}, {"mcc": 0.22193145523289443, "accuracy": 0.37004950495049505}, {"mcc": 0.15670256180559117, "accuracy": 0.36386138613861385}, {"mcc": 0.09521526078164112, "accuracy": 0.3180693069306931}, {"mcc": 0.24559802399652705, "accuracy": 0.40470297029702973}, {"mcc": 0.17307010917156507, "accuracy": 0.3341584158415842}, {"mcc": 0.24168088257362325, "accuracy": 0.3849009900990099}, {"mcc": 0.28736346126511314, "accuracy": 0.44183168316831684}], "total": {"test_mcc": 22.291872356504573, "test_mcc_se": 4.326394619441406, "test_accuracy": 38.663366336633665, "test_accuracy_se": 2.8388443686259865}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "danish-citizen-tests", "task": "knowledge", "dataset_languages": ["da"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"mcc": 0.2400970189568651, "accuracy": 0.4780952380952381}, {"mcc": 0.07390184625813942, "accuracy": 0.3238095238095238}, {"mcc": 0.23495033764660855, "accuracy": 0.37523809523809526}, {"mcc": 0.19783734211620238, "accuracy": 0.41523809523809524}, {"mcc": 0.40736058797185265, "accuracy": 0.5085714285714286}, {"mcc": 0.44116437941756226, "accuracy": 0.6076190476190476}, {"mcc": 0.4160889325777367, "accuracy": 0.5847619047619048}, {"mcc": 0.1235473533291602, "accuracy": 0.3219047619047619}, {"mcc": 0.26959786907100186, "accuracy": 0.47619047619047616}, {"mcc": 0.22053122826860386, "accuracy": 0.33904761904761904}], "total": {"test_mcc": 26.25076895613733, "test_mcc_se": 7.692319753574241, "test_accuracy": 44.30476190476191, "test_accuracy_se": 6.496238394944463}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "nrk-quiz-qa", "task": "knowledge", "dataset_languages": ["nb", "nn", "no"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"mcc": 0.19179719641995177, "accuracy": 0.3466796875}, {"mcc": 0.18387355742410202, "accuracy": 0.3662109375}, {"mcc": 0.09057910696017218, "accuracy": 0.31201171875}, {"mcc": 0.16398196358006187, "accuracy": 0.30712890625}, {"mcc": 0.14231639176493727, "accuracy": 0.37255859375}, {"mcc": 0.15455322331025717, "accuracy": 0.373046875}, {"mcc": 0.18873418866072375, "accuracy": 0.37841796875}, {"mcc": 0.2011137882849187, "accuracy": 0.400390625}, {"mcc": 0.19731332917149932, "accuracy": 0.3828125}, {"mcc": 0.09767275780009756, "accuracy": 0.322265625}], "total": {"test_mcc": 16.119355033767217, "test_mcc_se": 2.4910104519419543, "test_accuracy": 35.615234375, "test_accuracy_se": 2.0040456797456394}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-sv", "task": "knowledge", "dataset_languages": ["sv"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"mcc": 0.1256180813406201, "accuracy": 0.30712890625}, {"mcc": 0.1784818976723414, "accuracy": 0.3115234375}, {"mcc": 0.2062416894871472, "accuracy": 0.388671875}, {"mcc": 0.18552533392105872, "accuracy": 0.38916015625}, {"mcc": 0.11520814550268081, "accuracy": 0.30419921875}, {"mcc": 0.19591994566739993, "accuracy": 0.353515625}, {"mcc": 0.16452974721606337, "accuracy": 0.33935546875}, {"mcc": 0.0698214305110435, "accuracy": 0.28271484375}, {"mcc": 0.11082272423347728, "accuracy": 0.318359375}, {"mcc": 0.17828034992238506, "accuracy": 0.33349609375}], "total": {"test_mcc": 15.304493454742174, "test_mcc_se": 2.774292546132314, "test_accuracy": 33.28125, "test_accuracy_se": 2.207598182623193}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "icelandic-knowledge", "task": "knowledge", "dataset_languages": ["is"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"mcc": -0.052127440344746034, "accuracy": 0.2333984375}, {"mcc": 0.034557034526782555, "accuracy": 0.26953125}, {"mcc": 0.0, "accuracy": 0.2392578125}, {"mcc": -0.006978563628120095, "accuracy": 0.263671875}, {"mcc": 0.01771807100258048, "accuracy": 0.25}, {"mcc": 0.02188922465968068, "accuracy": 0.2705078125}, {"mcc": 0.023950772222679037, "accuracy": 0.2724609375}, {"mcc": 0.0, "accuracy": 0.2412109375}, {"mcc": 0.010350397451317554, "accuracy": 0.2587890625}, {"mcc": 0.0, "accuracy": 0.2578125}], "total": {"test_mcc": 0.49359495890174176, "test_mcc_se": 1.4884478327329989, "test_accuracy": 25.56640625, "test_accuracy_se": 0.8714533154887809}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-de", "task": "knowledge", "dataset_languages": ["de"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"mcc": 0.12823173172185684, "accuracy": 0.29833984375}, {"mcc": 0.25642343681882335, "accuracy": 0.43115234375}, {"mcc": 0.16919142964650005, "accuracy": 0.33935546875}, {"mcc": 0.16563102308746763, "accuracy": 0.3291015625}, {"mcc": 0.11685035673646124, "accuracy": 0.302734375}, {"mcc": 0.19610243527313032, "accuracy": 0.37158203125}, {"mcc": 0.15065284512778412, "accuracy": 0.3349609375}, {"mcc": 0.15531695659024708, "accuracy": 0.3515625}, {"mcc": 0.14691160812207477, "accuracy": 0.3291015625}, {"mcc": 0.19029488173687834, "accuracy": 0.3447265625}], "total": {"test_mcc": 16.756067048612238, "test_mcc_se": 2.463241610202199, "test_accuracy": 34.326171875, "test_accuracy_se": 2.332269426029369}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-nl", "task": "knowledge", "dataset_languages": ["nl"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"mcc": 0.14325629179949512, "accuracy": 0.29296875}, {"mcc": 0.23543725847615168, "accuracy": 0.40087890625}, {"mcc": 0.15446926992213345, "accuracy": 0.322265625}, {"mcc": 0.16587504148169882, "accuracy": 0.35546875}, {"mcc": 0.11485949015732057, "accuracy": 0.31298828125}, {"mcc": 0.16610088465103431, "accuracy": 0.3349609375}, {"mcc": 0.17741344583761404, "accuracy": 0.33251953125}, {"mcc": 0.14246812990158733, "accuracy": 0.283203125}, {"mcc": 0.11275577622529417, "accuracy": 0.30224609375}, {"mcc": 0.1632667938213887, "accuracy": 0.35986328125}], "total": {"test_mcc": 15.75902382273718, "test_mcc_se": 2.157616582980161, "test_accuracy": 32.9736328125, "test_accuracy_se": 2.193810768650319}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu", "task": "knowledge", "dataset_languages": ["en"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"mcc": 0.31658743454094584, "accuracy": 0.44677734375}, {"mcc": 0.1981033573349401, "accuracy": 0.36572265625}, {"mcc": 0.19765804338368465, "accuracy": 0.31591796875}, {"mcc": 0.26794898741323014, "accuracy": 0.44091796875}, {"mcc": 0.22058478820088942, "accuracy": 0.37353515625}, {"mcc": 0.12749466111746288, "accuracy": 0.31005859375}, {"mcc": 0.25994567208907443, "accuracy": 0.423828125}, {"mcc": 0.1517512070260746, "accuracy": 0.28369140625}, {"mcc": 0.11494352962721446, "accuracy": 0.31298828125}, {"mcc": 0.16463562404482004, "accuracy": 0.3212890625}], "total": {"test_mcc": 20.196533047783365, "test_mcc_se": 4.0415720199862255, "test_accuracy": 35.947265625, "test_accuracy_se": 3.7150622384522713}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-fr", "task": "knowledge", "dataset_languages": ["fr"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"mcc": 0.14319354841717596, "accuracy": 0.30126953125}, {"mcc": 0.20812399312831406, "accuracy": 0.37939453125}, {"mcc": 0.1837413410476204, "accuracy": 0.337890625}, {"mcc": 0.18193341780041078, "accuracy": 0.34619140625}, {"mcc": 0.18136448191708557, "accuracy": 0.35205078125}, {"mcc": 0.21651490880314844, "accuracy": 0.39208984375}, {"mcc": 0.20672057232888955, "accuracy": 0.38427734375}, {"mcc": 0.12235276585907583, "accuracy": 0.27978515625}, {"mcc": 0.24696355351165453, "accuracy": 0.4228515625}, {"mcc": 0.12096563243807804, "accuracy": 0.27880859375}], "total": {"test_mcc": 18.118742152514532, "test_mcc_se": 2.5714188212983995, "test_accuracy": 34.74609375, "test_accuracy_se": 3.035839629961259}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-it", "task": "knowledge", "dataset_languages": ["it"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"mcc": 0.1898997917794934, "accuracy": 0.3515625}, {"mcc": 0.21810152105150674, "accuracy": 0.3564453125}, {"mcc": 0.17151496080138628, "accuracy": 0.31787109375}, {"mcc": 0.1555660258867445, "accuracy": 0.34130859375}, {"mcc": 0.258285735957001, "accuracy": 0.4384765625}, {"mcc": 0.18907748977757258, "accuracy": 0.3330078125}, {"mcc": 0.23215976468564054, "accuracy": 0.39404296875}, {"mcc": 0.23063322621570942, "accuracy": 0.35888671875}, {"mcc": 0.09638780859497048, "accuracy": 0.27197265625}, {"mcc": 0.1622253743649678, "accuracy": 0.318359375}], "total": {"test_mcc": 19.03851699114993, "test_mcc_se": 2.9175987516542596, "test_accuracy": 34.8193359375, "test_accuracy_se": 2.798230560487562}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-da", "task": "common-sense-reasoning", "dataset_languages": ["da"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"mcc": 0.0, "accuracy": 0.23486328125}, {"mcc": 0.06296294436140419, "accuracy": 0.2646484375}, {"mcc": 0.04466000507111357, "accuracy": 0.25537109375}, {"mcc": 0.0, "accuracy": 0.23291015625}, {"mcc": 0.0, "accuracy": 0.24462890625}, {"mcc": 0.025391801107466663, "accuracy": 0.25146484375}, {"mcc": 0.0, "accuracy": 0.2548828125}, {"mcc": 0.012582887673237833, "accuracy": 0.24365234375}, {"mcc": 0.0, "accuracy": 0.23779296875}, {"mcc": 0.0, "accuracy": 0.26220703125}], "total": {"test_mcc": 1.4559763821322225, "test_mcc_se": 1.4076344987797795, "test_accuracy": 24.82421875, "test_accuracy_se": 0.6935377435868316}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "nor-common-sense-qa", "task": "common-sense-reasoning", "dataset_languages": ["nb", "nn", "no"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"mcc": 0.26230951298644056, "accuracy": 0.381903642773208}, {"mcc": 0.30242857948932084, "accuracy": 0.43243243243243246}, {"mcc": 0.2646405746038798, "accuracy": 0.35605170387779084}, {"mcc": 0.2869025881699478, "accuracy": 0.37485311398354876}, {"mcc": 0.23976487834297763, "accuracy": 0.3584018801410106}, {"mcc": 0.17038818534102582, "accuracy": 0.31139835487661577}, {"mcc": 0.25785563405783785, "accuracy": 0.37132784958871917}, {"mcc": 0.15802540789581698, "accuracy": 0.2796709753231492}, {"mcc": 0.3021590866827122, "accuracy": 0.41598119858989424}, {"mcc": 0.27870104846557375, "accuracy": 0.40423031727379555}], "total": {"test_mcc": 25.231754960355335, "test_mcc_se": 3.128564484361319, "test_accuracy": 36.86251468860164, "test_accuracy_se": 2.8668204968708237}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-sv", "task": "common-sense-reasoning", "dataset_languages": ["sv"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"mcc": 0.0914774665862669, "accuracy": 0.26513671875}, {"mcc": 0.010118668878064221, "accuracy": 0.2578125}, {"mcc": 0.04553715831505813, "accuracy": 0.24658203125}, {"mcc": 0.0, "accuracy": 0.2392578125}, {"mcc": 0.0, "accuracy": 0.26416015625}, {"mcc": 0.017853622098924018, "accuracy": 0.24658203125}, {"mcc": 0.0, "accuracy": 0.2353515625}, {"mcc": 0.0, "accuracy": 0.251953125}, {"mcc": -0.00028201940458873534, "accuracy": 0.2412109375}, {"mcc": 0.0, "accuracy": 0.25537109375}], "total": {"test_mcc": 1.6470489647372453, "test_mcc_se": 1.8651732073455274, "test_accuracy": 25.034179687500004, "test_accuracy_se": 0.6377097223252817}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "winogrande-is", "task": "common-sense-reasoning", "dataset_languages": ["is"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"mcc": 0.0, "accuracy": 0.4486607142857143}, {"mcc": 0.01698823971458752, "accuracy": 0.5625}, {"mcc": 0.0, "accuracy": 0.4341517857142857}, {"mcc": 0.0, "accuracy": 0.43638392857142855}, {"mcc": 0.0048590526036397476, "accuracy": 0.42857142857142855}, {"mcc": 0.0, "accuracy": 0.5479910714285714}, {"mcc": 0.0, "accuracy": 0.41629464285714285}, {"mcc": 0.0, "accuracy": 0.45982142857142855}, {"mcc": 0.0, "accuracy": 0.4095982142857143}, {"mcc": 0.0, "accuracy": 0.5580357142857143}], "total": {"test_mcc": 0.21847292318227265, "test_mcc_se": 0.33599394454915027, "test_accuracy": 47.02008928571429, "test_accuracy_se": 3.7877453681521853}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-de", "task": "common-sense-reasoning", "dataset_languages": ["de"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"mcc": 0.0, "accuracy": 0.2373046875}, {"mcc": 0.0, "accuracy": 0.2412109375}, {"mcc": 0.07163607897864668, "accuracy": 0.26416015625}, {"mcc": 0.0, "accuracy": 0.23291015625}, {"mcc": 0.0, "accuracy": 0.27392578125}, {"mcc": 0.0, "accuracy": 0.2646484375}, {"mcc": 0.0, "accuracy": 0.234375}, {"mcc": 0.0, "accuracy": 0.26513671875}, {"mcc": 0.020158175322786197, "accuracy": 0.24951171875}, {"mcc": 0.06339931288488965, "accuracy": 0.26611328125}], "total": {"test_mcc": 1.5519356718632253, "test_mcc_se": 1.7468633568988603, "test_accuracy": 25.29296875, "test_accuracy_se": 0.9613811733354223}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-nl", "task": "common-sense-reasoning", "dataset_languages": ["nl"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"mcc": 0.0, "accuracy": 0.2470703125}, {"mcc": 0.04191151136154642, "accuracy": 0.240234375}, {"mcc": 0.0081789325459812, "accuracy": 0.26171875}, {"mcc": 0.0, "accuracy": 0.25146484375}, {"mcc": 0.017579232506045827, "accuracy": 0.23681640625}, {"mcc": 0.0543546396770483, "accuracy": 0.27197265625}, {"mcc": 0.0, "accuracy": 0.248046875}, {"mcc": 0.009839253267251321, "accuracy": 0.2666015625}, {"mcc": 0.0, "accuracy": 0.2431640625}, {"mcc": 0.0, "accuracy": 0.22998046875}], "total": {"test_mcc": 1.318635693578731, "test_mcc_se": 1.2126220089155122, "test_accuracy": 24.970703125, "test_accuracy_se": 0.8331242718716863}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag", "task": "common-sense-reasoning", "dataset_languages": ["en"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"mcc": 0.05852081872067836, "accuracy": 0.25048828125}, {"mcc": 0.1779960142491021, "accuracy": 0.3505859375}, {"mcc": 0.0, "accuracy": 0.2451171875}, {"mcc": 0.0778531327783548, "accuracy": 0.271484375}, {"mcc": 0.11144737696822701, "accuracy": 0.2744140625}, {"mcc": 0.10819443728693326, "accuracy": 0.26171875}, {"mcc": 0.11450331631927137, "accuracy": 0.2724609375}, {"mcc": 0.13022771004778122, "accuracy": 0.2919921875}, {"mcc": 0.11435058697097925, "accuracy": 0.28076171875}, {"mcc": 0.08409794152154221, "accuracy": 0.26953125}], "total": {"test_mcc": 9.771913348628695, "test_mcc_se": 2.9235062573775323, "test_accuracy": 27.685546875, "test_accuracy_se": 1.815278594485372}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-fr", "task": "common-sense-reasoning", "dataset_languages": ["fr"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"mcc": 0.0, "accuracy": 0.23486328125}, {"mcc": 0.007780766507189119, "accuracy": 0.25390625}, {"mcc": 0.0, "accuracy": 0.26513671875}, {"mcc": 0.0, "accuracy": 0.2333984375}, {"mcc": 0.01834922009575427, "accuracy": 0.2607421875}, {"mcc": 0.0003741814154110259, "accuracy": 0.26708984375}, {"mcc": 0.008817705700935357, "accuracy": 0.22021484375}, {"mcc": 0.0, "accuracy": 0.2421875}, {"mcc": 0.018078541726049138, "accuracy": 0.240234375}, {"mcc": 0.048556104601112, "accuracy": 0.2705078125}], "total": {"test_mcc": 1.0195652004645093, "test_mcc_se": 0.9512995317848065, "test_accuracy": 24.8828125, "test_accuracy_se": 1.0548782620693526}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-it", "task": "common-sense-reasoning", "dataset_languages": ["it"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"mcc": 0.0908412146583628, "accuracy": 0.279296875}, {"mcc": -0.025578613467939083, "accuracy": 0.24169921875}, {"mcc": 0.0006524782581944044, "accuracy": 0.27392578125}, {"mcc": 0.0, "accuracy": 0.2451171875}, {"mcc": 0.0561312770440426, "accuracy": 0.28369140625}, {"mcc": 0.00020546189825470577, "accuracy": 0.23779296875}, {"mcc": 0.06767443410192493, "accuracy": 0.232421875}, {"mcc": 0.01350513381063921, "accuracy": 0.26708984375}, {"mcc": 0.034869907868493984, "accuracy": 0.2314453125}, {"mcc": 0.0, "accuracy": 0.2646484375}], "total": {"test_mcc": 2.3830129417197354, "test_mcc_se": 2.2966276632879223, "test_accuracy": 25.571289062500004, "test_accuracy_se": 1.2466778515672678}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "meta-llama/Llama-2-70b-chat-hf", "results": {"raw": [{"test_speed": 251.34, "test_speed_short": 28.24}, {"test_speed": 485.04, "test_speed_short": 52.8}, {"test_speed": 732.1700000000001, "test_speed_short": 102.66}, {"test_speed": 955.4, "test_speed_short": 125.28}, {"test_speed": 1228.5, "test_speed_short": 152.22}, {"test_speed": 1448.24, "test_speed_short": 202.35}, {"test_speed": 1679.22, "test_speed_short": 227.2}, {"test_speed": 1896.1799999999998, "test_speed_short": 251.34}, {"test_speed": 2075.9900000000002, "test_speed_short": 276.9}, {"test_speed": 2292.27, "test_speed_short": 300.05}], "total": {"test_speed": 1304.435, "test_speed_se": 429.31094371340373, "test_speed_short": 171.90399999999997, "test_speed_short_se": 58.407173622491115}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "swerec", "task": "sentiment-classification", "dataset_languages": ["sv"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"mcc": 0.6547326931279879, "macro_f1": 0.5651547477262077}, {"mcc": 0.5770066477647183, "macro_f1": 0.5343280887027824}, {"mcc": 0.695745306327939, "macro_f1": 0.5812815119035694}, {"mcc": 0.4711518612241441, "macro_f1": 0.49146113146113146}, {"mcc": 0.42633328211728055, "macro_f1": 0.45210859349903254}, {"mcc": 0.1309048402276982, "macro_f1": 0.2831022989604106}, {"mcc": 0.6826586918955837, "macro_f1": 0.5754062420098031}, {"mcc": 0.27692840495332355, "macro_f1": 0.36931943303094267}, {"mcc": 0.6477079005226881, "macro_f1": 0.5648321518593707}, {"mcc": 0.5133590044480393, "macro_f1": 0.5024433684617566}], "total": {"test_mcc": 50.76528632609403, "test_mcc_se": 11.59945910835034, "test_macro_f1": 49.194375676150074, "test_macro_f1_se": 6.12062159449565}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "angry-tweets", "task": "sentiment-classification", "dataset_languages": ["da"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"mcc": 0.163101106891645, "macro_f1": 0.3349140825999908}, {"mcc": 0.005898521498637211, "macro_f1": 0.19297564006307874}, {"mcc": 0.15809581319030525, "macro_f1": 0.31292185609324735}, {"mcc": 0.24156418044475644, "macro_f1": 0.37938895269978845}, {"mcc": 0.151215539801508, "macro_f1": 0.27860703189809816}, {"mcc": 0.09053717661764675, "macro_f1": 0.24393944043385118}, {"mcc": 0.2232014793736871, "macro_f1": 0.38172155046641204}, {"mcc": 0.28020207484185283, "macro_f1": 0.42605721168317306}, {"mcc": 0.18769001065141822, "macro_f1": 0.33620761165183094}, {"mcc": 0.05787120682047748, "macro_f1": 0.19313271022060904}], "total": {"test_mcc": 15.593771101319346, "test_mcc_se": 5.258844143760602, "test_macro_f1": 30.798660878100797, "test_macro_f1_se": 4.950086155440722}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "norec", "task": "sentiment-classification", "dataset_languages": ["nb", "nn", "no"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"mcc": 0.27049111021877065, "macro_f1": 0.3494977394319631}, {"mcc": 0.3983597454011184, "macro_f1": 0.4926265369797935}, {"mcc": 0.33121516440093607, "macro_f1": 0.48699319353671994}, {"mcc": 0.2846842990076604, "macro_f1": 0.36128196681617125}, {"mcc": 0.33199241590461653, "macro_f1": 0.38557505075339776}, {"mcc": 0.3199725079463099, "macro_f1": 0.3750833303678398}, {"mcc": 0.3390221395042556, "macro_f1": 0.36533315694166246}, {"mcc": 0.3245442018457646, "macro_f1": 0.37155825208828447}, {"mcc": 0.27603091561500637, "macro_f1": 0.3576928020247858}, {"mcc": 0.3167430281736914, "macro_f1": 0.41813491536260977}], "total": {"test_mcc": 31.9305552801813, "test_mcc_se": 2.3033808063040286, "test_macro_f1": 39.63776944303228, "test_macro_f1_se": 3.2682513531265083}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hotter-and-colder-sentiment", "task": "sentiment-classification", "dataset_languages": ["is"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"mcc": 0.0, "macro_f1": 0.17159581022797288}, {"mcc": 0.0, "macro_f1": 0.17590471827759965}, {"mcc": 0.0, "macro_f1": 0.18277627220716652}, {"mcc": 0.04510524862187941, "macro_f1": 0.18462163234897952}, {"mcc": 0.0, "macro_f1": 0.1783652385293224}, {"mcc": 0.0, "macro_f1": 0.1803601149947042}, {"mcc": 0.0, "macro_f1": 0.18058076225045372}, {"mcc": 0.03839969964076947, "macro_f1": 0.17985049269452938}, {"mcc": 0.0, "macro_f1": 0.1783652385293224}, {"mcc": 0.0, "macro_f1": 0.18364893297264803}], "total": {"test_mcc": 0.8350494826264886, "test_mcc_se": 1.0955199521515289, "test_macro_f1": 17.96069213032699, "test_macro_f1_se": 0.23922415759456686}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "sb10k", "task": "sentiment-classification", "dataset_languages": ["de"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"mcc": 0.34156224327230567, "macro_f1": 0.44259818555309205}, {"mcc": 0.4299310302356257, "macro_f1": 0.5694405486936608}, {"mcc": 0.46892893816339143, "macro_f1": 0.64198481736539}, {"mcc": 0.22169163135776024, "macro_f1": 0.34980557123728606}, {"mcc": 0.4083315770134553, "macro_f1": 0.5911745254459863}, {"mcc": 0.5421137025035583, "macro_f1": 0.6820417418146033}, {"mcc": 0.478610853562969, "macro_f1": 0.6269344500068339}, {"mcc": 0.38344276307238084, "macro_f1": 0.50392633736247}, {"mcc": 0.470556170606914, "macro_f1": 0.6364592367962268}, {"mcc": 0.49874335435650224, "macro_f1": 0.6699617768998554}], "total": {"test_mcc": 42.43912264144862, "test_mcc_se": 5.70889765263995, "test_macro_f1": 57.14327191175405, "test_macro_f1_se": 6.69137636265273}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "dutch-social", "task": "sentiment-classification", "dataset_languages": ["nl"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"mcc": 0.06404628030128806, "macro_f1": 0.1333431233849741}, {"mcc": -0.02396312385217439, "macro_f1": 0.20249446417010722}, {"mcc": 0.06433368549591921, "macro_f1": 0.3238432555961292}, {"mcc": -0.014673760073357812, "macro_f1": 0.10063063805996038}, {"mcc": 0.02626558843610337, "macro_f1": 0.1998614800563019}, {"mcc": 0.04358030018912398, "macro_f1": 0.16024334167531384}, {"mcc": 0.06179774069925674, "macro_f1": 0.1979358446335484}, {"mcc": 0.02872610090628946, "macro_f1": 0.2389566370961809}, {"mcc": -0.001222301548645808, "macro_f1": 0.09929589641795537}, {"mcc": 0.007858931897286857, "macro_f1": 0.09725399698940988}], "total": {"test_mcc": 2.5674944245108966, "test_mcc_se": 2.038442109242788, "test_macro_f1": 17.53858678079881, "test_macro_f1_se": 4.498178770553817}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "sst5", "task": "sentiment-classification", "dataset_languages": ["en"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"mcc": 0.06594322704078118, "macro_f1": 0.21576931095590535}, {"mcc": 0.35679966309288486, "macro_f1": 0.3914377076338118}, {"mcc": 0.09943072739870919, "macro_f1": 0.2182326022267285}, {"mcc": 0.10845338194418346, "macro_f1": 0.2307675125802118}, {"mcc": 0.16406830818379928, "macro_f1": 0.25976580151668266}, {"mcc": 0.14791101239985674, "macro_f1": 0.2505350418690984}, {"mcc": 0.11764389749500254, "macro_f1": 0.23597524139638526}, {"mcc": 0.06600868247031882, "macro_f1": 0.20582472678169347}, {"mcc": 0.1452556041493528, "macro_f1": 0.25329636615685297}, {"mcc": 0.5240327771587976, "macro_f1": 0.5229352425431556}], "total": {"test_mcc": 17.955472813336865, "test_mcc_se": 9.090657644070502, "test_macro_f1": 27.84539553660526, "test_macro_f1_se": 6.23950312947761}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fosent", "task": "sentiment-classification", "dataset_languages": ["fo"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"mcc": 0.09514828017475677, "macro_f1": 0.1692020971017836}, {"mcc": 0.2667597819309075, "macro_f1": 0.3427510558931542}, {"mcc": 0.3171546867826197, "macro_f1": 0.4110873293029427}, {"mcc": 0.05052600592254295, "macro_f1": 0.15078901227352426}, {"mcc": 0.13383450174065925, "macro_f1": 0.23368831087236516}, {"mcc": 0.0, "macro_f1": 0.15567765567765568}, {"mcc": 0.0, "macro_f1": 0.14124293785310735}, {"mcc": 0.049460894982076654, "macro_f1": 0.1503725538360763}, {"mcc": 0.03473653185800012, "macro_f1": 0.1566083576287658}, {"mcc": 0.24017440235939994, "macro_f1": 0.34814319747756645}], "total": {"test_mcc": 11.87795085750963, "test_mcc_se": 7.201993461765871, "test_macro_f1": 22.595625079169412, "test_macro_f1_se": 6.3481874555319475}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "allocine", "task": "sentiment-classification", "dataset_languages": ["fr"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"mcc": 0.3016990383376737, "macro_f1": 0.5093087230455231}, {"mcc": 0.3208091271763906, "macro_f1": 0.5802126061633265}, {"mcc": 0.5797614900305894, "macro_f1": 0.7456709956709957}, {"mcc": 0.4515212815954516, "macro_f1": 0.6491045849846118}, {"mcc": 0.2085623548773021, "macro_f1": 0.4328612849917217}, {"mcc": 0.1881821090955436, "macro_f1": 0.41307814992025516}, {"mcc": 0.3476602022593752, "macro_f1": 0.551032350244173}, {"mcc": 0.18480978937898243, "macro_f1": 0.4105263157894737}, {"mcc": 0.17709244863673204, "macro_f1": 0.4091569455874214}, {"mcc": 0.6370317171737411, "macro_f1": 0.7835566062516417}], "total": {"test_mcc": 33.97129558561782, "test_mcc_se": 10.35055930712374, "test_macro_f1": 54.84508562649143, "test_macro_f1_se": 8.691100117232194}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "sentipolc16", "task": "sentiment-classification", "dataset_languages": ["it"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"mcc": 0.360521718387707, "macro_f1": 0.51643550759629}, {"mcc": 0.05436212000874624, "macro_f1": 0.2093184177142198}, {"mcc": 0.18407508365419556, "macro_f1": 0.2828830457896145}, {"mcc": 0.11246790042247683, "macro_f1": 0.220400372517809}, {"mcc": 0.36081506051969175, "macro_f1": 0.4857212194222595}, {"mcc": 0.15668859853888983, "macro_f1": 0.2854093258269322}, {"mcc": 0.18532071514685577, "macro_f1": 0.2991724598996653}, {"mcc": 0.01712432908471354, "macro_f1": 0.17038029892943954}, {"mcc": 0.44782575401200775, "macro_f1": 0.595379957518437}, {"mcc": 0.24954084804267898, "macro_f1": 0.3447031587097902}], "total": {"test_mcc": 21.287421278179632, "test_mcc_se": 8.728882625227271, "test_macro_f1": 34.098037639244566, "test_macro_f1_se": 8.902072663565127}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "suc3", "task": "named-entity-recognition", "dataset_languages": ["sv"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"micro_f1_no_misc": 0.5349794238683128, "micro_f1": 0.29021430415698424}, {"micro_f1_no_misc": 0.5727495240685341, "micro_f1": 0.47530481504443073}, {"micro_f1_no_misc": 0.5463367297428433, "micro_f1": 0.38823529411764707}, {"micro_f1_no_misc": 0.5669064748201439, "micro_f1": 0.3831381733021077}, {"micro_f1_no_misc": 0.5594440307768678, "micro_f1": 0.4374420759962928}, {"micro_f1_no_misc": 0.5651201788708776, "micro_f1": 0.41576086956521746}, {"micro_f1_no_misc": 0.49042675893886967, "micro_f1": 0.32046551234743115}, {"micro_f1_no_misc": 0.5426695842450765, "micro_f1": 0.38969225064886914}, {"micro_f1_no_misc": 0.5210171250648676, "micro_f1": 0.3920983973164368}, {"micro_f1_no_misc": 0.5641690432978999, "micro_f1": 0.4095526644091962}], "total": {"test_micro_f1_no_misc": 54.63818873694293, "test_micro_f1_no_misc_se": 1.583276736601568, "test_micro_f1": 39.01904356904613, "test_micro_f1_se": 3.29400244362851}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "dansk", "task": "named-entity-recognition", "dataset_languages": ["da"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"micro_f1_no_misc": 0.6160635481023831, "micro_f1": 0.3849281648143128}, {"micro_f1_no_misc": 0.5793528505392912, "micro_f1": 0.34922458357265945}, {"micro_f1_no_misc": 0.5747368421052632, "micro_f1": 0.4485566247224278}, {"micro_f1_no_misc": 0.6051873198847262, "micro_f1": 0.4254385964912281}, {"micro_f1_no_misc": 0.5132275132275133, "micro_f1": 0.2667531344574146}, {"micro_f1_no_misc": 0.5291055371509702, "micro_f1": 0.3397790055248619}, {"micro_f1_no_misc": 0.5863247863247865, "micro_f1": 0.39166209544706526}, {"micro_f1_no_misc": 0.6150907354345749, "micro_f1": 0.44365053588827547}, {"micro_f1_no_misc": 0.5947219604147032, "micro_f1": 0.409105082631743}, {"micro_f1_no_misc": 0.5787720133269871, "micro_f1": 0.3068453805648636}], "total": {"test_micro_f1_no_misc": 57.925831065111986, "test_micro_f1_no_misc_se": 2.1166404143780873, "test_micro_f1": 37.65943204114852, "test_micro_f1_se": 3.721637146369611}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "norne-nb", "task": "named-entity-recognition", "dataset_languages": ["nb", "no"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"micro_f1_no_misc": 0.6769574111958342, "micro_f1": 0.5506790564689064}, {"micro_f1_no_misc": 0.593646936295782, "micro_f1": 0.48164582492940694}, {"micro_f1_no_misc": 0.6637521383767345, "micro_f1": 0.5078965758211041}, {"micro_f1_no_misc": 0.6711515631183222, "micro_f1": 0.5425642549398306}, {"micro_f1_no_misc": 0.6547892720306514, "micro_f1": 0.47830176004299335}, {"micro_f1_no_misc": 0.6639877535399924, "micro_f1": 0.46578249336870026}, {"micro_f1_no_misc": 0.6290731707317073, "micro_f1": 0.48057051375345655}, {"micro_f1_no_misc": 0.6072644721906925, "micro_f1": 0.5102806543598979}, {"micro_f1_no_misc": 0.6602941176470588, "micro_f1": 0.5494537939179215}, {"micro_f1_no_misc": 0.6515570296664825, "micro_f1": 0.5}], "total": {"test_micro_f1_no_misc": 64.72473864793258, "test_micro_f1_no_misc_se": 1.7352557960156847, "test_micro_f1": 50.67174927602218, "test_micro_f1_se": 1.948926626721684}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "norne-nn", "task": "named-entity-recognition", "dataset_languages": ["nn"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"micro_f1_no_misc": 0.6915676287492926, "micro_f1": 0.607334525939177}, {"micro_f1_no_misc": 0.641160471441523, "micro_f1": 0.4943535084380154}, {"micro_f1_no_misc": 0.5967246673490276, "micro_f1": 0.4293557422969188}, {"micro_f1_no_misc": 0.6646132785763177, "micro_f1": 0.552006552006552}, {"micro_f1_no_misc": 0.6645468998410176, "micro_f1": 0.5839416058394162}, {"micro_f1_no_misc": 0.6487262289199855, "micro_f1": 0.4105960264900662}, {"micro_f1_no_misc": 0.6412374758305501, "micro_f1": 0.5498909487459105}, {"micro_f1_no_misc": 0.6500650436721799, "micro_f1": 0.5128782547501759}, {"micro_f1_no_misc": 0.6507689649213755, "micro_f1": 0.49561674280775403}, {"micro_f1_no_misc": 0.6626270523846755, "micro_f1": 0.5017940199335548}], "total": {"test_micro_f1_no_misc": 65.12037711685946, "test_micro_f1_no_misc_se": 1.503146358582708, "test_micro_f1": 51.377679272475405, "test_micro_f1_se": 3.8688437897878023}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mim-gold-ner", "task": "named-entity-recognition", "dataset_languages": ["is"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"micro_f1_no_misc": 0.5683297180043384, "micro_f1": 0.4243378668575519}, {"micro_f1_no_misc": 0.5012626262626263, "micro_f1": 0.25196701109109865}, {"micro_f1_no_misc": 0.5972088546679499, "micro_f1": 0.45259938837920494}, {"micro_f1_no_misc": 0.5016453382084095, "micro_f1": 0.27421313159615207}, {"micro_f1_no_misc": 0.5326612056290695, "micro_f1": 0.4054743056487321}, {"micro_f1_no_misc": 0.5099709583736689, "micro_f1": 0.4330199764982373}, {"micro_f1_no_misc": 0.5568752855185016, "micro_f1": 0.4854651162790698}, {"micro_f1_no_misc": 0.5945404457801151, "micro_f1": 0.4367961934972244}, {"micro_f1_no_misc": 0.5903948772678762, "micro_f1": 0.43011635865845316}, {"micro_f1_no_misc": 0.49120703437250196, "micro_f1": 0.3104125736738703}], "total": {"test_micro_f1_no_misc": 54.440963440850574, "test_micro_f1_no_misc_se": 2.612359301664445, "test_micro_f1": 39.04401922179595, "test_micro_f1_se": 5.013781697369944}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fone", "task": "named-entity-recognition", "dataset_languages": ["fo"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"micro_f1_no_misc": 0.7396140317520677, "micro_f1": 0.6690451919809719}, {"micro_f1_no_misc": 0.736111111111111, "micro_f1": 0.7203107658157604}, {"micro_f1_no_misc": 0.7180072257083095, "micro_f1": 0.6788573354681048}, {"micro_f1_no_misc": 0.6596996753246753, "micro_f1": 0.5954811715481172}, {"micro_f1_no_misc": 0.7020311575626109, "micro_f1": 0.6851648351648352}, {"micro_f1_no_misc": 0.7173228346456693, "micro_f1": 0.7006310319227914}, {"micro_f1_no_misc": 0.6997437211686315, "micro_f1": 0.6600429645542428}, {"micro_f1_no_misc": 0.7464747641738791, "micro_f1": 0.6979129819596747}, {"micro_f1_no_misc": 0.7146085884528457, "micro_f1": 0.685453716370269}, {"micro_f1_no_misc": 0.7297379338257614, "micro_f1": 0.693993061895198}], "total": {"test_micro_f1_no_misc": 71.63351043725562, "test_micro_f1_no_misc_se": 1.561895102328045, "test_micro_f1": 67.86893056679966, "test_micro_f1_se": 2.0936806991234973}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "germeval", "task": "named-entity-recognition", "dataset_languages": ["de"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"micro_f1_no_misc": 0.6461858890913825, "micro_f1": 0.48719326526956835}, {"micro_f1_no_misc": 0.6593294087535193, "micro_f1": 0.5610574077727363}, {"micro_f1_no_misc": 0.6242069302098585, "micro_f1": 0.4416904988878297}, {"micro_f1_no_misc": 0.6158250910983863, "micro_f1": 0.41433265529651075}, {"micro_f1_no_misc": 0.6067853170189099, "micro_f1": 0.4432002889651436}, {"micro_f1_no_misc": 0.6215883943939021, "micro_f1": 0.45370370370370366}, {"micro_f1_no_misc": 0.6208971553610504, "micro_f1": 0.4328989080662205}, {"micro_f1_no_misc": 0.6655692729766804, "micro_f1": 0.4828471840029352}, {"micro_f1_no_misc": 0.6510249444307236, "micro_f1": 0.45874312385397564}, {"micro_f1_no_misc": 0.6141011840688912, "micro_f1": 0.3787040052100293}], "total": {"test_micro_f1_no_misc": 63.255135874033044, "test_micro_f1_no_misc_se": 1.2975079631123017, "test_micro_f1": 45.54371041028653, "test_micro_f1_se": 3.019048490988425}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "conll-nl", "task": "named-entity-recognition", "dataset_languages": ["nl"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"micro_f1_no_misc": 0.6565656565656566, "micro_f1": 0.49192405780674414}, {"micro_f1_no_misc": 0.5936254980079682, "micro_f1": 0.4763897190675433}, {"micro_f1_no_misc": 0.5702445089100705, "micro_f1": 0.42162429029869164}, {"micro_f1_no_misc": 0.5719643694327239, "micro_f1": 0.4518140589569161}, {"micro_f1_no_misc": 0.5646017699115045, "micro_f1": 0.4292969794172682}, {"micro_f1_no_misc": 0.6410856340664484, "micro_f1": 0.4787356321839081}, {"micro_f1_no_misc": 0.6216087577344123, "micro_f1": 0.47195582679453657}, {"micro_f1_no_misc": 0.5422093584177521, "micro_f1": 0.4303571428571428}, {"micro_f1_no_misc": 0.6002034587995931, "micro_f1": 0.4164691943127962}, {"micro_f1_no_misc": 0.6189701897018971, "micro_f1": 0.4941176470588235}], "total": {"test_micro_f1_no_misc": 59.81079201548026, "test_micro_f1_no_misc_se": 2.2616626123282275, "test_micro_f1": 45.62684548754371, "test_micro_f1_se": 1.8551116516753192}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "conll-en", "task": "named-entity-recognition", "dataset_languages": ["en"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"micro_f1_no_misc": 0.7111691967427365, "micro_f1": 0.6524198617221872}, {"micro_f1_no_misc": 0.7637482628548741, "micro_f1": 0.6859138136603645}, {"micro_f1_no_misc": 0.7718339464004205, "micro_f1": 0.6858606376402013}, {"micro_f1_no_misc": 0.7199297629499561, "micro_f1": 0.6445188428191261}, {"micro_f1_no_misc": 0.7453694280480359, "micro_f1": 0.6989197673345027}, {"micro_f1_no_misc": 0.7276401148569606, "micro_f1": 0.6671657153551043}, {"micro_f1_no_misc": 0.7070848782443132, "micro_f1": 0.6636940490481907}, {"micro_f1_no_misc": 0.7367122442685309, "micro_f1": 0.6857659286037053}, {"micro_f1_no_misc": 0.7421074210742107, "micro_f1": 0.6960985626283368}, {"micro_f1_no_misc": 0.7499508937340404, "micro_f1": 0.6936445731811212}], "total": {"test_micro_f1_no_misc": 73.75546149174079, "test_micro_f1_no_misc_se": 1.327872361651156, "test_micro_f1": 67.74001751992841, "test_micro_f1_se": 1.1847602296369621}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "eltec", "task": "named-entity-recognition", "dataset_languages": ["fr"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"micro_f1_no_misc": 0.5814525139664805, "micro_f1": 0.49952945605119514}, {"micro_f1_no_misc": 0.5794947994056464, "micro_f1": 0.5634194831013917}, {"micro_f1_no_misc": 0.5760702524698134, "micro_f1": 0.5416503715291358}, {"micro_f1_no_misc": 0.6338394793926247, "micro_f1": 0.5381750465549349}, {"micro_f1_no_misc": 0.636939685519831, "micro_f1": 0.5858458961474037}, {"micro_f1_no_misc": 0.5842696629213482, "micro_f1": 0.5413651476628203}, {"micro_f1_no_misc": 0.6335676625659051, "micro_f1": 0.56}, {"micro_f1_no_misc": 0.530779164873009, "micro_f1": 0.48692620569436373}, {"micro_f1_no_misc": 0.5410321489001693, "micro_f1": 0.46933136857616503}, {"micro_f1_no_misc": 0.5573214664123756, "micro_f1": 0.4561943874058863}], "total": {"test_micro_f1_no_misc": 58.547668364272035, "test_micro_f1_no_misc_se": 2.3738495171894733, "test_micro_f1": 52.42437362723295, "test_micro_f1_se": 2.697544143373193}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "multinerd-it", "task": "named-entity-recognition", "dataset_languages": ["it"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"micro_f1_no_misc": 0.7085189685432698, "micro_f1": 0.509831793413883}, {"micro_f1_no_misc": 0.7348059701492536, "micro_f1": 0.630652824212564}, {"micro_f1_no_misc": 0.7007392301809839, "micro_f1": 0.5938375350140056}, {"micro_f1_no_misc": 0.665428495622181, "micro_f1": 0.5733407696597881}, {"micro_f1_no_misc": 0.7540134645261523, "micro_f1": 0.6307121527463578}, {"micro_f1_no_misc": 0.7530562347188263, "micro_f1": 0.6508580918988743}, {"micro_f1_no_misc": 0.7421931735657226, "micro_f1": 0.5811743685535612}, {"micro_f1_no_misc": 0.7241889890967829, "micro_f1": 0.6248633608267911}, {"micro_f1_no_misc": 0.7100126742712294, "micro_f1": 0.579590302574704}, {"micro_f1_no_misc": 0.7552195824334054, "micro_f1": 0.6342906875543951}], "total": {"test_micro_f1_no_misc": 72.48176783107807, "test_micro_f1_no_misc_se": 1.7973005974643648, "test_micro_f1": 60.09151886454924, "test_micro_f1_se": 2.605563113284689}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-sv", "task": "linguistic-acceptability", "dataset_languages": ["sv"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"mcc": 0.0, "macro_f1": 0.3322464949462015}, {"mcc": 0.0, "macro_f1": 0.33956788132860366}, {"mcc": 0.0, "macro_f1": 0.3289646133682831}, {"mcc": 0.0, "macro_f1": 0.3359273670557717}, {"mcc": 0.0, "macro_f1": 0.33678756476683935}, {"mcc": 0.0, "macro_f1": 0.3274220032840722}, {"mcc": 0.0, "macro_f1": 0.33376707872478856}, {"mcc": 0.0, "macro_f1": 0.33914165859954826}, {"mcc": 0.0, "macro_f1": 0.3247609627431586}, {"mcc": 0.0, "macro_f1": 0.330718954248366}], "total": {"test_mcc": 0.0, "test_mcc_se": 0.0, "test_macro_f1": 33.29304579065633, "test_macro_f1_se": 0.31015328854355584}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-da", "task": "linguistic-acceptability", "dataset_languages": ["da"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"mcc": 0.0, "macro_f1": 0.3382875605815832}, {"mcc": 0.0, "macro_f1": 0.32520593080724874}, {"mcc": 0.0, "macro_f1": 0.3335502766026684}, {"mcc": -0.022715341972280156, "macro_f1": 0.3269799539927703}, {"mcc": 0.0, "macro_f1": 0.33093760209082}, {"mcc": 0.0, "macro_f1": 0.3331162487788994}, {"mcc": 0.0, "macro_f1": 0.33006215243702974}, {"mcc": 0.0, "macro_f1": 0.3372168284789644}, {"mcc": 0.0, "macro_f1": 0.3344166395840104}, {"mcc": 0.0, "macro_f1": 0.326758711374096}], "total": {"test_mcc": -0.22715341972280156, "test_mcc_se": 0.44522070265669106, "test_macro_f1": 33.1653190472809, "test_macro_f1_se": 0.2762223114483204}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-nb", "task": "linguistic-acceptability", "dataset_languages": ["nb", "no"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"mcc": 0.0, "macro_f1": 0.32830436208592984}, {"mcc": 0.0, "macro_f1": 0.335280753002272}, {"mcc": 0.0, "macro_f1": 0.33202870189171557}, {"mcc": 0.0, "macro_f1": 0.335280753002272}, {"mcc": 0.0, "macro_f1": 0.32653732324893125}, {"mcc": 0.0, "macro_f1": 0.33678756476683935}, {"mcc": 0.0, "macro_f1": 0.33571196886149857}, {"mcc": 0.0, "macro_f1": 0.33506493506493507}, {"mcc": 0.0, "macro_f1": 0.3322464949462015}, {"mcc": 0.0, "macro_f1": 0.3331162487788994}], "total": {"test_mcc": 0.0, "test_mcc_se": 0.0, "test_macro_f1": 33.30359105649494, "test_macro_f1_se": 0.20834579915016932}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-nn", "task": "linguistic-acceptability", "dataset_languages": ["nn"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"mcc": 0.0, "macro_f1": 0.33159268929503916}, {"mcc": 0.0, "macro_f1": 0.32275132275132273}, {"mcc": 0.0, "macro_f1": 0.3324641460234681}, {"mcc": 0.0, "macro_f1": 0.3247609627431586}, {"mcc": 0.0, "macro_f1": 0.3342002600780234}, {"mcc": 0.0, "macro_f1": 0.32520593080724874}, {"mcc": 0.0, "macro_f1": 0.32940406024885394}, {"mcc": 0.0, "macro_f1": 0.3326816552623004}, {"mcc": 0.0, "macro_f1": 0.33202870189171557}, {"mcc": 0.031052268087937544, "macro_f1": 0.3339635325160971}], "total": {"test_mcc": 0.3105226808793754, "test_mcc_se": 0.6086244545235758, "test_macro_f1": 32.990532616172274, "test_macro_f1_se": 0.2584648112839468}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-is", "task": "linguistic-acceptability", "dataset_languages": ["is"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"mcc": 0.0, "macro_f1": 0.342957972409368}, {"mcc": 0.0, "macro_f1": 0.33050016345210853}, {"mcc": 0.0, "macro_f1": 0.3331162487788994}, {"mcc": 0.0, "macro_f1": 0.3380736910148675}, {"mcc": 0.0, "macro_f1": 0.3402061855670103}, {"mcc": 0.0, "macro_f1": 0.3247609627431586}, {"mcc": 0.0, "macro_f1": 0.3372168284789644}, {"mcc": 0.0, "macro_f1": 0.34169077467052394}, {"mcc": 0.0, "macro_f1": 0.33289902280130296}, {"mcc": 0.0, "macro_f1": 0.32830436208592984}], "total": {"test_mcc": 0.0, "test_mcc_se": 0.0, "test_macro_f1": 33.49726212002133, "test_macro_f1_se": 0.37385008209322423}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-fo", "task": "linguistic-acceptability", "dataset_languages": ["fo"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"mcc": 0.0, "macro_f1": 0.330718954248366}, {"mcc": 0.0, "macro_f1": 0.3342002600780234}, {"mcc": 0.029949501262289895, "macro_f1": 0.3257290399095688}, {"mcc": 0.0, "macro_f1": 0.3324641460234681}, {"mcc": 0.0, "macro_f1": 0.3397807865892972}, {"mcc": 0.027292436725484488, "macro_f1": 0.36569067860679044}, {"mcc": -0.0365370047865803, "macro_f1": 0.3513374815575085}, {"mcc": 0.0, "macro_f1": 0.3333333333333333}, {"mcc": 0.0, "macro_f1": 0.3406310367031552}, {"mcc": 0.041798864382685944, "macro_f1": 0.32461347381044153}], "total": {"test_mcc": 0.6250379758388003, "test_mcc_se": 1.3600126384913975, "test_macro_f1": 33.78499190859952, "test_macro_f1_se": 0.774187887300399}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-de", "task": "linguistic-acceptability", "dataset_languages": ["de"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"mcc": 0.02231939254931135, "macro_f1": 0.33659054462197197}, {"mcc": 0.0, "macro_f1": 0.3302812295618051}, {"mcc": 0.0, "macro_f1": 0.3359273670557717}, {"mcc": 0.0, "macro_f1": 0.3247609627431586}, {"mcc": 0.0, "macro_f1": 0.33506493506493507}, {"mcc": 0.03225806451612903, "macro_f1": 0.34243760434950127}, {"mcc": 0.01764573687642094, "macro_f1": 0.33544283858252333}, {"mcc": 0.0, "macro_f1": 0.33785968315551246}, {"mcc": 0.022037824891242577, "macro_f1": 0.3337628539105819}, {"mcc": 0.0, "macro_f1": 0.3324641460234681}], "total": {"test_mcc": 0.9426101883310392, "test_mcc_se": 0.7859743370331219, "test_macro_f1": 33.4459216506923, "test_macro_f1_se": 0.29161451694104334}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-nl", "task": "linguistic-acceptability", "dataset_languages": ["nl"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"mcc": 0.0, "macro_f1": 0.3354964308890331}, {"mcc": 0.0, "macro_f1": 0.3302812295618051}, {"mcc": 0.0, "macro_f1": 0.3359273670557717}, {"mcc": 0.0, "macro_f1": 0.3247609627431586}, {"mcc": 0.0, "macro_f1": 0.33506493506493507}, {"mcc": 0.0, "macro_f1": 0.3402061855670103}, {"mcc": 0.0, "macro_f1": 0.33159268929503916}, {"mcc": 0.0, "macro_f1": 0.33785968315551246}, {"mcc": 0.0, "macro_f1": 0.3326816552623004}, {"mcc": 0.0, "macro_f1": 0.3324641460234681}], "total": {"test_mcc": 0.0, "test_mcc_se": 0.0, "test_macro_f1": 33.36335284618034, "test_macro_f1_se": 0.26797827056120405}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-en", "task": "linguistic-acceptability", "dataset_languages": ["en"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"mcc": -0.02452793116235671, "macro_f1": 0.33659301020042526}, {"mcc": 0.021802339034638404, "macro_f1": 0.3313517344342636}, {"mcc": -0.021844970231010515, "macro_f1": 0.33571196886149857}, {"mcc": -0.0315896781749236, "macro_f1": 0.32491255151668075}, {"mcc": -0.028681046287011288, "macro_f1": 0.3451082476029639}, {"mcc": -0.007523774028259369, "macro_f1": 0.34687088929242893}, {"mcc": 0.0, "macro_f1": 0.33159268929503916}, {"mcc": 0.02900304517806416, "macro_f1": 0.36325869958863166}, {"mcc": 0.0, "macro_f1": 0.3326816552623004}, {"mcc": 0.0, "macro_f1": 0.3324641460234681}], "total": {"test_mcc": -0.6336201567085892, "test_mcc_se": 1.2854212718360531, "test_macro_f1": 33.80545592077701, "test_macro_f1_se": 0.6820598329640268}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-fr", "task": "linguistic-acceptability", "dataset_languages": ["fr"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"mcc": 0.012323234755132747, "macro_f1": 0.35301964350161474}, {"mcc": 0.017641607644874314, "macro_f1": 0.37293716999406384}, {"mcc": 0.09384846653970899, "macro_f1": 0.4867337252414662}, {"mcc": 0.09018758630092101, "macro_f1": 0.3751046821469357}, {"mcc": 0.027393666209971773, "macro_f1": 0.4955858564230647}, {"mcc": 0.12632560835909626, "macro_f1": 0.506059593548506}, {"mcc": 0.11627985501553306, "macro_f1": 0.555625351393406}, {"mcc": 0.06275550984933574, "macro_f1": 0.5073159683354704}, {"mcc": 0.15679010294555318, "macro_f1": 0.5590532735988315}, {"mcc": 0.0740422661167124, "macro_f1": 0.4783083777402485}], "total": {"test_mcc": 7.775879037368395, "test_mcc_se": 3.0062814796298714, "test_macro_f1": 46.89743641923608, "test_macro_f1_se": 4.668424121284154}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-it", "task": "linguistic-acceptability", "dataset_languages": ["it"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"mcc": -0.02231939254931135, "macro_f1": 0.33093760209082}, {"mcc": -0.0173292044570927, "macro_f1": 0.33768278965129356}, {"mcc": 0.022147876666601694, "macro_f1": 0.34771100193450394}, {"mcc": 0.00998597379124145, "macro_f1": 0.41919139241422887}, {"mcc": -0.013891746699270286, "macro_f1": 0.3493922998894868}, {"mcc": 0.021422243704878722, "macro_f1": 0.3273692393087279}, {"mcc": 0.038601747521714164, "macro_f1": 0.33833567801629294}, {"mcc": 0.0072153852457266295, "macro_f1": 0.3479349186483104}, {"mcc": 0.008773406929651815, "macro_f1": 0.36295369211514394}, {"mcc": 0.0679319974877263, "macro_f1": 0.36516753625488524}], "total": {"test_mcc": 1.2253828764186643, "test_mcc_se": 1.7002892152553062, "test_macro_f1": 35.266761503236935, "test_macro_f1_se": 1.6382011450167662}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"f1": 0.07575757575757576, "em": 0.0}, {"f1": 0.0758150113722517, "em": 0.0}, {"f1": 0.06798756798756798, "em": 0.0}, {"f1": 0.076103500761035, "em": 0.0}, {"f1": 0.04818812644564379, "em": 0.0}, {"f1": 0.05725190839694656, "em": 0.0}, {"f1": 0.04875195007800312, "em": 0.0}, {"f1": 0.08844339622641509, "em": 0.0}, {"f1": 0.01934984520123839, "em": 0.0}, {"f1": 0.38451646090534974, "em": 0.15432098765432098}], "total": {"test_f1": 0.09421653431320272, "test_f1_se": 0.0643988561631681, "test_em": 0.015432098765432098, "test_em_se": 0.030246913580246913}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "norquad", "task": "reading-comprehension", "dataset_languages": ["nb", "nn", "no"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"f1": 11.70762800002718, "em": 0.32679738562091504}, {"f1": 2.784854130895435, "em": 0.08130081300813008}, {"f1": 5.407953417274518, "em": 0.16652789342214822}, {"f1": 2.7171505835195617, "em": 0.24529844644317253}, {"f1": 10.146397462855328, "em": 0.24813895781637718}, {"f1": 1.0037586459415995, "em": 0.0823045267489712}, {"f1": 0.3631985197123973, "em": 0.0}, {"f1": 5.259672604134618, "em": 0.6756756756756757}, {"f1": 5.007142019658011, "em": 0.16611295681063123}, {"f1": 1.1583510491592859, "em": 0.0}], "total": {"test_f1": 4.555610643317793, "test_f1_se": 2.374425651404611, "test_em": 0.1992156655546021, "test_em_se": 0.12342960996919722}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scandiqa-sv", "task": "reading-comprehension", "dataset_languages": ["sv"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"f1": 0.05817099567099568, "em": 0.0}, {"f1": 0.20352720314812806, "em": 0.0758150113722517}, {"f1": 0.34900284900284895, "em": 0.1554001554001554}, {"f1": 0.3774371240124665, "em": 0.076103500761035}, {"f1": 0.9452215735947422, "em": 0.3855050115651503}, {"f1": 0.08587786259541985, "em": 0.0}, {"f1": 1.6524589555010774, "em": 1.24804992199688}, {"f1": 0.1913933812518718, "em": 0.07861635220125786}, {"f1": 0.23358027421494912, "em": 0.15479876160990713}, {"f1": 0.8712705761316871, "em": 0.5401234567901234}], "total": {"test_f1": 0.49679407951241866, "test_f1_se": 0.31487470333931666, "test_em": 0.2714412171696761, "test_em_se": 0.23799676857787308}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "nqii", "task": "reading-comprehension", "dataset_languages": ["is"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"f1": 2.77266202767875, "em": 1.2307692307692308}, {"f1": 0.05192107995846314, "em": 0.0}, {"f1": 2.819252042726479, "em": 0.6230529595015576}, {"f1": 12.260450927154618, "em": 5.680119581464873}, {"f1": 12.496490073021445, "em": 4.702194357366771}, {"f1": 0.3774412764832809, "em": 0.15408320493066255}, {"f1": 2.265209526521744, "em": 0.6153846153846154}, {"f1": 1.1006554794898353, "em": 0.4601226993865031}, {"f1": 3.3378642987383613, "em": 0.7530120481927711}, {"f1": 1.7194472435550596, "em": 0.30120481927710846}], "total": {"test_f1": 3.920139397532803, "test_f1_se": 2.840516021885792, "test_em": 1.4519943516274094, "test_em_se": 1.2476170478027857}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "foqa", "task": "reading-comprehension", "dataset_languages": ["fo"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"f1": 0.6842320261437908, "em": 0.0}, {"f1": 0.09857259857259858, "em": 0.0}, {"f1": 14.891559382055956, "em": 5.185185185185185}, {"f1": 15.160776636522012, "em": 5.825242718446602}, {"f1": 25.177857257894843, "em": 9.405940594059405}, {"f1": 0.1649029982363316, "em": 0.0}, {"f1": 3.9260444743935303, "em": 0.7075471698113207}, {"f1": 22.026860780789832, "em": 8.651399491094148}, {"f1": 14.943254202362617, "em": 6.483790523690773}, {"f1": 26.973507717665328, "em": 12.039312039312039}], "total": {"test_f1": 12.404756807463686, "test_f1_se": 6.529166153290597, "test_em": 4.829841772159947, "test_em_se": 2.7591212014249638}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "germanquad", "task": "reading-comprehension", "dataset_languages": ["de"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"f1": 3.674101208992086, "em": 0.0}, {"f1": 3.998519751071863, "em": 0.0}, {"f1": 4.349435896003027, "em": 0.0777000777000777}, {"f1": 3.9021783420909237, "em": 0.076103500761035}, {"f1": 3.7159712716951976, "em": 0.0}, {"f1": 3.812365569337492, "em": 0.0}, {"f1": 3.833470625150652, "em": 0.0}, {"f1": 4.049669630090563, "em": 0.0}, {"f1": 4.09079869879471, "em": 0.15479876160990713}, {"f1": 3.72382261715739, "em": 0.0}], "total": {"test_f1": 3.9150333610383896, "test_f1_se": 0.1300592705291265, "test_em": 0.030860234007101982, "test_em_se": 0.033484949780516776}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "squad", "task": "reading-comprehension", "dataset_languages": ["en"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"f1": 0.010822510822510824, "em": 0.0}, {"f1": 0.010830715910321673, "em": 0.0}, {"f1": 0.0, "em": 0.0}, {"f1": 0.01087192868014786, "em": 0.0}, {"f1": 0.011014428901861441, "em": 0.0}, {"f1": 0.010905125408942205, "em": 0.0}, {"f1": 0.011143302874972144, "em": 0.0}, {"f1": 0.011230907457322553, "em": 0.0}, {"f1": 0.0, "em": 0.0}, {"f1": 0.009077705156136527, "em": 0.0}], "total": {"test_f1": 0.008589662521221524, "test_f1_se": 0.002830958808954303, "test_em": 0.0, "test_em_se": 0.0}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "squad-nl", "task": "reading-comprehension", "dataset_languages": ["nl"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"f1": 2.345879836683311, "em": 0.6060606060606061}, {"f1": 2.7752550721246143, "em": 0.6065200909780136}, {"f1": 0.674356404985842, "em": 0.0777000777000777}, {"f1": 2.1533594277398245, "em": 0.91324200913242}, {"f1": 2.721693270592463, "em": 1.079414032382421}, {"f1": 7.673164714949928, "em": 3.282442748091603}, {"f1": 1.4735938079105098, "em": 0.31201248049922}, {"f1": 0.9119453513326531, "em": 0.15723270440251572}, {"f1": 1.8652480309116455, "em": 0.38699690402476783}, {"f1": 10.27361413661515, "em": 5.246913580246914}], "total": {"test_f1": 3.286811005384594, "test_f1_se": 1.9444777523019408, "test_em": 1.2668535233518559, "test_em_se": 1.038523704613892}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "squad-it", "task": "reading-comprehension", "dataset_languages": ["it"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"f1": 0.7540504372934714, "em": 0.30303030303030304}, {"f1": 3.336639695823288, "em": 1.6679302501895374}, {"f1": 2.18605439099857, "em": 1.0101010101010102}, {"f1": 9.595152421926247, "em": 5.251141552511416}, {"f1": 0.13500234326575658, "em": 0.07710100231303008}, {"f1": 1.9625207120480437, "em": 0.6870229007633588}, {"f1": 2.1219158129304816, "em": 0.7020280811232449}, {"f1": 2.0853528297559785, "em": 0.6289308176100629}, {"f1": 0.07917261739305466, "em": 0.0}, {"f1": 4.432725410734668, "em": 2.623456790123457}], "total": {"test_f1": 2.6688586672169565, "test_f1_se": 1.7247967043297943, "test_em": 1.295074270776542, "test_em_se": 0.9896502315442071}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fquad", "task": "reading-comprehension", "dataset_languages": ["fr"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"f1": 10.838181331167352, "em": 0.6818181818181818}, {"f1": 17.038560347017434, "em": 0.9855951478392722}, {"f1": 18.823965471231613, "em": 1.4763014763014763}, {"f1": 25.120214406645506, "em": 1.2937595129375952}, {"f1": 5.897995137039953, "em": 0.3855050115651503}, {"f1": 10.229392113852715, "em": 0.7633587786259542}, {"f1": 15.768293434803683, "em": 0.9360374414976599}, {"f1": 12.845740604734766, "em": 0.4716981132075472}, {"f1": 8.52873880046634, "em": 0.30959752321981426}, {"f1": 12.68151856865554, "em": 1.3117283950617284}], "total": {"test_f1": 13.777260021561492, "test_f1_se": 3.4673223093372263, "test_em": 0.861539958207438, "test_em_se": 0.25417274058460726}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "nordjylland-news", "task": "summarization", "dataset_languages": ["da"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"bertscore": 0.48662443963985424, "rouge_l": 0.010238380781731695}, {"bertscore": 0.5103552687942283, "rouge_l": 0.010804448672186608}, {"bertscore": 0.5077019512245897, "rouge_l": 0.010354933044294662}, {"bertscore": 0.5173858139314689, "rouge_l": 0.010741854499538624}, {"bertscore": 0.5079849536268739, "rouge_l": 0.010482355708207065}, {"bertscore": 0.5128547674539732, "rouge_l": 0.011929369616634801}, {"bertscore": 0.5099578578519868, "rouge_l": 0.010506890865690243}, {"bertscore": 0.5141942617919995, "rouge_l": 0.010649203202899278}, {"bertscore": 0.5075283649639459, "rouge_l": 0.010776241740792418}, {"bertscore": 0.5092522192717297, "rouge_l": 0.010152219477951465}], "total": {"test_bertscore": 50.83839898550651, "test_bertscore_se": 0.5130773124212673, "test_rouge_l": 1.0663589760992687, "test_rouge_l_se": 0.030893992715789194}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mlsum", "task": "summarization", "dataset_languages": ["de"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"bertscore": 0.5312062691955362, "rouge_l": 0.048980158192351186}, {"bertscore": 0.5285464839253109, "rouge_l": 0.04993632930244285}, {"bertscore": 0.5299890645837877, "rouge_l": 0.04876386515847869}, {"bertscore": 0.5276012827816885, "rouge_l": 0.04990988616050626}, {"bertscore": 0.5287378268694738, "rouge_l": 0.05024015620309926}, {"bertscore": 0.5301532540324843, "rouge_l": 0.048750035593017105}, {"bertscore": 0.5281670029216912, "rouge_l": 0.05021962399157374}, {"bertscore": 0.5301953432062874, "rouge_l": 0.05009390404072549}, {"bertscore": 0.5304704224108718, "rouge_l": 0.049471327888066546}, {"bertscore": 0.5302895166387316, "rouge_l": 0.04921665188074566}], "total": {"test_bertscore": 52.95356466565864, "test_bertscore_se": 0.07300174926038855, "test_rouge_l": 4.955819384110067, "test_rouge_l_se": 0.036950152941659806}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "rrn", "task": "summarization", "dataset_languages": ["is"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"bertscore": 0.45531015971209854, "rouge_l": 0.011769615206571264}, {"bertscore": 0.4905568237300031, "rouge_l": 0.0130713695993771}, {"bertscore": 0.48535670078126714, "rouge_l": 0.013040501950985592}, {"bertscore": 0.49148382496787235, "rouge_l": 0.013377535140122947}, {"bertscore": 0.4969250420690514, "rouge_l": 0.0130214588170449}, {"bertscore": 0.4957379520346876, "rouge_l": 0.013084690401705864}, {"bertscore": 0.496391069580568, "rouge_l": 0.013798529353010071}, {"bertscore": 0.4126014105568174, "rouge_l": 0.010834262764947691}, {"bertscore": 0.4786531931895297, "rouge_l": 0.012754280374621618}, {"bertscore": 0.3867195387138054, "rouge_l": 0.010430306744951847}], "total": {"test_bertscore": 46.89735715335701, "test_bertscore_se": 2.4194708650953416, "test_rouge_l": 1.251825503533389, "test_rouge_l_se": 0.06957841371030866}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "no-sammendrag", "task": "summarization", "dataset_languages": ["nb", "nn", "no"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"bertscore": 0.505920821960899, "rouge_l": 0.007890331005319233}, {"bertscore": 0.4945932859845925, "rouge_l": 0.007627168667588533}, {"bertscore": 0.5098493711120682, "rouge_l": 0.007401192841478216}, {"bertscore": 0.4983686961058993, "rouge_l": 0.0068789929889467175}, {"bertscore": 0.49703620235959534, "rouge_l": 0.00705051099659949}, {"bertscore": 0.5013255456869956, "rouge_l": 0.007081264749204183}, {"bertscore": 0.5115705297648674, "rouge_l": 0.007633800967991584}, {"bertscore": 0.5106274466088507, "rouge_l": 0.007679227129238127}, {"bertscore": 0.5113834319199668, "rouge_l": 0.007525859705661981}, {"bertscore": 0.49970249927719124, "rouge_l": 0.007164539380109455}], "total": {"test_bertscore": 50.40377830780927, "test_bertscore_se": 0.40693542479488454, "test_rouge_l": 0.7393288843213752, "test_rouge_l_se": 0.020563683069606643}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "wiki-lingua-nl", "task": "summarization", "dataset_languages": ["nl"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"bertscore": 0.5276670572784496, "rouge_l": 0.011960967730929242}, {"bertscore": 0.5271856848557945, "rouge_l": 0.011191005223337495}, {"bertscore": 0.5271514011110412, "rouge_l": 0.011546710835043404}, {"bertscore": 0.525212275038939, "rouge_l": 0.011896156449156862}, {"bertscore": 0.5268923357944004, "rouge_l": 0.011755642373298632}, {"bertscore": 0.5275020669505466, "rouge_l": 0.01106282808655824}, {"bertscore": 0.5286176559311571, "rouge_l": 0.01085331281680487}, {"bertscore": 0.5277372656273656, "rouge_l": 0.01138440198119412}, {"bertscore": 0.5269833976926748, "rouge_l": 0.011566450617877626}, {"bertscore": 0.529174144990975, "rouge_l": 0.010802894208867628}], "total": {"test_bertscore": 52.74123285271344, "test_bertscore_se": 0.065819548325518, "test_rouge_l": 1.140203703230681, "test_rouge_l_se": 0.025717901442267887}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "swedn", "task": "summarization", "dataset_languages": ["sv"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"bertscore": 0.513290453251102, "rouge_l": 0.029273495137925147}, {"bertscore": 0.5266567680082517, "rouge_l": 0.03101533346764978}, {"bertscore": 0.5242859333666274, "rouge_l": 0.030160300916524836}, {"bertscore": 0.5266184456995688, "rouge_l": 0.029980368003618692}, {"bertscore": 0.5236640464136144, "rouge_l": 0.03133346558087247}, {"bertscore": 0.5104278154612985, "rouge_l": 0.030596306816376588}, {"bertscore": 0.5004583445988828, "rouge_l": 0.029206604781476755}, {"bertscore": 0.5270432133547729, "rouge_l": 0.029020774603301578}, {"bertscore": 0.5235458834213205, "rouge_l": 0.03273384398441416}, {"bertscore": 0.5262427890120307, "rouge_l": 0.027913145530750184}], "total": {"test_bertscore": 52.022336925874704, "test_bertscore_se": 0.561671200520521, "test_rouge_l": 3.012336388229102, "test_rouge_l_se": 0.08514300014415596}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "cnn-dailymail", "task": "summarization", "dataset_languages": ["en"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"bertscore": 0.48990983664407395, "rouge_l": 0.013878312164654603}, {"bertscore": 0.5035774298739852, "rouge_l": 0.015594079911668321}, {"bertscore": 0.502631214025314, "rouge_l": 0.01412225175143917}, {"bertscore": 0.5032101718970807, "rouge_l": 0.0149515729590748}, {"bertscore": 0.5034688729647314, "rouge_l": 0.0148267458438462}, {"bertscore": 0.5028620753728319, "rouge_l": 0.014769801082690437}, {"bertscore": 0.4999253380665323, "rouge_l": 0.013513665368893545}, {"bertscore": 0.5015837365499465, "rouge_l": 0.013952283459797753}, {"bertscore": 0.5035655625542859, "rouge_l": 0.014778955739692293}, {"bertscore": 0.5039051938219927, "rouge_l": 0.014641362638610574}], "total": {"test_bertscore": 50.146394317707745, "test_bertscore_se": 0.2622372262210998, "test_rouge_l": 1.450290309203677, "test_rouge_l_se": 0.03854245187637722}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "orange-sum", "task": "summarization", "dataset_languages": ["fr"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"bertscore": 0.5089236329949927, "rouge_l": 0.01132070831429918}, {"bertscore": 0.5126464285713155, "rouge_l": 0.012658996769267327}, {"bertscore": 0.5109645246120635, "rouge_l": 0.012853822621843768}, {"bertscore": 0.5120662116387393, "rouge_l": 0.014376100480757407}, {"bertscore": 0.5109010326559655, "rouge_l": 0.013027190134958119}, {"bertscore": 0.5152979189879261, "rouge_l": 0.013646487854467634}, {"bertscore": 0.5085825278947596, "rouge_l": 0.012395506135251342}, {"bertscore": 0.5097456941439305, "rouge_l": 0.012347457045048794}, {"bertscore": 0.5112693127302919, "rouge_l": 0.01367748198331896}, {"bertscore": 0.5102051609137561, "rouge_l": 0.012890890174908045}], "total": {"test_bertscore": 51.106024451437406, "test_bertscore_se": 0.12175065008497099, "test_rouge_l": 1.2919464151412055, "test_rouge_l_se": 0.05254060067359864}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "ilpost-sum", "task": "summarization", "dataset_languages": ["it"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"bertscore": 0.5166157313651638, "rouge_l": 0.022345021528823623}, {"bertscore": 0.5136173772043549, "rouge_l": 0.01434131455789434}, {"bertscore": 0.5096538349462207, "rouge_l": 0.009713584633385557}, {"bertscore": 0.5109890844905749, "rouge_l": 0.010853360344764133}, {"bertscore": 0.5082130936498288, "rouge_l": 0.006816210064217646}, {"bertscore": 0.5233593916054815, "rouge_l": 0.02746868519759334}, {"bertscore": 0.5080724898725748, "rouge_l": 0.007824326870856907}, {"bertscore": 0.5087895121396286, "rouge_l": 0.0075203496812276775}, {"bertscore": 0.5209835387649946, "rouge_l": 0.023433604010812993}, {"bertscore": 0.5288845758041134, "rouge_l": 0.036880415903652874}], "total": {"test_bertscore": 51.49178629842937, "test_bertscore_se": 0.45274159505313905, "test_rouge_l": 1.671968727932291, "test_rouge_l_se": 0.6366923668671987}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "danske-talemaader", "task": "knowledge", "dataset_languages": ["da"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"mcc": 0.4248040049774079, "accuracy": 0.5173267326732673}, {"mcc": 0.36214020165392485, "accuracy": 0.4801980198019802}, {"mcc": 0.40994902128025806, "accuracy": 0.5346534653465347}, {"mcc": 0.31648168805719645, "accuracy": 0.45173267326732675}, {"mcc": 0.2996140356583117, "accuracy": 0.4430693069306931}, {"mcc": 0.32993748307687104, "accuracy": 0.47277227722772275}, {"mcc": 0.2805700903724665, "accuracy": 0.4245049504950495}, {"mcc": 0.15371457633412772, "accuracy": 0.31435643564356436}, {"mcc": 0.13483969617954672, "accuracy": 0.3180693069306931}, {"mcc": 0.36394369784380176, "accuracy": 0.4566831683168317}], "total": {"test_mcc": 30.75994495433913, "test_mcc_se": 6.032656281427838, "test_accuracy": 44.13366336633664, "test_accuracy_se": 4.570048652146182}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "danish-citizen-tests", "task": "knowledge", "dataset_languages": ["da"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"mcc": 0.35742013043350945, "accuracy": 0.5485714285714286}, {"mcc": 0.3025233265595777, "accuracy": 0.4533333333333333}, {"mcc": 0.42809983247847466, "accuracy": 0.5847619047619048}, {"mcc": 0.21158381256249884, "accuracy": 0.43047619047619046}, {"mcc": 0.21455958064146635, "accuracy": 0.44571428571428573}, {"mcc": 0.17848658572943776, "accuracy": 0.38857142857142857}, {"mcc": 0.20728803571228, "accuracy": 0.44}, {"mcc": 0.4990563024596601, "accuracy": 0.6361904761904762}, {"mcc": 0.44311347003246926, "accuracy": 0.6}, {"mcc": 0.1851926301775987, "accuracy": 0.45904761904761904}], "total": {"test_mcc": 30.27323706786973, "test_mcc_se": 7.48921963906443, "test_accuracy": 49.86666666666667, "test_accuracy_se": 5.296657301345937}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "nrk-quiz-qa", "task": "knowledge", "dataset_languages": ["nb", "nn", "no"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"mcc": 0.20418007150818174, "accuracy": 0.365234375}, {"mcc": 0.23722896831725113, "accuracy": 0.3828125}, {"mcc": 0.08287034948540961, "accuracy": 0.2724609375}, {"mcc": 0.20145481530349899, "accuracy": 0.35400390625}, {"mcc": 0.28248435186234166, "accuracy": 0.43408203125}, {"mcc": 0.2036486098970415, "accuracy": 0.36328125}, {"mcc": 0.32626035188336183, "accuracy": 0.48291015625}, {"mcc": 0.25950127120178046, "accuracy": 0.39404296875}, {"mcc": 0.22008911243196147, "accuracy": 0.36962890625}, {"mcc": 0.36077717598741965, "accuracy": 0.5146484375}], "total": {"test_mcc": 23.78495077878248, "test_mcc_se": 4.772063098149749, "test_accuracy": 39.3310546875, "test_accuracy_se": 4.280431558137568}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-sv", "task": "knowledge", "dataset_languages": ["sv"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"mcc": 0.07354133994912607, "accuracy": 0.2568359375}, {"mcc": 0.10346103996924216, "accuracy": 0.29052734375}, {"mcc": 0.0011018785942899848, "accuracy": 0.26220703125}, {"mcc": 0.08094212556116803, "accuracy": 0.259765625}, {"mcc": 0.10817882258930545, "accuracy": 0.2607421875}, {"mcc": 0.09624821930996472, "accuracy": 0.275390625}, {"mcc": 0.15768522242074542, "accuracy": 0.34228515625}, {"mcc": 0.06418851955211927, "accuracy": 0.25244140625}, {"mcc": 0.27765560668504013, "accuracy": 0.4482421875}, {"mcc": 0.0893367901832778, "accuracy": 0.287109375}], "total": {"test_mcc": 10.52339564814279, "test_mcc_se": 4.476566967840308, "test_accuracy": 29.355468750000004, "test_accuracy_se": 3.7487402578800184}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "icelandic-knowledge", "task": "knowledge", "dataset_languages": ["is"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"mcc": 0.0, "accuracy": 0.2900390625}, {"mcc": 0.0, "accuracy": 0.2509765625}, {"mcc": 0.0, "accuracy": 0.275390625}, {"mcc": 0.014155439560574901, "accuracy": 0.25390625}, {"mcc": 0.06684470938720723, "accuracy": 0.26171875}, {"mcc": 0.0, "accuracy": 0.255859375}, {"mcc": 0.0, "accuracy": 0.2607421875}, {"mcc": 0.0, "accuracy": 0.2236328125}, {"mcc": 0.025383739994601442, "accuracy": 0.2548828125}, {"mcc": 0.0, "accuracy": 0.2578125}], "total": {"test_mcc": 1.0638388894238358, "test_mcc_se": 1.3359305165124065, "test_accuracy": 25.849609375, "test_accuracy_se": 1.0553605186124204}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-de", "task": "knowledge", "dataset_languages": ["de"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"mcc": 0.07424111612355158, "accuracy": 0.275390625}, {"mcc": 0.1480921951826431, "accuracy": 0.32421875}, {"mcc": 0.03958043715173386, "accuracy": 0.27783203125}, {"mcc": 0.10742569232309344, "accuracy": 0.2998046875}, {"mcc": 0.21072173671659755, "accuracy": 0.37158203125}, {"mcc": 0.15220740600740112, "accuracy": 0.3310546875}, {"mcc": 0.17191491083335284, "accuracy": 0.35498046875}, {"mcc": 0.23874166334674185, "accuracy": 0.38623046875}, {"mcc": 0.10931163586881437, "accuracy": 0.32470703125}, {"mcc": 0.10695313881478893, "accuracy": 0.29541015625}], "total": {"test_mcc": 13.591899323687187, "test_mcc_se": 3.771339463097075, "test_accuracy": 32.412109375, "test_accuracy_se": 2.359590473455024}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-nl", "task": "knowledge", "dataset_languages": ["nl"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"mcc": 0.07341287924469962, "accuracy": 0.30224609375}, {"mcc": 0.04635111521252492, "accuracy": 0.25537109375}, {"mcc": 0.031002590945238252, "accuracy": 0.24267578125}, {"mcc": 0.14622853831119562, "accuracy": 0.33447265625}, {"mcc": 0.1854974808947582, "accuracy": 0.38232421875}, {"mcc": 0.18873135285365206, "accuracy": 0.349609375}, {"mcc": 0.2663589636610286, "accuracy": 0.421875}, {"mcc": 0.1446309493266306, "accuracy": 0.326171875}, {"mcc": 0.20988901869818402, "accuracy": 0.34765625}, {"mcc": 0.059878109693937795, "accuracy": 0.27880859375}], "total": {"test_mcc": 13.519809988418496, "test_mcc_se": 4.916497069572134, "test_accuracy": 32.412109375, "test_accuracy_se": 3.4663619616674053}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu", "task": "knowledge", "dataset_languages": ["en"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"mcc": 0.12844390384000842, "accuracy": 0.3466796875}, {"mcc": 0.08412403774932682, "accuracy": 0.32568359375}, {"mcc": 0.035676825492023785, "accuracy": 0.29345703125}, {"mcc": 0.08155792664591209, "accuracy": 0.30615234375}, {"mcc": 0.05729049799087943, "accuracy": 0.30322265625}, {"mcc": 0.1284769813012972, "accuracy": 0.32080078125}, {"mcc": 0.08984310563266892, "accuracy": 0.3125}, {"mcc": 0.038665768266507736, "accuracy": 0.2861328125}, {"mcc": 0.10337559724373555, "accuracy": 0.31494140625}, {"mcc": 0.11018802541952294, "accuracy": 0.326171875}], "total": {"test_mcc": 8.576426695818828, "test_mcc_se": 2.0791405937729777, "test_accuracy": 31.357421875000004, "test_accuracy_se": 1.0886437881971749}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-fr", "task": "knowledge", "dataset_languages": ["fr"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"mcc": 0.06126539441451623, "accuracy": 0.26806640625}, {"mcc": 0.037437871881899835, "accuracy": 0.263671875}, {"mcc": 0.11641643427098695, "accuracy": 0.30126953125}, {"mcc": 0.04320510831274151, "accuracy": 0.24658203125}, {"mcc": -0.00997147344163172, "accuracy": 0.22607421875}, {"mcc": 0.03165179594604765, "accuracy": 0.24658203125}, {"mcc": 0.07081828371943912, "accuracy": 0.2626953125}, {"mcc": 0.027136776626121, "accuracy": 0.2470703125}, {"mcc": 0.062277273316389825, "accuracy": 0.2529296875}, {"mcc": 0.0721282386594016, "accuracy": 0.24365234375}], "total": {"test_mcc": 5.123657037059119, "test_mcc_se": 2.0923494386186454, "test_accuracy": 25.5859375, "test_accuracy_se": 1.2418481817849745}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-it", "task": "knowledge", "dataset_languages": ["it"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"mcc": -0.03610027060254387, "accuracy": 0.23046875}, {"mcc": 0.09295042626745294, "accuracy": 0.265625}, {"mcc": 0.027996569839919532, "accuracy": 0.2314453125}, {"mcc": -0.033120307361350146, "accuracy": 0.23486328125}, {"mcc": 0.01681510208265664, "accuracy": 0.23388671875}, {"mcc": 0.022001646326571506, "accuracy": 0.24169921875}, {"mcc": 0.004391812641597896, "accuracy": 0.2255859375}, {"mcc": -0.004505357654504045, "accuracy": 0.22802734375}, {"mcc": 0.04245071357397592, "accuracy": 0.2529296875}, {"mcc": -0.0006054920638984038, "accuracy": 0.22314453125}], "total": {"test_mcc": 1.3227484304987795, "test_mcc_se": 2.32672415739582, "test_accuracy": 23.6767578125, "test_accuracy_se": 0.8225830663846065}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-da", "task": "common-sense-reasoning", "dataset_languages": ["da"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"mcc": 0.014648181678199165, "accuracy": 0.2529296875}, {"mcc": 0.021822742070306392, "accuracy": 0.25439453125}, {"mcc": 0.1572919808822974, "accuracy": 0.29736328125}, {"mcc": 0.11484804291025981, "accuracy": 0.3076171875}, {"mcc": 0.01104912221184613, "accuracy": 0.25048828125}, {"mcc": 0.054751605652503976, "accuracy": 0.25390625}, {"mcc": 0.03766633041367292, "accuracy": 0.267578125}, {"mcc": 0.03007178831038101, "accuracy": 0.25830078125}, {"mcc": 0.09606465248567925, "accuracy": 0.2890625}, {"mcc": 0.029155137975676156, "accuracy": 0.26708984375}], "total": {"test_mcc": 5.673695845908222, "test_mcc_se": 3.061070107071447, "test_accuracy": 26.987304687500004, "test_accuracy_se": 1.282409971571}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "nor-common-sense-qa", "task": "common-sense-reasoning", "dataset_languages": ["nb", "nn", "no"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"mcc": 0.26020745282923957, "accuracy": 0.3419506462984724}, {"mcc": 0.1696065307212883, "accuracy": 0.30317273795534666}, {"mcc": 0.22929069332562738, "accuracy": 0.34312573443008226}, {"mcc": 0.4321685419513681, "accuracy": 0.5146886016451234}, {"mcc": 0.16835597131737504, "accuracy": 0.2444183313748531}, {"mcc": 0.5045061070669253, "accuracy": 0.5898942420681551}, {"mcc": 0.43370529130034235, "accuracy": 0.5088131609870741}, {"mcc": 0.17111393931627075, "accuracy": 0.25499412455934195}, {"mcc": 0.51256918772679, "accuracy": 0.5934195064629847}, {"mcc": 0.15617231750533628, "accuracy": 0.27144535840188017}], "total": {"test_mcc": 30.376960330605634, "test_mcc_se": 9.249359215842873, "test_accuracy": 39.65922444183314, "test_accuracy_se": 8.669592120769721}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-sv", "task": "common-sense-reasoning", "dataset_languages": ["sv"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"mcc": -0.022767547342127922, "accuracy": 0.24853515625}, {"mcc": 0.02479323234467442, "accuracy": 0.265625}, {"mcc": 0.007293605191706631, "accuracy": 0.25244140625}, {"mcc": 0.020764546612005937, "accuracy": 0.24169921875}, {"mcc": -0.040620241103301244, "accuracy": 0.24853515625}, {"mcc": 0.07643274570112124, "accuracy": 0.27587890625}, {"mcc": 0.04077488289206806, "accuracy": 0.2607421875}, {"mcc": 0.045216814824190446, "accuracy": 0.27685546875}, {"mcc": 0.0017736655184233201, "accuracy": 0.24462890625}, {"mcc": -0.005139335528145548, "accuracy": 0.2548828125}], "total": {"test_mcc": 1.4852236911061534, "test_mcc_se": 2.131130664110401, "test_accuracy": 25.698242187499996, "test_accuracy_se": 0.7715898557681792}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "winogrande-is", "task": "common-sense-reasoning", "dataset_languages": ["is"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"mcc": 0.0, "accuracy": 0.5513392857142857}, {"mcc": 0.03790187692636212, "accuracy": 0.5636160714285714}, {"mcc": 0.0, "accuracy": 0.5658482142857143}, {"mcc": 0.026913502766955314, "accuracy": 0.5647321428571429}, {"mcc": 0.0, "accuracy": 0.5803571428571429}, {"mcc": 0.0, "accuracy": 0.5479910714285714}, {"mcc": 0.0, "accuracy": 0.5837053571428571}, {"mcc": 0.0, "accuracy": 0.5401785714285714}, {"mcc": 0.0, "accuracy": 0.5904017857142857}, {"mcc": -0.0297475410936319, "accuracy": 0.5569196428571429}], "total": {"test_mcc": 0.3506783859968554, "test_mcc_se": 1.116957318848934, "test_accuracy": 56.45089285714286, "test_accuracy_se": 1.0097312967649696}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-de", "task": "common-sense-reasoning", "dataset_languages": ["de"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"mcc": 0.08812510256418526, "accuracy": 0.26513671875}, {"mcc": 0.0, "accuracy": 0.2412109375}, {"mcc": 0.08796601118351835, "accuracy": 0.2509765625}, {"mcc": 0.10067203618686592, "accuracy": 0.2724609375}, {"mcc": 0.07138912740929301, "accuracy": 0.2568359375}, {"mcc": 0.2005708936420062, "accuracy": 0.31591796875}, {"mcc": 0.06343498008634876, "accuracy": 0.25390625}, {"mcc": 0.06403363651085234, "accuracy": 0.25}, {"mcc": 0.01819694763044587, "accuracy": 0.25634765625}, {"mcc": 0.21372594467683956, "accuracy": 0.330078125}], "total": {"test_mcc": 9.081146798903553, "test_mcc_se": 4.265770736596543, "test_accuracy": 26.9287109375, "test_accuracy_se": 1.8423369429274405}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-nl", "task": "common-sense-reasoning", "dataset_languages": ["nl"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"mcc": 0.044026723130404385, "accuracy": 0.24462890625}, {"mcc": 0.06275500925494508, "accuracy": 0.24267578125}, {"mcc": 0.06581233566621286, "accuracy": 0.24267578125}, {"mcc": 0.10940474236756166, "accuracy": 0.25927734375}, {"mcc": 0.10342478662139906, "accuracy": 0.2568359375}, {"mcc": 0.03743730391140687, "accuracy": 0.2373046875}, {"mcc": 0.17591741629310753, "accuracy": 0.29541015625}, {"mcc": 0.09911951183322361, "accuracy": 0.251953125}, {"mcc": 0.050605932518731786, "accuracy": 0.24658203125}, {"mcc": 0.10521482126480024, "accuracy": 0.248046875}], "total": {"test_mcc": 8.537185828617929, "test_mcc_se": 2.596507697916475, "test_accuracy": 25.253906250000004, "test_accuracy_se": 1.0220090072443941}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag", "task": "common-sense-reasoning", "dataset_languages": ["en"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"mcc": 0.1249151123847713, "accuracy": 0.2734375}, {"mcc": 0.14008024381713524, "accuracy": 0.2734375}, {"mcc": 0.13990052729407426, "accuracy": 0.27685546875}, {"mcc": 0.1599135973943347, "accuracy": 0.27587890625}, {"mcc": 0.36642473624565985, "accuracy": 0.435546875}, {"mcc": 0.1612320606290556, "accuracy": 0.291015625}, {"mcc": 0.3529640082881086, "accuracy": 0.4228515625}, {"mcc": 0.3735645502172036, "accuracy": 0.4580078125}, {"mcc": 0.08141683201635956, "accuracy": 0.27197265625}, {"mcc": 0.28571180004982616, "accuracy": 0.37353515625}], "total": {"test_mcc": 21.86123468336529, "test_mcc_se": 7.009692968850333, "test_accuracy": 33.525390625, "test_accuracy_se": 4.836690295543129}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-fr", "task": "common-sense-reasoning", "dataset_languages": ["fr"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"mcc": 0.0710523732432887, "accuracy": 0.28076171875}, {"mcc": 0.05434287530711263, "accuracy": 0.25830078125}, {"mcc": -0.0003000522125272316, "accuracy": 0.25244140625}, {"mcc": 0.06780164557321497, "accuracy": 0.251953125}, {"mcc": 0.0364408551531694, "accuracy": 0.2568359375}, {"mcc": 0.03271065629720853, "accuracy": 0.26318359375}, {"mcc": 0.0, "accuracy": 0.251953125}, {"mcc": 0.05204189773128214, "accuracy": 0.263671875}, {"mcc": 0.07486409423395396, "accuracy": 0.25341796875}, {"mcc": 0.032323213884682886, "accuracy": 0.267578125}], "total": {"test_mcc": 4.21277559211386, "test_mcc_se": 1.6793294621289034, "test_accuracy": 26.0009765625, "test_accuracy_se": 0.5676682577657811}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-it", "task": "common-sense-reasoning", "dataset_languages": ["it"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"mcc": 0.0, "accuracy": 0.248046875}, {"mcc": 0.0, "accuracy": 0.24267578125}, {"mcc": 0.0, "accuracy": 0.25244140625}, {"mcc": 0.0, "accuracy": 0.27197265625}, {"mcc": 0.0, "accuracy": 0.25390625}, {"mcc": 0.0, "accuracy": 0.279296875}, {"mcc": 0.0, "accuracy": 0.26611328125}, {"mcc": 0.0, "accuracy": 0.2578125}, {"mcc": 0.0, "accuracy": 0.26318359375}, {"mcc": 0.0, "accuracy": 0.2646484375}], "total": {"test_mcc": 0.0, "test_mcc_se": 0.0, "test_accuracy": 26.0009765625, "test_accuracy_se": 0.6935450803978843}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "DiscoResearch/DiscoLM-70b", "results": {"raw": [{"test_speed": 259.15, "test_speed_short": 29.2}, {"test_speed": 506.19, "test_speed_short": 54.449999999999996}, {"test_speed": 753.27, "test_speed_short": 105.56}, {"test_speed": 994.74, "test_speed_short": 129.96}, {"test_speed": 1253.07, "test_speed_short": 156.09}, {"test_speed": 1465.08, "test_speed_short": 205.76999999999998}, {"test_speed": 1698.86, "test_speed_short": 230.4}, {"test_speed": 1924.23, "test_speed_short": 257.02}, {"test_speed": 2107.54, "test_speed_short": 283.14}, {"test_speed": 2306.29, "test_speed_short": 309.40000000000003}], "total": {"test_speed": 1326.842, "test_speed_se": 430.4738970765015, "test_speed_short": 176.09900000000002, "test_speed_short_se": 59.695800183811926}}, "num_model_parameters": 68977172480, "max_sequence_length": 8192, "vocabulary_size": 32032, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "swerec", "task": "sentiment-classification", "dataset_languages": ["sv"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"mcc": 0.7830686150982432, "macro_f1": 0.703808472099939}, {"mcc": 0.7889079405645558, "macro_f1": 0.7776397018428308}, {"mcc": 0.7980751208928804, "macro_f1": 0.7616900825083505}, {"mcc": 0.7990833267983665, "macro_f1": 0.7666946945564099}, {"mcc": 0.7865235076386297, "macro_f1": 0.7633456775546777}, {"mcc": 0.8072982443584212, "macro_f1": 0.7825814294306604}, {"mcc": 0.7956841313808228, "macro_f1": 0.7301924889811722}, {"mcc": 0.785496476255143, "macro_f1": 0.7615051781572223}, {"mcc": 0.7971016350284658, "macro_f1": 0.7469177009471765}, {"mcc": 0.8132413119995481, "macro_f1": 0.8041487244797046}], "total": {"test_mcc": 79.54480310015077, "test_mcc_se": 0.6039849574934336, "test_macro_f1": 75.98524150558144, "test_macro_f1_se": 1.7345020094529664}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "angry-tweets", "task": "sentiment-classification", "dataset_languages": ["da"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"mcc": 0.5001741536559232, "macro_f1": 0.6553821198153392}, {"mcc": 0.5429629597017283, "macro_f1": 0.6865801494207616}, {"mcc": 0.5028959239166759, "macro_f1": 0.6247871717148632}, {"mcc": 0.4980181727307191, "macro_f1": 0.6471224778123933}, {"mcc": 0.4679318962225438, "macro_f1": 0.650047381901786}, {"mcc": 0.5244265069917403, "macro_f1": 0.6673075599806415}, {"mcc": 0.507883868042396, "macro_f1": 0.652558293450971}, {"mcc": 0.523763612906349, "macro_f1": 0.6836499365090871}, {"mcc": 0.5392532871475564, "macro_f1": 0.6794723705375754}, {"mcc": 0.5012867959815616, "macro_f1": 0.663738768993034}], "total": {"test_mcc": 51.08597177297194, "test_mcc_se": 1.3817936909095412, "test_macro_f1": 66.10646230136453, "test_macro_f1_se": 1.1837011645848847}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "norec", "task": "sentiment-classification", "dataset_languages": ["nb", "nn", "no"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"mcc": 0.3896510408000918, "macro_f1": 0.5049619967075766}, {"mcc": 0.38601269838932273, "macro_f1": 0.5413341436206164}, {"mcc": 0.39189020011555503, "macro_f1": 0.5287469796322418}, {"mcc": 0.37341019557407906, "macro_f1": 0.4585243648113632}, {"mcc": 0.3382098626793279, "macro_f1": 0.47314721504403856}, {"mcc": 0.39934713835423363, "macro_f1": 0.5660036771349212}, {"mcc": 0.3538032626609088, "macro_f1": 0.46276512734089864}, {"mcc": 0.32559412241263486, "macro_f1": 0.4307403007059902}, {"mcc": 0.3530014105105481, "macro_f1": 0.4945508221920518}, {"mcc": 0.40506476115659346, "macro_f1": 0.5404604321492337}], "total": {"test_mcc": 37.15984692653295, "test_mcc_se": 1.6951957189413638, "test_macro_f1": 50.01235059338931, "test_macro_f1_se": 2.7101316190778926}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hotter-and-colder-sentiment", "task": "sentiment-classification", "dataset_languages": ["is"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"mcc": 0.30768502738119, "macro_f1": 0.4459829714828403}, {"mcc": 0.40547578453318317, "macro_f1": 0.5779719222087399}, {"mcc": 0.34775104723556116, "macro_f1": 0.4178466196310118}, {"mcc": 0.4348817791823584, "macro_f1": 0.613872864134252}, {"mcc": 0.43553634641541006, "macro_f1": 0.5956165546190381}, {"mcc": 0.39594679360615753, "macro_f1": 0.5413289648403405}, {"mcc": 0.36970779468609527, "macro_f1": 0.4946926496388862}, {"mcc": 0.23820989164731632, "macro_f1": 0.34901295680481575}, {"mcc": 0.37562912772122903, "macro_f1": 0.5378537654457874}, {"mcc": 0.361783013859729, "macro_f1": 0.5167573226893103}], "total": {"test_mcc": 36.7260660626823, "test_mcc_se": 3.7077735154225127, "test_macro_f1": 50.90936591495022, "test_macro_f1_se": 5.197031576431373}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "sb10k", "task": "sentiment-classification", "dataset_languages": ["de"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"mcc": 0.41758287423124035, "macro_f1": 0.538877802138093}, {"mcc": 0.4614133048839717, "macro_f1": 0.5705160595717079}, {"mcc": 0.4880454251184854, "macro_f1": 0.5876118544581916}, {"mcc": 0.41975936949722065, "macro_f1": 0.5181492238884621}, {"mcc": 0.45388378940736407, "macro_f1": 0.5393958241191213}, {"mcc": 0.5588726834438218, "macro_f1": 0.6879097924381998}, {"mcc": 0.5031323322110405, "macro_f1": 0.658680069987092}, {"mcc": 0.434089117752108, "macro_f1": 0.4890422224342215}, {"mcc": 0.5613553114439868, "macro_f1": 0.6930465612751678}, {"mcc": 0.6267161491344848, "macro_f1": 0.751937984496124}], "total": {"test_mcc": 49.24850357123724, "test_mcc_se": 4.336994266091949, "test_macro_f1": 60.35167394806381, "test_macro_f1_se": 5.4752824970398715}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "dutch-social", "task": "sentiment-classification", "dataset_languages": ["nl"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"mcc": 0.1772220142427469, "macro_f1": 0.41428145117045156}, {"mcc": 0.15237963884034836, "macro_f1": 0.4068141222060256}, {"mcc": 0.12058611463502537, "macro_f1": 0.3782512340435911}, {"mcc": 0.1366925263212315, "macro_f1": 0.32807871548417794}, {"mcc": 0.12318560429660731, "macro_f1": 0.387310606060606}, {"mcc": 0.10969089146205448, "macro_f1": 0.35718458809694936}, {"mcc": 0.08971485573447398, "macro_f1": 0.32134173756975454}, {"mcc": 0.10691655319271108, "macro_f1": 0.3273367740762961}, {"mcc": 0.043439721230445955, "macro_f1": 0.30325495942144787}, {"mcc": 0.11754895718823657, "macro_f1": 0.3246928652205202}], "total": {"test_mcc": 11.773768771438817, "test_mcc_se": 2.2273665092686046, "test_macro_f1": 35.4854705334982, "test_macro_f1_se": 2.445170162061201}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "sst5", "task": "sentiment-classification", "dataset_languages": ["en"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"mcc": 0.6474179467953457, "macro_f1": 0.5806275162726429}, {"mcc": 0.6563171615377229, "macro_f1": 0.6154821626213666}, {"mcc": 0.661397099195261, "macro_f1": 0.5926831500131083}, {"mcc": 0.6450819125287558, "macro_f1": 0.5922389754677292}, {"mcc": 0.6319039651889314, "macro_f1": 0.6560341179577521}, {"mcc": 0.6496805283931779, "macro_f1": 0.6416334535466506}, {"mcc": 0.6379789839797632, "macro_f1": 0.6264373977291567}, {"mcc": 0.5915584614986253, "macro_f1": 0.5965062744057513}, {"mcc": 0.6446060178288633, "macro_f1": 0.637016361116315}, {"mcc": 0.6788909817701245, "macro_f1": 0.6063343480792871}], "total": {"test_mcc": 64.44833058716571, "test_mcc_se": 1.4084731014661505, "test_macro_f1": 61.44993757209759, "test_macro_f1_se": 1.5489767583996945}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fosent", "task": "sentiment-classification", "dataset_languages": ["fo"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"mcc": 0.20020253244834327, "macro_f1": 0.38629793472311197}, {"mcc": 0.2491049699255196, "macro_f1": 0.45397603485838783}, {"mcc": 0.2365616435284589, "macro_f1": 0.3605028403899258}, {"mcc": -0.054211383331718016, "macro_f1": 0.16726050420168068}, {"mcc": 0.23157498247270392, "macro_f1": 0.46795053708761536}, {"mcc": 0.3340881054773374, "macro_f1": 0.4913466461853559}, {"mcc": 0.049614368827231733, "macro_f1": 0.17585506539930132}, {"mcc": 0.4902014737861802, "macro_f1": 0.6345130726978713}, {"mcc": 0.3680201444127737, "macro_f1": 0.5211767957597632}, {"mcc": 0.30566681418205394, "macro_f1": 0.5003372522526498}], "total": {"test_mcc": 24.108236517288844, "test_mcc_se": 9.617534697355998, "test_macro_f1": 41.592166835556625, "test_macro_f1_se": 9.222050764455366}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "allocine", "task": "sentiment-classification", "dataset_languages": ["fr"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"mcc": 0.9168834910512255, "macro_f1": 0.9584139886474585}, {"mcc": 0.9013105502202402, "macro_f1": 0.9506368822540441}, {"mcc": 0.9360804928693818, "macro_f1": 0.9680006076807676}, {"mcc": 0.9355301063609114, "macro_f1": 0.967741935483871}, {"mcc": 0.9048299015435222, "macro_f1": 0.9519301791984212}, {"mcc": 0.921803203852525, "macro_f1": 0.9608684977076163}, {"mcc": 0.9209486093117297, "macro_f1": 0.9604227131369595}, {"mcc": 0.9111142198012115, "macro_f1": 0.955529498621154}, {"mcc": 0.9187118965343802, "macro_f1": 0.9591606777370868}, {"mcc": 0.9334270023908063, "macro_f1": 0.966555460241862}], "total": {"test_mcc": 92.00639473935934, "test_mcc_se": 0.7602054009340591, "test_macro_f1": 95.99260440709241, "test_macro_f1_se": 0.38307562806358536}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "sentipolc16", "task": "sentiment-classification", "dataset_languages": ["it"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"mcc": 0.40781493178120604, "macro_f1": 0.511692961268993}, {"mcc": 0.42446029794485807, "macro_f1": 0.49131126129753994}, {"mcc": 0.43531716705363904, "macro_f1": 0.5116673891987166}, {"mcc": 0.41097884473587964, "macro_f1": 0.5252186707332526}, {"mcc": 0.4388121055085991, "macro_f1": 0.49412666699528934}, {"mcc": 0.39928853076946574, "macro_f1": 0.4669931183973577}, {"mcc": 0.45256920577642806, "macro_f1": 0.49700731290541383}, {"mcc": 0.45420491512616123, "macro_f1": 0.4872042922856643}, {"mcc": 0.38088002800203624, "macro_f1": 0.4502571471717922}, {"mcc": 0.47665673159467675, "macro_f1": 0.568946178740212}], "total": {"test_mcc": 42.8098275829295, "test_mcc_se": 1.7990400701330314, "test_macro_f1": 50.044249989942315, "test_macro_f1_se": 2.0144433741493843}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "suc3", "task": "named-entity-recognition", "dataset_languages": ["sv"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"micro_f1_no_misc": 0.4858604901696741, "micro_f1": 0.2941786402014516}, {"micro_f1_no_misc": 0.4396875903962973, "micro_f1": 0.3738738738738739}, {"micro_f1_no_misc": 0.46881011266831546, "micro_f1": 0.2943429581614614}, {"micro_f1_no_misc": 0.46683974510266696, "micro_f1": 0.32410099464422343}, {"micro_f1_no_misc": 0.49917808219178084, "micro_f1": 0.3390010626992561}, {"micro_f1_no_misc": 0.3661971830985915, "micro_f1": 0.2491971740526654}, {"micro_f1_no_misc": 0.455748767194394, "micro_f1": 0.2875912408759124}, {"micro_f1_no_misc": 0.5208053691275167, "micro_f1": 0.34940513462742645}, {"micro_f1_no_misc": 0.43681415929203543, "micro_f1": 0.3002442815900511}, {"micro_f1_no_misc": 0.5143233454066513, "micro_f1": 0.3926645091693635}], "total": {"test_micro_f1_no_misc": 46.542648446479234, "test_micro_f1_no_misc_se": 2.8062288423610493, "test_micro_f1": 32.04599869895685, "test_micro_f1_se": 2.7096886731424257}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "dansk", "task": "named-entity-recognition", "dataset_languages": ["da"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"micro_f1_no_misc": 0.41958041958041953, "micro_f1": 0.311969111969112}, {"micro_f1_no_misc": 0.33481858688733296, "micro_f1": 0.2476635514018692}, {"micro_f1_no_misc": 0.3692870201096893, "micro_f1": 0.27246848312322086}, {"micro_f1_no_misc": 0.39764359351988215, "micro_f1": 0.2819585934932632}, {"micro_f1_no_misc": 0.3514056224899598, "micro_f1": 0.20488768205381386}, {"micro_f1_no_misc": 0.3410918324164198, "micro_f1": 0.212807881773399}, {"micro_f1_no_misc": 0.3952225841476656, "micro_f1": 0.30375567478332643}, {"micro_f1_no_misc": 0.3440453686200378, "micro_f1": 0.2707670554966419}, {"micro_f1_no_misc": 0.3569553805774278, "micro_f1": 0.2115491393670183}, {"micro_f1_no_misc": 0.3945885005636978, "micro_f1": 0.22005367162722614}], "total": {"test_micro_f1_no_misc": 37.04638908912533, "test_micro_f1_no_misc_se": 1.8158066446851893, "test_micro_f1": 25.378808450888908, "test_micro_f1_se": 2.4743043364179003}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "norne-nb", "task": "named-entity-recognition", "dataset_languages": ["nb", "no"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"micro_f1_no_misc": 0.48862052949372975, "micro_f1": 0.33087742799732084}, {"micro_f1_no_misc": 0.4803551609322974, "micro_f1": 0.3142711258616288}, {"micro_f1_no_misc": 0.4454670961803957, "micro_f1": 0.3324771689497717}, {"micro_f1_no_misc": 0.46852425180598556, "micro_f1": 0.39536188516925375}, {"micro_f1_no_misc": 0.43391405342624856, "micro_f1": 0.32145839131644866}, {"micro_f1_no_misc": 0.48677007299270075, "micro_f1": 0.38063597532036075}, {"micro_f1_no_misc": 0.4508611955420466, "micro_f1": 0.3674242424242424}, {"micro_f1_no_misc": 0.42410602650662665, "micro_f1": 0.36318502959709764}, {"micro_f1_no_misc": 0.4662473794549266, "micro_f1": 0.37818181818181823}, {"micro_f1_no_misc": 0.45187528242205155, "micro_f1": 0.26616888108048753}], "total": {"test_micro_f1_no_misc": 45.967410487570085, "test_micro_f1_no_misc_se": 1.368935657360357, "test_micro_f1": 34.500419458984304, "test_micro_f1_se": 2.4278961748274828}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "norne-nn", "task": "named-entity-recognition", "dataset_languages": ["nn"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"micro_f1_no_misc": 0.4258153422140561, "micro_f1": 0.30108073744437386}, {"micro_f1_no_misc": 0.39493791786055393, "micro_f1": 0.28873812754409767}, {"micro_f1_no_misc": 0.5136286201022147, "micro_f1": 0.32189854909459004}, {"micro_f1_no_misc": 0.501709077098367, "micro_f1": 0.3706345302508608}, {"micro_f1_no_misc": 0.45601359966000854, "micro_f1": 0.3607915324436264}, {"micro_f1_no_misc": 0.43218298555377205, "micro_f1": 0.30009727626459143}, {"micro_f1_no_misc": 0.5286674574930802, "micro_f1": 0.4222255236963304}, {"micro_f1_no_misc": 0.4373915199603273, "micro_f1": 0.34797707935405453}, {"micro_f1_no_misc": 0.5247800901094186, "micro_f1": 0.36212307272479194}, {"micro_f1_no_misc": 0.43659244917715395, "micro_f1": 0.33357193987115247}], "total": {"test_micro_f1_no_misc": 46.51719059228952, "test_micro_f1_no_misc_se": 2.9588333183551603, "test_micro_f1": 34.0913836868847, "test_micro_f1_se": 2.5117425775362507}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mim-gold-ner", "task": "named-entity-recognition", "dataset_languages": ["is"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"micro_f1_no_misc": 0.4417593040115998, "micro_f1": 0.32970112079701125}, {"micro_f1_no_misc": 0.4559808612440192, "micro_f1": 0.2263575350823673}, {"micro_f1_no_misc": 0.40879689521345414, "micro_f1": 0.3275237273511648}, {"micro_f1_no_misc": 0.5232530407822561, "micro_f1": 0.3964314679643147}, {"micro_f1_no_misc": 0.38295486403139894, "micro_f1": 0.2835292451494123}, {"micro_f1_no_misc": 0.41049307748360453, "micro_f1": 0.30627126883811373}, {"micro_f1_no_misc": 0.40803795701925755, "micro_f1": 0.31847806940285367}, {"micro_f1_no_misc": 0.46650573325286665, "micro_f1": 0.3138586956521739}, {"micro_f1_no_misc": 0.4468452895419187, "micro_f1": 0.3618215845290081}, {"micro_f1_no_misc": 0.42640990371389276, "micro_f1": 0.2868757259001162}], "total": {"test_micro_f1_no_misc": 43.710369262942685, "test_micro_f1_no_misc_se": 2.458459604528835, "test_micro_f1": 31.50848440666536, "test_micro_f1_se": 2.842165171999587}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fone", "task": "named-entity-recognition", "dataset_languages": ["fo"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"micro_f1_no_misc": 0.541111901616018, "micro_f1": 0.4608418581251048}, {"micro_f1_no_misc": 0.57331840846647, "micro_f1": 0.5326596896427281}, {"micro_f1_no_misc": 0.5859269282814614, "micro_f1": 0.4698526127735596}, {"micro_f1_no_misc": 0.5525147165134773, "micro_f1": 0.4563218390804597}, {"micro_f1_no_misc": 0.5562595586628797, "micro_f1": 0.5059715639810427}, {"micro_f1_no_misc": 0.584214307708098, "micro_f1": 0.4732886783205359}, {"micro_f1_no_misc": 0.537125152152263, "micro_f1": 0.47086614173228347}, {"micro_f1_no_misc": 0.5141848145968215, "micro_f1": 0.47419449613162273}, {"micro_f1_no_misc": 0.5324675324675324, "micro_f1": 0.47480971366437114}, {"micro_f1_no_misc": 0.5789923142613151, "micro_f1": 0.529522376833396}], "total": {"test_micro_f1_no_misc": 55.56115634726336, "test_micro_f1_no_misc_se": 1.5197760514374261, "test_micro_f1": 48.48328970285103, "test_micro_f1_se": 1.714453383296268}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "germeval", "task": "named-entity-recognition", "dataset_languages": ["de"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"micro_f1_no_misc": 0.47869542346133614, "micro_f1": 0.3734830646621989}, {"micro_f1_no_misc": 0.4907905922357609, "micro_f1": 0.36336051583538775}, {"micro_f1_no_misc": 0.48309178743961356, "micro_f1": 0.3476257973068746}, {"micro_f1_no_misc": 0.3941965507801807, "micro_f1": 0.27054322876817133}, {"micro_f1_no_misc": 0.4648896531957581, "micro_f1": 0.33251184834123226}, {"micro_f1_no_misc": 0.49841964502796005, "micro_f1": 0.3783517025826616}, {"micro_f1_no_misc": 0.4508060033351862, "micro_f1": 0.39284906087350085}, {"micro_f1_no_misc": 0.42398884239888424, "micro_f1": 0.29941567600241786}, {"micro_f1_no_misc": 0.4938450398262128, "micro_f1": 0.35564570655848554}, {"micro_f1_no_misc": 0.5114828466118514, "micro_f1": 0.37402496099843996}], "total": {"test_micro_f1_no_misc": 46.902063843127436, "test_micro_f1_no_misc_se": 2.265519030291117, "test_micro_f1": 34.8781156192937, "test_micro_f1_se": 2.3689672371637376}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "conll-nl", "task": "named-entity-recognition", "dataset_languages": ["nl"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"micro_f1_no_misc": 0.49610894941634237, "micro_f1": 0.34917127071823195}, {"micro_f1_no_misc": 0.4241469816272965, "micro_f1": 0.3451078462019381}, {"micro_f1_no_misc": 0.4591549295774648, "micro_f1": 0.29192951925168587}, {"micro_f1_no_misc": 0.4548780487804878, "micro_f1": 0.33002481389578164}, {"micro_f1_no_misc": 0.4465841357175607, "micro_f1": 0.3015026660203587}, {"micro_f1_no_misc": 0.5180878552971576, "micro_f1": 0.34867591424968475}, {"micro_f1_no_misc": 0.5366132723112128, "micro_f1": 0.32736705577172504}, {"micro_f1_no_misc": 0.5124747134187457, "micro_f1": 0.32562683165092804}, {"micro_f1_no_misc": 0.5093432995194873, "micro_f1": 0.3229946524064171}, {"micro_f1_no_misc": 0.5062295081967213, "micro_f1": 0.31612492033142126}], "total": {"test_micro_f1_no_misc": 48.63621693862477, "test_micro_f1_no_misc_se": 2.302099146896388, "test_micro_f1": 32.585254904981724, "test_micro_f1_se": 1.187139352399259}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "conll-en", "task": "named-entity-recognition", "dataset_languages": ["en"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"micro_f1_no_misc": 0.36684748328497535, "micro_f1": 0.33969732246798606}, {"micro_f1_no_misc": 0.3436719094517125, "micro_f1": 0.31343676045227453}, {"micro_f1_no_misc": 0.5213886360681316, "micro_f1": 0.4809404245606026}, {"micro_f1_no_misc": 0.3781838316722038, "micro_f1": 0.3407981984236207}, {"micro_f1_no_misc": 0.5577542567878508, "micro_f1": 0.5277659120034174}, {"micro_f1_no_misc": 0.563217031342401, "micro_f1": 0.5325535201942178}, {"micro_f1_no_misc": 0.34923245614035087, "micro_f1": 0.32427088482451805}, {"micro_f1_no_misc": 0.52251632863527, "micro_f1": 0.496421052631579}, {"micro_f1_no_misc": 0.5184407096171801, "micro_f1": 0.4882231629219581}, {"micro_f1_no_misc": 0.4989444053483462, "micro_f1": 0.46731601731601735}], "total": {"test_micro_f1_no_misc": 46.201970483484224, "test_micro_f1_no_misc_se": 5.618311410351341, "test_micro_f1": 43.11423255796192, "test_micro_f1_se": 5.571697128482676}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "eltec", "task": "named-entity-recognition", "dataset_languages": ["fr"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"micro_f1_no_misc": 0.3981900452488688, "micro_f1": 0.34310409620443444}, {"micro_f1_no_misc": 0.4486948694869487, "micro_f1": 0.4243146603098928}, {"micro_f1_no_misc": 0.4968713789107764, "micro_f1": 0.40817057874932816}, {"micro_f1_no_misc": 0.45529513888888884, "micro_f1": 0.37754924176398813}, {"micro_f1_no_misc": 0.5120359955005624, "micro_f1": 0.4839178252749533}, {"micro_f1_no_misc": 0.4826067811536768, "micro_f1": 0.3724115441056579}, {"micro_f1_no_misc": 0.48538644728296165, "micro_f1": 0.41469445425644647}, {"micro_f1_no_misc": 0.5003806140573458, "micro_f1": 0.4209670865501829}, {"micro_f1_no_misc": 0.46163963174046474, "micro_f1": 0.40052800301716013}, {"micro_f1_no_misc": 0.48168950797573584, "micro_f1": 0.38127659574468087}], "total": {"test_micro_f1_no_misc": 47.2279041024623, "test_micro_f1_no_misc_se": 2.044125641917875, "test_micro_f1": 40.26934085976725, "test_micro_f1_se": 2.367944602731736}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "multinerd-it", "task": "named-entity-recognition", "dataset_languages": ["it"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"micro_f1_no_misc": 0.4938235294117647, "micro_f1": 0.3611612268328686}, {"micro_f1_no_misc": 0.4847993514389947, "micro_f1": 0.44044982109388314}, {"micro_f1_no_misc": 0.45376112159611753, "micro_f1": 0.4030185967118858}, {"micro_f1_no_misc": 0.40052242054854154, "micro_f1": 0.3820478974165566}, {"micro_f1_no_misc": 0.6134253117537808, "micro_f1": 0.5363680387409201}, {"micro_f1_no_misc": 0.5198191008246874, "micro_f1": 0.4057996640438511}, {"micro_f1_no_misc": 0.5164590163934425, "micro_f1": 0.4233123826626398}, {"micro_f1_no_misc": 0.4698196278937651, "micro_f1": 0.41899915182357933}, {"micro_f1_no_misc": 0.5179738562091504, "micro_f1": 0.41917126089276185}, {"micro_f1_no_misc": 0.587229763700352, "micro_f1": 0.44849484536082473}], "total": {"test_micro_f1_no_misc": 50.576330997705966, "test_micro_f1_no_misc_se": 3.828712669331248, "test_micro_f1": 42.38822885579771, "test_micro_f1_se": 2.9236934198176194}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-sv", "task": "linguistic-acceptability", "dataset_languages": ["sv"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"mcc": 0.22406483361831775, "macro_f1": 0.5704879893835193}, {"mcc": 0.17186282619311585, "macro_f1": 0.5222113172332792}, {"mcc": 0.2972502030289459, "macro_f1": 0.6402313386409098}, {"mcc": 0.25976635903055, "macro_f1": 0.6225191824226255}, {"mcc": 0.28713408686921166, "macro_f1": 0.6388889687456578}, {"mcc": 0.25757416578703113, "macro_f1": 0.5986796119642994}, {"mcc": 0.2211979664182185, "macro_f1": 0.5982864288813297}, {"mcc": 0.23472973303162348, "macro_f1": 0.6029664152666467}, {"mcc": 0.2588804361565435, "macro_f1": 0.6278785844003235}, {"mcc": 0.23465309994556627, "macro_f1": 0.5896884860263303}], "total": {"test_mcc": 24.47113710079124, "test_mcc_se": 2.2289828349579235, "test_macro_f1": 60.11838322964922, "test_macro_f1_se": 2.2100704813363037}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-da", "task": "linguistic-acceptability", "dataset_languages": ["da"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"mcc": 0.10640414262012653, "macro_f1": 0.41635094811359163}, {"mcc": 0.20066980976938356, "macro_f1": 0.5929440993788819}, {"mcc": 0.2963624432005725, "macro_f1": 0.6477132860709603}, {"mcc": 0.2055765053450238, "macro_f1": 0.49136699283702634}, {"mcc": 0.2813041074000935, "macro_f1": 0.6291805385199463}, {"mcc": 0.15915184217883818, "macro_f1": 0.42168841042212496}, {"mcc": 0.26586267706955363, "macro_f1": 0.5796037998628059}, {"mcc": 0.24696876504739823, "macro_f1": 0.6176091149428202}, {"mcc": 0.27451022425669475, "macro_f1": 0.6353080231709052}, {"mcc": 0.3282749654169275, "macro_f1": 0.6147794152938902}], "total": {"test_mcc": 23.650854823046124, "test_mcc_se": 4.212750751201802, "test_macro_f1": 56.46544628612953, "test_macro_f1_se": 5.473946160618015}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-nb", "task": "linguistic-acceptability", "dataset_languages": ["nb", "no"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"mcc": 0.13878958269648708, "macro_f1": 0.4152270401848458}, {"mcc": 0.22644383751487654, "macro_f1": 0.5576241663724515}, {"mcc": 0.3218871415189306, "macro_f1": 0.6531258078606512}, {"mcc": 0.1613028464092419, "macro_f1": 0.44249761443399155}, {"mcc": 0.2267839136966299, "macro_f1": 0.5886948160910324}, {"mcc": 0.20525503557679076, "macro_f1": 0.5637790127492645}, {"mcc": 0.2094431219771468, "macro_f1": 0.5918453476512584}, {"mcc": 0.1484158705264888, "macro_f1": 0.47786113949301806}, {"mcc": 0.2494349961818073, "macro_f1": 0.5864529750237915}, {"mcc": 0.22983986437246662, "macro_f1": 0.6081053463666686}], "total": {"test_mcc": 21.175962104708663, "test_mcc_se": 3.3471060892918905, "test_macro_f1": 54.852132662269725, "test_macro_f1_se": 4.789381553144099}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-nn", "task": "linguistic-acceptability", "dataset_languages": ["nn"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"mcc": 0.03639173622774329, "macro_f1": 0.338639197171296}, {"mcc": 0.17713616611356692, "macro_f1": 0.5224175634103854}, {"mcc": 0.22201310673587443, "macro_f1": 0.6071537154952731}, {"mcc": 0.14625823802861754, "macro_f1": 0.49381798691085044}, {"mcc": 0.1692028240173201, "macro_f1": 0.557873273410427}, {"mcc": 0.1694155821012445, "macro_f1": 0.5249447520269578}, {"mcc": 0.2085007781656273, "macro_f1": 0.5281385281385281}, {"mcc": 0.1436708453468528, "macro_f1": 0.42118240348861213}, {"mcc": 0.06434606494340771, "macro_f1": 0.37962986429161916}, {"mcc": 0.12931842707547897, "macro_f1": 0.5198681568654759}], "total": {"test_mcc": 14.662537687557336, "test_mcc_se": 3.620059405700978, "test_macro_f1": 48.93665441209425, "test_macro_f1_se": 5.176962707667025}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-is", "task": "linguistic-acceptability", "dataset_languages": ["is"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"mcc": 0.03428637472218052, "macro_f1": 0.3521782507482541}, {"mcc": 0.0770712875504861, "macro_f1": 0.3797952285827473}, {"mcc": 0.10545700218495221, "macro_f1": 0.4446526491814274}, {"mcc": 0.10432038770564091, "macro_f1": 0.38574209875130233}, {"mcc": 0.008650093741974296, "macro_f1": 0.346661065426668}, {"mcc": 0.04762141567538988, "macro_f1": 0.3299788738317526}, {"mcc": 0.11587650224434085, "macro_f1": 0.45855081020295574}, {"mcc": 0.06308944934738035, "macro_f1": 0.3660057879545695}, {"mcc": 0.010918665375177425, "macro_f1": 0.3409882511466665}, {"mcc": 0.12805528772683114, "macro_f1": 0.5426089705265906}], "total": {"test_mcc": 6.953464662743538, "test_mcc_se": 2.696647112540296, "test_macro_f1": 39.47161986352934, "test_macro_f1_se": 4.171349025029598}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-fo", "task": "linguistic-acceptability", "dataset_languages": ["fo"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"mcc": 0.047980175739443395, "macro_f1": 0.42822943310430145}, {"mcc": 0.0752844793304108, "macro_f1": 0.5225038703299573}, {"mcc": -0.008066063228769757, "macro_f1": 0.4648949274009929}, {"mcc": 0.03638999530910996, "macro_f1": 0.44544332040811907}, {"mcc": 0.03716263075631179, "macro_f1": 0.5183829855574548}, {"mcc": 0.05140322303286482, "macro_f1": 0.5060728744939271}, {"mcc": 0.07563280217335422, "macro_f1": 0.4141788621135888}, {"mcc": -0.03138824102871722, "macro_f1": 0.33712685418072896}, {"mcc": 0.018942418860342983, "macro_f1": 0.5018112331625655}, {"mcc": -0.018128046319872217, "macro_f1": 0.49010989010989015}], "total": {"test_mcc": 2.8521337462447884, "test_mcc_se": 2.3228561291345606, "test_macro_f1": 46.28754250861526, "test_macro_f1_se": 3.6053543598133078}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-de", "task": "linguistic-acceptability", "dataset_languages": ["de"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"mcc": 0.14750826583250398, "macro_f1": 0.5519548808139412}, {"mcc": 0.10030373493119636, "macro_f1": 0.5426353374898978}, {"mcc": 0.048183766785283966, "macro_f1": 0.46880856760374834}, {"mcc": 0.13924703032481478, "macro_f1": 0.502247333063043}, {"mcc": 0.20866222641159815, "macro_f1": 0.6038292599443524}, {"mcc": 0.09875530240250269, "macro_f1": 0.4711181287986802}, {"mcc": 0.15985581181424552, "macro_f1": 0.5745425800605395}, {"mcc": 0.12342798340037177, "macro_f1": 0.5570095984858726}, {"mcc": 0.08492439705158562, "macro_f1": 0.5169690640256243}, {"mcc": 0.07593429804327424, "macro_f1": 0.4501333996845178}], "total": {"test_mcc": 11.868028169973769, "test_mcc_se": 2.902734258971385, "test_macro_f1": 52.39248149970217, "test_macro_f1_se": 3.1310324407799723}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-nl", "task": "linguistic-acceptability", "dataset_languages": ["nl"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"mcc": 0.049931809044471955, "macro_f1": 0.34777130977519305}, {"mcc": 0.06016204016487315, "macro_f1": 0.3770582657714389}, {"mcc": 0.09854935223661103, "macro_f1": 0.35470206577671715}, {"mcc": 0.039788950879807335, "macro_f1": 0.3450570292675556}, {"mcc": 0.07392599016072825, "macro_f1": 0.4116401196103243}, {"mcc": 0.04813811904902458, "macro_f1": 0.346925797278511}, {"mcc": 0.01028100901240377, "macro_f1": 0.3378976453819386}, {"mcc": 0.0368192227693039, "macro_f1": 0.3375702745419605}, {"mcc": -0.012654956768592069, "macro_f1": 0.3346354166666667}, {"mcc": -0.02201631383001683, "macro_f1": 0.3339837398373984}], "total": {"test_mcc": 3.8292522271861507, "test_mcc_se": 2.3221538613502273, "test_macro_f1": 35.272416639077036, "test_macro_f1_se": 1.5057823227807374}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-en", "task": "linguistic-acceptability", "dataset_languages": ["en"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"mcc": 0.22805119238382762, "macro_f1": 0.5369089090766516}, {"mcc": 0.1859561519685992, "macro_f1": 0.586432509204973}, {"mcc": 0.19590087681220428, "macro_f1": 0.5947982097235829}, {"mcc": 0.21869951435243462, "macro_f1": 0.5760120683168839}, {"mcc": 0.18383274952511944, "macro_f1": 0.5863033167586997}, {"mcc": 0.17034423098616328, "macro_f1": 0.5666931805703971}, {"mcc": 0.18434798648300044, "macro_f1": 0.5874917428609587}, {"mcc": 0.17356132317891929, "macro_f1": 0.553125}, {"mcc": 0.20204099192299463, "macro_f1": 0.4914390578509452}, {"mcc": 0.16248238089339645, "macro_f1": 0.5779331260957377}], "total": {"test_mcc": 19.052173985066588, "test_mcc_se": 1.2982722550467305, "test_macro_f1": 56.571371204588296, "test_macro_f1_se": 1.9528697467633895}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-fr", "task": "linguistic-acceptability", "dataset_languages": ["fr"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"mcc": 0.09879472037651489, "macro_f1": 0.45695800467793163}, {"mcc": 0.08881655885715692, "macro_f1": 0.38528834548984625}, {"mcc": 0.0869080688461517, "macro_f1": 0.42001777652824696}, {"mcc": 0.0763319199384925, "macro_f1": 0.48197646128854876}, {"mcc": 0.07994930976066521, "macro_f1": 0.5171963513013764}, {"mcc": 0.12283718844453653, "macro_f1": 0.5206818131637632}, {"mcc": 0.10200797916141893, "macro_f1": 0.504392316307466}, {"mcc": 0.08218216022825314, "macro_f1": 0.44093567251461985}, {"mcc": 0.12544147458986496, "macro_f1": 0.5052833263063458}, {"mcc": 0.07230904151064765, "macro_f1": 0.43550593620007333}], "total": {"test_mcc": 9.355784217137025, "test_mcc_se": 1.15072334936386, "test_macro_f1": 46.682360037782175, "test_macro_f1_se": 2.8586885798853783}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-it", "task": "linguistic-acceptability", "dataset_languages": ["it"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"mcc": 0.09295280429489841, "macro_f1": 0.5463814193462248}, {"mcc": 0.053874789683248124, "macro_f1": 0.42160068187045363}, {"mcc": 0.013775460964291962, "macro_f1": 0.3624154523722752}, {"mcc": -0.0021733867621604517, "macro_f1": 0.342299527660012}, {"mcc": 0.037270436274687825, "macro_f1": 0.375158791153247}, {"mcc": 0.09629916935043958, "macro_f1": 0.4181588023116565}, {"mcc": 0.03579404028977776, "macro_f1": 0.4401903610652814}, {"mcc": 0.07286534409106914, "macro_f1": 0.44534156654445495}, {"mcc": 0.09473112003381115, "macro_f1": 0.45696012917327833}, {"mcc": -0.00017280364464369852, "macro_f1": 0.33418341530552353}], "total": {"test_mcc": 4.9521697457541975, "test_mcc_se": 2.395197043010434, "test_macro_f1": 41.42690146802408, "test_macro_f1_se": 3.9630057237275187}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"f1": 65.94589412370159, "em": 59.166666666666664}, {"f1": 63.88737915273168, "em": 59.13570887035633}, {"f1": 64.69179683188067, "em": 59.82905982905983}, {"f1": 63.915335262543934, "em": 59.208523592085236}, {"f1": 65.83384560571736, "em": 60.755589822667694}, {"f1": 64.76343556159453, "em": 58.85496183206107}, {"f1": 63.23068478227672, "em": 58.03432137285491}, {"f1": 65.14248586270168, "em": 60.613207547169814}, {"f1": 64.07562488876411, "em": 59.21052631578947}, {"f1": 64.86800860202702, "em": 58.641975308641975}], "total": {"test_f1": 64.63544906739392, "test_f1_se": 0.5406934195973309, "test_em": 59.345054115735294, "test_em_se": 0.5225442357121111}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "norquad", "task": "reading-comprehension", "dataset_languages": ["nb", "nn", "no"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"f1": 65.1288054141303, "em": 42.8921568627451}, {"f1": 53.38934731253737, "em": 31.951219512195124}, {"f1": 63.09326635746483, "em": 41.96502914238135}, {"f1": 56.97653797591348, "em": 34.75061324611611}, {"f1": 68.7398556188004, "em": 48.717948717948715}, {"f1": 56.096152412689136, "em": 33.251028806584365}, {"f1": 55.519439025325084, "em": 33.02521008403362}, {"f1": 67.36135179828463, "em": 46.19932432432432}, {"f1": 54.73796809202938, "em": 32.89036544850498}, {"f1": 55.34848682261859, "em": 33.02980132450331}], "total": {"test_f1": 59.639121082979315, "test_f1_se": 3.5940949275135536, "test_em": 37.8672697469337, "test_em_se": 3.9570679402653357}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scandiqa-sv", "task": "reading-comprehension", "dataset_languages": ["sv"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"f1": 62.93559279154194, "em": 56.59090909090909}, {"f1": 64.31424463549348, "em": 58.605003790750565}, {"f1": 65.13470222691251, "em": 59.28515928515929}, {"f1": 65.50261198595874, "em": 59.817351598173516}, {"f1": 65.55290306118003, "em": 60.98689282960679}, {"f1": 62.53265833472012, "em": 56.25954198473283}, {"f1": 65.53029607815326, "em": 59.90639625585023}, {"f1": 65.4741295515435, "em": 59.5125786163522}, {"f1": 64.0672267593593, "em": 58.204334365325074}, {"f1": 63.610977316902634, "em": 57.25308641975309}], "total": {"test_f1": 64.46553427417656, "test_f1_se": 0.7110597478414289, "test_em": 58.64212542366126, "test_em_se": 0.9614286371839631}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "nqii", "task": "reading-comprehension", "dataset_languages": ["is"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"f1": 57.496406209092136, "em": 29.23076923076923}, {"f1": 49.52979708808042, "em": 21.339563862928348}, {"f1": 49.655638145393866, "em": 26.947040498442366}, {"f1": 50.53889316496926, "em": 26.00896860986547}, {"f1": 56.98729665673566, "em": 31.661442006269592}, {"f1": 53.70294427937911, "em": 29.892141756548536}, {"f1": 56.89041304302081, "em": 30.76923076923077}, {"f1": 54.16065038869812, "em": 30.368098159509202}, {"f1": 53.84906399059191, "em": 27.259036144578314}, {"f1": 58.22764079460344, "em": 33.734939759036145}], "total": {"test_f1": 54.10387437605648, "test_f1_se": 2.0477661691706848, "test_em": 28.721123079717803, "test_em_se": 2.161441483723378}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "foqa", "task": "reading-comprehension", "dataset_languages": ["fo"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"f1": 60.348136896625476, "em": 41.42156862745098}, {"f1": 52.78432691194415, "em": 37.1007371007371}, {"f1": 60.99978806884587, "em": 40.98765432098765}, {"f1": 59.77367406226873, "em": 41.262135922330096}, {"f1": 60.807816531308966, "em": 41.08910891089109}, {"f1": 60.19023197473409, "em": 42.71604938271605}, {"f1": 63.6683815750838, "em": 44.10377358490566}, {"f1": 61.00901030282839, "em": 40.20356234096692}, {"f1": 63.53807079796343, "em": 46.38403990024938}, {"f1": 60.33970753284194, "em": 39.31203931203931}], "total": {"test_f1": 60.34591446544448, "test_f1_se": 1.8460406586916702, "test_em": 41.45806694032743, "test_em_se": 1.5789888342545724}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "germanquad", "task": "reading-comprehension", "dataset_languages": ["de"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"f1": 52.00160534415341, "em": 28.939393939393938}, {"f1": 49.223088847853504, "em": 25.170583775587566}, {"f1": 50.55125112345117, "em": 26.651126651126653}, {"f1": 52.6872594306721, "em": 29.14764079147641}, {"f1": 53.515651341063226, "em": 28.758673862760215}, {"f1": 54.06190721215048, "em": 27.480916030534353}, {"f1": 46.842452323817014, "em": 25.039001560062403}, {"f1": 50.17540058358813, "em": 26.17924528301887}, {"f1": 53.319947676906395, "em": 28.25077399380805}, {"f1": 45.40017434368021, "em": 24.305555555555557}], "total": {"test_f1": 50.77787382273356, "test_f1_se": 1.8143925953752718, "test_em": 26.992291144332405, "test_em_se": 1.1031340448602538}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "squad", "task": "reading-comprehension", "dataset_languages": ["en"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"f1": 78.532300070501, "em": 68.18181818181819}, {"f1": 80.57334230907688, "em": 70.12888551933283}, {"f1": 82.02793473065562, "em": 72.88267288267288}, {"f1": 78.4480524870209, "em": 69.558599695586}, {"f1": 78.8568690760271, "em": 69.54510408635312}, {"f1": 70.75919562733273, "em": 59.61832061068702}, {"f1": 77.04650152011142, "em": 66.84867394695787}, {"f1": 81.20221606578329, "em": 70.75471698113208}, {"f1": 79.17215445951713, "em": 68.57585139318886}, {"f1": 76.05959805862778, "em": 65.12345679012346}], "total": {"test_f1": 78.2678164404654, "test_f1_se": 1.9825099619763502, "test_em": 68.12181000878522, "test_em_se": 2.271182330521317}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "squad-nl", "task": "reading-comprehension", "dataset_languages": ["nl"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"f1": 63.2933452244201, "em": 50.378787878787875}, {"f1": 60.476442085635334, "em": 48.59742228961334}, {"f1": 57.85790433682261, "em": 46.697746697746695}, {"f1": 62.22321282560759, "em": 50.91324200913242}, {"f1": 63.0433270819267, "em": 51.349267540478024}, {"f1": 64.76086123804447, "em": 48.85496183206107}, {"f1": 58.49792972900451, "em": 46.957878315132604}, {"f1": 61.44744798415418, "em": 50.471698113207545}, {"f1": 60.250554325436845, "em": 50.61919504643963}, {"f1": 62.058264278154276, "em": 47.608024691358025}], "total": {"test_f1": 61.390928910920664, "test_f1_se": 1.3363138578495974, "test_em": 49.244822441395726, "test_em_se": 1.0707542969232549}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "squad-it", "task": "reading-comprehension", "dataset_languages": ["it"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"f1": 57.269686807819845, "em": 44.166666666666664}, {"f1": 58.836183873806405, "em": 42.68385140257771}, {"f1": 54.25909355921545, "em": 41.64724164724165}, {"f1": 58.70231494524181, "em": 45.58599695585997}, {"f1": 58.62111589600526, "em": 42.713955281418656}, {"f1": 60.11120169255268, "em": 46.030534351145036}, {"f1": 59.11608883602281, "em": 45.78783151326053}, {"f1": 59.35549822791694, "em": 43.710691823899374}, {"f1": 57.243364313614364, "em": 44.969040247678016}, {"f1": 59.67951274067723, "em": 42.824074074074076}], "total": {"test_f1": 58.319406089287284, "test_f1_se": 1.0538579253163218, "test_em": 44.01198839638217, "test_em_se": 0.9510053438916218}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fquad", "task": "reading-comprehension", "dataset_languages": ["fr"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"f1": 59.64928183417452, "em": 35.303030303030305}, {"f1": 64.85696678972998, "em": 41.62244124336619}, {"f1": 65.06424158449285, "em": 39.23853923853924}, {"f1": 61.31358114255448, "em": 35.99695585996956}, {"f1": 57.75583083091683, "em": 35.31225905936777}, {"f1": 64.8961573508033, "em": 39.00763358778626}, {"f1": 61.28364927076581, "em": 37.67550702028081}, {"f1": 61.10001506684312, "em": 37.971698113207545}, {"f1": 58.731889045991856, "em": 38.62229102167183}, {"f1": 64.31882875399329, "em": 38.19444444444444}], "total": {"test_f1": 61.89704416702661, "test_f1_se": 1.693938874751517, "test_em": 37.89447998916639, "test_em_se": 1.215287369882454}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "nordjylland-news", "task": "summarization", "dataset_languages": ["da"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"bertscore": 0.6755982581817079, "rouge_l": 0.22780686849866466}, {"bertscore": 0.6727546528127277, "rouge_l": 0.21947516072147702}, {"bertscore": 0.6761359542288119, "rouge_l": 0.23098929626492348}, {"bertscore": 0.6386142538685817, "rouge_l": 0.17694492537414935}, {"bertscore": 0.6742672528780531, "rouge_l": 0.22717656809398934}, {"bertscore": 0.6762768817861797, "rouge_l": 0.22861512244108867}, {"bertscore": 0.6662315816793125, "rouge_l": 0.20383268022633488}, {"bertscore": 0.6520740341511555, "rouge_l": 0.1902463806953059}, {"bertscore": 0.671774224487308, "rouge_l": 0.219631117253635}, {"bertscore": 0.6815172193892067, "rouge_l": 0.23624972648049208}], "total": {"test_bertscore": 66.85244313463045, "test_bertscore_se": 0.8183152464204604, "test_rouge_l": 21.6096784605006, "test_rouge_l_se": 1.2082161484129408}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mlsum", "task": "summarization", "dataset_languages": ["de"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"bertscore": 0.5775251517989091, "rouge_l": 0.14678672380162358}, {"bertscore": 0.592892655840842, "rouge_l": 0.14217844766431875}, {"bertscore": 0.6309063238295494, "rouge_l": 0.18922998889149878}, {"bertscore": 0.5930287066366873, "rouge_l": 0.1524288753521163}, {"bertscore": 0.6560065487283282, "rouge_l": 0.1802698318285349}, {"bertscore": 0.5995740587677574, "rouge_l": 0.15227235146262394}, {"bertscore": 0.6559776889916975, "rouge_l": 0.1999292299389142}, {"bertscore": 0.5494373234541854, "rouge_l": 0.11909848793492786}, {"bertscore": 0.6735741187003441, "rouge_l": 0.21620001671473327}, {"bertscore": 0.6209046477015363, "rouge_l": 0.1747671568999941}], "total": {"test_bertscore": 61.49827224449837, "test_bertscore_se": 2.4438308026966764, "test_rouge_l": 16.731611104892856, "test_rouge_l_se": 1.845944548819375}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "rrn", "task": "summarization", "dataset_languages": ["is"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"bertscore": 0.6663574867125135, "rouge_l": 0.18089122293805837}, {"bertscore": 0.6435948581201956, "rouge_l": 0.14981630977712398}, {"bertscore": 0.6649261729908176, "rouge_l": 0.18900420389376954}, {"bertscore": 0.6339238843938801, "rouge_l": 0.14553799220106617}, {"bertscore": 0.6206715877051465, "rouge_l": 0.12789640333257585}, {"bertscore": 0.6568165820499416, "rouge_l": 0.1697791797596807}, {"bertscore": 0.6342409856442828, "rouge_l": 0.1411917297901238}, {"bertscore": 0.6211824136262294, "rouge_l": 0.13079919155768355}, {"bertscore": 0.5912430407188367, "rouge_l": 0.1196625802256141}, {"bertscore": 0.6299470811500214, "rouge_l": 0.13716777602654495}], "total": {"test_bertscore": 63.62904093111865, "test_bertscore_se": 1.4239396288127397, "test_rouge_l": 14.917465895022413, "test_rouge_l_se": 1.4461845195043384}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "no-sammendrag", "task": "summarization", "dataset_languages": ["nb", "nn", "no"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"bertscore": 0.5596199082792737, "rouge_l": 0.06956773038096248}, {"bertscore": 0.5955545183096547, "rouge_l": 0.12048186826456331}, {"bertscore": 0.5450138321320992, "rouge_l": 0.08447669288048018}, {"bertscore": 0.6037737727092463, "rouge_l": 0.1352946310371792}, {"bertscore": 0.6103896800050279, "rouge_l": 0.13687683020433578}, {"bertscore": 0.6072858501938754, "rouge_l": 0.13182435567416634}, {"bertscore": 0.5842696737527149, "rouge_l": 0.1309013712616579}, {"bertscore": 0.6179164788482012, "rouge_l": 0.14415479886461113}, {"bertscore": 0.6028977887326619, "rouge_l": 0.13520711999960427}, {"bertscore": 0.5115830429349444, "rouge_l": 0.07217138333971862}], "total": {"test_bertscore": 58.38304545897699, "test_bertscore_se": 2.1299989387466063, "test_rouge_l": 11.609567819072792, "test_rouge_l_se": 1.7931363712227457}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "wiki-lingua-nl", "task": "summarization", "dataset_languages": ["nl"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"bertscore": 0.5960396400623722, "rouge_l": 0.14722788772632972}, {"bertscore": 0.5516578363240114, "rouge_l": 0.1299030521498031}, {"bertscore": 0.6023945653432747, "rouge_l": 0.11202307875136716}, {"bertscore": 0.6144500051086652, "rouge_l": 0.1649158047959597}, {"bertscore": 0.5647127390620881, "rouge_l": 0.14708004874466052}, {"bertscore": 0.6106350989066414, "rouge_l": 0.17580632917193767}, {"bertscore": 0.5746254673649673, "rouge_l": 0.14877401544498786}, {"bertscore": 0.5440374842728488, "rouge_l": 0.12248566959308368}, {"bertscore": 0.6016018110312871, "rouge_l": 0.16455639085394572}, {"bertscore": 0.6011466598429251, "rouge_l": 0.1617074002693722}], "total": {"test_bertscore": 58.613013073190814, "test_bertscore_se": 1.5708964790261368, "test_rouge_l": 14.744796775014473, "test_rouge_l_se": 1.2724338694656279}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "swedn", "task": "summarization", "dataset_languages": ["sv"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"bertscore": 0.5949173653934849, "rouge_l": 0.1480722392982261}, {"bertscore": 0.638114927438437, "rouge_l": 0.18051245367931745}, {"bertscore": 0.6233412651054095, "rouge_l": 0.17970164796713517}, {"bertscore": 0.5876500649173977, "rouge_l": 0.1436335059067365}, {"bertscore": 0.6289788687718101, "rouge_l": 0.17310757935787968}, {"bertscore": 0.6350262536579976, "rouge_l": 0.17807739118342708}, {"bertscore": 0.6142044252192136, "rouge_l": 0.16704556662569797}, {"bertscore": 0.6353731668641558, "rouge_l": 0.17530192945537382}, {"bertscore": 0.6437680405651918, "rouge_l": 0.1753020929129151}, {"bertscore": 0.6328050269730738, "rouge_l": 0.17853713876695465}], "total": {"test_bertscore": 62.34179404906172, "test_bertscore_se": 1.1693330967662863, "test_rouge_l": 16.992915451536636, "test_rouge_l_se": 0.8246861644048846}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "cnn-dailymail", "task": "summarization", "dataset_languages": ["en"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"bertscore": 0.6629634045966668, "rouge_l": 0.18334861342118275}, {"bertscore": 0.6589251717668958, "rouge_l": 0.18697653622562538}, {"bertscore": 0.6547425992685021, "rouge_l": 0.18459796540767076}, {"bertscore": 0.6731486048083752, "rouge_l": 0.1896119938708928}, {"bertscore": 0.6360504250478698, "rouge_l": 0.17352443270142848}, {"bertscore": 0.6637936410988914, "rouge_l": 0.1901186252200841}, {"bertscore": 0.6614922706794459, "rouge_l": 0.18649816919241663}, {"bertscore": 0.6354386281309417, "rouge_l": 0.17888563661056128}, {"bertscore": 0.6432388029061258, "rouge_l": 0.17199066091787704}, {"bertscore": 0.6566868846712168, "rouge_l": 0.18427723517951727}], "total": {"test_bertscore": 65.46480432974931, "test_bertscore_se": 0.7756266082314281, "test_rouge_l": 18.298298687472563, "test_rouge_l_se": 0.3890854713665369}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "orange-sum", "task": "summarization", "dataset_languages": ["fr"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"bertscore": 0.6492435184773058, "rouge_l": 0.19084485608529303}, {"bertscore": 0.6293725316936616, "rouge_l": 0.16541069590987345}, {"bertscore": 0.6187800545012578, "rouge_l": 0.16545959787192283}, {"bertscore": 0.6373321470746305, "rouge_l": 0.1752420882845936}, {"bertscore": 0.6385885660420172, "rouge_l": 0.18169440723170666}, {"bertscore": 0.6159045849053655, "rouge_l": 0.16297506725891858}, {"bertscore": 0.646499095892068, "rouge_l": 0.17566046983728073}, {"bertscore": 0.6219884334423114, "rouge_l": 0.174137243371944}, {"bertscore": 0.6356785930984188, "rouge_l": 0.1765232868777329}, {"bertscore": 0.6063005719333887, "rouge_l": 0.1562647775669448}], "total": {"test_bertscore": 62.99688097060425, "test_bertscore_se": 0.8650447699408479, "test_rouge_l": 17.242124902962104, "test_rouge_l_se": 0.623759255122043}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "ilpost-sum", "task": "summarization", "dataset_languages": ["it"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"bertscore": 0.6713848580693593, "rouge_l": 0.2414105717313444}, {"bertscore": 0.6302536516159307, "rouge_l": 0.18342040954033337}, {"bertscore": 0.6533272189408308, "rouge_l": 0.2137579343227943}, {"bertscore": 0.6512320119945798, "rouge_l": 0.21429693319290766}, {"bertscore": 0.6094756160018733, "rouge_l": 0.1720911874785379}, {"bertscore": 0.6572567054390674, "rouge_l": 0.22670547345541409}, {"bertscore": 0.6662462813837919, "rouge_l": 0.23033882114677173}, {"bertscore": 0.6677830501721473, "rouge_l": 0.23532049026464988}, {"bertscore": 0.6670699564128881, "rouge_l": 0.22907735061677909}, {"bertscore": 0.6586504758452065, "rouge_l": 0.21264339497353146}], "total": {"test_bertscore": 65.32679825875675, "test_bertscore_se": 1.2036768570969143, "test_rouge_l": 21.590625667230636, "test_rouge_l_se": 1.388809692794564}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "danske-talemaader", "task": "knowledge", "dataset_languages": ["da"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"mcc": 0.11912893822540369, "accuracy": 0.3353960396039604}, {"mcc": 0.06485300801638251, "accuracy": 0.24876237623762376}, {"mcc": 0.12338569548472017, "accuracy": 0.32425742574257427}, {"mcc": 0.08276139610127627, "accuracy": 0.32425742574257427}, {"mcc": 0.04354329395668365, "accuracy": 0.25}, {"mcc": 0.06466315000187224, "accuracy": 0.2871287128712871}, {"mcc": 0.13384189867087307, "accuracy": 0.33663366336633666}, {"mcc": 0.08600275922154238, "accuracy": 0.29826732673267325}, {"mcc": 0.139391801433131, "accuracy": 0.3551980198019802}, {"mcc": 0.03460679120712156, "accuracy": 0.2957920792079208}], "total": {"test_mcc": 8.921787323190063, "test_mcc_se": 2.345455767518374, "test_accuracy": 30.556930693069305, "test_accuracy_se": 2.246181738429036}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "danish-citizen-tests", "task": "knowledge", "dataset_languages": ["da"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"mcc": 0.3083138259718379, "accuracy": 0.5295238095238095}, {"mcc": 0.33559631708166815, "accuracy": 0.5447619047619048}, {"mcc": 0.363144143047394, "accuracy": 0.5504761904761905}, {"mcc": 0.21657213677956372, "accuracy": 0.4666666666666667}, {"mcc": 0.3521979273139211, "accuracy": 0.5657142857142857}, {"mcc": 0.3309161092220263, "accuracy": 0.5504761904761905}, {"mcc": 0.1806528817639182, "accuracy": 0.4361904761904762}, {"mcc": 0.3660883297646061, "accuracy": 0.579047619047619}, {"mcc": 0.3652810135577448, "accuracy": 0.5638095238095238}, {"mcc": 0.39477362150623413, "accuracy": 0.5866666666666667}], "total": {"test_mcc": 32.13536306008915, "test_mcc_se": 4.297091825155849, "test_accuracy": 53.73333333333334, "test_accuracy_se": 3.019900284511901}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "nrk-quiz-qa", "task": "knowledge", "dataset_languages": ["nb", "nn", "no"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"mcc": 0.087323225428229, "accuracy": 0.349609375}, {"mcc": 0.07239306256127236, "accuracy": 0.33837890625}, {"mcc": 0.08652481061487316, "accuracy": 0.3173828125}, {"mcc": 0.09624900561166731, "accuracy": 0.3427734375}, {"mcc": 0.1395295876107373, "accuracy": 0.3662109375}, {"mcc": 0.09687788084629474, "accuracy": 0.35107421875}, {"mcc": 0.11106310520468844, "accuracy": 0.35107421875}, {"mcc": 0.09063570472331173, "accuracy": 0.3447265625}, {"mcc": 0.12518584178093292, "accuracy": 0.35205078125}, {"mcc": 0.08511868594270071, "accuracy": 0.341796875}], "total": {"test_mcc": 9.909009103247076, "test_mcc_se": 1.2649068769557383, "test_accuracy": 34.55078125, "test_accuracy_se": 0.7768673294028328}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-sv", "task": "knowledge", "dataset_languages": ["sv"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"mcc": 0.041157837774114144, "accuracy": 0.28515625}, {"mcc": 0.04785924141760337, "accuracy": 0.28955078125}, {"mcc": 0.0281736469455987, "accuracy": 0.26416015625}, {"mcc": 0.012369985520891823, "accuracy": 0.26220703125}, {"mcc": 0.034300361822016685, "accuracy": 0.2763671875}, {"mcc": 0.04620791261277499, "accuracy": 0.28857421875}, {"mcc": 0.022524410033443246, "accuracy": 0.271484375}, {"mcc": 0.06908651489273396, "accuracy": 0.29248046875}, {"mcc": 0.04240212754923664, "accuracy": 0.28466796875}, {"mcc": 0.058461075572669836, "accuracy": 0.2861328125}], "total": {"test_mcc": 4.0254311414108335, "test_mcc_se": 1.0401692533404883, "test_accuracy": 28.0078125, "test_accuracy_se": 0.6725296466505508}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "icelandic-knowledge", "task": "knowledge", "dataset_languages": ["is"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"mcc": 0.011156960252629598, "accuracy": 0.2412109375}, {"mcc": 0.059119719618916926, "accuracy": 0.2392578125}, {"mcc": 0.036260239495335016, "accuracy": 0.2666015625}, {"mcc": -0.03163708957535406, "accuracy": 0.21875}, {"mcc": 0.046768066145475316, "accuracy": 0.26171875}, {"mcc": -0.011367545582878735, "accuracy": 0.2412109375}, {"mcc": 0.024225215239690207, "accuracy": 0.23828125}, {"mcc": 0.036288601314731724, "accuracy": 0.28125}, {"mcc": 0.0003826100180365727, "accuracy": 0.2275390625}, {"mcc": 0.026034324683006184, "accuracy": 0.2470703125}], "total": {"test_mcc": 1.9723110160958877, "test_mcc_se": 1.721560962196989, "test_accuracy": 24.62890625, "test_accuracy_se": 1.1593033422561672}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-de", "task": "knowledge", "dataset_languages": ["de"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"mcc": 0.04450963833799819, "accuracy": 0.28564453125}, {"mcc": 0.06512599857051896, "accuracy": 0.291015625}, {"mcc": 0.017821135596797422, "accuracy": 0.25927734375}, {"mcc": 0.03391506474837692, "accuracy": 0.2734375}, {"mcc": 0.055090862104825375, "accuracy": 0.27685546875}, {"mcc": 0.031182478981321576, "accuracy": 0.2568359375}, {"mcc": 0.06699732185065299, "accuracy": 0.291015625}, {"mcc": 0.04662607735610645, "accuracy": 0.2861328125}, {"mcc": 0.03304636056291183, "accuracy": 0.2705078125}, {"mcc": 0.04501122349401676, "accuracy": 0.27734375}], "total": {"test_mcc": 4.393261616035264, "test_mcc_se": 0.9641117060887473, "test_accuracy": 27.6806640625, "test_accuracy_se": 0.7524116144836304}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-nl", "task": "knowledge", "dataset_languages": ["nl"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"mcc": 0.05303501752596467, "accuracy": 0.27978515625}, {"mcc": 0.07058113017080234, "accuracy": 0.29833984375}, {"mcc": 0.04885236428789588, "accuracy": 0.2861328125}, {"mcc": 0.01606083141752909, "accuracy": 0.26220703125}, {"mcc": 0.05097931206420178, "accuracy": 0.28076171875}, {"mcc": 0.08236918617123053, "accuracy": 0.3046875}, {"mcc": 0.033244518763153506, "accuracy": 0.26953125}, {"mcc": 0.0709646881854684, "accuracy": 0.29248046875}, {"mcc": 0.03619124350646716, "accuracy": 0.26806640625}, {"mcc": 0.06325837698116746, "accuracy": 0.29248046875}], "total": {"test_mcc": 5.255366690738809, "test_mcc_se": 1.249254693119964, "test_accuracy": 28.3447265625, "test_accuracy_se": 0.8631280624086102}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu", "task": "knowledge", "dataset_languages": ["en"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"mcc": 0.08410550123565934, "accuracy": 0.29736328125}, {"mcc": 0.07274412512416496, "accuracy": 0.2890625}, {"mcc": 0.12246489178116642, "accuracy": 0.33203125}, {"mcc": 0.08802817595679549, "accuracy": 0.28173828125}, {"mcc": 0.13250575415398394, "accuracy": 0.322265625}, {"mcc": 0.10129308180977752, "accuracy": 0.31396484375}, {"mcc": 0.06680766354048637, "accuracy": 0.2841796875}, {"mcc": 0.0959705580962282, "accuracy": 0.31787109375}, {"mcc": 0.0967968087559649, "accuracy": 0.31640625}, {"mcc": 0.09152457734681725, "accuracy": 0.31298828125}], "total": {"test_mcc": 9.522411378010444, "test_mcc_se": 1.2522509700801165, "test_accuracy": 30.678710937499996, "test_accuracy_se": 1.0782410509733793}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-fr", "task": "knowledge", "dataset_languages": ["fr"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"mcc": 0.0652200763184641, "accuracy": 0.26220703125}, {"mcc": 0.018260017738530223, "accuracy": 0.26025390625}, {"mcc": 0.04552788195922398, "accuracy": 0.2802734375}, {"mcc": 0.027144333954742517, "accuracy": 0.24169921875}, {"mcc": 0.05564023832512749, "accuracy": 0.2763671875}, {"mcc": 0.029564363611031208, "accuracy": 0.2587890625}, {"mcc": 0.013411278572366533, "accuracy": 0.25390625}, {"mcc": 0.034888029260955314, "accuracy": 0.271484375}, {"mcc": 0.042138013709942884, "accuracy": 0.25634765625}, {"mcc": 0.04395114374233548, "accuracy": 0.27490234375}], "total": {"test_mcc": 3.757453771927198, "test_mcc_se": 1.0020673634171133, "test_accuracy": 26.3623046875, "test_accuracy_se": 0.743020391691836}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-it", "task": "knowledge", "dataset_languages": ["it"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"mcc": 0.03925294041843184, "accuracy": 0.2587890625}, {"mcc": 0.021512747342122926, "accuracy": 0.2568359375}, {"mcc": 0.02488276761496888, "accuracy": 0.26708984375}, {"mcc": 0.01750429325769672, "accuracy": 0.25341796875}, {"mcc": 0.027983116105610138, "accuracy": 0.25927734375}, {"mcc": 0.03068795569715005, "accuracy": 0.25830078125}, {"mcc": 0.0042089743179913154, "accuracy": 0.25634765625}, {"mcc": 0.06040703739175168, "accuracy": 0.28271484375}, {"mcc": 0.016759133684456484, "accuracy": 0.25537109375}, {"mcc": 0.02086585477323956, "accuracy": 0.265625}], "total": {"test_mcc": 2.6406482060341956, "test_mcc_se": 0.9385980010318855, "test_accuracy": 26.137695312499996, "test_accuracy_se": 0.5355290870052718}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-da", "task": "common-sense-reasoning", "dataset_languages": ["da"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"mcc": -0.0026942219894503937, "accuracy": 0.2529296875}, {"mcc": 0.011051940220934746, "accuracy": 0.255859375}, {"mcc": 0.04502782497012814, "accuracy": 0.279296875}, {"mcc": 0.005395736394189919, "accuracy": 0.240234375}, {"mcc": 0.02156048618380363, "accuracy": 0.26416015625}, {"mcc": -0.012355285229665565, "accuracy": 0.23974609375}, {"mcc": 0.03102784950844959, "accuracy": 0.24755859375}, {"mcc": 0.05101677435655236, "accuracy": 0.287109375}, {"mcc": 0.007590007201510007, "accuracy": 0.25537109375}, {"mcc": 0.014674180066743247, "accuracy": 0.25732421875}], "total": {"test_mcc": 1.722952916831957, "test_mcc_se": 1.252176453038163, "test_accuracy": 25.795898437499996, "test_accuracy_se": 0.9544810858450513}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "nor-common-sense-qa", "task": "common-sense-reasoning", "dataset_languages": ["nb", "nn", "no"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"mcc": 0.05138848022402307, "accuracy": 0.209165687426557}, {"mcc": 0.05519532810411693, "accuracy": 0.199764982373678}, {"mcc": 0.1033339103476324, "accuracy": 0.22796709753231492}, {"mcc": 0.021394565252828356, "accuracy": 0.2009400705052879}, {"mcc": 0.1020292010696722, "accuracy": 0.27497062279670975}, {"mcc": 0.05053794935153056, "accuracy": 0.23031727379553465}, {"mcc": 0.0865894046795957, "accuracy": 0.25499412455934195}, {"mcc": 0.05178833101593917, "accuracy": 0.21974148061104584}, {"mcc": 0.06634225825340313, "accuracy": 0.22209165687426558}, {"mcc": 0.049496304250975405, "accuracy": 0.2209165687426557}], "total": {"test_mcc": 6.380957325497169, "test_mcc_se": 1.6137843820554585, "test_accuracy": 22.60869565217391, "test_accuracy_se": 1.4522778016475446}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-sv", "task": "common-sense-reasoning", "dataset_languages": ["sv"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"mcc": 0.03440645363717912, "accuracy": 0.26904296875}, {"mcc": 0.008473150282746807, "accuracy": 0.255859375}, {"mcc": -0.004204872974226217, "accuracy": 0.2578125}, {"mcc": -0.0027632738589478526, "accuracy": 0.259765625}, {"mcc": -0.032861572005883986, "accuracy": 0.23193359375}, {"mcc": 0.007396419556315142, "accuracy": 0.2578125}, {"mcc": 0.043493341425698724, "accuracy": 0.2822265625}, {"mcc": 0.0425148142109345, "accuracy": 0.27783203125}, {"mcc": -0.011266756870331022, "accuracy": 0.25634765625}, {"mcc": 0.02310605403735608, "accuracy": 0.25634765625}], "total": {"test_mcc": 1.082937574408413, "test_mcc_se": 1.5464499135426064, "test_accuracy": 26.0498046875, "test_accuracy_se": 0.8599387183028698}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "winogrande-is", "task": "common-sense-reasoning", "dataset_languages": ["is"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"mcc": -0.05243203166911292, "accuracy": 0.44642857142857145}, {"mcc": 0.0, "accuracy": 0.4375}, {"mcc": 0.05215066689971256, "accuracy": 0.43973214285714285}, {"mcc": -0.03798794431377352, "accuracy": 0.43526785714285715}, {"mcc": 0.0, "accuracy": 0.41964285714285715}, {"mcc": 0.013820891850023666, "accuracy": 0.453125}, {"mcc": 0.041579247668781384, "accuracy": 0.42075892857142855}, {"mcc": -0.09405658297117317, "accuracy": 0.4497767857142857}, {"mcc": -0.04249737252509374, "accuracy": 0.40736607142857145}, {"mcc": 0.0297475410936319, "accuracy": 0.44308035714285715}], "total": {"test_mcc": -0.8967558396700385, "test_mcc_se": 2.8898579132054745, "test_accuracy": 43.52678571428571, "test_accuracy_se": 0.9177077421488824}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-de", "task": "common-sense-reasoning", "dataset_languages": ["de"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"mcc": -0.01303649932771442, "accuracy": 0.24365234375}, {"mcc": -0.017413737186181475, "accuracy": 0.2314453125}, {"mcc": 0.007896783978140892, "accuracy": 0.244140625}, {"mcc": -0.002876501442304079, "accuracy": 0.25244140625}, {"mcc": 0.015522356866139779, "accuracy": 0.25390625}, {"mcc": 0.0384268545487759, "accuracy": 0.271484375}, {"mcc": 0.007900258943421331, "accuracy": 0.25439453125}, {"mcc": 0.013975024273936341, "accuracy": 0.25244140625}, {"mcc": 0.015781253436407336, "accuracy": 0.2578125}, {"mcc": 0.00496221671192885, "accuracy": 0.2587890625}], "total": {"test_mcc": 0.7113801080255046, "test_mcc_se": 0.9879631447728665, "test_accuracy": 25.205078125000004, "test_accuracy_se": 0.6608676337391638}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-nl", "task": "common-sense-reasoning", "dataset_languages": ["nl"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"mcc": 0.027316785710236016, "accuracy": 0.255859375}, {"mcc": -0.000623142770857209, "accuracy": 0.26025390625}, {"mcc": 0.0009708250208692696, "accuracy": 0.2412109375}, {"mcc": 0.010990341100906844, "accuracy": 0.24658203125}, {"mcc": 0.023241835293711568, "accuracy": 0.24169921875}, {"mcc": 0.010079357224029827, "accuracy": 0.24365234375}, {"mcc": 0.004178479652981225, "accuracy": 0.24755859375}, {"mcc": -0.030178369338573104, "accuracy": 0.2294921875}, {"mcc": -0.01544864737239115, "accuracy": 0.23779296875}, {"mcc": 0.032265599330186065, "accuracy": 0.2724609375}], "total": {"test_mcc": 0.6279306385109934, "test_mcc_se": 1.1910981484693959, "test_accuracy": 24.765625, "test_accuracy_se": 0.7626684450960209}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag", "task": "common-sense-reasoning", "dataset_languages": ["en"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"mcc": -0.018862459664446222, "accuracy": 0.244140625}, {"mcc": 0.038100620728558035, "accuracy": 0.28271484375}, {"mcc": 0.047095788079228616, "accuracy": 0.2783203125}, {"mcc": 0.03965037407756654, "accuracy": 0.2744140625}, {"mcc": 0.02074261995005555, "accuracy": 0.2705078125}, {"mcc": -0.01857780989239654, "accuracy": 0.2451171875}, {"mcc": 0.015051950401165031, "accuracy": 0.265625}, {"mcc": 0.022154371788651817, "accuracy": 0.271484375}, {"mcc": 0.0011971449053791337, "accuracy": 0.2490234375}, {"mcc": 0.019115803154695032, "accuracy": 0.27490234375}], "total": {"test_mcc": 1.65668403528457, "test_mcc_se": 1.4182945620656937, "test_accuracy": 26.5625, "test_accuracy_se": 0.8844128711800752}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-fr", "task": "common-sense-reasoning", "dataset_languages": ["fr"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"mcc": 0.006227530935902376, "accuracy": 0.2607421875}, {"mcc": 0.015531948549396414, "accuracy": 0.25244140625}, {"mcc": 0.020186387044637585, "accuracy": 0.25537109375}, {"mcc": -0.0024779441254860553, "accuracy": 0.23828125}, {"mcc": 0.003406521716538307, "accuracy": 0.25390625}, {"mcc": -0.01468837267131438, "accuracy": 0.2451171875}, {"mcc": -0.024449142867166693, "accuracy": 0.2412109375}, {"mcc": 0.03794043131285063, "accuracy": 0.26806640625}, {"mcc": 0.0015297477292727432, "accuracy": 0.2412109375}, {"mcc": 0.05878217751016968, "accuracy": 0.27294921875}], "total": {"test_mcc": 1.0198928513480061, "test_mcc_se": 1.5147962264625023, "test_accuracy": 25.29296875, "test_accuracy_se": 0.7299694474095992}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-it", "task": "common-sense-reasoning", "dataset_languages": ["it"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"mcc": -0.02109398371667793, "accuracy": 0.2412109375}, {"mcc": 0.007044607131525998, "accuracy": 0.2734375}, {"mcc": 0.004542236269285131, "accuracy": 0.25244140625}, {"mcc": -0.00019008760486887734, "accuracy": 0.259765625}, {"mcc": 0.013790217561080717, "accuracy": 0.25830078125}, {"mcc": -0.025768620511665653, "accuracy": 0.2451171875}, {"mcc": -0.010450803616382478, "accuracy": 0.25634765625}, {"mcc": -0.01337275146468166, "accuracy": 0.25390625}, {"mcc": 0.0035655094412469527, "accuracy": 0.26611328125}, {"mcc": -0.02111624622843051, "accuracy": 0.2578125}], "total": {"test_mcc": -0.630499227395683, "test_mcc_se": 0.8564828225654498, "test_accuracy": 25.644531250000004, "test_accuracy_se": 0.5775410256760773}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "LumiOpen/Viking-33B", "results": {"raw": [{"test_speed": 350.03, "test_speed_short": 39.12}, {"test_speed": 669.75, "test_speed_short": 72.9}, {"test_speed": 1017.0200000000001, "test_speed_short": 142.10000000000002}, {"test_speed": 1331.94, "test_speed_short": 176.4}, {"test_speed": 1681.29, "test_speed_short": 209.41}, {"test_speed": 1991.3300000000002, "test_speed_short": 276.45}, {"test_speed": 2327.34, "test_speed_short": 312.96}, {"test_speed": 2653.53, "test_speed_short": 346.48}, {"test_speed": 2978.3199999999997, "test_speed_short": 379.86}, {"test_speed": 3315.7300000000005, "test_speed_short": 415.65}], "total": {"test_speed": 1831.6280000000002, "test_speed_se": 617.9161674357864, "test_speed_short": 237.13300000000004, "test_speed_short_se": 80.32589253575684}}, "num_model_parameters": 33119034368, "max_sequence_length": 4096, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "allocine", "task": "sentiment-classification", "dataset_languages": ["fr"], "model": "CohereForAI/c4ai-command-r-08-2024", "results": {"raw": [{"mcc": 0.9055423477685502, "macro_f1": 0.9511681028521833}, {"mcc": 0.8864417000485872, "macro_f1": 0.9418872018646685}, {"mcc": 0.9225161234776738, "macro_f1": 0.960281711385895}, {"mcc": 0.9209981782565588, "macro_f1": 0.9594710232264817}, {"mcc": 0.8902082607259834, "macro_f1": 0.9433591589323385}, {"mcc": 0.9188056291665275, "macro_f1": 0.9584948963838726}, {"mcc": 0.9069121152737734, "macro_f1": 0.9516550731390129}, {"mcc": 0.9141727783465551, "macro_f1": 0.9565383990845671}, {"mcc": 0.9010760576714326, "macro_f1": 0.9491559086395234}, {"mcc": 0.9265041327503919, "macro_f1": 0.9623374672414826}], "total": {"test_mcc": 90.93177323486033, "test_mcc_se": 0.8508509384547827, "test_macro_f1": 95.34348942750024, "test_macro_f1_se": 0.4427703161542812}}, "num_model_parameters": 32296476672, "max_sequence_length": 131072, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "sentipolc16", "task": "sentiment-classification", "dataset_languages": ["it"], "model": "CohereForAI/c4ai-command-r-08-2024", "results": {"raw": [{"mcc": 0.5537169245870621, "macro_f1": 0.6929617894314734}, {"mcc": 0.5675026249526696, "macro_f1": 0.696759375959191}, {"mcc": 0.5273019057776149, "macro_f1": 0.6685682787066698}, {"mcc": 0.5713047965146658, "macro_f1": 0.7093112902500147}, {"mcc": 0.5523186022783025, "macro_f1": 0.693150364767177}, {"mcc": 0.5965971833242362, "macro_f1": 0.7105157309734436}, {"mcc": 0.5422649234792684, "macro_f1": 0.6778137163516168}, {"mcc": 0.6162200595502697, "macro_f1": 0.7409977997451372}, {"mcc": 0.5334611140864792, "macro_f1": 0.6617937451274707}, {"mcc": 0.5466648522173738, "macro_f1": 0.6887119197738767}], "total": {"test_mcc": 56.07352986767942, "test_mcc_se": 1.7338644357467865, "test_macro_f1": 69.40584011086071, "test_macro_f1_se": 1.416295397430205}}, "num_model_parameters": 32296476672, "max_sequence_length": 131072, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "eltec", "task": "named-entity-recognition", "dataset_languages": ["fr"], "model": "CohereForAI/c4ai-command-r-08-2024", "results": {"raw": [{"micro_f1_no_misc": 0.6812377210216111, "micro_f1": 0.595561035758323}, {"micro_f1_no_misc": 0.6858176883058131, "micro_f1": 0.6537902388369679}, {"micro_f1_no_misc": 0.7140783744557329, "micro_f1": 0.6520916544040362}, {"micro_f1_no_misc": 0.7211808809746953, "micro_f1": 0.619209945859234}, {"micro_f1_no_misc": 0.7152449639393187, "micro_f1": 0.6461937716262975}, {"micro_f1_no_misc": 0.672774254169603, "micro_f1": 0.5808912083500603}, {"micro_f1_no_misc": 0.67666815942678, "micro_f1": 0.5942393195437851}, {"micro_f1_no_misc": 0.734413352970054, "micro_f1": 0.6549326379834854}, {"micro_f1_no_misc": 0.6744186046511628, "micro_f1": 0.5559092717867091}, {"micro_f1_no_misc": 0.680029332681496, "micro_f1": 0.5523514611960518}], "total": {"test_micro_f1_no_misc": 69.55863332596267, "test_micro_f1_no_misc_se": 1.4255137524794423, "test_micro_f1": 61.0517054534495, "test_micro_f1_se": 2.500973183575002}}, "num_model_parameters": 32296476672, "max_sequence_length": 131072, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "multinerd-it", "task": "named-entity-recognition", "dataset_languages": ["it"], "model": "CohereForAI/c4ai-command-r-08-2024", "results": {"raw": [{"micro_f1_no_misc": 0.7944430784361939, "micro_f1": 0.5323879163896891}, {"micro_f1_no_misc": 0.8055492381168978, "micro_f1": 0.6359417506801088}, {"micro_f1_no_misc": 0.7840735068912711, "micro_f1": 0.6009111981456319}, {"micro_f1_no_misc": 0.784106286136056, "micro_f1": 0.5879201169020946}, {"micro_f1_no_misc": 0.8137205808713069, "micro_f1": 0.5732414225285916}, {"micro_f1_no_misc": 0.7916270218839201, "micro_f1": 0.5904895979978101}, {"micro_f1_no_misc": 0.8069529181535627, "micro_f1": 0.5390770110015716}, {"micro_f1_no_misc": 0.8161367349503582, "micro_f1": 0.6320067739204065}, {"micro_f1_no_misc": 0.8119325551232166, "micro_f1": 0.5510756732360463}, {"micro_f1_no_misc": 0.8019176800748363, "micro_f1": 0.524220209860572}], "total": {"test_micro_f1_no_misc": 80.10459600637618, "test_micro_f1_no_misc_se": 0.736315123386363, "test_micro_f1": 57.67271670662522, "test_micro_f1_se": 2.4682159753178965}}, "num_model_parameters": 32296476672, "max_sequence_length": 131072, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-fr", "task": "linguistic-acceptability", "dataset_languages": ["fr"], "model": "CohereForAI/c4ai-command-r-08-2024", "results": {"raw": [{"mcc": 0.3704394753345814, "macro_f1": 0.6645199113709059}, {"mcc": 0.3873586858932433, "macro_f1": 0.6793289388992907}, {"mcc": 0.34564785177670176, "macro_f1": 0.6676653424822439}, {"mcc": 0.38090770693669534, "macro_f1": 0.6525688962747542}, {"mcc": 0.4273550687509007, "macro_f1": 0.6875364087769424}, {"mcc": 0.4516744230517674, "macro_f1": 0.7162812655288331}, {"mcc": 0.4017618428814222, "macro_f1": 0.7006521195375242}, {"mcc": 0.37308726508631734, "macro_f1": 0.6739767581087324}, {"mcc": 0.44664242781159846, "macro_f1": 0.7137686695512067}, {"mcc": 0.47963736202765445, "macro_f1": 0.7346679270451961}], "total": {"test_mcc": 40.64512109550883, "test_mcc_se": 2.66185740453578, "test_macro_f1": 68.9096623757563, "test_macro_f1_se": 1.6375131753098398}}, "num_model_parameters": 32296476672, "max_sequence_length": 131072, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-it", "task": "linguistic-acceptability", "dataset_languages": ["it"], "model": "CohereForAI/c4ai-command-r-08-2024", "results": {"raw": [{"mcc": 0.29622044006901244, "macro_f1": 0.5842093390480487}, {"mcc": 0.21736203574944124, "macro_f1": 0.4801378787686699}, {"mcc": 0.2875085487790741, "macro_f1": 0.5819289904637719}, {"mcc": 0.27231664335054423, "macro_f1": 0.5450671595725601}, {"mcc": 0.23673988342989805, "macro_f1": 0.5009092842784678}, {"mcc": 0.28045681733884065, "macro_f1": 0.5320663925431646}, {"mcc": 0.29517138230073947, "macro_f1": 0.5799138808424019}, {"mcc": 0.29934921931181135, "macro_f1": 0.5838508527958751}, {"mcc": 0.3037971732378228, "macro_f1": 0.5587557755909068}, {"mcc": 0.24217310896760794, "macro_f1": 0.48957005116514474}], "total": {"test_mcc": 27.310952525347922, "test_mcc_se": 1.882306706945077, "test_macro_f1": 54.36409605069012, "test_macro_f1_se": 2.5464155799457178}}, "num_model_parameters": 32296476672, "max_sequence_length": 131072, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "squad-it", "task": "reading-comprehension", "dataset_languages": ["it"], "model": "CohereForAI/c4ai-command-r-08-2024", "results": {"raw": [{"f1": 73.09943051288374, "em": 52.803030303030305}, {"f1": 73.0541559483903, "em": 51.630022744503414}, {"f1": 71.7306833209752, "em": 51.981351981351985}, {"f1": 73.06564673609134, "em": 52.28310502283105}, {"f1": 71.91831013997057, "em": 51.50346954510409}, {"f1": 71.84634694930564, "em": 50.916030534351144}, {"f1": 74.09268141730698, "em": 53.51014040561623}, {"f1": 71.72998468095996, "em": 49.764150943396224}, {"f1": 72.57866818272468, "em": 53.250773993808046}, {"f1": 72.51622513777065, "em": 51.46604938271605}], "total": {"test_f1": 72.56321330263792, "test_f1_se": 0.48186980575409616, "test_em": 51.91081248567085, "test_em_se": 0.6955257024135016}}, "num_model_parameters": 32296476672, "max_sequence_length": 131072, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fquad", "task": "reading-comprehension", "dataset_languages": ["fr"], "model": "CohereForAI/c4ai-command-r-08-2024", "results": {"raw": [{"f1": 70.1058558137653, "em": 40.15151515151515}, {"f1": 70.84234956633166, "em": 43.13874147081122}, {"f1": 71.56643144417629, "em": 41.8026418026418}, {"f1": 71.42551950264894, "em": 41.70471841704718}, {"f1": 66.65352739531666, "em": 37.548188126445645}, {"f1": 71.81293329208162, "em": 41.603053435114504}, {"f1": 69.43737002847502, "em": 41.41965678627145}, {"f1": 68.99251315587377, "em": 40.25157232704402}, {"f1": 67.8892161013146, "em": 40.86687306501548}, {"f1": 72.67779939935447, "em": 43.20987654320987}], "total": {"test_f1": 70.14035156993383, "test_f1_se": 1.176957695547649, "test_em": 41.169683712511635, "test_em_se": 1.0134733064397485}}, "num_model_parameters": 32296476672, "max_sequence_length": 131072, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "orange-sum", "task": "summarization", "dataset_languages": ["fr"], "model": "CohereForAI/c4ai-command-r-08-2024", "results": {"raw": [{"bertscore": 0.6681550696957856, "rouge_l": 0.18571115968876234}, {"bertscore": 0.6682759752147831, "rouge_l": 0.1846046111721763}, {"bertscore": 0.6687492211931385, "rouge_l": 0.18805695575336923}, {"bertscore": 0.6688867666525766, "rouge_l": 0.19235718143001596}, {"bertscore": 0.6680229833582416, "rouge_l": 0.1905024836872516}, {"bertscore": 0.6698589368024841, "rouge_l": 0.18737342702409637}, {"bertscore": 0.6686400065373164, "rouge_l": 0.1888679392599817}, {"bertscore": 0.6682647074921988, "rouge_l": 0.18864475968598243}, {"bertscore": 0.6714990336913615, "rouge_l": 0.19239310617532457}, {"bertscore": 0.6701794850523584, "rouge_l": 0.18598145094545873}], "total": {"test_bertscore": 66.90532185690245, "test_bertscore_se": 0.06940247617537836, "test_rouge_l": 18.844930748224193, "test_rouge_l_se": 0.1664845777153738}}, "num_model_parameters": 32296476672, "max_sequence_length": 131072, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "ilpost-sum", "task": "summarization", "dataset_languages": ["it"], "model": "CohereForAI/c4ai-command-r-08-2024", "results": {"raw": [{"bertscore": 0.6548104745743331, "rouge_l": 0.16413625736203175}, {"bertscore": 0.654972935502883, "rouge_l": 0.17077608324702562}, {"bertscore": 0.6540288471151143, "rouge_l": 0.1647012042413576}, {"bertscore": 0.654944824957056, "rouge_l": 0.16897091794933705}, {"bertscore": 0.6535414541867794, "rouge_l": 0.16155873034828594}, {"bertscore": 0.6558262122271117, "rouge_l": 0.17253290200191787}, {"bertscore": 0.6559635015728418, "rouge_l": 0.1678663555993684}, {"bertscore": 0.6580288851109799, "rouge_l": 0.17443818253884763}, {"bertscore": 0.659424106910592, "rouge_l": 0.18171728392218753}, {"bertscore": 0.6563466255320236, "rouge_l": 0.1700562070760145}], "total": {"test_bertscore": 65.57887867689715, "test_bertscore_se": 0.11137287699818273, "test_rouge_l": 16.96754124286374, "test_rouge_l_se": 0.3592067358264912}}, "num_model_parameters": 32296476672, "max_sequence_length": 131072, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-fr", "task": "knowledge", "dataset_languages": ["fr"], "model": "CohereForAI/c4ai-command-r-08-2024", "results": {"raw": [{"mcc": 0.5261095657867516, "accuracy": 0.640625}, {"mcc": 0.5177211314546218, "accuracy": 0.63525390625}, {"mcc": 0.514227911724826, "accuracy": 0.630859375}, {"mcc": 0.5156010365213831, "accuracy": 0.63330078125}, {"mcc": 0.48909431034381784, "accuracy": 0.6083984375}, {"mcc": 0.5232719764358925, "accuracy": 0.6376953125}, {"mcc": 0.514064673900287, "accuracy": 0.63037109375}, {"mcc": 0.48907844120952476, "accuracy": 0.6123046875}, {"mcc": 0.515613855652825, "accuracy": 0.6337890625}, {"mcc": 0.5388516222343179, "accuracy": 0.64990234375}], "total": {"test_mcc": 51.43634525264248, "test_mcc_se": 0.9488733949684606, "test_accuracy": 63.125, "test_accuracy_se": 0.7691790340665058}}, "num_model_parameters": 32296476672, "max_sequence_length": 131072, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-it", "task": "knowledge", "dataset_languages": ["it"], "model": "CohereForAI/c4ai-command-r-08-2024", "results": {"raw": [{"mcc": 0.5498978187073675, "accuracy": 0.65771484375}, {"mcc": 0.5569876861599465, "accuracy": 0.666015625}, {"mcc": 0.5392630771940249, "accuracy": 0.64794921875}, {"mcc": 0.4959857213357761, "accuracy": 0.6162109375}, {"mcc": 0.5286603175932679, "accuracy": 0.63818359375}, {"mcc": 0.5261390835230365, "accuracy": 0.64013671875}, {"mcc": 0.532715308724097, "accuracy": 0.64306640625}, {"mcc": 0.5215195947910684, "accuracy": 0.63671875}, {"mcc": 0.5275087688857131, "accuracy": 0.64306640625}, {"mcc": 0.5199114644660081, "accuracy": 0.6357421875}], "total": {"test_mcc": 52.98588841380306, "test_mcc_se": 1.0475383331076373, "test_accuracy": 64.248046875, "test_accuracy_se": 0.8287888169184265}}, "num_model_parameters": 32296476672, "max_sequence_length": 131072, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-fr", "task": "common-sense-reasoning", "dataset_languages": ["fr"], "model": "CohereForAI/c4ai-command-r-08-2024", "results": {"raw": [{"mcc": 0.45853555060448015, "accuracy": 0.57470703125}, {"mcc": 0.446145862634104, "accuracy": 0.55517578125}, {"mcc": 0.5041838936703296, "accuracy": 0.61376953125}, {"mcc": 0.533112490224354, "accuracy": 0.64111328125}, {"mcc": 0.4997810840858889, "accuracy": 0.60986328125}, {"mcc": 0.5164964407048908, "accuracy": 0.623046875}, {"mcc": 0.4617830207370226, "accuracy": 0.5771484375}, {"mcc": 0.5012452479980801, "accuracy": 0.61083984375}, {"mcc": 0.5199238138454836, "accuracy": 0.625}, {"mcc": 0.44332571683178434, "accuracy": 0.55126953125}], "total": {"test_mcc": 48.845331213364176, "test_mcc_se": 2.039347531996882, "test_accuracy": 59.81933593749999, "test_accuracy_se": 1.9341320328280363}}, "num_model_parameters": 32296476672, "max_sequence_length": 131072, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-it", "task": "common-sense-reasoning", "dataset_languages": ["it"], "model": "CohereForAI/c4ai-command-r-08-2024", "results": {"raw": [{"mcc": 0.5193019108137793, "accuracy": 0.6328125}, {"mcc": 0.5406224873329193, "accuracy": 0.64453125}, {"mcc": 0.46562694467814314, "accuracy": 0.59326171875}, {"mcc": 0.5083825717662583, "accuracy": 0.62841796875}, {"mcc": 0.5123350315921897, "accuracy": 0.626953125}, {"mcc": 0.5081009047412867, "accuracy": 0.625}, {"mcc": 0.5200229638418659, "accuracy": 0.63427734375}, {"mcc": 0.502153249764263, "accuracy": 0.6171875}, {"mcc": 0.5395096264936696, "accuracy": 0.65087890625}, {"mcc": 0.47687885996988394, "accuracy": 0.59375}], "total": {"test_mcc": 50.92934550994259, "test_mcc_se": 1.4797347420169105, "test_accuracy": 62.47070312500001, "test_accuracy_se": 1.179234421651472}}, "num_model_parameters": 32296476672, "max_sequence_length": 131072, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "CohereForAI/c4ai-command-r-08-2024", "results": {"raw": [{"test_speed": 399.73, "test_speed_short": 45.2}, {"test_speed": 771.27, "test_speed_short": 84.3}, {"test_speed": 1164.7199999999998, "test_speed_short": 163.27}, {"test_speed": 1545.5, "test_speed_short": 202.68}, {"test_speed": 1930.5, "test_speed_short": 238.65}, {"test_speed": 2294.4500000000003, "test_speed_short": 322.05}, {"test_speed": 2695.59, "test_speed_short": 362.24}, {"test_speed": 3074.28, "test_speed_short": 400.44}, {"test_speed": 3438.9500000000003, "test_speed_short": 440.70000000000005}, {"test_speed": 3385.83, "test_speed_short": 475.15}], "total": {"test_speed": 2070.082, "test_speed_se": 674.1167118615637, "test_speed_short": 273.468, "test_speed_short_se": 92.68642316089328}}, "num_model_parameters": 32296476672, "max_sequence_length": 131072, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "swerec", "task": "sentiment-classification", "dataset_languages": ["sv"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.8037987606015371, "macro_f1": 0.7810110154206006}, {"mcc": 0.7963158186364466, "macro_f1": 0.7772650990271884}, {"mcc": 0.802812625118714, "macro_f1": 0.7793189349722752}, {"mcc": 0.7752094308473652, "macro_f1": 0.7700186732692704}, {"mcc": 0.7812917899497112, "macro_f1": 0.7799136347318522}, {"mcc": 0.8020203984270092, "macro_f1": 0.7885307667256646}, {"mcc": 0.7711643133669939, "macro_f1": 0.7347995167186593}, {"mcc": 0.7772808730244798, "macro_f1": 0.7697838427117017}, {"mcc": 0.7990034862802823, "macro_f1": 0.7758641616424903}, {"mcc": 0.7658464261919189, "macro_f1": 0.7631970645996452}], "total": {"test_mcc": 78.74743922444458, "test_mcc_se": 0.9125111104886809, "test_macro_f1": 77.19702709819349, "test_macro_f1_se": 0.9201350262928683}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "norec", "task": "sentiment-classification", "dataset_languages": ["nb", "nn", "no"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.376547836575547, "macro_f1": 0.39042800265428007}, {"mcc": 0.3640535826244135, "macro_f1": 0.3807009363565264}, {"mcc": 0.3640166233607444, "macro_f1": 0.39414652385173016}, {"mcc": 0.3652005455170701, "macro_f1": 0.3933836545406915}, {"mcc": 0.3727748695244694, "macro_f1": 0.38658498539380015}, {"mcc": 0.3720414028717185, "macro_f1": 0.3890869376729917}, {"mcc": 0.3509777790620508, "macro_f1": 0.3741706451321521}, {"mcc": 0.37482417001194984, "macro_f1": 0.38323468500447383}, {"mcc": 0.34877690532603695, "macro_f1": 0.38424321302134806}, {"mcc": 0.37433254555103146, "macro_f1": 0.3944443070769011}], "total": {"test_mcc": 36.63546260425032, "test_mcc_se": 0.6095139640435184, "test_macro_f1": 38.70423890704895, "test_macro_f1_se": 0.4077712662740591}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hotter-and-colder-sentiment", "task": "sentiment-classification", "dataset_languages": ["is"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.398226619835788, "macro_f1": 0.5620819295487802}, {"mcc": 0.41117146097995333, "macro_f1": 0.6055008349551724}, {"mcc": 0.4029535744197568, "macro_f1": 0.5752147045086982}, {"mcc": 0.3989498216719121, "macro_f1": 0.5673212447990501}, {"mcc": 0.3469701410501745, "macro_f1": 0.5541816384266092}, {"mcc": 0.39645235608121987, "macro_f1": 0.5863947330445786}, {"mcc": 0.3431465030389103, "macro_f1": 0.5412182568893493}, {"mcc": 0.39242976547115005, "macro_f1": 0.5870524477727438}, {"mcc": 0.3554752853946983, "macro_f1": 0.4857030350867317}, {"mcc": 0.34387191583488697, "macro_f1": 0.5351553194746271}], "total": {"test_mcc": 37.8964744377845, "test_mcc_se": 1.7236297194409507, "test_macro_f1": 55.9982414450634, "test_macro_f1_se": 2.0980207184320383}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "sb10k", "task": "sentiment-classification", "dataset_languages": ["de"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.5528180957894937, "macro_f1": 0.6829464412076193}, {"mcc": 0.576769551645263, "macro_f1": 0.7129607272643493}, {"mcc": 0.6220019347079564, "macro_f1": 0.7438539462240561}, {"mcc": 0.5397196666339042, "macro_f1": 0.6861412250432467}, {"mcc": 0.571685251678409, "macro_f1": 0.7150335715659711}, {"mcc": 0.6190304464382017, "macro_f1": 0.7439008836296042}, {"mcc": 0.5885678957881159, "macro_f1": 0.7252020446244319}, {"mcc": 0.5311380651848374, "macro_f1": 0.683132387585608}, {"mcc": 0.6195016076880335, "macro_f1": 0.7314864357185945}, {"mcc": 0.6166562255935193, "macro_f1": 0.7457052065362633}], "total": {"test_mcc": 58.37888741147734, "test_mcc_se": 2.1619182048974848, "test_macro_f1": 71.70362869399744, "test_macro_f1_se": 1.5768230294640762}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "dutch-social", "task": "sentiment-classification", "dataset_languages": ["nl"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.13384006485775102, "macro_f1": 0.37461079162365873}, {"mcc": 0.1316954248045042, "macro_f1": 0.33025894440988784}, {"mcc": 0.12092993916024858, "macro_f1": 0.34991438207686715}, {"mcc": 0.12231620354698432, "macro_f1": 0.32859279079163145}, {"mcc": 0.1361276458864765, "macro_f1": 0.35183230167424323}, {"mcc": 0.12195976075772871, "macro_f1": 0.31868179518057355}, {"mcc": 0.1056678351463652, "macro_f1": 0.3289647045460999}, {"mcc": 0.14434834669914318, "macro_f1": 0.3345775555133265}, {"mcc": 0.08728446445476405, "macro_f1": 0.29289085486354516}, {"mcc": 0.14038006115535448, "macro_f1": 0.34822750778376266}], "total": {"test_mcc": 12.445497464693203, "test_mcc_se": 1.0703386088291333, "test_macro_f1": 33.585516284635965, "test_macro_f1_se": 1.3676423485829783}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "sst5", "task": "sentiment-classification", "dataset_languages": ["en"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.6859156146486706, "macro_f1": 0.6783654712208734}, {"mcc": 0.6730322730363569, "macro_f1": 0.6866656849142755}, {"mcc": 0.682344810668138, "macro_f1": 0.6923772991899648}, {"mcc": 0.7079055013563201, "macro_f1": 0.7104521737363024}, {"mcc": 0.7065684879446914, "macro_f1": 0.6860481162572708}, {"mcc": 0.6938118488675107, "macro_f1": 0.7035134940342402}, {"mcc": 0.6901554488819365, "macro_f1": 0.6963646360428601}, {"mcc": 0.6725018140293328, "macro_f1": 0.6733533563343407}, {"mcc": 0.7028325100240628, "macro_f1": 0.6858014859965275}, {"mcc": 0.6931485611665225, "macro_f1": 0.690934991080951}], "total": {"test_mcc": 69.08216870623542, "test_mcc_se": 0.7858721726392512, "test_macro_f1": 69.03876708807606, "test_macro_f1_se": 0.6862404353802118}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fosent", "task": "sentiment-classification", "dataset_languages": ["fo"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.41477703249850334, "macro_f1": 0.57623066428393}, {"mcc": 0.512645877194201, "macro_f1": 0.6559458670575694}, {"mcc": 0.4644861332414223, "macro_f1": 0.6058441558441559}, {"mcc": 0.4593432341325406, "macro_f1": 0.6370288305200367}, {"mcc": 0.4085549487313322, "macro_f1": 0.6118125983452617}, {"mcc": 0.5263740058814165, "macro_f1": 0.6883472259837702}, {"mcc": 0.43897597627789275, "macro_f1": 0.5977759168307569}, {"mcc": 0.5069871453111633, "macro_f1": 0.6609705515448024}, {"mcc": 0.5503428915183541, "macro_f1": 0.6948113614780281}, {"mcc": 0.45633600621723514, "macro_f1": 0.62591062192481}], "total": {"test_mcc": 47.38823251004061, "test_mcc_se": 2.97874425505442, "test_macro_f1": 63.546777938131214, "test_macro_f1_se": 2.4312652650221858}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "allocine", "task": "sentiment-classification", "dataset_languages": ["fr"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.9093024823785567, "macro_f1": 0.9531205292252756}, {"mcc": 0.9287063216647645, "macro_f1": 0.9638666361486229}, {"mcc": 0.9524208221869737, "macro_f1": 0.9759216936777411}, {"mcc": 0.9386552988639156, "macro_f1": 0.9687498807902557}, {"mcc": 0.9297851460276501, "macro_f1": 0.964339748429023}, {"mcc": 0.9354089002849433, "macro_f1": 0.9672776588988785}, {"mcc": 0.9162289455085961, "macro_f1": 0.9565406374081176}, {"mcc": 0.9371395482067619, "macro_f1": 0.968253476151354}, {"mcc": 0.9251126545034509, "macro_f1": 0.9618336587449718}, {"mcc": 0.9544670147962082, "macro_f1": 0.9769781915146784}], "total": {"test_mcc": 93.27227134421821, "test_mcc_se": 0.8828404720646335, "test_macro_f1": 96.56882110988919, "test_macro_f1_se": 0.46761845177046124}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "sentipolc16", "task": "sentiment-classification", "dataset_languages": ["it"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.6595696924083942, "macro_f1": 0.7685763313759167}, {"mcc": 0.6485981946543243, "macro_f1": 0.759094238006985}, {"mcc": 0.6499817621889322, "macro_f1": 0.7432241373915837}, {"mcc": 0.6572702321448575, "macro_f1": 0.7682000537738243}, {"mcc": 0.7080061210018298, "macro_f1": 0.7991130189047807}, {"mcc": 0.6488281738073004, "macro_f1": 0.7448609105209224}, {"mcc": 0.6345865770741114, "macro_f1": 0.74012130795334}, {"mcc": 0.6967239094809571, "macro_f1": 0.7961765965257691}, {"mcc": 0.593950554014968, "macro_f1": 0.6848748473748474}, {"mcc": 0.6356462720643354, "macro_f1": 0.7576062000005347}], "total": {"test_mcc": 65.3316148884001, "test_mcc_se": 1.9785089474243784, "test_macro_f1": 75.61847641828504, "test_macro_f1_se": 2.0020090376743807}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "suc3", "task": "named-entity-recognition", "dataset_languages": ["sv"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"micro_f1_no_misc": 0.5542168674698795, "micro_f1": 0.326956025128498}, {"micro_f1_no_misc": 0.567617054669108, "micro_f1": 0.4800984817398441}, {"micro_f1_no_misc": 0.5739090701393649, "micro_f1": 0.42314647377938513}, {"micro_f1_no_misc": 0.5813132501765121, "micro_f1": 0.4005130671797338}, {"micro_f1_no_misc": 0.5934897804693414, "micro_f1": 0.411340206185567}, {"micro_f1_no_misc": 0.583378598587724, "micro_f1": 0.36701237872198056}, {"micro_f1_no_misc": 0.528531337698784, "micro_f1": 0.3314791042647269}, {"micro_f1_no_misc": 0.4972839506172839, "micro_f1": 0.36301950805767597}, {"micro_f1_no_misc": 0.4458077709611452, "micro_f1": 0.29215742602700373}, {"micro_f1_no_misc": 0.5409134157944814, "micro_f1": 0.3580055744812636}], "total": {"test_micro_f1_no_misc": 54.664610965836246, "test_micro_f1_no_misc_se": 2.844198242126892, "test_micro_f1": 37.537282455656786, "test_micro_f1_se": 3.3882272151388046}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "norne-nb", "task": "named-entity-recognition", "dataset_languages": ["nb", "no"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"micro_f1_no_misc": 0.6413852813852814, "micro_f1": 0.4266877042713851}, {"micro_f1_no_misc": 0.6290739782721159, "micro_f1": 0.5117161045645402}, {"micro_f1_no_misc": 0.6379084967320262, "micro_f1": 0.4050396640223985}, {"micro_f1_no_misc": 0.6706152082537256, "micro_f1": 0.5720403793208932}, {"micro_f1_no_misc": 0.6713821449053717, "micro_f1": 0.44269552385636995}, {"micro_f1_no_misc": 0.6039355992844365, "micro_f1": 0.4051203034253882}, {"micro_f1_no_misc": 0.6207792207792207, "micro_f1": 0.5164675021859516}, {"micro_f1_no_misc": 0.5840230298668586, "micro_f1": 0.44357264286648357}, {"micro_f1_no_misc": 0.639402560455192, "micro_f1": 0.48993199093212425}, {"micro_f1_no_misc": 0.6341375585163845, "micro_f1": 0.38846327174402884}], "total": {"test_micro_f1_no_misc": 63.32643078450613, "test_micro_f1_no_misc_se": 1.6569867178054536, "test_micro_f1": 46.01735087189564, "test_micro_f1_se": 3.70392979032017}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "norne-nn", "task": "named-entity-recognition", "dataset_languages": ["nn"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"micro_f1_no_misc": 0.7034530626409855, "micro_f1": 0.5723768189941282}, {"micro_f1_no_misc": 0.6702279202279203, "micro_f1": 0.45190003423485114}, {"micro_f1_no_misc": 0.6502043596730245, "micro_f1": 0.4531144591859681}, {"micro_f1_no_misc": 0.7042491958693076, "micro_f1": 0.5224025187696779}, {"micro_f1_no_misc": 0.657859078590786, "micro_f1": 0.5889720898570456}, {"micro_f1_no_misc": 0.6679035250463822, "micro_f1": 0.46933033202025887}, {"micro_f1_no_misc": 0.6615384615384616, "micro_f1": 0.5343983509404793}, {"micro_f1_no_misc": 0.6757310486124046, "micro_f1": 0.5595238095238095}, {"micro_f1_no_misc": 0.6608177454063897, "micro_f1": 0.5136994568497284}, {"micro_f1_no_misc": 0.6549728117874057, "micro_f1": 0.48039333253387695}], "total": {"test_micro_f1_no_misc": 67.06957209393069, "test_micro_f1_no_misc_se": 1.1770092901692226, "test_micro_f1": 51.461112029098246, "test_micro_f1_se": 3.0833164826216715}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mim-gold-ner", "task": "named-entity-recognition", "dataset_languages": ["is"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"micro_f1_no_misc": 0.5895582329317268, "micro_f1": 0.4276845859199578}, {"micro_f1_no_misc": 0.5333877884419033, "micro_f1": 0.2571454170773228}, {"micro_f1_no_misc": 0.5285013212533031, "micro_f1": 0.43724578733294595}, {"micro_f1_no_misc": 0.5675877520537714, "micro_f1": 0.33090573392142786}, {"micro_f1_no_misc": 0.5232644017725259, "micro_f1": 0.40642650935107316}, {"micro_f1_no_misc": 0.46242774566473993, "micro_f1": 0.35531660692951017}, {"micro_f1_no_misc": 0.5692497938994229, "micro_f1": 0.43832270321617584}, {"micro_f1_no_misc": 0.6040268456375838, "micro_f1": 0.4145578618140461}, {"micro_f1_no_misc": 0.5478473397255123, "micro_f1": 0.3795918367346938}, {"micro_f1_no_misc": 0.5846679478966474, "micro_f1": 0.309989701338826}], "total": {"test_micro_f1_no_misc": 55.10519169277137, "test_micro_f1_no_misc_se": 2.5705925673268943, "test_micro_f1": 37.571867436359796, "test_micro_f1_se": 3.791367146222095}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fone", "task": "named-entity-recognition", "dataset_languages": ["fo"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"micro_f1_no_misc": 0.6921595598349382, "micro_f1": 0.5792724776938916}, {"micro_f1_no_misc": 0.7341634099790968, "micro_f1": 0.7094617757970518}, {"micro_f1_no_misc": 0.6808209483368719, "micro_f1": 0.6603096984515078}, {"micro_f1_no_misc": 0.7070367255396692, "micro_f1": 0.615860919266492}, {"micro_f1_no_misc": 0.7501174701625787, "micro_f1": 0.705004770578541}, {"micro_f1_no_misc": 0.7487037037037036, "micro_f1": 0.6384490615586622}, {"micro_f1_no_misc": 0.7510024823372159, "micro_f1": 0.7012831479897348}, {"micro_f1_no_misc": 0.7179534033805389, "micro_f1": 0.6876164662036253}, {"micro_f1_no_misc": 0.7259175393905086, "micro_f1": 0.6804774083546462}, {"micro_f1_no_misc": 0.7306196033779296, "micro_f1": 0.6898535564853556}], "total": {"test_micro_f1_no_misc": 72.38494846043052, "test_micro_f1_no_misc_se": 1.5146419695137556, "test_micro_f1": 66.6758928237951, "test_micro_f1_se": 2.671671830756813}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "germeval", "task": "named-entity-recognition", "dataset_languages": ["de"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"micro_f1_no_misc": 0.6529984623270118, "micro_f1": 0.4902477596204533}, {"micro_f1_no_misc": 0.6343424787133396, "micro_f1": 0.5012224938875306}, {"micro_f1_no_misc": 0.6375420067210753, "micro_f1": 0.44805697476389533}, {"micro_f1_no_misc": 0.6174807197943445, "micro_f1": 0.39065108514190316}, {"micro_f1_no_misc": 0.6337554923752907, "micro_f1": 0.5052316890881914}, {"micro_f1_no_misc": 0.6421388275692912, "micro_f1": 0.4626937210417}, {"micro_f1_no_misc": 0.6140617096611026, "micro_f1": 0.37945833949977803}, {"micro_f1_no_misc": 0.6774954152475765, "micro_f1": 0.503781058696435}, {"micro_f1_no_misc": 0.6596679687500001, "micro_f1": 0.460744800902789}, {"micro_f1_no_misc": 0.6507282220390218, "micro_f1": 0.46932344763670064}], "total": {"test_micro_f1_no_misc": 64.20211303198055, "test_micro_f1_no_misc_se": 1.1863840542739181, "test_micro_f1": 46.11411370279376, "test_micro_f1_se": 2.779354814624139}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "conll-nl", "task": "named-entity-recognition", "dataset_languages": ["nl"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"micro_f1_no_misc": 0.6136263736263736, "micro_f1": 0.4072484501669051}, {"micro_f1_no_misc": 0.568359375, "micro_f1": 0.44431452791581405}, {"micro_f1_no_misc": 0.5835671342685371, "micro_f1": 0.42355769230769236}, {"micro_f1_no_misc": 0.563288183092013, "micro_f1": 0.4366005973391257}, {"micro_f1_no_misc": 0.5684560965459842, "micro_f1": 0.4103692834433847}, {"micro_f1_no_misc": 0.5506515342580917, "micro_f1": 0.36636918741193053}, {"micro_f1_no_misc": 0.5838680109990834, "micro_f1": 0.4200375033485133}, {"micro_f1_no_misc": 0.569055036344756, "micro_f1": 0.44338213325366005}, {"micro_f1_no_misc": 0.6009852216748768, "micro_f1": 0.40229885057471265}, {"micro_f1_no_misc": 0.5811283075386919, "micro_f1": 0.4028021015761822}], "total": {"test_micro_f1_no_misc": 57.829852733484074, "test_micro_f1_no_misc_se": 1.1503671050386721, "test_micro_f1": 41.5698032733792, "test_micro_f1_se": 1.4582371895673745}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "conll-en", "task": "named-entity-recognition", "dataset_languages": ["en"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"micro_f1_no_misc": 0.7602536160095106, "micro_f1": 0.6393521488133419}, {"micro_f1_no_misc": 0.7794420520501778, "micro_f1": 0.6749787036319987}, {"micro_f1_no_misc": 0.7797826737114945, "micro_f1": 0.6636493568365888}, {"micro_f1_no_misc": 0.7340560601621252, "micro_f1": 0.6209866875489428}, {"micro_f1_no_misc": 0.7499006754072308, "micro_f1": 0.6767609168479171}, {"micro_f1_no_misc": 0.7871871871871872, "micro_f1": 0.6286833855799372}, {"micro_f1_no_misc": 0.7481662591687042, "micro_f1": 0.6340435142663555}, {"micro_f1_no_misc": 0.7642970611596506, "micro_f1": 0.6652166908329306}, {"micro_f1_no_misc": 0.7731834604576475, "micro_f1": 0.7116177115300307}, {"micro_f1_no_misc": 0.7551020408163265, "micro_f1": 0.6575025098463201}], "total": {"test_micro_f1_no_misc": 76.31371086130055, "test_micro_f1_no_misc_se": 1.0415536415291107, "test_micro_f1": 65.72791625734362, "test_micro_f1_se": 1.6968136318105034}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "eltec", "task": "named-entity-recognition", "dataset_languages": ["fr"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"micro_f1_no_misc": 0.5996563573883162, "micro_f1": 0.5065190212537953}, {"micro_f1_no_misc": 0.5815459328207203, "micro_f1": 0.548975791433892}, {"micro_f1_no_misc": 0.594536459453646, "micro_f1": 0.5261992619926199}, {"micro_f1_no_misc": 0.6740185266872518, "micro_f1": 0.5701475807958154}, {"micro_f1_no_misc": 0.6385082004044035, "micro_f1": 0.5829046299960428}, {"micro_f1_no_misc": 0.5960919046596522, "micro_f1": 0.49203793165145815}, {"micro_f1_no_misc": 0.6002897950734838, "micro_f1": 0.502154058245735}, {"micro_f1_no_misc": 0.5852696185883385, "micro_f1": 0.5013948298307607}, {"micro_f1_no_misc": 0.5726662334849469, "micro_f1": 0.4950642577761223}, {"micro_f1_no_misc": 0.5709473684210526, "micro_f1": 0.41894409937888194}], "total": {"test_micro_f1_no_misc": 60.135303969818125, "test_micro_f1_no_misc_se": 1.9749013243266982, "test_micro_f1": 51.443414623551234, "test_micro_f1_se": 2.8852151823218817}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "multinerd-it", "task": "named-entity-recognition", "dataset_languages": ["it"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"micro_f1_no_misc": 0.7108022146306985, "micro_f1": 0.4899069434502506}, {"micro_f1_no_misc": 0.6950701232469187, "micro_f1": 0.5435321569199358}, {"micro_f1_no_misc": 0.7148450609687884, "micro_f1": 0.5373773153272821}, {"micro_f1_no_misc": 0.7110646211064621, "micro_f1": 0.5182156133828996}, {"micro_f1_no_misc": 0.753303430718875, "micro_f1": 0.5146799765120376}, {"micro_f1_no_misc": 0.7434751939807195, "micro_f1": 0.5350197397280304}, {"micro_f1_no_misc": 0.7412633667732335, "micro_f1": 0.4649071358748778}, {"micro_f1_no_misc": 0.7466566679165104, "micro_f1": 0.5867623737584509}, {"micro_f1_no_misc": 0.7472355634982688, "micro_f1": 0.5385988398036591}, {"micro_f1_no_misc": 0.7679723502304147, "micro_f1": 0.5333138941536666}], "total": {"test_micro_f1_no_misc": 73.31688593070889, "test_micro_f1_no_misc_se": 1.4527462904078818, "test_micro_f1": 52.623139889110895, "test_micro_f1_se": 2.0290935004134663}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-sv", "task": "linguistic-acceptability", "dataset_languages": ["sv"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.2534677170192706, "macro_f1": 0.6163792726944508}, {"mcc": 0.30907506328299306, "macro_f1": 0.6481871076019652}, {"mcc": 0.26112377824513966, "macro_f1": 0.6290866972907231}, {"mcc": 0.29262821148069984, "macro_f1": 0.6278589934467356}, {"mcc": 0.31798626574483424, "macro_f1": 0.6569718918864518}, {"mcc": 0.31572170973305436, "macro_f1": 0.657218389949559}, {"mcc": 0.31039175611744974, "macro_f1": 0.6443010424501393}, {"mcc": 0.30112931485348415, "macro_f1": 0.6503636164216329}, {"mcc": 0.3086257172691474, "macro_f1": 0.6311992151682707}, {"mcc": 0.26713831672939903, "macro_f1": 0.6205520100651187}], "total": {"test_mcc": 29.372878504754716, "test_mcc_se": 1.4975830498642195, "test_macro_f1": 63.821182369750474, "test_macro_f1_se": 0.9298463616405526}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-nb", "task": "linguistic-acceptability", "dataset_languages": ["nb", "no"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.313429402581259, "macro_f1": 0.6277021842446739}, {"mcc": 0.29541174976181817, "macro_f1": 0.6022700075856222}, {"mcc": 0.3729640282729722, "macro_f1": 0.6640152620312765}, {"mcc": 0.2543184491069778, "macro_f1": 0.5620169918712757}, {"mcc": 0.25593566382659216, "macro_f1": 0.5998011448935091}, {"mcc": 0.3252478200493812, "macro_f1": 0.6615304326945269}, {"mcc": 0.2711136354550532, "macro_f1": 0.6049504487798378}, {"mcc": 0.3450985071623489, "macro_f1": 0.6498113207547169}, {"mcc": 0.2999760607512191, "macro_f1": 0.6313854157995709}, {"mcc": 0.30244977194712985, "macro_f1": 0.6505268755075979}], "total": {"test_mcc": 30.359450889147517, "test_mcc_se": 2.349836377698681, "test_macro_f1": 62.54010084162609, "test_macro_f1_se": 2.0338332905520926}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-nn", "task": "linguistic-acceptability", "dataset_languages": ["nn"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.1805303726511076, "macro_f1": 0.5795765877957659}, {"mcc": 0.21786946720126874, "macro_f1": 0.6088643410519601}, {"mcc": 0.18161623483461464, "macro_f1": 0.5419169066227889}, {"mcc": 0.21911915333655388, "macro_f1": 0.5946765675359285}, {"mcc": 0.20188240023990503, "macro_f1": 0.588274297628264}, {"mcc": 0.20342628706357094, "macro_f1": 0.5954099527331739}, {"mcc": 0.21822173265449402, "macro_f1": 0.6050059052345299}, {"mcc": 0.16721715208376936, "macro_f1": 0.5500959301746365}, {"mcc": 0.20643755494945512, "macro_f1": 0.5535498801553848}, {"mcc": 0.20131584745885567, "macro_f1": 0.5742529064113708}], "total": {"test_mcc": 19.97636202473595, "test_mcc_se": 1.1061482149910988, "test_macro_f1": 57.916232753438024, "test_macro_f1_se": 1.4682063629369941}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-is", "task": "linguistic-acceptability", "dataset_languages": ["is"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.04353399677754831, "macro_f1": 0.4806946932040007}, {"mcc": 0.041088535996545376, "macro_f1": 0.4479374966988946}, {"mcc": 0.04102090194181629, "macro_f1": 0.5205059833754859}, {"mcc": 0.08681973967124691, "macro_f1": 0.5268261156922365}, {"mcc": 0.05471286189947072, "macro_f1": 0.5271930528948183}, {"mcc": 0.04949202894750107, "macro_f1": 0.521853813127088}, {"mcc": 0.0603347574429604, "macro_f1": 0.5049464448148421}, {"mcc": 0.07780926266925901, "macro_f1": 0.5321245124359254}, {"mcc": 0.09842311948104528, "macro_f1": 0.49461348154249013}, {"mcc": 0.05195231783021599, "macro_f1": 0.5218481259573873}], "total": {"test_mcc": 6.051875226576095, "test_mcc_se": 1.257109220289303, "test_macro_f1": 50.78543719743169, "test_macro_f1_se": 1.6510447466696947}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-fo", "task": "linguistic-acceptability", "dataset_languages": ["fo"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.10959986464774955, "macro_f1": 0.49783549783549785}, {"mcc": 0.027448990242931657, "macro_f1": 0.38832683174534355}, {"mcc": 0.08267958511728508, "macro_f1": 0.5413268643958531}, {"mcc": 0.07636860932941496, "macro_f1": 0.5357264387053113}, {"mcc": 0.06530665236103944, "macro_f1": 0.5322261163960613}, {"mcc": 0.06545122114674011, "macro_f1": 0.49279638203940995}, {"mcc": 0.02487939662853241, "macro_f1": 0.3643074608402777}, {"mcc": 0.05976977632975246, "macro_f1": 0.49859413727479696}, {"mcc": 0.051450116244258264, "macro_f1": 0.47406266050333845}, {"mcc": 0.05665487660975264, "macro_f1": 0.5139221747536846}], "total": {"test_mcc": 6.196090886574566, "test_mcc_se": 1.551475419960228, "test_macro_f1": 48.39124564489574, "test_macro_f1_se": 3.7669189863877093}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-de", "task": "linguistic-acceptability", "dataset_languages": ["de"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.42213085491463703, "macro_f1": 0.702282308475069}, {"mcc": 0.39630728299127677, "macro_f1": 0.6979829641988553}, {"mcc": 0.44516835435081337, "macro_f1": 0.6997319458362747}, {"mcc": 0.41666423521229007, "macro_f1": 0.7070242649141529}, {"mcc": 0.42051269479381254, "macro_f1": 0.7031485335390728}, {"mcc": 0.4513284287369613, "macro_f1": 0.7043140710398781}, {"mcc": 0.4187173887487383, "macro_f1": 0.701194655110148}, {"mcc": 0.43833389378886917, "macro_f1": 0.7185263266084168}, {"mcc": 0.3427830747482012, "macro_f1": 0.6713452676884191}, {"mcc": 0.3986743516106627, "macro_f1": 0.6806806806806807}], "total": {"test_mcc": 41.50620559896262, "test_mcc_se": 1.9274299732620288, "test_macro_f1": 69.86231018090967, "test_macro_f1_se": 0.8282754956963736}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-nl", "task": "linguistic-acceptability", "dataset_languages": ["nl"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.5426790847130217, "macro_f1": 0.765782643773099}, {"mcc": 0.48314270632223005, "macro_f1": 0.724016809885216}, {"mcc": 0.5254549498073766, "macro_f1": 0.7624914572825949}, {"mcc": 0.48930498551910245, "macro_f1": 0.7265119591667162}, {"mcc": 0.538050452603224, "macro_f1": 0.7671864671020203}, {"mcc": 0.5423338271746314, "macro_f1": 0.7710422139007082}, {"mcc": 0.4986638482763968, "macro_f1": 0.7384934131254841}, {"mcc": 0.5014306410194045, "macro_f1": 0.7436296873261378}, {"mcc": 0.47489544786574417, "macro_f1": 0.7143677300000517}, {"mcc": 0.5290862959845161, "macro_f1": 0.7639074756283989}], "total": {"test_mcc": 51.250422392856485, "test_mcc_se": 1.6036009531031312, "test_macro_f1": 74.77429857190427, "test_macro_f1_se": 1.2993303441593438}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-en", "task": "linguistic-acceptability", "dataset_languages": ["en"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.4603638977878672, "macro_f1": 0.7269312378416333}, {"mcc": 0.4770104248700011, "macro_f1": 0.7287004307458715}, {"mcc": 0.43379702594620373, "macro_f1": 0.7153475371536333}, {"mcc": 0.45025692560378616, "macro_f1": 0.7136572639347334}, {"mcc": 0.41420008479736153, "macro_f1": 0.6985311019599568}, {"mcc": 0.4747841037646109, "macro_f1": 0.7334721499219157}, {"mcc": 0.4658488225747295, "macro_f1": 0.7213021517298123}, {"mcc": 0.46792700967416007, "macro_f1": 0.7256603577056164}, {"mcc": 0.43127767412009194, "macro_f1": 0.7104772328248626}, {"mcc": 0.44247250069149396, "macro_f1": 0.7079641124584944}], "total": {"test_mcc": 45.17938469830306, "test_mcc_se": 1.2973288959554654, "test_macro_f1": 71.8204357627653, "test_macro_f1_se": 0.6742530690269587}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-fr", "task": "linguistic-acceptability", "dataset_languages": ["fr"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.2887424531181012, "macro_f1": 0.6427960818850706}, {"mcc": 0.4051331123102699, "macro_f1": 0.7023368771805973}, {"mcc": 0.3436153434253539, "macro_f1": 0.6656099121563266}, {"mcc": 0.35298889940000655, "macro_f1": 0.6629942059300495}, {"mcc": 0.40864154012819576, "macro_f1": 0.6917851565700537}, {"mcc": 0.43397122398128934, "macro_f1": 0.6860305558541389}, {"mcc": 0.42252539015480256, "macro_f1": 0.6874829625591126}, {"mcc": 0.3697346168500856, "macro_f1": 0.6848475305350478}, {"mcc": 0.3908233868852559, "macro_f1": 0.6944774167492731}, {"mcc": 0.39981723991320645, "macro_f1": 0.6854422350648152}], "total": {"test_mcc": 38.15993206166567, "test_mcc_se": 2.7051758900502763, "test_macro_f1": 68.03802934484486, "test_macro_f1_se": 1.1071749438427612}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-it", "task": "linguistic-acceptability", "dataset_languages": ["it"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.3139190393409632, "macro_f1": 0.6515468029283822}, {"mcc": 0.32457735631047735, "macro_f1": 0.6609453231461775}, {"mcc": 0.2705728966718733, "macro_f1": 0.6270120259019427}, {"mcc": 0.2767040412320848, "macro_f1": 0.6372000248913803}, {"mcc": 0.23854370911159106, "macro_f1": 0.5897435897435898}, {"mcc": 0.3308451177069874, "macro_f1": 0.6619389495788093}, {"mcc": 0.3047806292187636, "macro_f1": 0.6431149818450734}, {"mcc": 0.2773527938047021, "macro_f1": 0.6360158272267669}, {"mcc": 0.32743100332665853, "macro_f1": 0.6625965207176319}, {"mcc": 0.30672604101428547, "macro_f1": 0.6363059884240005}], "total": {"test_mcc": 29.714526277383868, "test_mcc_se": 1.8693434132598397, "test_macro_f1": 64.06420034403753, "test_macro_f1_se": 1.3549840940360354}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "norquad", "task": "reading-comprehension", "dataset_languages": ["nb", "nn", "no"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"f1": 73.64691209163922, "em": 41.911764705882355}, {"f1": 65.98945641226622, "em": 36.016260162601625}, {"f1": 71.33022134695287, "em": 42.464612822647794}, {"f1": 67.23090323808056, "em": 38.021259198691745}, {"f1": 72.02388420601699, "em": 40.52936311000827}, {"f1": 65.45657139342805, "em": 35.63786008230453}, {"f1": 65.74634861205743, "em": 37.47899159663866}, {"f1": 71.2002912611563, "em": 42.145270270270274}, {"f1": 65.40031532430714, "em": 35.79734219269103}, {"f1": 66.75475681198338, "em": 35.84437086092715}], "total": {"test_f1": 68.47796606978882, "test_f1_se": 1.9775369686155573, "test_em": 38.58470950026634, "test_em_se": 1.7854302008460279}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scandiqa-sv", "task": "reading-comprehension", "dataset_languages": ["sv"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"f1": 66.62691264963986, "em": 58.10606060606061}, {"f1": 64.76632256694772, "em": 53.525398028809704}, {"f1": 66.41101678321928, "em": 55.78865578865579}, {"f1": 66.10923453299077, "em": 55.09893455098935}, {"f1": 65.05160631719647, "em": 53.508095605242865}, {"f1": 64.70505236917445, "em": 55.343511450381676}, {"f1": 66.04982126723975, "em": 53.51014040561623}, {"f1": 67.02816184322468, "em": 57.075471698113205}, {"f1": 66.1520854624761, "em": 55.804953560371516}, {"f1": 66.77584728435396, "em": 56.867283950617285}], "total": {"test_f1": 65.9676061076463, "test_f1_se": 0.5204717863048499, "test_em": 55.46285056448583, "test_em_se": 0.9976394826545103}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "nqii", "task": "reading-comprehension", "dataset_languages": ["is"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"f1": 59.63796676040363, "em": 27.692307692307693}, {"f1": 58.13532808935817, "em": 24.766355140186917}, {"f1": 51.194891743285744, "em": 24.299065420560748}, {"f1": 53.03586442986816, "em": 24.364723467862483}, {"f1": 55.124624653947635, "em": 24.137931034482758}, {"f1": 57.383659406142236, "em": 28.043143297380585}, {"f1": 55.55333371981666, "em": 24.307692307692307}, {"f1": 51.74941161474248, "em": 25.460122699386503}, {"f1": 55.72266971834554, "em": 24.397590361445783}, {"f1": 57.96537765682462, "em": 30.72289156626506}], "total": {"test_f1": 55.55031277927348, "test_f1_se": 1.7595764250783585, "test_em": 25.819182298757084, "test_em_se": 1.3904903006459872}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "foqa", "task": "reading-comprehension", "dataset_languages": ["fo"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"f1": 67.63256689273439, "em": 48.77450980392157}, {"f1": 59.63347949948827, "em": 39.06633906633907}, {"f1": 75.8902480114195, "em": 55.55555555555556}, {"f1": 73.63198218640856, "em": 56.310679611650485}, {"f1": 70.83394686454582, "em": 53.960396039603964}, {"f1": 70.31885999033462, "em": 51.358024691358025}, {"f1": 69.77932887137227, "em": 51.41509433962264}, {"f1": 72.87527800392547, "em": 51.908396946564885}, {"f1": 76.4772107137301, "em": 58.85286783042394}, {"f1": 71.86246426433033, "em": 52.08845208845209}], "total": {"test_f1": 70.89353652982894, "test_f1_se": 2.976131033042897, "test_em": 51.92903159734923, "test_em_se": 3.3341281173010633}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "germanquad", "task": "reading-comprehension", "dataset_languages": ["de"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"f1": 55.44060757723055, "em": 26.742424242424242}, {"f1": 53.95829124838333, "em": 25.549658832448824}, {"f1": 58.93021622841678, "em": 27.03962703962704}, {"f1": 56.76994734340497, "em": 27.77777777777778}, {"f1": 56.96295638462367, "em": 28.218966846569007}, {"f1": 57.19940118983707, "em": 27.709923664122137}, {"f1": 53.756400177141025, "em": 26.365054602184088}, {"f1": 57.810729097748464, "em": 28.930817610062892}, {"f1": 56.13254583474899, "em": 25.851393188854487}, {"f1": 51.42159424161441, "em": 25.617283950617285}], "total": {"test_f1": 55.83826893231493, "test_f1_se": 1.3868007222363465, "test_em": 26.980292775468776, "test_em_se": 0.7194011417778883}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "squad", "task": "reading-comprehension", "dataset_languages": ["en"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"f1": 85.45745299421112, "em": 70.22727272727273}, {"f1": 84.02076111229294, "em": 67.02047005307051}, {"f1": 86.48947145211763, "em": 71.87257187257187}, {"f1": 84.74802871744411, "em": 69.71080669710807}, {"f1": 85.77575369904287, "em": 71.85813415574403}, {"f1": 82.95467269202634, "em": 67.17557251908397}, {"f1": 85.10934566961441, "em": 69.65678627145085}, {"f1": 83.43477009714863, "em": 65.33018867924528}, {"f1": 83.94232535861876, "em": 66.56346749226006}, {"f1": 84.0946973752664, "em": 67.74691358024691}], "total": {"test_f1": 84.60272791677832, "test_f1_se": 0.6870023802904415, "test_em": 68.71621840480543, "test_em_se": 1.4054959820106814}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "squad-nl", "task": "reading-comprehension", "dataset_languages": ["nl"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"f1": 76.33565287419286, "em": 55.303030303030305}, {"f1": 76.13349467490048, "em": 56.86125852918878}, {"f1": 74.64357525721312, "em": 56.64335664335665}, {"f1": 75.42184302826406, "em": 56.69710806697108}, {"f1": 75.89201527079582, "em": 56.51503469545104}, {"f1": 78.36219529439784, "em": 57.48091603053435}, {"f1": 75.49221485565602, "em": 56.63026521060842}, {"f1": 76.46091073920879, "em": 57.38993710691824}, {"f1": 76.32732402391818, "em": 59.05572755417957}, {"f1": 76.31321100841495, "em": 56.32716049382716}], "total": {"test_f1": 76.13824370269622, "test_f1_se": 0.5981008131983053, "test_em": 56.89037946340657, "test_em_se": 0.599918040638232}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "squad-it", "task": "reading-comprehension", "dataset_languages": ["it"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"f1": 72.4253939932978, "em": 50.75757575757576}, {"f1": 72.35167997461805, "em": 48.59742228961334}, {"f1": 72.59576171842798, "em": 54.001554001554}, {"f1": 71.1281286878156, "em": 47.48858447488585}, {"f1": 72.19491572860107, "em": 50.424055512721665}, {"f1": 71.52759696286489, "em": 49.23664122137404}, {"f1": 73.43367692261981, "em": 51.5600624024961}, {"f1": 69.97014383833965, "em": 46.4622641509434}, {"f1": 73.69266961159316, "em": 56.037151702786375}, {"f1": 71.66765945022162, "em": 49.76851851851852}], "total": {"test_f1": 72.09876268883997, "test_f1_se": 0.6769073370756491, "test_em": 50.43338300324691, "test_em_se": 1.791554759174558}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fquad", "task": "reading-comprehension", "dataset_languages": ["fr"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"f1": 68.51103341454557, "em": 37.27272727272727}, {"f1": 69.24776211662075, "em": 39.575435936315394}, {"f1": 70.0874476318926, "em": 39.08313908313908}, {"f1": 69.24208213510758, "em": 38.50837138508371}, {"f1": 65.47640628130485, "em": 35.774865073245955}, {"f1": 69.73688148212811, "em": 38.01526717557252}, {"f1": 67.80790707744754, "em": 38.92355694227769}, {"f1": 68.3581134115447, "em": 40.172955974842765}, {"f1": 66.08685910778462, "em": 37.84829721362229}, {"f1": 70.66899548304171, "em": 39.19753086419753}], "total": {"test_f1": 68.52234881414181, "test_f1_se": 1.0396802026745624, "test_em": 38.437214692102415, "test_em_se": 0.7871036256916663}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mlsum", "task": "summarization", "dataset_languages": ["de"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"bertscore": 0.6611195646401029, "rouge_l": 0.1765718666483803}, {"bertscore": 0.6633599490160123, "rouge_l": 0.1847529702509289}, {"bertscore": 0.6552798897901084, "rouge_l": 0.16686100220285577}, {"bertscore": 0.6625044771644752, "rouge_l": 0.18376538172269619}, {"bertscore": 0.6562618380703498, "rouge_l": 0.16338524052757497}, {"bertscore": 0.6605799106764607, "rouge_l": 0.17774391539463497}, {"bertscore": 0.6564689974475186, "rouge_l": 0.1673801259498412}, {"bertscore": 0.6644116969255265, "rouge_l": 0.184922080711026}, {"bertscore": 0.6546788683917839, "rouge_l": 0.16043332766856438}, {"bertscore": 0.6572726734739263, "rouge_l": 0.16902132025450206}], "total": {"test_bertscore": 65.91937865596265, "test_bertscore_se": 0.22308121809611123, "test_rouge_l": 17.348372313310048, "test_rouge_l_se": 0.5712651437061225}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "rrn", "task": "summarization", "dataset_languages": ["is"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"bertscore": 0.6612481942283921, "rouge_l": 0.16534014798281266}, {"bertscore": 0.6592803800594993, "rouge_l": 0.16159352362455742}, {"bertscore": 0.6620556872221641, "rouge_l": 0.1676425207334527}, {"bertscore": 0.6605022621806711, "rouge_l": 0.16217773242990224}, {"bertscore": 0.660960236331448, "rouge_l": 0.16611498609942843}, {"bertscore": 0.6595516541274264, "rouge_l": 0.16652337701152425}, {"bertscore": 0.6589053289499134, "rouge_l": 0.16234365120321836}, {"bertscore": 0.6568969477084465, "rouge_l": 0.156754933675613}, {"bertscore": 0.6605598555761389, "rouge_l": 0.16642683857531565}, {"bertscore": 0.6592049017781392, "rouge_l": 0.16537421624756432}], "total": {"test_bertscore": 65.99165448162239, "test_bertscore_se": 0.09095244033075155, "test_rouge_l": 16.40291927583389, "test_rouge_l_se": 0.20454687697888657}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "no-sammendrag", "task": "summarization", "dataset_languages": ["nb", "nn", "no"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"bertscore": 0.6405142651201459, "rouge_l": 0.14017581977343374}, {"bertscore": 0.6419513177825138, "rouge_l": 0.14255316161850906}, {"bertscore": 0.6480505671352148, "rouge_l": 0.155504398160008}, {"bertscore": 0.6415244383679237, "rouge_l": 0.1451224521852601}, {"bertscore": 0.6434280806424795, "rouge_l": 0.1475036421319404}, {"bertscore": 0.6401754972466733, "rouge_l": 0.13978908832236253}, {"bertscore": 0.6369825614237925, "rouge_l": 0.13662945467214455}, {"bertscore": 0.645357347355457, "rouge_l": 0.151798242779963}, {"bertscore": 0.6444260342977941, "rouge_l": 0.14882414218589501}, {"bertscore": 0.6397365824959707, "rouge_l": 0.1420454551972944}], "total": {"test_bertscore": 64.22146691867965, "test_bertscore_se": 0.19752706301978937, "test_rouge_l": 14.499458570268109, "test_rouge_l_se": 0.36532620107359337}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "wiki-lingua-nl", "task": "summarization", "dataset_languages": ["nl"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"bertscore": 0.7556709107593633, "rouge_l": 0.3371112693079518}, {"bertscore": 0.7532878501224332, "rouge_l": 0.3332909368596978}, {"bertscore": 0.7521426433959277, "rouge_l": 0.3315555272884846}, {"bertscore": 0.7559242988645565, "rouge_l": 0.33619529084005567}, {"bertscore": 0.7577818971330998, "rouge_l": 0.3464398646147641}, {"bertscore": 0.755725047973101, "rouge_l": 0.33832055206238654}, {"bertscore": 0.7542063759465236, "rouge_l": 0.3376616500146907}, {"bertscore": 0.7498824051290285, "rouge_l": 0.3270501298292138}, {"bertscore": 0.7562933737062849, "rouge_l": 0.34527460609148}, {"bertscore": 0.7557770992279984, "rouge_l": 0.34144953255532756}], "total": {"test_bertscore": 75.46691902258317, "test_bertscore_se": 0.144129248416655, "test_rouge_l": 33.743493594640526, "test_rouge_l_se": 0.37000114377302035}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "swedn", "task": "summarization", "dataset_languages": ["sv"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"bertscore": 0.6551893088035285, "rouge_l": 0.17210788321820042}, {"bertscore": 0.6541451512021013, "rouge_l": 0.1714362165627058}, {"bertscore": 0.6552301220071968, "rouge_l": 0.17428177879412157}, {"bertscore": 0.6548079745844007, "rouge_l": 0.1771203369969539}, {"bertscore": 0.6548787163337693, "rouge_l": 0.17162430318336785}, {"bertscore": 0.6524185590387788, "rouge_l": 0.16959585901885466}, {"bertscore": 0.6544030509248842, "rouge_l": 0.17197119295116664}, {"bertscore": 0.6570389073749539, "rouge_l": 0.17878307718351721}, {"bertscore": 0.6549050688045099, "rouge_l": 0.1706586856718253}, {"bertscore": 0.6543822471285239, "rouge_l": 0.16989998374284931}], "total": {"test_bertscore": 65.47399106202647, "test_bertscore_se": 0.07085772297525131, "test_rouge_l": 17.27479317323563, "test_rouge_l_se": 0.1896929875338642}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "cnn-dailymail", "task": "summarization", "dataset_languages": ["en"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"bertscore": 0.727965661615599, "rouge_l": 0.32286220677152694}, {"bertscore": 0.7304377709224354, "rouge_l": 0.3201091819083084}, {"bertscore": 0.7288949886278715, "rouge_l": 0.3187870366928221}, {"bertscore": 0.7321407627023291, "rouge_l": 0.3151526581205678}, {"bertscore": 0.728835900867125, "rouge_l": 0.31569192066210683}, {"bertscore": 0.7372033491556067, "rouge_l": 0.3245261838752782}, {"bertscore": 0.7355126363981981, "rouge_l": 0.3195017656383963}, {"bertscore": 0.7303174440166913, "rouge_l": 0.31765070424612046}, {"bertscore": 0.7298864251351915, "rouge_l": 0.31984944216997063}, {"bertscore": 0.7338378234999254, "rouge_l": 0.31889264971870146}], "total": {"test_bertscore": 73.15032762940973, "test_bertscore_se": 0.1916069508682317, "test_rouge_l": 31.930237498037993, "test_rouge_l_se": 0.17816671114631633}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "orange-sum", "task": "summarization", "dataset_languages": ["fr"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"bertscore": 0.6722303770366125, "rouge_l": 0.19748738880669142}, {"bertscore": 0.6697946776403114, "rouge_l": 0.19016822628663743}, {"bertscore": 0.6745165072497912, "rouge_l": 0.20348843563316915}, {"bertscore": 0.6691481752204709, "rouge_l": 0.19473870284509048}, {"bertscore": 0.6696583747980185, "rouge_l": 0.19039740827313528}, {"bertscore": 0.6744023148785345, "rouge_l": 0.2002641991930152}, {"bertscore": 0.6716472051921301, "rouge_l": 0.19192083809686128}, {"bertscore": 0.6729440664639696, "rouge_l": 0.20123132234023194}, {"bertscore": 0.6749857978429645, "rouge_l": 0.1986016932450631}, {"bertscore": 0.6742680868774187, "rouge_l": 0.1975305148797134}], "total": {"test_bertscore": 67.23595583200222, "test_bertscore_se": 0.1376948160057069, "test_rouge_l": 19.65828729599609, "test_rouge_l_se": 0.2871562010131101}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "ilpost-sum", "task": "summarization", "dataset_languages": ["it"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"bertscore": 0.6568493898957968, "rouge_l": 0.17550226330713628}, {"bertscore": 0.6532749837206211, "rouge_l": 0.17186646238046077}, {"bertscore": 0.6580651980184484, "rouge_l": 0.1824710178097071}, {"bertscore": 0.6549025206186343, "rouge_l": 0.17344015154242715}, {"bertscore": 0.655109293787973, "rouge_l": 0.16957078018749455}, {"bertscore": 0.655632575872005, "rouge_l": 0.1752208429126126}, {"bertscore": 0.658505422557937, "rouge_l": 0.18073876829348035}, {"bertscore": 0.6584159458579961, "rouge_l": 0.179803879751735}, {"bertscore": 0.6597264725423884, "rouge_l": 0.18539077201949766}, {"bertscore": 0.6556288364517968, "rouge_l": 0.17160368273459936}], "total": {"test_bertscore": 65.66110639323597, "test_bertscore_se": 0.12558511954122759, "test_rouge_l": 17.656086209391507, "test_rouge_l_se": 0.3260105129194156}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "nrk-quiz-qa", "task": "knowledge", "dataset_languages": ["nb", "nn", "no"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.4085635714058815, "accuracy": 0.56298828125}, {"mcc": 0.39784673075609883, "accuracy": 0.5576171875}, {"mcc": 0.39427056358409796, "accuracy": 0.55126953125}, {"mcc": 0.4055653606100973, "accuracy": 0.55712890625}, {"mcc": 0.405486514812808, "accuracy": 0.560546875}, {"mcc": 0.3783072995000451, "accuracy": 0.5400390625}, {"mcc": 0.3907187205546391, "accuracy": 0.55224609375}, {"mcc": 0.3873354412143969, "accuracy": 0.54638671875}, {"mcc": 0.3934738330714974, "accuracy": 0.55078125}, {"mcc": 0.39965164474781906, "accuracy": 0.556640625}], "total": {"test_mcc": 39.612196802573806, "test_mcc_se": 0.5774890859878068, "test_accuracy": 55.3564453125, "test_accuracy_se": 0.42610287152850446}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-sv", "task": "knowledge", "dataset_languages": ["sv"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.5051829113763449, "accuracy": 0.62548828125}, {"mcc": 0.5020794854385962, "accuracy": 0.6259765625}, {"mcc": 0.4828997681395652, "accuracy": 0.607421875}, {"mcc": 0.4727838593774654, "accuracy": 0.6015625}, {"mcc": 0.5121048351628732, "accuracy": 0.6318359375}, {"mcc": 0.5019319465990222, "accuracy": 0.62548828125}, {"mcc": 0.5098507793083286, "accuracy": 0.62939453125}, {"mcc": 0.5402053219839853, "accuracy": 0.65234375}, {"mcc": 0.5238875540791289, "accuracy": 0.64013671875}, {"mcc": 0.488172904298165, "accuracy": 0.61474609375}], "total": {"test_mcc": 50.390993657634745, "test_mcc_se": 1.2197766023056948, "test_accuracy": 62.54394531250001, "test_accuracy_se": 0.9252896997935878}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "icelandic-knowledge", "task": "knowledge", "dataset_languages": ["is"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.1251356715769414, "accuracy": 0.34765625}, {"mcc": 0.19533200671300105, "accuracy": 0.3974609375}, {"mcc": 0.13722235078130593, "accuracy": 0.353515625}, {"mcc": 0.2073257051928513, "accuracy": 0.4013671875}, {"mcc": 0.13997531381084796, "accuracy": 0.353515625}, {"mcc": 0.17783625094370348, "accuracy": 0.3828125}, {"mcc": 0.16020586510755758, "accuracy": 0.3681640625}, {"mcc": 0.12296904203899262, "accuracy": 0.3369140625}, {"mcc": 0.11547714458039077, "accuracy": 0.3359375}, {"mcc": 0.16927427389649202, "accuracy": 0.376953125}], "total": {"test_mcc": 15.507536246420841, "test_mcc_se": 1.9750393276096523, "test_accuracy": 36.54296875, "test_accuracy_se": 1.467114998467502}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-de", "task": "knowledge", "dataset_languages": ["de"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.5777986138865668, "accuracy": 0.681640625}, {"mcc": 0.554834574986583, "accuracy": 0.66650390625}, {"mcc": 0.5744340886269822, "accuracy": 0.67919921875}, {"mcc": 0.5917554906217833, "accuracy": 0.693359375}, {"mcc": 0.5449558742999544, "accuracy": 0.6572265625}, {"mcc": 0.5560787570249109, "accuracy": 0.66650390625}, {"mcc": 0.5645037376485231, "accuracy": 0.671875}, {"mcc": 0.5648480757812532, "accuracy": 0.6708984375}, {"mcc": 0.5721978964425426, "accuracy": 0.677734375}, {"mcc": 0.5737298367588086, "accuracy": 0.68017578125}], "total": {"test_mcc": 56.751369460779074, "test_mcc_se": 0.8311614011048305, "test_accuracy": 67.451171875, "test_accuracy_se": 0.6258135574135303}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-nl", "task": "knowledge", "dataset_languages": ["nl"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.5955549869199539, "accuracy": 0.69677734375}, {"mcc": 0.5817678496097093, "accuracy": 0.68603515625}, {"mcc": 0.5903066228667003, "accuracy": 0.69140625}, {"mcc": 0.5758920671469409, "accuracy": 0.68115234375}, {"mcc": 0.5613706311906215, "accuracy": 0.669921875}, {"mcc": 0.5761131468904118, "accuracy": 0.681640625}, {"mcc": 0.5859854846892926, "accuracy": 0.6884765625}, {"mcc": 0.5916008654928492, "accuracy": 0.69287109375}, {"mcc": 0.5986754665406324, "accuracy": 0.69873046875}, {"mcc": 0.5433271258204587, "accuracy": 0.6572265625}], "total": {"test_mcc": 58.005942471675695, "test_mcc_se": 1.0523310095439384, "test_accuracy": 68.4423828125, "test_accuracy_se": 0.790740534835075}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu", "task": "knowledge", "dataset_languages": ["en"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.6450675162943179, "accuracy": 0.73291015625}, {"mcc": 0.6609351602185153, "accuracy": 0.744140625}, {"mcc": 0.6430135349040677, "accuracy": 0.73193359375}, {"mcc": 0.644560164020961, "accuracy": 0.7314453125}, {"mcc": 0.6362904856184398, "accuracy": 0.72607421875}, {"mcc": 0.6477332309252526, "accuracy": 0.7353515625}, {"mcc": 0.644713149883444, "accuracy": 0.73193359375}, {"mcc": 0.6718203667824827, "accuracy": 0.75244140625}, {"mcc": 0.6278352125160617, "accuracy": 0.71923828125}, {"mcc": 0.654440748553109, "accuracy": 0.740234375}], "total": {"test_mcc": 64.76409569716651, "test_mcc_se": 0.7660525050542555, "test_accuracy": 73.45703125, "test_accuracy_se": 0.576023639123236}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-fr", "task": "knowledge", "dataset_languages": ["fr"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.59929302486797, "accuracy": 0.69873046875}, {"mcc": 0.5773273883561415, "accuracy": 0.6826171875}, {"mcc": 0.5763607418658778, "accuracy": 0.681640625}, {"mcc": 0.5737867086860354, "accuracy": 0.67822265625}, {"mcc": 0.5701048615898745, "accuracy": 0.67626953125}, {"mcc": 0.5963278769426166, "accuracy": 0.69677734375}, {"mcc": 0.5948042429247172, "accuracy": 0.6953125}, {"mcc": 0.5890181222220685, "accuracy": 0.69140625}, {"mcc": 0.6027440827023978, "accuracy": 0.701171875}, {"mcc": 0.6063352103686577, "accuracy": 0.70361328125}], "total": {"test_mcc": 58.86102260526357, "test_mcc_se": 0.8175011985032774, "test_accuracy": 69.0576171875, "test_accuracy_se": 0.6235082872443829}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-it", "task": "knowledge", "dataset_languages": ["it"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.6069272313116468, "accuracy": 0.70458984375}, {"mcc": 0.6274057187536813, "accuracy": 0.72021484375}, {"mcc": 0.6165864909601776, "accuracy": 0.7119140625}, {"mcc": 0.5763049087235472, "accuracy": 0.68212890625}, {"mcc": 0.6090763950635332, "accuracy": 0.7060546875}, {"mcc": 0.5956558139196638, "accuracy": 0.6962890625}, {"mcc": 0.5873356177493444, "accuracy": 0.68994140625}, {"mcc": 0.6066405342997766, "accuracy": 0.7041015625}, {"mcc": 0.6079813667354457, "accuracy": 0.70556640625}, {"mcc": 0.5845598831583031, "accuracy": 0.68896484375}], "total": {"test_mcc": 60.184739606751194, "test_mcc_se": 0.9711038761891438, "test_accuracy": 70.09765625, "test_accuracy_se": 0.7164891577609663}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "nor-common-sense-qa", "task": "common-sense-reasoning", "dataset_languages": ["nb", "nn", "no"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.7019320387431742, "accuracy": 0.7602820211515864}, {"mcc": 0.6769041623441717, "accuracy": 0.7391304347826086}, {"mcc": 0.661957349520165, "accuracy": 0.7285546415981199}, {"mcc": 0.658165106563664, "accuracy": 0.7262044653349001}, {"mcc": 0.6683473758427416, "accuracy": 0.7344300822561692}, {"mcc": 0.665638076387603, "accuracy": 0.7320799059929495}, {"mcc": 0.6996576982495988, "accuracy": 0.7591069330199764}, {"mcc": 0.6708511898844879, "accuracy": 0.7356051703877791}, {"mcc": 0.652404184602053, "accuracy": 0.7203290246768508}, {"mcc": 0.6472028234727896, "accuracy": 0.7168037602820212}], "total": {"test_mcc": 67.03060005610449, "test_mcc_se": 1.1320817465031854, "test_accuracy": 73.52526439482962, "test_accuracy_se": 0.9032105273902593}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-sv", "task": "common-sense-reasoning", "dataset_languages": ["sv"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.5756750187588359, "accuracy": 0.67431640625}, {"mcc": 0.5893178962706042, "accuracy": 0.6875}, {"mcc": 0.5602299651378414, "accuracy": 0.66357421875}, {"mcc": 0.5717234032729972, "accuracy": 0.6708984375}, {"mcc": 0.5551787331184573, "accuracy": 0.6591796875}, {"mcc": 0.578227856492008, "accuracy": 0.67724609375}, {"mcc": 0.6014752218377835, "accuracy": 0.697265625}, {"mcc": 0.5565176227554266, "accuracy": 0.65966796875}, {"mcc": 0.5432998446248508, "accuracy": 0.64697265625}, {"mcc": 0.564153349384705, "accuracy": 0.669921875}], "total": {"test_mcc": 56.9579891165351, "test_mcc_se": 1.0753158179058502, "test_accuracy": 67.0654296875, "test_accuracy_se": 0.9035656808270296}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "winogrande-is", "task": "common-sense-reasoning", "dataset_languages": ["is"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.11448768776303533, "accuracy": 0.5669642857142857}, {"mcc": 0.16306879101347488, "accuracy": 0.5803571428571429}, {"mcc": 0.10934812537838606, "accuracy": 0.5770089285714286}, {"mcc": 0.10010517816889812, "accuracy": 0.5680803571428571}, {"mcc": 0.1343890780650303, "accuracy": 0.5870535714285714}, {"mcc": 0.11876130724752046, "accuracy": 0.5647321428571429}, {"mcc": 0.09958713943687668, "accuracy": 0.5758928571428571}, {"mcc": 0.16004317198186913, "accuracy": 0.5892857142857143}, {"mcc": 0.14559640018215173, "accuracy": 0.6082589285714286}, {"mcc": 0.11369805613715568, "accuracy": 0.5636160714285714}], "total": {"test_mcc": 12.590849353743986, "test_mcc_se": 1.4563737000742736, "test_accuracy": 57.8125, "test_accuracy_se": 0.8615282367849466}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-de", "task": "common-sense-reasoning", "dataset_languages": ["de"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.7027046368832116, "accuracy": 0.77490234375}, {"mcc": 0.6999763453285217, "accuracy": 0.77001953125}, {"mcc": 0.7360582651845444, "accuracy": 0.7998046875}, {"mcc": 0.7052519438294426, "accuracy": 0.77587890625}, {"mcc": 0.7088528693377893, "accuracy": 0.77880859375}, {"mcc": 0.6924377469638927, "accuracy": 0.76708984375}, {"mcc": 0.7316184356485441, "accuracy": 0.796875}, {"mcc": 0.6925816355777833, "accuracy": 0.765625}, {"mcc": 0.7314739411628179, "accuracy": 0.79638671875}, {"mcc": 0.694256284724711, "accuracy": 0.767578125}], "total": {"test_mcc": 70.9521210464126, "test_mcc_se": 1.062039572659239, "test_accuracy": 77.9296875, "test_accuracy_se": 0.8295497705736069}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-nl", "task": "common-sense-reasoning", "dataset_languages": ["nl"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.6995428777090597, "accuracy": 0.7724609375}, {"mcc": 0.6940675112355101, "accuracy": 0.76708984375}, {"mcc": 0.6713441609519519, "accuracy": 0.75048828125}, {"mcc": 0.6439395771677424, "accuracy": 0.72705078125}, {"mcc": 0.6976190184529887, "accuracy": 0.7666015625}, {"mcc": 0.6464259699692244, "accuracy": 0.728515625}, {"mcc": 0.6874245995594229, "accuracy": 0.759765625}, {"mcc": 0.6401230998624905, "accuracy": 0.7216796875}, {"mcc": 0.6824755525713289, "accuracy": 0.75634765625}, {"mcc": 0.6280865612660456, "accuracy": 0.7119140625}], "total": {"test_mcc": 66.91048928745766, "test_mcc_se": 1.6723462069099706, "test_accuracy": 74.619140625, "test_accuracy_se": 1.3553851251521551}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag", "task": "common-sense-reasoning", "dataset_languages": ["en"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.8384087448320903, "accuracy": 0.8779296875}, {"mcc": 0.8516323549625238, "accuracy": 0.88818359375}, {"mcc": 0.8337518639238762, "accuracy": 0.87451171875}, {"mcc": 0.844992683158355, "accuracy": 0.8828125}, {"mcc": 0.8496652700745464, "accuracy": 0.88671875}, {"mcc": 0.8653629332109348, "accuracy": 0.8984375}, {"mcc": 0.8256245576235896, "accuracy": 0.86865234375}, {"mcc": 0.845673324947246, "accuracy": 0.88330078125}, {"mcc": 0.8466444310413505, "accuracy": 0.8837890625}, {"mcc": 0.8252554138026391, "accuracy": 0.8681640625}], "total": {"test_mcc": 84.27011577577153, "test_mcc_se": 0.7635715500029936, "test_accuracy": 88.125, "test_accuracy_se": 0.574360514907727}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-fr", "task": "common-sense-reasoning", "dataset_languages": ["fr"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.7278189246441602, "accuracy": 0.79296875}, {"mcc": 0.7488273461519814, "accuracy": 0.80908203125}, {"mcc": 0.7441625406722395, "accuracy": 0.80615234375}, {"mcc": 0.7328718809667786, "accuracy": 0.7978515625}, {"mcc": 0.7654876854369634, "accuracy": 0.8232421875}, {"mcc": 0.7532263526839921, "accuracy": 0.814453125}, {"mcc": 0.7350569989142098, "accuracy": 0.79931640625}, {"mcc": 0.7494998809402661, "accuracy": 0.81103515625}, {"mcc": 0.7206336305406291, "accuracy": 0.787109375}, {"mcc": 0.7214020140052487, "accuracy": 0.7900390625}], "total": {"test_mcc": 73.9898725495647, "test_mcc_se": 0.9133861138986631, "test_accuracy": 80.3125, "test_accuracy_se": 0.7196920249562241}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-it", "task": "common-sense-reasoning", "dataset_languages": ["it"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.7453043633468757, "accuracy": 0.80908203125}, {"mcc": 0.7651190017162365, "accuracy": 0.82275390625}, {"mcc": 0.7434160175504946, "accuracy": 0.8076171875}, {"mcc": 0.7531499285755584, "accuracy": 0.814453125}, {"mcc": 0.7403715851344417, "accuracy": 0.80419921875}, {"mcc": 0.7345676663228636, "accuracy": 0.80078125}, {"mcc": 0.7531745279496506, "accuracy": 0.814453125}, {"mcc": 0.7737815424291253, "accuracy": 0.830078125}, {"mcc": 0.7458393839640916, "accuracy": 0.80859375}, {"mcc": 0.7270632892515884, "accuracy": 0.7939453125}], "total": {"test_mcc": 74.81787306240926, "test_mcc_se": 0.8569980257036505, "test_accuracy": 81.0595703125, "test_accuracy_se": 0.6490039366511444}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"test_speed": 377.72, "test_speed_short": 42.56}, {"test_speed": 734.61, "test_speed_short": 79.95}, {"test_speed": 1103.5300000000002, "test_speed_short": 154.28}, {"test_speed": 1472.44, "test_speed_short": 193.68}, {"test_speed": 1835.7300000000002, "test_speed_short": 230.91}, {"test_speed": 2168.15, "test_speed_short": 303.81}, {"test_speed": 2548.2900000000004, "test_speed_short": 341.76}, {"test_speed": 2900.37, "test_speed_short": 382.69}, {"test_speed": 3331.6800000000003, "test_speed_short": 420.41999999999996}, {"test_speed": 3659.22, "test_speed_short": 459.85}], "total": {"test_speed": 2013.1740000000002, "test_speed_se": 685.4141288706874, "test_speed_short": 260.991, "test_speed_short_se": 88.96881011619395}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "swerec", "task": "sentiment-classification", "dataset_languages": ["sv"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.8140692409477088, "macro_f1": 0.8220284032501123}, {"mcc": 0.8015846572758863, "macro_f1": 0.7954961985271666}, {"mcc": 0.8260541890158107, "macro_f1": 0.8255597401334839}, {"mcc": 0.7901830671596717, "macro_f1": 0.7875237801774387}, {"mcc": 0.8188165200273164, "macro_f1": 0.8321598020545163}, {"mcc": 0.8379517180864778, "macro_f1": 0.8405909690140296}, {"mcc": 0.8352006622208646, "macro_f1": 0.8248155797678015}, {"mcc": 0.7997424668528489, "macro_f1": 0.8216328550759754}, {"mcc": 0.819625366839233, "macro_f1": 0.8208743423358197}, {"mcc": 0.8099546191644751, "macro_f1": 0.818016486920059}], "total": {"test_mcc": 81.53182507590294, "test_mcc_se": 0.9572963838800059, "test_macro_f1": 81.88698157256404, "test_macro_f1_se": 0.9864677208090317}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "angry-tweets", "task": "sentiment-classification", "dataset_languages": ["da"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.4903137064855648, "macro_f1": 0.6445020832869828}, {"mcc": 0.5313992851952006, "macro_f1": 0.6813160945442851}, {"mcc": 0.5348090946240154, "macro_f1": 0.6831707277564418}, {"mcc": 0.5407745110703118, "macro_f1": 0.6798170262537319}, {"mcc": 0.5667630738015856, "macro_f1": 0.7107874265311973}, {"mcc": 0.4840475654755527, "macro_f1": 0.6095280699655717}, {"mcc": 0.5017220575795789, "macro_f1": 0.6455684579911846}, {"mcc": 0.4471509484223559, "macro_f1": 0.6006143095373185}, {"mcc": 0.5471144869257865, "macro_f1": 0.6927019919418677}, {"mcc": 0.5050586273217416, "macro_f1": 0.6527874552894303}], "total": {"test_mcc": 51.49153356901694, "test_mcc_se": 2.213869078141019, "test_macro_f1": 66.00793643098012, "test_macro_f1_se": 2.225867892629781}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hotter-and-colder-sentiment", "task": "sentiment-classification", "dataset_languages": ["is"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.4354696555344564, "macro_f1": 0.6002243903045164}, {"mcc": 0.45000778590436247, "macro_f1": 0.6313719883277847}, {"mcc": 0.36696908416390267, "macro_f1": 0.544485974605628}, {"mcc": 0.42348011866111657, "macro_f1": 0.60000539436918}, {"mcc": 0.3429836783514535, "macro_f1": 0.5513049244354656}, {"mcc": 0.43544074144305805, "macro_f1": 0.6143796784742205}, {"mcc": 0.4007652618445978, "macro_f1": 0.5666087119713205}, {"mcc": 0.34545002244160655, "macro_f1": 0.5505784405546699}, {"mcc": 0.3856400676312019, "macro_f1": 0.5349378331205987}, {"mcc": 0.43084853346673063, "macro_f1": 0.6071979350461668}], "total": {"test_mcc": 40.170549494424876, "test_mcc_se": 2.4456997142463894, "test_macro_f1": 58.01095271209551, "test_macro_f1_se": 2.1200742120155627}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "sb10k", "task": "sentiment-classification", "dataset_languages": ["de"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.3743669267630068, "macro_f1": 0.5064367156800073}, {"mcc": 0.5800032441595232, "macro_f1": 0.7075557395256662}, {"mcc": 0.528902383656654, "macro_f1": 0.6589853080283178}, {"mcc": 0.5376101507367178, "macro_f1": 0.686382521777019}, {"mcc": 0.5432630317509884, "macro_f1": 0.6576302652648051}, {"mcc": 0.6086629341280704, "macro_f1": 0.7204388273640472}, {"mcc": 0.5136145894021935, "macro_f1": 0.6346084269790953}, {"mcc": 0.5113694582770805, "macro_f1": 0.6503397916904671}, {"mcc": 0.5537544277639822, "macro_f1": 0.6599168123772667}, {"mcc": 0.5948118697047861, "macro_f1": 0.7141349934230424}], "total": {"test_mcc": 53.46359016343003, "test_mcc_se": 4.04383127429227, "test_macro_f1": 65.96429402109733, "test_macro_f1_se": 3.8020489757025238}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "dutch-social", "task": "sentiment-classification", "dataset_languages": ["nl"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.07319533612833484, "macro_f1": 0.2012525982265887}, {"mcc": 0.12710977011699953, "macro_f1": 0.22161237315986515}, {"mcc": 0.04723003626060067, "macro_f1": 0.19297872717289266}, {"mcc": 0.09806833782213947, "macro_f1": 0.20496994327321985}, {"mcc": 0.055342394547684415, "macro_f1": 0.1722443719544118}, {"mcc": 0.05594023484286141, "macro_f1": 0.1846360425623145}, {"mcc": 0.10070846676166362, "macro_f1": 0.227631473217348}, {"mcc": 0.10769059234404457, "macro_f1": 0.217874930069453}, {"mcc": 0.05969640381943265, "macro_f1": 0.20879000908477696}, {"mcc": 0.09739390961965, "macro_f1": 0.2147927525440556}], "total": {"test_mcc": 8.223754822634112, "test_mcc_se": 1.6927722233150189, "test_macro_f1": 20.467832212649263, "test_macro_f1_se": 1.0753879777024329}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "sst5", "task": "sentiment-classification", "dataset_languages": ["en"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.6871941770934422, "macro_f1": 0.679752168361898}, {"mcc": 0.6916487386049398, "macro_f1": 0.6567897436808158}, {"mcc": 0.7028661670227268, "macro_f1": 0.6959691316879691}, {"mcc": 0.6838256716184186, "macro_f1": 0.6563653700577049}, {"mcc": 0.572975519656039, "macro_f1": 0.6612553522122296}, {"mcc": 0.6771407424773482, "macro_f1": 0.6902647301612898}, {"mcc": 0.6942284716108131, "macro_f1": 0.7098395545322469}, {"mcc": 0.6875048799072677, "macro_f1": 0.6721486890402643}, {"mcc": 0.6888770174193946, "macro_f1": 0.6351764675568713}, {"mcc": 0.7030912050081591, "macro_f1": 0.7195116338425361}], "total": {"test_mcc": 67.8935259041855, "test_mcc_se": 2.3598633766259365, "test_macro_f1": 67.77072841133827, "test_macro_f1_se": 1.6371062525504378}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fosent", "task": "sentiment-classification", "dataset_languages": ["fo"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.503437761954918, "macro_f1": 0.65110036104012}, {"mcc": 0.5785800601723293, "macro_f1": 0.7239045223734989}, {"mcc": 0.4273917877100382, "macro_f1": 0.5052790346907994}, {"mcc": 0.5173254607149779, "macro_f1": 0.675789313904068}, {"mcc": 0.547467187269832, "macro_f1": 0.69521211849174}, {"mcc": 0.5425123970417245, "macro_f1": 0.6866856900827436}, {"mcc": 0.43653668169598886, "macro_f1": 0.6082858078999274}, {"mcc": 0.555406015908697, "macro_f1": 0.6992667513279972}, {"mcc": 0.4928913298819275, "macro_f1": 0.5839267440449034}, {"mcc": 0.5502805289183178, "macro_f1": 0.6822588090306146}], "total": {"test_mcc": 51.518292112687504, "test_mcc_se": 3.146197139396272, "test_macro_f1": 65.11709152886414, "test_macro_f1_se": 4.133188948429795}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "allocine", "task": "sentiment-classification", "dataset_languages": ["fr"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.948757097535919, "macro_f1": 0.9740933668909009}, {"mcc": 0.9513446097498229, "macro_f1": 0.9755792068393484}, {"mcc": 0.9552646650051927, "macro_f1": 0.9773905213370235}, {"mcc": 0.9541377432803404, "macro_f1": 0.9770406599474533}, {"mcc": 0.9569585618279564, "macro_f1": 0.978464279842}, {"mcc": 0.9608855077828218, "macro_f1": 0.9804418163135116}, {"mcc": 0.9397724993682326, "macro_f1": 0.9692366309742855}, {"mcc": 0.9354952877612495, "macro_f1": 0.9672802806318748}, {"mcc": 0.9502035895385118, "macro_f1": 0.9749984620588114}, {"mcc": 0.9666394844692903, "macro_f1": 0.9833159541188738}], "total": {"test_mcc": 95.19459046319338, "test_mcc_se": 0.5729786044607883, "test_macro_f1": 97.57841178954084, "test_macro_f1_se": 0.29830095725857225}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "sentipolc16", "task": "sentiment-classification", "dataset_languages": ["it"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.5684932719949326, "macro_f1": 0.7060071452626362}, {"mcc": 0.5671157826959836, "macro_f1": 0.700686247940166}, {"mcc": 0.5983225374826808, "macro_f1": 0.7238449591039776}, {"mcc": 0.6120395265325852, "macro_f1": 0.7363886509602183}, {"mcc": 0.5098865619523306, "macro_f1": 0.6568435651723358}, {"mcc": 0.6025707794668279, "macro_f1": 0.7238067307741779}, {"mcc": 0.3859284900021991, "macro_f1": 0.5483969479320954}, {"mcc": 0.5786219232757499, "macro_f1": 0.7171729186901333}, {"mcc": 0.5492033787752681, "macro_f1": 0.6760311502494009}, {"mcc": 0.6365581245639605, "macro_f1": 0.7516386010359207}], "total": {"test_mcc": 56.087403767425194, "test_mcc_se": 4.394407024684581, "test_macro_f1": 69.40816917121062, "test_macro_f1_se": 3.609788917692974}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "suc3", "task": "named-entity-recognition", "dataset_languages": ["sv"], "model": "google/gemma-2-27b", "results": {"raw": [{"micro_f1_no_misc": 0.284326710816777, "micro_f1": 0.22949369887242982}, {"micro_f1_no_misc": 0.3393994540491356, "micro_f1": 0.3630017452006981}, {"micro_f1_no_misc": 0.3652525252525253, "micro_f1": 0.35492227979274615}, {"micro_f1_no_misc": 0.333992876929165, "micro_f1": 0.3388746803069054}, {"micro_f1_no_misc": 0.3745791245791245, "micro_f1": 0.37770122142186024}, {"micro_f1_no_misc": 0.3614678899082569, "micro_f1": 0.3398570984777881}, {"micro_f1_no_misc": 0.3211382113821138, "micro_f1": 0.3223570190641248}, {"micro_f1_no_misc": 0.3380634390651085, "micro_f1": 0.2715248315447966}, {"micro_f1_no_misc": 0.33458177278401996, "micro_f1": 0.3384126984126984}, {"micro_f1_no_misc": 0.3426603529918209, "micro_f1": 0.3410997204100652}], "total": {"test_micro_f1_no_misc": 33.95462357758047, "test_micro_f1_no_misc_se": 1.5767047088478807, "test_micro_f1": 32.772449935041124, "test_micro_f1_se": 2.762838397830368}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "dansk", "task": "named-entity-recognition", "dataset_languages": ["da"], "model": "google/gemma-2-27b", "results": {"raw": [{"micro_f1_no_misc": 0.46674311926605505, "micro_f1": 0.39688359303391385}, {"micro_f1_no_misc": 0.45989304812834225, "micro_f1": 0.34502923976608185}, {"micro_f1_no_misc": 0.41904761904761906, "micro_f1": 0.3640606767794633}, {"micro_f1_no_misc": 0.46292134831460674, "micro_f1": 0.40566873339238263}, {"micro_f1_no_misc": 0.3850063532401525, "micro_f1": 0.2688142182727021}, {"micro_f1_no_misc": 0.39534883720930236, "micro_f1": 0.34152542372881356}, {"micro_f1_no_misc": 0.4147465437788019, "micro_f1": 0.3340982101881597}, {"micro_f1_no_misc": 0.4302991725015914, "micro_f1": 0.37191283292978206}, {"micro_f1_no_misc": 0.4392991239048811, "micro_f1": 0.3640275387263339}, {"micro_f1_no_misc": 0.3963011889035667, "micro_f1": 0.33662477558348297}], "total": {"test_micro_f1_no_misc": 42.69606354294919, "test_micro_f1_no_misc_se": 1.8519454395012125, "test_micro_f1": 35.28645242401115, "test_micro_f1_se": 2.374495262616556}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mim-gold-ner", "task": "named-entity-recognition", "dataset_languages": ["is"], "model": "google/gemma-2-27b", "results": {"raw": [{"micro_f1_no_misc": 0.4120832145910515, "micro_f1": 0.4242965609647164}, {"micro_f1_no_misc": 0.356140350877193, "micro_f1": 0.3033284144941794}, {"micro_f1_no_misc": 0.36247723132969034, "micro_f1": 0.3289928248737709}, {"micro_f1_no_misc": 0.38404255319148933, "micro_f1": 0.40415116506755433}, {"micro_f1_no_misc": 0.3705980994969256, "micro_f1": 0.4098518375986146}, {"micro_f1_no_misc": 0.3647226173541963, "micro_f1": 0.4121956048307266}, {"micro_f1_no_misc": 0.36857307914500287, "micro_f1": 0.43221003134796243}, {"micro_f1_no_misc": 0.38327936414483366, "micro_f1": 0.38967591694158743}, {"micro_f1_no_misc": 0.36176715754376215, "micro_f1": 0.37379361544172235}, {"micro_f1_no_misc": 0.33449477351916374, "micro_f1": 0.34916327453640894}], "total": {"test_micro_f1_no_misc": 36.981784411933084, "test_micro_f1_no_misc_se": 1.265432169170786, "test_micro_f1": 38.27659246097244, "test_micro_f1_se": 2.6687565016243076}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fone", "task": "named-entity-recognition", "dataset_languages": ["fo"], "model": "google/gemma-2-27b", "results": {"raw": [{"micro_f1_no_misc": 0.5088058419243986, "micro_f1": 0.5015397249024841}, {"micro_f1_no_misc": 0.4255266724159172, "micro_f1": 0.42042613974526843}, {"micro_f1_no_misc": 0.41787579802669766, "micro_f1": 0.4089260195668902}, {"micro_f1_no_misc": 0.3310004116920544, "micro_f1": 0.34033508377094274}, {"micro_f1_no_misc": 0.45761277058894667, "micro_f1": 0.4587155963302752}, {"micro_f1_no_misc": 0.45031169783645036, "micro_f1": 0.45120551090700345}, {"micro_f1_no_misc": 0.4593844296922149, "micro_f1": 0.47805078647064875}, {"micro_f1_no_misc": 0.5091918140825529, "micro_f1": 0.5031903190319031}, {"micro_f1_no_misc": 0.46693339609319845, "micro_f1": 0.4716981132075471}, {"micro_f1_no_misc": 0.47956143487069475, "micro_f1": 0.4753363228699552}], "total": {"test_micro_f1_no_misc": 45.06204267223126, "test_micro_f1_no_misc_se": 3.2044315691478094, "test_micro_f1": 45.09423616802918, "test_micro_f1_se": 3.067311331003138}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "germeval", "task": "named-entity-recognition", "dataset_languages": ["de"], "model": "google/gemma-2-27b", "results": {"raw": [{"micro_f1_no_misc": 0.4169705004971826, "micro_f1": 0.400893355667225}, {"micro_f1_no_misc": 0.4363399544122436, "micro_f1": 0.4043062200956937}, {"micro_f1_no_misc": 0.4085808580858086, "micro_f1": 0.4029553850525717}, {"micro_f1_no_misc": 0.34798947763998495, "micro_f1": 0.32075471698113206}, {"micro_f1_no_misc": 0.3369122547204739, "micro_f1": 0.34369501466275654}, {"micro_f1_no_misc": 0.39068219633943424, "micro_f1": 0.3772714162099798}, {"micro_f1_no_misc": 0.39093898428936796, "micro_f1": 0.3897373243738546}, {"micro_f1_no_misc": 0.3976305072195483, "micro_f1": 0.3971493359248462}, {"micro_f1_no_misc": 0.4061251664447404, "micro_f1": 0.38816568047337274}, {"micro_f1_no_misc": 0.4441281138790036, "micro_f1": 0.42988437592647494}], "total": {"test_micro_f1_no_misc": 39.762980135277886, "test_micro_f1_no_misc_se": 2.1140706734927477, "test_micro_f1": 38.54812825367907, "test_micro_f1_se": 1.963630799857048}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "conll-nl", "task": "named-entity-recognition", "dataset_languages": ["nl"], "model": "google/gemma-2-27b", "results": {"raw": [{"micro_f1_no_misc": 0.4141351518908865, "micro_f1": 0.4436363636363636}, {"micro_f1_no_misc": 0.39080459770114945, "micro_f1": 0.45013723696248853}, {"micro_f1_no_misc": 0.43029917250159133, "micro_f1": 0.43072939145446704}, {"micro_f1_no_misc": 0.4167832167832168, "micro_f1": 0.43553008595988546}, {"micro_f1_no_misc": 0.40507859733978235, "micro_f1": 0.42893127024109395}, {"micro_f1_no_misc": 0.4523506988564168, "micro_f1": 0.43783979702790865}, {"micro_f1_no_misc": 0.4211238997968855, "micro_f1": 0.4874151018777467}, {"micro_f1_no_misc": 0.41926729986431477, "micro_f1": 0.4235611510791367}, {"micro_f1_no_misc": 0.4263005780346821, "micro_f1": 0.43355036384527}, {"micro_f1_no_misc": 0.426917510853835, "micro_f1": 0.42673031026252983}], "total": {"test_micro_f1_no_misc": 42.03060723622761, "test_micro_f1_no_misc_se": 1.0038434283519, "test_micro_f1": 43.98061072346891, "test_micro_f1_se": 1.1474937414242885}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "conll-en", "task": "named-entity-recognition", "dataset_languages": ["en"], "model": "google/gemma-2-27b", "results": {"raw": [{"micro_f1_no_misc": 0.3549437537004145, "micro_f1": 0.3450369588173179}, {"micro_f1_no_misc": 0.32575201760821715, "micro_f1": 0.3515718742500599}, {"micro_f1_no_misc": 0.4395172607353354, "micro_f1": 0.44678680720843256}, {"micro_f1_no_misc": 0.4065902578796562, "micro_f1": 0.3946315045914764}, {"micro_f1_no_misc": 0.3520566121185317, "micro_f1": 0.3686613201727329}, {"micro_f1_no_misc": 0.30849714648065946, "micro_f1": 0.3307169021454736}, {"micro_f1_no_misc": 0.3336437408540641, "micro_f1": 0.39869945133103024}, {"micro_f1_no_misc": 0.3782339707536558, "micro_f1": 0.38466133078485243}, {"micro_f1_no_misc": 0.35687842440396866, "micro_f1": 0.3390189078408039}, {"micro_f1_no_misc": 0.3947226709746904, "micro_f1": 0.431739358906989}], "total": {"test_micro_f1_no_misc": 36.50835855509194, "test_micro_f1_no_misc_se": 2.4785272650110954, "test_micro_f1": 37.91524416049168, "test_micro_f1_se": 2.4430775728548006}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "eltec", "task": "named-entity-recognition", "dataset_languages": ["fr"], "model": "google/gemma-2-27b", "results": {"raw": [{"micro_f1_no_misc": 0.20301783264746226, "micro_f1": 0.21134328358208956}, {"micro_f1_no_misc": 0.23560209424083772, "micro_f1": 0.26271432723045623}, {"micro_f1_no_misc": 0.2503467406380028, "micro_f1": 0.2702384456873712}, {"micro_f1_no_misc": 0.2739469578783151, "micro_f1": 0.294468546637744}, {"micro_f1_no_misc": 0.23213610586011343, "micro_f1": 0.2841328413284132}, {"micro_f1_no_misc": 0.2733908427339084, "micro_f1": 0.30242045145499047}, {"micro_f1_no_misc": 0.25216, "micro_f1": 0.2493975903614458}, {"micro_f1_no_misc": 0.26058152373941845, "micro_f1": 0.25883175935641833}, {"micro_f1_no_misc": 0.228631727048892, "micro_f1": 0.24091778202676864}, {"micro_f1_no_misc": 0.25188536953242835, "micro_f1": 0.2677024200776815}], "total": {"test_micro_f1_no_misc": 24.616991943193785, "test_micro_f1_no_misc_se": 1.3523444147075192, "test_micro_f1": 26.421674477433786, "test_micro_f1_se": 1.653987067354686}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "multinerd-it", "task": "named-entity-recognition", "dataset_languages": ["it"], "model": "google/gemma-2-27b", "results": {"raw": [{"micro_f1_no_misc": 0.46515590390185724, "micro_f1": 0.37649317406143346}, {"micro_f1_no_misc": 0.4878797177048174, "micro_f1": 0.4182815356489945}, {"micro_f1_no_misc": 0.4768639508070715, "micro_f1": 0.406518786781349}, {"micro_f1_no_misc": 0.37083482622715874, "micro_f1": 0.3372199659373772}, {"micro_f1_no_misc": 0.4745317121261913, "micro_f1": 0.4645850796311819}, {"micro_f1_no_misc": 0.4761754992923416, "micro_f1": 0.4283057466989027}, {"micro_f1_no_misc": 0.4786576390933176, "micro_f1": 0.4445034519383962}, {"micro_f1_no_misc": 0.4275814275814276, "micro_f1": 0.43842203037919353}, {"micro_f1_no_misc": 0.49067303387554095, "micro_f1": 0.40493767396829244}, {"micro_f1_no_misc": 0.4996892479801119, "micro_f1": 0.4185758513931888}], "total": {"test_micro_f1_no_misc": 46.480429585898364, "test_micro_f1_no_misc_se": 2.3733029731781463, "test_micro_f1": 41.378432964383094, "test_micro_f1_se": 2.2413462068551224}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-sv", "task": "linguistic-acceptability", "dataset_languages": ["sv"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.6208056909649732, "macro_f1": 0.8065795996168563}, {"mcc": 0.5288644693605131, "macro_f1": 0.7386010240671147}, {"mcc": 0.6152025007269915, "macro_f1": 0.8036077066632379}, {"mcc": 0.532982589553519, "macro_f1": 0.7534500897332755}, {"mcc": 0.5739237525579494, "macro_f1": 0.7866149378794391}, {"mcc": 0.5858971524797991, "macro_f1": 0.7883228614114074}, {"mcc": 0.5857975856644339, "macro_f1": 0.7797619047619048}, {"mcc": 0.6001880052766236, "macro_f1": 0.7930054655278953}, {"mcc": 0.6184594450757973, "macro_f1": 0.8092018449172569}, {"mcc": 0.5913839263414826, "macro_f1": 0.7859233449477352}], "total": {"test_mcc": 58.53505118002082, "test_mcc_se": 2.0205971910987435, "test_macro_f1": 78.45068779526123, "test_macro_f1_se": 1.4087648969754423}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-da", "task": "linguistic-acceptability", "dataset_languages": ["da"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.5323392778192503, "macro_f1": 0.7576269107335714}, {"mcc": 0.5483551301772851, "macro_f1": 0.7629101651797752}, {"mcc": 0.5156537458391259, "macro_f1": 0.7578088044556344}, {"mcc": 0.3824716090952433, "macro_f1": 0.6044493167368391}, {"mcc": 0.5644246597640755, "macro_f1": 0.7816882616089359}, {"mcc": 0.5104695535768845, "macro_f1": 0.7392181052224599}, {"mcc": 0.5714156838541039, "macro_f1": 0.7837833179280671}, {"mcc": 0.5134505775842335, "macro_f1": 0.7566667652815726}, {"mcc": 0.5931047233522082, "macro_f1": 0.7938496417029076}, {"mcc": 0.5541013635294899, "macro_f1": 0.7651971976660227}], "total": {"test_mcc": 52.85786324591901, "test_mcc_se": 3.6029032626908104, "test_macro_f1": 75.03198486515787, "test_macro_f1_se": 3.3268524540370743}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-is", "task": "linguistic-acceptability", "dataset_languages": ["is"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.1654794874907243, "macro_f1": 0.5826454346103949}, {"mcc": 0.14326309904268092, "macro_f1": 0.5655196143244299}, {"mcc": 0.11280858007571833, "macro_f1": 0.45519345106973974}, {"mcc": 0.18250225934940228, "macro_f1": 0.5145521069484993}, {"mcc": 0.15445997217929514, "macro_f1": 0.5607986407844496}, {"mcc": 0.18987907616963956, "macro_f1": 0.5888849464612932}, {"mcc": 0.1449941892195807, "macro_f1": 0.5674396992354158}, {"mcc": 0.09013164275306255, "macro_f1": 0.46360630371736744}, {"mcc": 0.17792409568486445, "macro_f1": 0.5668016194331984}, {"mcc": 0.17158458614000313, "macro_f1": 0.5851628648331295}], "total": {"test_mcc": 15.330269881049713, "test_mcc_se": 1.968765624138937, "test_macro_f1": 54.50604681417918, "test_macro_f1_se": 3.0798838057318663}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-fo", "task": "linguistic-acceptability", "dataset_languages": ["fo"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.12520073514030586, "macro_f1": 0.5208333333333334}, {"mcc": 0.16637523182285965, "macro_f1": 0.5799546095810587}, {"mcc": 0.07640876927106796, "macro_f1": 0.414577931431864}, {"mcc": 0.1045360197366781, "macro_f1": 0.5462794219069672}, {"mcc": 0.10911856164432965, "macro_f1": 0.4586968754795914}, {"mcc": 0.17394690294297913, "macro_f1": 0.5868474743110261}, {"mcc": 0.12010228393013253, "macro_f1": 0.5598574392091955}, {"mcc": 0.12323658986891405, "macro_f1": 0.5116104496120173}, {"mcc": 0.12448607632951794, "macro_f1": 0.531350114416476}, {"mcc": 0.0727766138726653, "macro_f1": 0.5329465648854962}], "total": {"test_mcc": 11.9618778455945, "test_mcc_se": 2.0247979136396768, "test_macro_f1": 52.42954214167026, "test_macro_f1_se": 3.2925187119954393}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-de", "task": "linguistic-acceptability", "dataset_languages": ["de"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.4285007245764886, "macro_f1": 0.7132105049346429}, {"mcc": 0.20257780361152164, "macro_f1": 0.46691413632711265}, {"mcc": 0.48014727485456876, "macro_f1": 0.738298434431031}, {"mcc": 0.38149856751628847, "macro_f1": 0.621882136987471}, {"mcc": 0.43339454673519856, "macro_f1": 0.6889475235936853}, {"mcc": 0.4871329221772211, "macro_f1": 0.7426727750220241}, {"mcc": 0.26305743669012915, "macro_f1": 0.6262467798242446}, {"mcc": 0.44975036934392515, "macro_f1": 0.7222109189555781}, {"mcc": 0.3658869430049541, "macro_f1": 0.6332452241852842}, {"mcc": 0.4550258476608731, "macro_f1": 0.7128320578947522}], "total": {"test_mcc": 39.46972436171169, "test_mcc_se": 5.858173097348467, "test_macro_f1": 66.66460492155825, "test_macro_f1_se": 5.203942301305694}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-nl", "task": "linguistic-acceptability", "dataset_languages": ["nl"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.4594911328980277, "macro_f1": 0.6963705643974746}, {"mcc": 0.4843705607895436, "macro_f1": 0.7421675830595282}, {"mcc": 0.5285575192428205, "macro_f1": 0.7538451994618466}, {"mcc": 0.5460738194136604, "macro_f1": 0.7730184208514868}, {"mcc": 0.4886176537349629, "macro_f1": 0.7256570786951608}, {"mcc": 0.5523532919985604, "macro_f1": 0.7714809454757974}, {"mcc": 0.5083387967487205, "macro_f1": 0.7470256033144445}, {"mcc": 0.46742622437237646, "macro_f1": 0.7095107496218414}, {"mcc": 0.5535075451811654, "macro_f1": 0.7747407826112738}, {"mcc": 0.4289869854407404, "macro_f1": 0.6900774437294643}], "total": {"test_mcc": 50.177235298205794, "test_mcc_se": 2.669814597839799, "test_macro_f1": 73.8389437121832, "test_macro_f1_se": 1.9642528342978482}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-en", "task": "linguistic-acceptability", "dataset_languages": ["en"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.5099711191997448, "macro_f1": 0.7548818774485682}, {"mcc": 0.4947306699374899, "macro_f1": 0.7262635275757625}, {"mcc": 0.47369759631398595, "macro_f1": 0.7342671679934623}, {"mcc": 0.4517119662836721, "macro_f1": 0.7211880461667656}, {"mcc": 0.4684737255554942, "macro_f1": 0.7334489177690771}, {"mcc": 0.4048486122417564, "macro_f1": 0.6372008776739394}, {"mcc": 0.44358237205830464, "macro_f1": 0.7195857960593994}, {"mcc": 0.5016026141744845, "macro_f1": 0.7486152665899501}, {"mcc": 0.4685891003959913, "macro_f1": 0.7292419023762308}, {"mcc": 0.4719052960614402, "macro_f1": 0.7357122462507862}], "total": {"test_mcc": 46.891130722223636, "test_mcc_se": 1.9041991736304138, "test_macro_f1": 72.40405625903941, "test_macro_f1_se": 2.0121754274073576}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-fr", "task": "linguistic-acceptability", "dataset_languages": ["fr"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.4000795394091756, "macro_f1": 0.6885980636479784}, {"mcc": 0.3865731504408424, "macro_f1": 0.6770252200120379}, {"mcc": 0.4481065686866102, "macro_f1": 0.7208448117539027}, {"mcc": 0.3697981331303181, "macro_f1": 0.681000197298055}, {"mcc": 0.46417721409056784, "macro_f1": 0.7151705964637326}, {"mcc": 0.49276252865517145, "macro_f1": 0.7423034980286889}, {"mcc": 0.4581300734153977, "macro_f1": 0.708829680420125}, {"mcc": 0.35519235664891413, "macro_f1": 0.6176706827309237}, {"mcc": 0.3852125425957027, "macro_f1": 0.6871951869680031}, {"mcc": 0.4311059334578665, "macro_f1": 0.6860761725477611}], "total": {"test_mcc": 41.91138040530566, "test_mcc_se": 2.851557600538197, "test_macro_f1": 69.24714109871208, "test_macro_f1_se": 2.078400663007376}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-it", "task": "linguistic-acceptability", "dataset_languages": ["it"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.3343002732741468, "macro_f1": 0.5799754537820554}, {"mcc": 0.2987367757521245, "macro_f1": 0.6493659102354754}, {"mcc": 0.2582830892146707, "macro_f1": 0.5361177038299045}, {"mcc": 0.16742825929238378, "macro_f1": 0.4006183509479777}, {"mcc": 0.1024055262429822, "macro_f1": 0.4719752313593365}, {"mcc": 0.28151063163509854, "macro_f1": 0.5328546084311904}, {"mcc": 0.2143272152369982, "macro_f1": 0.5999960503617411}, {"mcc": 0.2353944383019212, "macro_f1": 0.4859479373304725}, {"mcc": 0.2351127501248828, "macro_f1": 0.6132120365368643}, {"mcc": 0.23636854258362672, "macro_f1": 0.5068181818181818}], "total": {"test_mcc": 23.638675016588355, "test_mcc_se": 4.089766006164443, "test_macro_f1": 53.768814646332, "test_macro_f1_se": 4.647786954435237}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "google/gemma-2-27b", "results": {"raw": [{"f1": 67.92858725095563, "em": 62.34848484848485}, {"f1": 69.07886344620711, "em": 64.2153146322972}, {"f1": 69.65681347827274, "em": 64.72416472416472}, {"f1": 69.1335268575397, "em": 64.38356164383562}, {"f1": 69.21632338363256, "em": 64.91904394757132}, {"f1": 69.46913999536359, "em": 63.81679389312977}, {"f1": 66.98667282948985, "em": 60.29641185647426}, {"f1": 69.64013332501554, "em": 64.30817610062893}, {"f1": 68.30200154434307, "em": 63.54489164086687}, {"f1": 69.84574124679182, "em": 63.58024691358025}], "total": {"test_f1": 68.92578033576116, "test_f1_se": 0.5650830174319661, "test_em": 63.613709020103386, "test_em_se": 0.8524998596604397}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scandiqa-sv", "task": "reading-comprehension", "dataset_languages": ["sv"], "model": "google/gemma-2-27b", "results": {"raw": [{"f1": 65.2146464646463, "em": 53.25757575757576}, {"f1": 68.01407866341184, "em": 60.19711902956785}, {"f1": 68.73949224696514, "em": 63.79176379176379}, {"f1": 68.67953568799656, "em": 63.54642313546423}, {"f1": 68.58967411713562, "em": 62.37471087124133}, {"f1": 67.07033805888766, "em": 61.29770992366412}, {"f1": 69.39241132161195, "em": 63.182527301092044}, {"f1": 69.23972036000332, "em": 64.54402515723271}, {"f1": 69.50344774511588, "em": 63.69969040247678}, {"f1": 66.72227121301188, "em": 60.80246913580247}], "total": {"test_f1": 68.11656158787862, "test_f1_se": 0.8571323184231134, "test_em": 61.6694014505881, "test_em_se": 2.034966390491385}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "nqii", "task": "reading-comprehension", "dataset_languages": ["is"], "model": "google/gemma-2-27b", "results": {"raw": [{"f1": 64.32781963300813, "em": 37.23076923076923}, {"f1": 62.441815187730235, "em": 30.373831775700936}, {"f1": 56.16808109338472, "em": 35.51401869158879}, {"f1": 58.6846602171698, "em": 35.57548579970105}, {"f1": 63.88562860201331, "em": 34.32601880877743}, {"f1": 64.64919804761605, "em": 40.36979969183359}, {"f1": 64.61784252860348, "em": 39.23076923076923}, {"f1": 57.49057804979832, "em": 34.66257668711656}, {"f1": 63.76426184499306, "em": 36.144578313253014}, {"f1": 62.90101111254753, "em": 40.21084337349398}], "total": {"test_f1": 61.893089631686465, "test_f1_se": 1.9836890812783265, "test_em": 36.36386916030038, "test_em_se": 1.8950832041019638}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "foqa", "task": "reading-comprehension", "dataset_languages": ["fo"], "model": "google/gemma-2-27b", "results": {"raw": [{"f1": 73.93294160634206, "em": 55.14705882352941}, {"f1": 71.95018988443528, "em": 53.562653562653566}, {"f1": 76.69379387021179, "em": 56.79012345679013}, {"f1": 71.712241074241, "em": 53.398058252427184}, {"f1": 78.29156611411524, "em": 59.9009900990099}, {"f1": 72.62856192604231, "em": 55.30864197530864}, {"f1": 77.32869476661132, "em": 59.198113207547166}, {"f1": 76.83365917036907, "em": 55.216284987277355}, {"f1": 78.94490817713707, "em": 59.35162094763092}, {"f1": 76.34003169960538, "em": 56.51105651105651}], "total": {"test_f1": 75.46565882891106, "test_f1_se": 1.6599181140114527, "test_em": 56.438460182323084, "test_em_se": 1.4634151521838648}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "germanquad", "task": "reading-comprehension", "dataset_languages": ["de"], "model": "google/gemma-2-27b", "results": {"raw": [{"f1": 72.3486424903738, "em": 44.84848484848485}, {"f1": 67.56788296219511, "em": 37.5284306292646}, {"f1": 72.2078163304827, "em": 38.92773892773893}, {"f1": 75.11288587654248, "em": 46.34703196347032}, {"f1": 75.19023893781724, "em": 45.41249036237471}, {"f1": 73.70630825662677, "em": 43.969465648854964}, {"f1": 64.2574717918658, "em": 36.349453978159126}, {"f1": 69.46609922028195, "em": 40.801886792452834}, {"f1": 70.2984023782226, "em": 41.95046439628483}, {"f1": 59.13316286367582, "em": 33.25617283950617}], "total": {"test_f1": 69.92889111080842, "test_f1_se": 3.1638538029513623, "test_em": 40.939162038659134, "test_em_se": 2.6980016816006205}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "squad", "task": "reading-comprehension", "dataset_languages": ["en"], "model": "google/gemma-2-27b", "results": {"raw": [{"f1": 90.01298840817414, "em": 80.9090909090909}, {"f1": 86.75749858669589, "em": 74.45034116755117}, {"f1": 90.54801511709915, "em": 80.65268065268066}, {"f1": 88.80805195793398, "em": 80.36529680365297}, {"f1": 89.89629412199798, "em": 81.11025443330763}, {"f1": 83.75227760196843, "em": 72.21374045801527}, {"f1": 87.95836843420707, "em": 77.76911076443058}, {"f1": 88.90756710800225, "em": 76.33647798742139}, {"f1": 88.31926264162885, "em": 78.17337461300309}, {"f1": 85.56487692330764, "em": 73.37962962962963}], "total": {"test_f1": 88.05252009010154, "test_f1_se": 1.326281450941358, "test_em": 77.53599974187833, "test_em_se": 2.0545671112521453}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "squad-nl", "task": "reading-comprehension", "dataset_languages": ["nl"], "model": "google/gemma-2-27b", "results": {"raw": [{"f1": 85.24301833527082, "em": 71.51515151515152}, {"f1": 82.9460251056309, "em": 70.73540561031083}, {"f1": 79.04504486094095, "em": 67.28826728826729}, {"f1": 80.40629010552331, "em": 69.48249619482496}, {"f1": 83.19658900799281, "em": 70.77872012336161}, {"f1": 84.82472684661039, "em": 69.31297709923665}, {"f1": 80.73881429169205, "em": 67.94071762870514}, {"f1": 82.85739875277581, "em": 71.4622641509434}, {"f1": 80.97887994991615, "em": 70.58823529411765}, {"f1": 84.53164223882486, "em": 71.21913580246914}], "total": {"test_f1": 82.4768429495178, "test_f1_se": 1.3000929203638283, "test_em": 70.03233707073882, "test_em_se": 0.917856182050373}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "squad-it", "task": "reading-comprehension", "dataset_languages": ["it"], "model": "google/gemma-2-27b", "results": {"raw": [{"f1": 77.6962441807079, "em": 64.0909090909091}, {"f1": 79.05371828934538, "em": 62.09249431387415}, {"f1": 69.15499286581135, "em": 54.93395493395494}, {"f1": 78.5460126724987, "em": 63.926940639269404}, {"f1": 74.81749336849522, "em": 56.28373168851195}, {"f1": 78.1499660404893, "em": 64.73282442748092}, {"f1": 79.08405209760765, "em": 65.36661466458658}, {"f1": 77.91916316472896, "em": 62.106918238993714}, {"f1": 56.581177690489284, "em": 45.89783281733746}, {"f1": 76.9524050672872, "em": 59.1820987654321}], "total": {"test_f1": 74.7955225437461, "test_f1_se": 4.374134045276119, "test_em": 59.86143195803503, "test_em_se": 3.7587837242543873}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fquad", "task": "reading-comprehension", "dataset_languages": ["fr"], "model": "google/gemma-2-27b", "results": {"raw": [{"f1": 78.52674803250352, "em": 50.53030303030303}, {"f1": 77.00223336110628, "em": 50.11372251705838}, {"f1": 77.64946130759022, "em": 48.64024864024864}, {"f1": 79.32488308810187, "em": 51.90258751902587}, {"f1": 67.92491575632302, "em": 41.94294525828836}, {"f1": 78.30195032706052, "em": 48.3206106870229}, {"f1": 75.22653396047475, "em": 47.03588143525741}, {"f1": 75.5452298602286, "em": 49.056603773584904}, {"f1": 69.642001079755, "em": 45.27863777089783}, {"f1": 79.74846005126321, "em": 48.99691358024691}], "total": {"test_f1": 75.8892416824407, "test_f1_se": 2.503451537123123, "test_em": 48.181845421193415, "test_em_se": 1.7702316528265436}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "nordjylland-news", "task": "summarization", "dataset_languages": ["da"], "model": "google/gemma-2-27b", "results": {"raw": [{"bertscore": 0.6919260598951951, "rouge_l": 0.2640803780861195}, {"bertscore": 0.68800543731777, "rouge_l": 0.25507669404152533}, {"bertscore": 0.6804467459878651, "rouge_l": 0.2537829789526023}, {"bertscore": 0.6848307036270853, "rouge_l": 0.2513241636673351}, {"bertscore": 0.6908946980402106, "rouge_l": 0.2587491223767945}, {"bertscore": 0.6874005147838034, "rouge_l": 0.2551327771856246}, {"bertscore": 0.682397690921789, "rouge_l": 0.2460398469738146}, {"bertscore": 0.6861525215499569, "rouge_l": 0.2530731140626728}, {"bertscore": 0.6808998978813179, "rouge_l": 0.24724750297377396}, {"bertscore": 0.6884630864660721, "rouge_l": 0.2599608833541804}], "total": {"test_bertscore": 68.61417356471065, "test_bertscore_se": 0.2463181036807469, "test_rouge_l": 25.444674616744432, "test_rouge_l_se": 0.34396217212851304}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mlsum", "task": "summarization", "dataset_languages": ["de"], "model": "google/gemma-2-27b", "results": {"raw": [{"bertscore": 0.6893411736382404, "rouge_l": 0.2725565460681548}, {"bertscore": 0.6981072222260991, "rouge_l": 0.2977757081844473}, {"bertscore": 0.6850200409098761, "rouge_l": 0.2677175811894345}, {"bertscore": 0.6746423660079017, "rouge_l": 0.2477424267878183}, {"bertscore": 0.7245866572484374, "rouge_l": 0.35494622152842015}, {"bertscore": 0.6891915950691327, "rouge_l": 0.26800485333592405}, {"bertscore": 0.7258415188553045, "rouge_l": 0.3618371625564536}, {"bertscore": 0.6801749413571088, "rouge_l": 0.25427957094492304}, {"bertscore": 0.7028102274925914, "rouge_l": 0.2914574062457489}, {"bertscore": 0.6748170694627333, "rouge_l": 0.27082464934635586}], "total": {"test_bertscore": 69.44532812267425, "test_bertscore_se": 1.150527386498444, "test_rouge_l": 28.871421261876804, "test_rouge_l_se": 2.4566163158701357}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "rrn", "task": "summarization", "dataset_languages": ["is"], "model": "google/gemma-2-27b", "results": {"raw": [{"bertscore": 0.6872963474597782, "rouge_l": 0.23838519463755845}, {"bertscore": 0.690098278777441, "rouge_l": 0.2405107933350396}, {"bertscore": 0.6876796631258912, "rouge_l": 0.2287409925467653}, {"bertscore": 0.6875289947493002, "rouge_l": 0.22695921037867106}, {"bertscore": 0.6898036823549774, "rouge_l": 0.23879264139889728}, {"bertscore": 0.6869509979733266, "rouge_l": 0.23016585168082435}, {"bertscore": 0.6860823810857255, "rouge_l": 0.23124400160748126}, {"bertscore": 0.6842183136905078, "rouge_l": 0.224391021525655}, {"bertscore": 0.6810837413941044, "rouge_l": 0.22269652477411778}, {"bertscore": 0.682507069373969, "rouge_l": 0.2263410022841646}], "total": {"test_bertscore": 68.63249469985021, "test_bertscore_se": 0.18188422839882787, "test_rouge_l": 23.08227234169175, "test_rouge_l_se": 0.3930473491791344}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "wiki-lingua-nl", "task": "summarization", "dataset_languages": ["nl"], "model": "google/gemma-2-27b", "results": {"raw": [{"bertscore": 0.6994589350360911, "rouge_l": 0.25084259575533474}, {"bertscore": 0.6928467747784453, "rouge_l": 0.24332186852777427}, {"bertscore": 0.6198353734071134, "rouge_l": 0.12684026124403114}, {"bertscore": 0.7096325747770607, "rouge_l": 0.2729520318981721}, {"bertscore": 0.6822070110938512, "rouge_l": 0.24539188421172742}, {"bertscore": 0.7101086645707255, "rouge_l": 0.2765340571869467}, {"bertscore": 0.702933601976838, "rouge_l": 0.2788567935306165}, {"bertscore": 0.6985153800051194, "rouge_l": 0.2588941323835623}, {"bertscore": 0.7073294361834996, "rouge_l": 0.27153183258068475}, {"bertscore": 0.7149273168033687, "rouge_l": 0.2818670026717017}], "total": {"test_bertscore": 69.37795068632113, "test_bertscore_se": 1.714897452192038, "test_rouge_l": 25.070324599905515, "test_rouge_l_se": 2.8366794091767575}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "swedn", "task": "summarization", "dataset_languages": ["sv"], "model": "google/gemma-2-27b", "results": {"raw": [{"bertscore": 0.6521600640553515, "rouge_l": 0.19915602249500836}, {"bertscore": 0.6584000669972738, "rouge_l": 0.20174285969508496}, {"bertscore": 0.6629814013431314, "rouge_l": 0.21464458217728105}, {"bertscore": 0.6587217900378164, "rouge_l": 0.2051661407768573}, {"bertscore": 0.6521585012524156, "rouge_l": 0.1911250242879951}, {"bertscore": 0.6572911356633995, "rouge_l": 0.20140204837101702}, {"bertscore": 0.6547722336254083, "rouge_l": 0.1983036853446533}, {"bertscore": 0.6519162640179275, "rouge_l": 0.1986140816110227}, {"bertscore": 0.6574693164875498, "rouge_l": 0.20269541819206466}, {"bertscore": 0.6641553148801904, "rouge_l": 0.21239453505874112}], "total": {"test_bertscore": 65.70026088360464, "test_bertscore_se": 0.2692793967230998, "test_rouge_l": 20.252443980097254, "test_rouge_l_se": 0.42721888688464027}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "cnn-dailymail", "task": "summarization", "dataset_languages": ["en"], "model": "google/gemma-2-27b", "results": {"raw": [{"bertscore": 0.7078674654767383, "rouge_l": 0.250994697199024}, {"bertscore": 0.7125342566578183, "rouge_l": 0.2552819606269552}, {"bertscore": 0.7123515779239824, "rouge_l": 0.2598125816367185}, {"bertscore": 0.702729566139169, "rouge_l": 0.21192231972812436}, {"bertscore": 0.7030423015094129, "rouge_l": 0.23488922821315136}, {"bertscore": 0.712813906633528, "rouge_l": 0.2503724496329348}, {"bertscore": 0.7146920267987298, "rouge_l": 0.27537317841401965}, {"bertscore": 0.7022721841931343, "rouge_l": 0.2487373449700801}, {"bertscore": 0.6974813282140531, "rouge_l": 0.2617545115208312}, {"bertscore": 0.7121031323331408, "rouge_l": 0.257452471680973}], "total": {"test_bertscore": 70.77887745879707, "test_bertscore_se": 0.3693883977089906, "test_rouge_l": 25.065907436228123, "test_rouge_l_se": 1.0612023430948085}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "orange-sum", "task": "summarization", "dataset_languages": ["fr"], "model": "google/gemma-2-27b", "results": {"raw": [{"bertscore": 0.6853183644707315, "rouge_l": 0.24845205303534404}, {"bertscore": 0.6858840078348294, "rouge_l": 0.25136100683685186}, {"bertscore": 0.6818068890715949, "rouge_l": 0.25074414183083316}, {"bertscore": 0.6544017109263223, "rouge_l": 0.20993045784246633}, {"bertscore": 0.6578092183335684, "rouge_l": 0.2070406341211177}, {"bertscore": 0.6872896861750633, "rouge_l": 0.25240665636294773}, {"bertscore": 0.6888369367807172, "rouge_l": 0.26395872210860616}, {"bertscore": 0.6489796296227723, "rouge_l": 0.19759951470008918}, {"bertscore": 0.6859971581143327, "rouge_l": 0.25124912595113397}, {"bertscore": 0.6807745684636757, "rouge_l": 0.24548418217312268}], "total": {"test_bertscore": 67.57098169793608, "test_bertscore_se": 0.9601728851933092, "test_rouge_l": 23.78226494962513, "test_rouge_l_se": 1.4523117416862195}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "ilpost-sum", "task": "summarization", "dataset_languages": ["it"], "model": "google/gemma-2-27b", "results": {"raw": [{"bertscore": 0.6812265873304568, "rouge_l": 0.2566482473677348}, {"bertscore": 0.672225330315996, "rouge_l": 0.24448846835632532}, {"bertscore": 0.6856040449638385, "rouge_l": 0.2669747701222571}, {"bertscore": 0.6845793057873379, "rouge_l": 0.2659210757187389}, {"bertscore": 0.6664974412124138, "rouge_l": 0.23425033044287757}, {"bertscore": 0.6869808444898808, "rouge_l": 0.27262153839980596}, {"bertscore": 0.6814212661993224, "rouge_l": 0.25794841042526917}, {"bertscore": 0.6802216227515601, "rouge_l": 0.25763089171486786}, {"bertscore": 0.6891091809375212, "rouge_l": 0.2710468560387764}, {"bertscore": 0.6876177132216981, "rouge_l": 0.2677399295009732}], "total": {"test_bertscore": 68.15483337210026, "test_bertscore_se": 0.44526434104240004, "test_rouge_l": 25.95270518087626, "test_rouge_l_se": 0.7583794613394129}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "danske-talemaader", "task": "knowledge", "dataset_languages": ["da"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.7913787695899632, "accuracy": 0.8428217821782178}, {"mcc": 0.7926268190298221, "accuracy": 0.844059405940594}, {"mcc": 0.7968456625985025, "accuracy": 0.8465346534653465}, {"mcc": 0.7818816031027797, "accuracy": 0.8353960396039604}, {"mcc": 0.8057490614873747, "accuracy": 0.8539603960396039}, {"mcc": 0.8005893017999719, "accuracy": 0.8502475247524752}, {"mcc": 0.7821034526371466, "accuracy": 0.8366336633663366}, {"mcc": 0.795622582263084, "accuracy": 0.8465346534653465}, {"mcc": 0.8069234140521234, "accuracy": 0.8551980198019802}, {"mcc": 0.8111838334073914, "accuracy": 0.8589108910891089}], "total": {"test_mcc": 79.64904499968159, "test_mcc_se": 0.616216685177134, "test_accuracy": 84.7029702970297, "test_accuracy_se": 0.4786361589178144}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "danish-citizen-tests", "task": "knowledge", "dataset_languages": ["da"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.8071277872405535, "accuracy": 0.8704761904761905}, {"mcc": 0.9230145798520524, "accuracy": 0.9485714285714286}, {"mcc": 0.8566759654895754, "accuracy": 0.9047619047619048}, {"mcc": 0.8786842206481076, "accuracy": 0.9180952380952381}, {"mcc": 0.8547619127075377, "accuracy": 0.9028571428571428}, {"mcc": 0.847101032656314, "accuracy": 0.8971428571428571}, {"mcc": 0.8172040657412414, "accuracy": 0.8761904761904762}, {"mcc": 0.7952960187454431, "accuracy": 0.8590476190476191}, {"mcc": 0.8679070547684594, "accuracy": 0.9104761904761904}, {"mcc": 0.8195882596622963, "accuracy": 0.88}], "total": {"test_mcc": 84.67360897511581, "test_mcc_se": 2.3803841753569497, "test_accuracy": 89.67619047619048, "test_accuracy_se": 1.6328325158115136}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-sv", "task": "knowledge", "dataset_languages": ["sv"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.5297686475340503, "accuracy": 0.64501953125}, {"mcc": 0.5631974446962615, "accuracy": 0.6708984375}, {"mcc": 0.5517777289716103, "accuracy": 0.66015625}, {"mcc": 0.5590962950972443, "accuracy": 0.666015625}, {"mcc": 0.5671552868849312, "accuracy": 0.67138671875}, {"mcc": 0.5467604416820003, "accuracy": 0.65673828125}, {"mcc": 0.5540436940176671, "accuracy": 0.6640625}, {"mcc": 0.5449118918386846, "accuracy": 0.65576171875}, {"mcc": 0.5588107782577932, "accuracy": 0.66650390625}, {"mcc": 0.5689908909904504, "accuracy": 0.67529296875}], "total": {"test_mcc": 55.44513099970694, "test_mcc_se": 0.7323642668676843, "test_accuracy": 66.318359375, "test_accuracy_se": 0.5578214357944402}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "icelandic-knowledge", "task": "knowledge", "dataset_languages": ["is"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.21766942767089228, "accuracy": 0.4091796875}, {"mcc": 0.20592469578348868, "accuracy": 0.40234375}, {"mcc": 0.22311637595318629, "accuracy": 0.41015625}, {"mcc": 0.27269682863889166, "accuracy": 0.4462890625}, {"mcc": 0.2345018327087583, "accuracy": 0.4248046875}, {"mcc": 0.23037001287495595, "accuracy": 0.419921875}, {"mcc": 0.25658347721091984, "accuracy": 0.435546875}, {"mcc": 0.27881199501111753, "accuracy": 0.4541015625}, {"mcc": 0.21045213851820688, "accuracy": 0.404296875}, {"mcc": 0.20234563317031157, "accuracy": 0.3955078125}], "total": {"test_mcc": 23.324724175407287, "test_mcc_se": 1.697812013655088, "test_accuracy": 42.021484375, "test_accuracy_se": 1.2206224001086161}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-de", "task": "knowledge", "dataset_languages": ["de"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.5612530396010842, "accuracy": 0.66650390625}, {"mcc": 0.5916259069630723, "accuracy": 0.69384765625}, {"mcc": 0.5786017460077972, "accuracy": 0.6796875}, {"mcc": 0.5837014003508558, "accuracy": 0.685546875}, {"mcc": 0.5889673265978872, "accuracy": 0.69091796875}, {"mcc": 0.5661559206853238, "accuracy": 0.671875}, {"mcc": 0.5974361876274765, "accuracy": 0.697265625}, {"mcc": 0.5872585934564054, "accuracy": 0.6884765625}, {"mcc": 0.5846800181890801, "accuracy": 0.6875}, {"mcc": 0.6006644389973381, "accuracy": 0.69921875}], "total": {"test_mcc": 58.4034457847632, "test_mcc_se": 0.7777680529858219, "test_accuracy": 68.6083984375, "test_accuracy_se": 0.658345067044406}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-nl", "task": "knowledge", "dataset_languages": ["nl"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.5680312350534549, "accuracy": 0.67578125}, {"mcc": 0.5798463112540905, "accuracy": 0.68359375}, {"mcc": 0.5678434149835483, "accuracy": 0.67236328125}, {"mcc": 0.5748456332815882, "accuracy": 0.677734375}, {"mcc": 0.5430175952377525, "accuracy": 0.65625}, {"mcc": 0.5638104445996464, "accuracy": 0.671875}, {"mcc": 0.5680144638769452, "accuracy": 0.67236328125}, {"mcc": 0.5599704321274996, "accuracy": 0.66845703125}, {"mcc": 0.571066486519368, "accuracy": 0.67724609375}, {"mcc": 0.5779646071233964, "accuracy": 0.68115234375}], "total": {"test_mcc": 56.7441062405729, "test_mcc_se": 0.6529879533451561, "test_accuracy": 67.3681640625, "test_accuracy_se": 0.4736096160556816}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu", "task": "knowledge", "dataset_languages": ["en"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.6516418335703295, "accuracy": 0.73681640625}, {"mcc": 0.6487328688386299, "accuracy": 0.73583984375}, {"mcc": 0.6605395441362251, "accuracy": 0.7451171875}, {"mcc": 0.6631122127488634, "accuracy": 0.74658203125}, {"mcc": 0.665264851483888, "accuracy": 0.74755859375}, {"mcc": 0.6204786938772712, "accuracy": 0.71435546875}, {"mcc": 0.6612180417369617, "accuracy": 0.7451171875}, {"mcc": 0.656005262912247, "accuracy": 0.740234375}, {"mcc": 0.6503509279284034, "accuracy": 0.736328125}, {"mcc": 0.6628417722885507, "accuracy": 0.74658203125}], "total": {"test_mcc": 65.40186009521369, "test_mcc_se": 0.8153031798751459, "test_accuracy": 73.9453125, "test_accuracy_se": 0.6169697746898622}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-fr", "task": "knowledge", "dataset_languages": ["fr"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.5717776344003158, "accuracy": 0.67578125}, {"mcc": 0.5712791016586672, "accuracy": 0.6728515625}, {"mcc": 0.6003604033783949, "accuracy": 0.69482421875}, {"mcc": 0.5752763923849284, "accuracy": 0.6787109375}, {"mcc": 0.5693143994931448, "accuracy": 0.67529296875}, {"mcc": 0.5873332387710402, "accuracy": 0.68896484375}, {"mcc": 0.5971223709862754, "accuracy": 0.69482421875}, {"mcc": 0.573507724934593, "accuracy": 0.677734375}, {"mcc": 0.60982155394066, "accuracy": 0.7060546875}, {"mcc": 0.5852671255639648, "accuracy": 0.6826171875}], "total": {"test_mcc": 58.41059945511985, "test_mcc_se": 0.8843631856432114, "test_accuracy": 68.4765625, "test_accuracy_se": 0.6778051791184865}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-it", "task": "knowledge", "dataset_languages": ["it"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.5790159897877094, "accuracy": 0.68310546875}, {"mcc": 0.5844102702104117, "accuracy": 0.68701171875}, {"mcc": 0.5671506757664418, "accuracy": 0.67041015625}, {"mcc": 0.5718783511693926, "accuracy": 0.67822265625}, {"mcc": 0.5629344957654948, "accuracy": 0.6728515625}, {"mcc": 0.5626238631722472, "accuracy": 0.67041015625}, {"mcc": 0.5718500503708268, "accuracy": 0.6767578125}, {"mcc": 0.5553931171312863, "accuracy": 0.66552734375}, {"mcc": 0.5633257512990114, "accuracy": 0.67041015625}, {"mcc": 0.570589620896226, "accuracy": 0.67724609375}], "total": {"test_mcc": 56.89172185569048, "test_mcc_se": 0.5287046710522105, "test_accuracy": 67.51953125, "test_accuracy_se": 0.40573309462689977}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-da", "task": "common-sense-reasoning", "dataset_languages": ["da"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.4249082256279692, "accuracy": 0.52978515625}, {"mcc": 0.32993559877422807, "accuracy": 0.46533203125}, {"mcc": 0.4721733327341961, "accuracy": 0.5654296875}, {"mcc": 0.3793072858187624, "accuracy": 0.5009765625}, {"mcc": 0.37026263543101284, "accuracy": 0.46923828125}, {"mcc": 0.46681200465728495, "accuracy": 0.57373046875}, {"mcc": 0.2766300250638106, "accuracy": 0.4033203125}, {"mcc": 0.41438803663862306, "accuracy": 0.53466796875}, {"mcc": 0.441100576892725, "accuracy": 0.55419921875}, {"mcc": 0.5451291123845855, "accuracy": 0.64697265625}], "total": {"test_mcc": 41.20646834023198, "test_mcc_se": 4.767009870224605, "test_accuracy": 52.43652343750001, "test_accuracy_se": 4.229251648498504}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-sv", "task": "common-sense-reasoning", "dataset_languages": ["sv"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.3680566581163614, "accuracy": 0.47509765625}, {"mcc": 0.41024843214513074, "accuracy": 0.5400390625}, {"mcc": 0.46558339582100927, "accuracy": 0.5576171875}, {"mcc": 0.39248115215934326, "accuracy": 0.498046875}, {"mcc": 0.33684031352032445, "accuracy": 0.462890625}, {"mcc": 0.32211792013637364, "accuracy": 0.4306640625}, {"mcc": 0.44379375709765106, "accuracy": 0.560546875}, {"mcc": 0.40869337030684727, "accuracy": 0.51611328125}, {"mcc": 0.36272474626493606, "accuracy": 0.4794921875}, {"mcc": 0.4523560561953994, "accuracy": 0.57080078125}], "total": {"test_mcc": 39.628958017633764, "test_mcc_se": 3.0310923531978893, "test_accuracy": 50.9130859375, "test_accuracy_se": 2.94047494654779}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "winogrande-is", "task": "common-sense-reasoning", "dataset_languages": ["is"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.2854474658313605, "accuracy": 0.6484375}, {"mcc": 0.32231414785967166, "accuracy": 0.6640625}, {"mcc": 0.3031586950460407, "accuracy": 0.6629464285714286}, {"mcc": 0.23331901270881739, "accuracy": 0.6272321428571429}, {"mcc": 0.28457231817260537, "accuracy": 0.6607142857142857}, {"mcc": 0.30682269932268624, "accuracy": 0.6584821428571429}, {"mcc": 0.26809589668710077, "accuracy": 0.6484375}, {"mcc": 0.24456351598790518, "accuracy": 0.6261160714285714}, {"mcc": 0.3201601791570373, "accuracy": 0.6774553571428571}, {"mcc": 0.23541483485297227, "accuracy": 0.6205357142857143}], "total": {"test_mcc": 28.038687656261974, "test_mcc_se": 2.093991663405694, "test_accuracy": 64.94419642857142, "test_accuracy_se": 1.1804620571635398}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-de", "task": "common-sense-reasoning", "dataset_languages": ["de"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.4650634971829186, "accuracy": 0.56884765625}, {"mcc": 0.5549202357242324, "accuracy": 0.65234375}, {"mcc": 0.4550699714545515, "accuracy": 0.55322265625}, {"mcc": 0.43430946870793447, "accuracy": 0.5234375}, {"mcc": 0.43236596580513537, "accuracy": 0.5390625}, {"mcc": 0.39584629187776255, "accuracy": 0.5185546875}, {"mcc": 0.5043361217159029, "accuracy": 0.61181640625}, {"mcc": 0.5136720890734789, "accuracy": 0.6083984375}, {"mcc": 0.5393798796915292, "accuracy": 0.63232421875}, {"mcc": 0.3181807323145926, "accuracy": 0.4580078125}], "total": {"test_mcc": 46.131442535480396, "test_mcc_se": 4.4235112644716, "test_accuracy": 56.66015625000001, "test_accuracy_se": 3.7170723595321977}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-nl", "task": "common-sense-reasoning", "dataset_languages": ["nl"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.444480684610376, "accuracy": 0.53857421875}, {"mcc": 0.5338908279492145, "accuracy": 0.6337890625}, {"mcc": 0.3967205696000896, "accuracy": 0.50439453125}, {"mcc": 0.37740732909295077, "accuracy": 0.4873046875}, {"mcc": 0.49659741367517185, "accuracy": 0.59375}, {"mcc": 0.40045419071955446, "accuracy": 0.5107421875}, {"mcc": 0.3609985845640232, "accuracy": 0.47705078125}, {"mcc": 0.40478148740546244, "accuracy": 0.52001953125}, {"mcc": 0.5575244990101093, "accuracy": 0.65576171875}, {"mcc": 0.43676162895928033, "accuracy": 0.55126953125}], "total": {"test_mcc": 44.09617215586232, "test_mcc_se": 4.167450791718561, "test_accuracy": 54.7265625, "test_accuracy_se": 3.8067394190403827}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag", "task": "common-sense-reasoning", "dataset_languages": ["en"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.4679041700963181, "accuracy": 0.57421875}, {"mcc": 0.6585440091711886, "accuracy": 0.7373046875}, {"mcc": 0.4837939524815827, "accuracy": 0.578125}, {"mcc": 0.5648417971139005, "accuracy": 0.650390625}, {"mcc": 0.6117602063679336, "accuracy": 0.69580078125}, {"mcc": 0.6380455010488005, "accuracy": 0.7177734375}, {"mcc": 0.6017298254690725, "accuracy": 0.6845703125}, {"mcc": 0.6195420189231777, "accuracy": 0.69873046875}, {"mcc": 0.6398586317920195, "accuracy": 0.71923828125}, {"mcc": 0.504108581541385, "accuracy": 0.609375}], "total": {"test_mcc": 57.9012869400538, "test_mcc_se": 4.332076796516825, "test_accuracy": 66.6552734375, "test_accuracy_se": 3.7243547357176237}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-fr", "task": "common-sense-reasoning", "dataset_languages": ["fr"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.5413795131644926, "accuracy": 0.63330078125}, {"mcc": 0.41554259186381387, "accuracy": 0.53662109375}, {"mcc": 0.46870952189958437, "accuracy": 0.56787109375}, {"mcc": 0.4703182880200374, "accuracy": 0.58251953125}, {"mcc": 0.47354422674827673, "accuracy": 0.5732421875}, {"mcc": 0.5355156543667355, "accuracy": 0.63916015625}, {"mcc": 0.4772228884193684, "accuracy": 0.5859375}, {"mcc": 0.4692841856966297, "accuracy": 0.5810546875}, {"mcc": 0.5881442894628288, "accuracy": 0.681640625}, {"mcc": 0.567029111561503, "accuracy": 0.66357421875}], "total": {"test_mcc": 50.0669027120327, "test_mcc_se": 3.3560471816118786, "test_accuracy": 60.44921875, "test_accuracy_se": 2.90530757230764}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-it", "task": "common-sense-reasoning", "dataset_languages": ["it"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.328876798993421, "accuracy": 0.4404296875}, {"mcc": 0.5556766457601094, "accuracy": 0.6640625}, {"mcc": 0.4191753519188767, "accuracy": 0.54443359375}, {"mcc": 0.5671590103318568, "accuracy": 0.66943359375}, {"mcc": 0.4970093309968627, "accuracy": 0.60205078125}, {"mcc": 0.5491938939698217, "accuracy": 0.6552734375}, {"mcc": 0.4826246203654248, "accuracy": 0.58984375}, {"mcc": 0.5283501042962198, "accuracy": 0.62548828125}, {"mcc": 0.447639546614835, "accuracy": 0.57080078125}, {"mcc": 0.46607210483559064, "accuracy": 0.5888671875}], "total": {"test_mcc": 48.417774080830185, "test_mcc_se": 4.542893612992482, "test_accuracy": 59.5068359375, "test_accuracy_se": 4.235133320064677}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "google/gemma-2-27b", "results": {"raw": [{"test_speed": 309.56, "test_speed_short": 34.96}, {"test_speed": 590.7900000000001, "test_speed_short": 64.80000000000001}, {"test_speed": 894.6400000000001, "test_speed_short": 125.28}, {"test_speed": 1194.25, "test_speed_short": 155.16}, {"test_speed": 1491.75, "test_speed_short": 187.04999999999998}, {"test_speed": 1772.41, "test_speed_short": 246.81}, {"test_speed": 2067.11, "test_speed_short": 275.2}, {"test_speed": 2378.6400000000003, "test_speed_short": 304.59}, {"test_speed": 2662.8199999999997, "test_speed_short": 333.05999999999995}, {"test_speed": 2958.22, "test_speed_short": 371.45}], "total": {"test_speed": 1632.019, "test_speed_se": 553.4437931728172, "test_speed_short": 209.83599999999996, "test_speed_short_se": 71.02924182622127}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "swerec", "task": "sentiment-classification", "dataset_languages": ["sv"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"mcc": 0.772945291315836, "macro_f1": 0.7192059518331474}, {"mcc": 0.7638313839355196, "macro_f1": 0.6976679586873583}, {"mcc": 0.7888401072054582, "macro_f1": 0.7419101777793219}, {"mcc": 0.7638343344092147, "macro_f1": 0.721313857330551}, {"mcc": 0.7594190528499126, "macro_f1": 0.7080210952185664}, {"mcc": 0.7794262847149646, "macro_f1": 0.7280454557363507}, {"mcc": 0.7913016550437965, "macro_f1": 0.7233881797013254}, {"mcc": 0.7691065319044742, "macro_f1": 0.7276566044350576}, {"mcc": 0.7674315692805809, "macro_f1": 0.7101615549769479}, {"mcc": 0.7638027235412342, "macro_f1": 0.720303017893359}], "total": {"test_mcc": 77.19938934200992, "test_mcc_se": 0.6853179162515762, "test_macro_f1": 71.97673853591986, "test_macro_f1_se": 0.7607351412924168}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "angry-tweets", "task": "sentiment-classification", "dataset_languages": ["da"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"mcc": 0.4344037602840875, "macro_f1": 0.5934161401270778}, {"mcc": 0.42311457922968787, "macro_f1": 0.5954536358823227}, {"mcc": 0.4341996258563359, "macro_f1": 0.5954186604070741}, {"mcc": 0.41692940320108496, "macro_f1": 0.5716539497569113}, {"mcc": 0.4557460974143048, "macro_f1": 0.6191953750081612}, {"mcc": 0.45027291400509173, "macro_f1": 0.606830627986934}, {"mcc": 0.4392707939130305, "macro_f1": 0.6082479656319822}, {"mcc": 0.43778608158490556, "macro_f1": 0.5968831024130816}, {"mcc": 0.4688350418796533, "macro_f1": 0.6291592449017269}, {"mcc": 0.3755522824567581, "macro_f1": 0.5425117787679947}], "total": {"test_mcc": 43.3611057982494, "test_mcc_se": 1.5758773479479273, "test_macro_f1": 59.58770480883266, "test_macro_f1_se": 1.5134981838517148}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "norec", "task": "sentiment-classification", "dataset_languages": ["nb", "nn", "no"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"mcc": 0.3167771143626782, "macro_f1": 0.334921936249804}, {"mcc": 0.3035436829137863, "macro_f1": 0.3294959602338045}, {"mcc": 0.3275780523858529, "macro_f1": 0.3416944033271299}, {"mcc": 0.3005323034945078, "macro_f1": 0.32490625084306796}, {"mcc": 0.32285157260921055, "macro_f1": 0.33771610117041906}, {"mcc": 0.3267351629443361, "macro_f1": 0.34143024052786153}, {"mcc": 0.29273894704003717, "macro_f1": 0.30536068818361045}, {"mcc": 0.3183310736317006, "macro_f1": 0.33316273540083885}, {"mcc": 0.3112654753407787, "macro_f1": 0.3224959973686629}, {"mcc": 0.3229637831392663, "macro_f1": 0.34249781751110936}], "total": {"test_mcc": 31.433171678621548, "test_mcc_se": 0.739405145692702, "test_macro_f1": 33.136821308163086, "test_macro_f1_se": 0.7121805299020769}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hotter-and-colder-sentiment", "task": "sentiment-classification", "dataset_languages": ["is"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"mcc": 0.2652951523408245, "macro_f1": 0.4050179809057181}, {"mcc": 0.21652632577010045, "macro_f1": 0.41921476766087307}, {"mcc": 0.2420216593983784, "macro_f1": 0.40453316298296577}, {"mcc": 0.23678619067199597, "macro_f1": 0.4545161686879016}, {"mcc": 0.25174063694831883, "macro_f1": 0.4049302525943295}, {"mcc": 0.22204288984493536, "macro_f1": 0.41564828219068645}, {"mcc": 0.2350494443641449, "macro_f1": 0.42350251887221374}, {"mcc": 0.2706743639449353, "macro_f1": 0.45470491806548896}, {"mcc": 0.24394596026397636, "macro_f1": 0.39984822055859587}, {"mcc": 0.22903497001231707, "macro_f1": 0.392242709699854}], "total": {"test_mcc": 24.131175935599273, "test_mcc_se": 1.0833508077570522, "test_macro_f1": 41.74158982218626, "test_macro_f1_se": 1.3434201048684509}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "sb10k", "task": "sentiment-classification", "dataset_languages": ["de"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"mcc": 0.5336036899407401, "macro_f1": 0.6640490524398414}, {"mcc": 0.5007923798373449, "macro_f1": 0.6429967135265559}, {"mcc": 0.5070951578648452, "macro_f1": 0.6407696661522265}, {"mcc": 0.4715767051498156, "macro_f1": 0.6137031712999935}, {"mcc": 0.49382730239801115, "macro_f1": 0.6340631352871254}, {"mcc": 0.5624364273805725, "macro_f1": 0.6856146293392699}, {"mcc": 0.4425399732140365, "macro_f1": 0.5830459068732659}, {"mcc": 0.508873019324493, "macro_f1": 0.6529097915551825}, {"mcc": 0.5309350664224269, "macro_f1": 0.6449986054756572}, {"mcc": 0.5186588201276308, "macro_f1": 0.648841716953172}], "total": {"test_mcc": 50.70338541659917, "test_mcc_se": 2.0778911513079836, "test_macro_f1": 64.1099238890229, "test_macro_f1_se": 1.7157811800249982}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "dutch-social", "task": "sentiment-classification", "dataset_languages": ["nl"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"mcc": 0.1145062603844512, "macro_f1": 0.21763155822207417}, {"mcc": 0.09874453301710248, "macro_f1": 0.2094942416705877}, {"mcc": 0.04541241461592328, "macro_f1": 0.1857790079594591}, {"mcc": 0.11344974697262737, "macro_f1": 0.21354688021354687}, {"mcc": 0.09710160926025245, "macro_f1": 0.2196846045032449}, {"mcc": 0.09219759800286105, "macro_f1": 0.20492202119288527}, {"mcc": 0.0737409790188824, "macro_f1": 0.18983640015433267}, {"mcc": 0.10373945162182868, "macro_f1": 0.2196978307381552}, {"mcc": 0.07093170087019371, "macro_f1": 0.19774781598747662}, {"mcc": 0.13053332353333189, "macro_f1": 0.23092035665709534}], "total": {"test_mcc": 9.403576172974546, "test_mcc_se": 1.5415114816946065, "test_macro_f1": 20.89260717298858, "test_macro_f1_se": 0.8896311957876929}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "sst5", "task": "sentiment-classification", "dataset_languages": ["en"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"mcc": 0.5477017154224714, "macro_f1": 0.6566639356185024}, {"mcc": 0.5399199165985771, "macro_f1": 0.6441545521857616}, {"mcc": 0.5247064406871426, "macro_f1": 0.6401188972553117}, {"mcc": 0.5087229768319289, "macro_f1": 0.6251371477190859}, {"mcc": 0.5452995031691381, "macro_f1": 0.6558941214920343}, {"mcc": 0.5095928713019949, "macro_f1": 0.6228020764025409}, {"mcc": 0.5156252503875928, "macro_f1": 0.626858892491255}, {"mcc": 0.5167073282034277, "macro_f1": 0.6282772520872124}, {"mcc": 0.5338597973600788, "macro_f1": 0.6470147386983087}, {"mcc": 0.5470815452372517, "macro_f1": 0.6524760438659979}], "total": {"test_mcc": 52.89217345199605, "test_mcc_se": 0.973750291948779, "test_macro_f1": 63.99397657816011, "test_macro_f1_se": 0.8210169205153819}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fosent", "task": "sentiment-classification", "dataset_languages": ["fo"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"mcc": 0.2696998583083437, "macro_f1": 0.30730135619725524}, {"mcc": 0.19867900532522578, "macro_f1": 0.23667395909296107}, {"mcc": 0.18894399337229406, "macro_f1": 0.2617283950617284}, {"mcc": 0.18905781917734854, "macro_f1": 0.23902894491129786}, {"mcc": 0.1931799989603052, "macro_f1": 0.23678519338128048}, {"mcc": 0.20669950616056218, "macro_f1": 0.26953708124000697}, {"mcc": 0.22427506003372452, "macro_f1": 0.2750287343453794}, {"mcc": 0.18507196162644668, "macro_f1": 0.25519096917587886}, {"mcc": 0.22361898379567965, "macro_f1": 0.2640275849715642}, {"mcc": 0.22930598078997763, "macro_f1": 0.27247884416924667}], "total": {"test_mcc": 21.08532167549908, "test_mcc_se": 1.6325591212032187, "test_macro_f1": 26.17781062546599, "test_macro_f1_se": 1.345849882223877}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "allocine", "task": "sentiment-classification", "dataset_languages": ["fr"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"mcc": 0.9133178590927573, "macro_f1": 0.9557710692113659}, {"mcc": 0.8956927507625149, "macro_f1": 0.946565340478636}, {"mcc": 0.9276940229551486, "macro_f1": 0.9633469157157064}, {"mcc": 0.8990966941246893, "macro_f1": 0.9480129354606341}, {"mcc": 0.9115199942850587, "macro_f1": 0.9548159602183998}, {"mcc": 0.9013333151807595, "macro_f1": 0.949446580687062}, {"mcc": 0.9129238647678305, "macro_f1": 0.9553666775634739}, {"mcc": 0.9077256125978352, "macro_f1": 0.9529234632746122}, {"mcc": 0.9208707346017627, "macro_f1": 0.9601167013476914}, {"mcc": 0.9120268651805977, "macro_f1": 0.9550896998710534}], "total": {"test_mcc": 91.02201713548956, "test_mcc_se": 0.6060898462773386, "test_macro_f1": 95.41455343828635, "test_macro_f1_se": 0.32301136943679193}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "sentipolc16", "task": "sentiment-classification", "dataset_languages": ["it"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"mcc": 0.4302706385175812, "macro_f1": 0.46647896519361814}, {"mcc": 0.39017428844812985, "macro_f1": 0.43522082617151253}, {"mcc": 0.39593587835766925, "macro_f1": 0.44124054623709236}, {"mcc": 0.38749665325059823, "macro_f1": 0.43022137459370224}, {"mcc": 0.4038254408453947, "macro_f1": 0.44098186955498003}, {"mcc": 0.40287966328872266, "macro_f1": 0.4430113337131469}, {"mcc": 0.4202049961676504, "macro_f1": 0.4548684708738812}, {"mcc": 0.40253601180814175, "macro_f1": 0.4385026737967914}, {"mcc": 0.39264610075194334, "macro_f1": 0.43683594775923407}, {"mcc": 0.4179313771480656, "macro_f1": 0.4387090549471755}], "total": {"test_mcc": 40.43901048583896, "test_mcc_se": 0.8780446411860814, "test_macro_f1": 44.260710628411346, "test_macro_f1_se": 0.6527417708871278}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "suc3", "task": "named-entity-recognition", "dataset_languages": ["sv"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"micro_f1_no_misc": 0.5264100268576545, "micro_f1": 0.3009913863156184}, {"micro_f1_no_misc": 0.5389876880984951, "micro_f1": 0.20609404990403074}, {"micro_f1_no_misc": 0.5314972165250513, "micro_f1": 0.27072263549415515}, {"micro_f1_no_misc": 0.5381479324403029, "micro_f1": 0.28834087261053554}, {"micro_f1_no_misc": 0.5370483772198408, "micro_f1": 0.30853355426677714}, {"micro_f1_no_misc": 0.5533081933493316, "micro_f1": 0.3880392604428213}, {"micro_f1_no_misc": 0.5312409183376926, "micro_f1": 0.31453154875717015}, {"micro_f1_no_misc": 0.495748031496063, "micro_f1": 0.28911806543385493}, {"micro_f1_no_misc": 0.5022508038585208, "micro_f1": 0.31051940746296647}, {"micro_f1_no_misc": 0.5469468154957321, "micro_f1": 0.36400817995910023}], "total": {"test_micro_f1_no_misc": 53.015860036786854, "test_micro_f1_no_misc_se": 1.129141944991684, "test_micro_f1": 30.4089896064703, "test_micro_f1_se": 3.0648337762144635}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "dansk", "task": "named-entity-recognition", "dataset_languages": ["da"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"micro_f1_no_misc": 0.4060701203558346, "micro_f1": 0.21798631476050834}, {"micro_f1_no_misc": 0.43339706819630336, "micro_f1": 0.18933333333333333}, {"micro_f1_no_misc": 0.401328903654485, "micro_f1": 0.21198156682027652}, {"micro_f1_no_misc": 0.46627565982404695, "micro_f1": 0.2521681120747165}, {"micro_f1_no_misc": 0.4272965879265092, "micro_f1": 0.20619469026548673}, {"micro_f1_no_misc": 0.4445634708087841, "micro_f1": 0.2206148282097649}, {"micro_f1_no_misc": 0.42379958246346555, "micro_f1": 0.24131274131274133}, {"micro_f1_no_misc": 0.5225130890052355, "micro_f1": 0.24834209924536932}, {"micro_f1_no_misc": 0.4311512415349887, "micro_f1": 0.21115537848605578}, {"micro_f1_no_misc": 0.4524197933659598, "micro_f1": 0.21533728850802022}], "total": {"test_micro_f1_no_misc": 44.08815517135613, "test_micro_f1_no_misc_se": 2.152884043025085, "test_micro_f1": 22.144263530162732, "test_micro_f1_se": 1.2346462397193967}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "norne-nb", "task": "named-entity-recognition", "dataset_languages": ["nb", "no"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"micro_f1_no_misc": 0.5015948963317384, "micro_f1": 0.28829765121596346}, {"micro_f1_no_misc": 0.5115216940004151, "micro_f1": 0.2464508801817149}, {"micro_f1_no_misc": 0.517384649026897, "micro_f1": 0.27734818022476543}, {"micro_f1_no_misc": 0.5201987470295961, "micro_f1": 0.33044585987261144}, {"micro_f1_no_misc": 0.5361522198731501, "micro_f1": 0.2819373942470389}, {"micro_f1_no_misc": 0.5309106098579783, "micro_f1": 0.25014398157035894}, {"micro_f1_no_misc": 0.4694648012436154, "micro_f1": 0.2516614010240767}, {"micro_f1_no_misc": 0.4479231893132958, "micro_f1": 0.26134261777826134}, {"micro_f1_no_misc": 0.5010473397570172, "micro_f1": 0.3089260808926081}, {"micro_f1_no_misc": 0.5311629899336047, "micro_f1": 0.2822435829724306}], "total": {"test_micro_f1_no_misc": 50.673611363673075, "test_micro_f1_no_misc_se": 1.7616787160351814, "test_micro_f1": 27.787976299798295, "test_micro_f1_se": 1.677084980775619}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "norne-nn", "task": "named-entity-recognition", "dataset_languages": ["nn"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"micro_f1_no_misc": 0.46048371738716437, "micro_f1": 0.2588450962419798}, {"micro_f1_no_misc": 0.5162593415471621, "micro_f1": 0.26150121065375304}, {"micro_f1_no_misc": 0.4959898691430983, "micro_f1": 0.2684458398744113}, {"micro_f1_no_misc": 0.49898804047838086, "micro_f1": 0.30704715147795847}, {"micro_f1_no_misc": 0.5430387059503177, "micro_f1": 0.33153610318215454}, {"micro_f1_no_misc": 0.4981617647058824, "micro_f1": 0.2786563114063816}, {"micro_f1_no_misc": 0.49698149951314513, "micro_f1": 0.3328635189100306}, {"micro_f1_no_misc": 0.4511120436424675, "micro_f1": 0.29345116700657153}, {"micro_f1_no_misc": 0.4870267153565251, "micro_f1": 0.28681633864108397}, {"micro_f1_no_misc": 0.49664429530201337, "micro_f1": 0.2830578512396694}], "total": {"test_micro_f1_no_misc": 49.446859930261574, "test_micro_f1_no_misc_se": 1.5965231521101644, "test_micro_f1": 29.022205886339936, "test_micro_f1_se": 1.6397171464431637}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mim-gold-ner", "task": "named-entity-recognition", "dataset_languages": ["is"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"micro_f1_no_misc": 0.2740331491712707, "micro_f1": 0.15308836567825831}, {"micro_f1_no_misc": 0.30738916256157633, "micro_f1": 0.12789017341040462}, {"micro_f1_no_misc": 0.24723115398356554, "micro_f1": 0.17330282227307398}, {"micro_f1_no_misc": 0.3532823622462431, "micro_f1": 0.18004000889086463}, {"micro_f1_no_misc": 0.2352941176470588, "micro_f1": 0.1560266793711291}, {"micro_f1_no_misc": 0.24286162535299652, "micro_f1": 0.15875265768958186}, {"micro_f1_no_misc": 0.3158872716011149, "micro_f1": 0.1619696176008381}, {"micro_f1_no_misc": 0.30424242424242426, "micro_f1": 0.16917098445595852}, {"micro_f1_no_misc": 0.3119621342512909, "micro_f1": 0.1990490598660039}, {"micro_f1_no_misc": 0.3044291952588896, "micro_f1": 0.1514020480428749}], "total": {"test_micro_f1_no_misc": 28.96612596316431, "test_micro_f1_no_misc_se": 2.3708261178381447, "test_micro_f1": 16.30692417278988, "test_micro_f1_se": 1.1809367478221813}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fone", "task": "named-entity-recognition", "dataset_languages": ["fo"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"micro_f1_no_misc": 0.4080927707870713, "micro_f1": 0.2977721995607154}, {"micro_f1_no_misc": 0.36607142857142855, "micro_f1": 0.2692176289716433}, {"micro_f1_no_misc": 0.34456257205072366, "micro_f1": 0.2738439306358381}, {"micro_f1_no_misc": 0.3957570400303069, "micro_f1": 0.3000793021411578}, {"micro_f1_no_misc": 0.42282122567387886, "micro_f1": 0.3577525560453991}, {"micro_f1_no_misc": 0.4715730728985002, "micro_f1": 0.36193497224425053}, {"micro_f1_no_misc": 0.39754355182353673, "micro_f1": 0.2695634946840124}, {"micro_f1_no_misc": 0.42570784264595335, "micro_f1": 0.3571971135586782}, {"micro_f1_no_misc": 0.392141656972987, "micro_f1": 0.3047474491947045}, {"micro_f1_no_misc": 0.44053413203544234, "micro_f1": 0.33474704634959107}], "total": {"test_micro_f1_no_misc": 40.64805293489829, "test_micro_f1_no_misc_se": 2.2540652613970584, "test_micro_f1": 31.268556933859905, "test_micro_f1_se": 2.3215418088068906}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "germeval", "task": "named-entity-recognition", "dataset_languages": ["de"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"micro_f1_no_misc": 0.4871997888624967, "micro_f1": 0.2858311741442793}, {"micro_f1_no_misc": 0.5545645330535152, "micro_f1": 0.32194983704123564}, {"micro_f1_no_misc": 0.4831975560081467, "micro_f1": 0.26769190668578136}, {"micro_f1_no_misc": 0.5107587768969423, "micro_f1": 0.2835820895522388}, {"micro_f1_no_misc": 0.5548245614035088, "micro_f1": 0.29736477115117893}, {"micro_f1_no_misc": 0.5126353790613719, "micro_f1": 0.28706454226303}, {"micro_f1_no_misc": 0.48890106878596873, "micro_f1": 0.2537889225682006}, {"micro_f1_no_misc": 0.5218145044784746, "micro_f1": 0.3082787010214971}, {"micro_f1_no_misc": 0.5115638766519824, "micro_f1": 0.2717864923747277}, {"micro_f1_no_misc": 0.5060923774440351, "micro_f1": 0.2709884654694116}], "total": {"test_micro_f1_no_misc": 51.315524226464426, "test_micro_f1_no_misc_se": 1.5658053888113268, "test_micro_f1": 28.483269022715806, "test_micro_f1_se": 1.2586430644576596}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "conll-nl", "task": "named-entity-recognition", "dataset_languages": ["nl"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"micro_f1_no_misc": 0.587651598676957, "micro_f1": 0.36757990867579904}, {"micro_f1_no_misc": 0.5447263017356475, "micro_f1": 0.3242617255356109}, {"micro_f1_no_misc": 0.6495633187772926, "micro_f1": 0.32801295396715247}, {"micro_f1_no_misc": 0.5202863961813843, "micro_f1": 0.30448891292590585}, {"micro_f1_no_misc": 0.6096866096866097, "micro_f1": 0.3346265761396702}, {"micro_f1_no_misc": 0.5699177438307874, "micro_f1": 0.350485991995426}, {"micro_f1_no_misc": 0.5939759036144578, "micro_f1": 0.3502145922746781}, {"micro_f1_no_misc": 0.5397030342156229, "micro_f1": 0.33100824350031705}, {"micro_f1_no_misc": 0.6031347962382445, "micro_f1": 0.3228476821192053}, {"micro_f1_no_misc": 0.5302631578947368, "micro_f1": 0.3538681948424069}], "total": {"test_micro_f1_no_misc": 57.489088608517406, "test_micro_f1_no_misc_se": 2.5519847620519287, "test_micro_f1": 33.67394781976172, "test_micro_f1_se": 1.153519036895784}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "conll-en", "task": "named-entity-recognition", "dataset_languages": ["en"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"micro_f1_no_misc": 0.654390003288392, "micro_f1": 0.4302447920319843}, {"micro_f1_no_misc": 0.6668781055079818, "micro_f1": 0.44676128337929644}, {"micro_f1_no_misc": 0.6697622996130459, "micro_f1": 0.4445223734044046}, {"micro_f1_no_misc": 0.6373239436619719, "micro_f1": 0.43198439785470505}, {"micro_f1_no_misc": 0.657672127636792, "micro_f1": 0.4491203377902885}, {"micro_f1_no_misc": 0.6596445029624753, "micro_f1": 0.43711656441717794}, {"micro_f1_no_misc": 0.6586337179618096, "micro_f1": 0.4567901234567901}, {"micro_f1_no_misc": 0.6314053221904552, "micro_f1": 0.42746369660228395}, {"micro_f1_no_misc": 0.6468417625352724, "micro_f1": 0.4356279934234041}, {"micro_f1_no_misc": 0.6385224274406331, "micro_f1": 0.43558493424223643}], "total": {"test_micro_f1_no_misc": 65.21074212798828, "test_micro_f1_no_misc_se": 0.8067678576976146, "test_micro_f1": 43.95216496602572, "test_micro_f1_se": 0.581599425659973}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "eltec", "task": "named-entity-recognition", "dataset_languages": ["fr"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"micro_f1_no_misc": 0.3950807071483474, "micro_f1": 0.26116034108008696}, {"micro_f1_no_misc": 0.39388710870101057, "micro_f1": 0.2889397905759163}, {"micro_f1_no_misc": 0.38817624093697717, "micro_f1": 0.2588890709487138}, {"micro_f1_no_misc": 0.3771885942971485, "micro_f1": 0.279427549194991}, {"micro_f1_no_misc": 0.3728059332509271, "micro_f1": 0.2708225108225108}, {"micro_f1_no_misc": 0.3978411719352351, "micro_f1": 0.28102315935015554}, {"micro_f1_no_misc": 0.437891804039935, "micro_f1": 0.2799290570499557}, {"micro_f1_no_misc": 0.3758974358974359, "micro_f1": 0.24667201283079393}, {"micro_f1_no_misc": 0.3713858424725823, "micro_f1": 0.25209520616828696}, {"micro_f1_no_misc": 0.41065754094353457, "micro_f1": 0.26940921744509744}], "total": {"test_micro_f1_no_misc": 39.208123796231334, "test_micro_f1_no_misc_se": 1.2715541643358692, "test_micro_f1": 26.883679154665085, "test_micro_f1_se": 0.8572257471878444}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "multinerd-it", "task": "named-entity-recognition", "dataset_languages": ["it"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"micro_f1_no_misc": 0.4041213508872353, "micro_f1": 0.25192862611153644}, {"micro_f1_no_misc": 0.35821287779237837, "micro_f1": 0.24561600719424456}, {"micro_f1_no_misc": 0.3640911617565314, "micro_f1": 0.23556213691764438}, {"micro_f1_no_misc": 0.4243008678881388, "micro_f1": 0.2866945293493856}, {"micro_f1_no_misc": 0.4300414272350661, "micro_f1": 0.2517259085981628}, {"micro_f1_no_misc": 0.40231330148353034, "micro_f1": 0.2558058925476603}, {"micro_f1_no_misc": 0.37635518843572535, "micro_f1": 0.24248496993987975}, {"micro_f1_no_misc": 0.41584731058415264, "micro_f1": 0.2906649616368287}, {"micro_f1_no_misc": 0.37007132283676497, "micro_f1": 0.2415325139922682}, {"micro_f1_no_misc": 0.3783923494443009, "micro_f1": 0.22495719178082188}], "total": {"test_micro_f1_no_misc": 39.23747158343824, "test_micro_f1_no_misc_se": 1.6192054076738964, "test_micro_f1": 25.269727380684326, "test_micro_f1_se": 1.2986007079709168}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-sv", "task": "linguistic-acceptability", "dataset_languages": ["sv"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"mcc": 0.25128418631891214, "macro_f1": 0.5607885352889727}, {"mcc": 0.2410330858415918, "macro_f1": 0.5089191235141273}, {"mcc": 0.25135610180028384, "macro_f1": 0.5605806440148702}, {"mcc": 0.251437750009689, "macro_f1": 0.5480880354580207}, {"mcc": 0.33353824095805423, "macro_f1": 0.6154037522142963}, {"mcc": 0.286442707086626, "macro_f1": 0.5559044740040716}, {"mcc": 0.20266532493682327, "macro_f1": 0.48716880043829425}, {"mcc": 0.2823541783554917, "macro_f1": 0.5792501284026708}, {"mcc": 0.3045255791830671, "macro_f1": 0.5945597412713302}, {"mcc": 0.277841055935601, "macro_f1": 0.5688768557432995}], "total": {"test_mcc": 26.824782104261402, "test_mcc_se": 2.27247020326265, "test_macro_f1": 55.79540090349953, "test_macro_f1_se": 2.3323838438755025}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-da", "task": "linguistic-acceptability", "dataset_languages": ["da"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"mcc": 0.25605105789955246, "macro_f1": 0.4994209560461097}, {"mcc": 0.23211002433905042, "macro_f1": 0.48532521898586456}, {"mcc": 0.2563665063798642, "macro_f1": 0.5435087821064379}, {"mcc": 0.22682859402808095, "macro_f1": 0.46190818377527537}, {"mcc": 0.2462135537853362, "macro_f1": 0.5112615413573006}, {"mcc": 0.23976708561489637, "macro_f1": 0.4944543928419042}, {"mcc": 0.29362376360189646, "macro_f1": 0.5635618138167301}, {"mcc": 0.22019360768488302, "macro_f1": 0.496870900924376}, {"mcc": 0.24522146228838254, "macro_f1": 0.5058350190302645}, {"mcc": 0.2385177506434536, "macro_f1": 0.4917044190486892}], "total": {"test_mcc": 24.548934062653963, "test_mcc_se": 1.272882555783081, "test_macro_f1": 50.53851227932953, "test_macro_f1_se": 1.7987472454610167}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-nb", "task": "linguistic-acceptability", "dataset_languages": ["nb", "no"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"mcc": 0.30950264806145217, "macro_f1": 0.6494253914241006}, {"mcc": 0.26504501395729296, "macro_f1": 0.5963760986192226}, {"mcc": 0.3023710417072585, "macro_f1": 0.6364182473647029}, {"mcc": 0.2632518643184555, "macro_f1": 0.5985896907401826}, {"mcc": 0.2635167833863873, "macro_f1": 0.6284548126871556}, {"mcc": 0.23981234017914962, "macro_f1": 0.589754893382531}, {"mcc": 0.2427501901158055, "macro_f1": 0.6026633230374018}, {"mcc": 0.27580813215802746, "macro_f1": 0.6358505500556365}, {"mcc": 0.27400947744815524, "macro_f1": 0.6285160376591867}, {"mcc": 0.26703940631441997, "macro_f1": 0.5812162948693516}], "total": {"test_mcc": 27.03106897646404, "test_mcc_se": 1.375405853923749, "test_macro_f1": 61.472653398394726, "test_macro_f1_se": 1.4592251476692855}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-nn", "task": "linguistic-acceptability", "dataset_languages": ["nn"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"mcc": 0.21345938134494172, "macro_f1": 0.6053087644764132}, {"mcc": 0.1950618742892113, "macro_f1": 0.5972659050348229}, {"mcc": 0.17391200497348583, "macro_f1": 0.5864846055368707}, {"mcc": 0.2150152387622831, "macro_f1": 0.6074900681772346}, {"mcc": 0.15166615437496042, "macro_f1": 0.575529666095288}, {"mcc": 0.19604277818295615, "macro_f1": 0.5947615517667408}, {"mcc": 0.25355956878704977, "macro_f1": 0.6267648302032256}, {"mcc": 0.18557918196171505, "macro_f1": 0.5925853837258561}, {"mcc": 0.18544661113946445, "macro_f1": 0.5893062887905784}, {"mcc": 0.18658289973981645, "macro_f1": 0.5903351609607276}], "total": {"test_mcc": 19.563256935558844, "test_mcc_se": 1.696285828646869, "test_macro_f1": 59.65832224767759, "test_macro_f1_se": 0.8674872816102474}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-is", "task": "linguistic-acceptability", "dataset_languages": ["is"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"mcc": 0.04542636816427132, "macro_f1": 0.5201984922225403}, {"mcc": 0.02929484874148557, "macro_f1": 0.4371361327883067}, {"mcc": 0.06489080580183831, "macro_f1": 0.5301546431253544}, {"mcc": 0.04477613133002181, "macro_f1": 0.4789427763794891}, {"mcc": 0.06054574337734942, "macro_f1": 0.5087620850848837}, {"mcc": 0.056929448262625124, "macro_f1": 0.4812833259172492}, {"mcc": 0.08709525869083953, "macro_f1": 0.5240924609088113}, {"mcc": 0.050824962188378425, "macro_f1": 0.5248831999923703}, {"mcc": 0.09872577771587505, "macro_f1": 0.548774092244865}, {"mcc": 0.047052481075522644, "macro_f1": 0.48591706406725704}], "total": {"test_mcc": 5.855618253482072, "test_mcc_se": 1.2878162430452438, "test_macro_f1": 50.40144272731128, "test_macro_f1_se": 2.0405920206869026}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-fo", "task": "linguistic-acceptability", "dataset_languages": ["fo"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"mcc": 0.05371927633294966, "macro_f1": 0.5145873734109028}, {"mcc": 0.0943345247030096, "macro_f1": 0.4889977990345168}, {"mcc": 0.04824318623865845, "macro_f1": 0.47464962968119917}, {"mcc": 0.06770421420687422, "macro_f1": 0.5157292998504228}, {"mcc": 0.14929973330686275, "macro_f1": 0.5524054443956595}, {"mcc": 0.09012761444327204, "macro_f1": 0.4812850386729043}, {"mcc": 0.06186700713557128, "macro_f1": 0.5030075744876598}, {"mcc": 0.09186014735754239, "macro_f1": 0.5457420471945165}, {"mcc": 0.029699305791715414, "macro_f1": 0.513521557793355}, {"mcc": 0.09190935164495222, "macro_f1": 0.5002216049323385}], "total": {"test_mcc": 7.7876436116140795, "test_mcc_se": 2.071924272335117, "test_macro_f1": 50.90147369453476, "test_macro_f1_se": 1.5723603294984259}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-de", "task": "linguistic-acceptability", "dataset_languages": ["de"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"mcc": 0.3773407202998364, "macro_f1": 0.6884575474577306}, {"mcc": 0.345851601870061, "macro_f1": 0.6554054269813365}, {"mcc": 0.3543729876963985, "macro_f1": 0.6744042047835749}, {"mcc": 0.4075498258002919, "macro_f1": 0.6927546148945819}, {"mcc": 0.3829338876864077, "macro_f1": 0.6907261797889783}, {"mcc": 0.39055327355839364, "macro_f1": 0.6864163611526303}, {"mcc": 0.3837816659591729, "macro_f1": 0.6917321768709916}, {"mcc": 0.3738255155626471, "macro_f1": 0.6859868703528069}, {"mcc": 0.36231265548070946, "macro_f1": 0.6811522677307768}, {"mcc": 0.3555288489119377, "macro_f1": 0.6735847607343114}], "total": {"test_mcc": 37.34050982825856, "test_mcc_se": 1.1756511707913755, "test_macro_f1": 68.20620410747719, "test_macro_f1_se": 0.7157165585840677}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-nl", "task": "linguistic-acceptability", "dataset_languages": ["nl"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"mcc": 0.39718351150965353, "macro_f1": 0.6658732766115316}, {"mcc": 0.3480558283335481, "macro_f1": 0.6003694783716227}, {"mcc": 0.359793868809447, "macro_f1": 0.6326966629507114}, {"mcc": 0.36290502130485397, "macro_f1": 0.6281433434740793}, {"mcc": 0.39043343533508895, "macro_f1": 0.6580441842235767}, {"mcc": 0.33108899493298366, "macro_f1": 0.5957516986608615}, {"mcc": 0.39191691071455126, "macro_f1": 0.6332384896434797}, {"mcc": 0.3578669732748525, "macro_f1": 0.6350695437867357}, {"mcc": 0.3386019701227453, "macro_f1": 0.6227450946421388}, {"mcc": 0.3791739209125725, "macro_f1": 0.6450953303419308}], "total": {"test_mcc": 36.57020435250297, "test_mcc_se": 1.4342741882477128, "test_macro_f1": 63.17027102706668, "test_macro_f1_se": 1.3739741168556359}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-en", "task": "linguistic-acceptability", "dataset_languages": ["en"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"mcc": 0.21717718974064787, "macro_f1": 0.5320124366522953}, {"mcc": 0.2077139850030879, "macro_f1": 0.5466055325020066}, {"mcc": 0.24411678014825008, "macro_f1": 0.5484427219944032}, {"mcc": 0.2312709532730581, "macro_f1": 0.5696237888947044}, {"mcc": 0.20583887872529819, "macro_f1": 0.5555727092435403}, {"mcc": 0.24125546936879072, "macro_f1": 0.5503512880562061}, {"mcc": 0.2278958490144558, "macro_f1": 0.5657957614244647}, {"mcc": 0.21964686500417194, "macro_f1": 0.5429665396706853}, {"mcc": 0.18852427924908804, "macro_f1": 0.5237847939076873}, {"mcc": 0.20492856914033736, "macro_f1": 0.5286553867086168}], "total": {"test_mcc": 21.88368818667186, "test_mcc_se": 1.0906381898162798, "test_macro_f1": 54.6381095905461, "test_macro_f1_se": 0.938758855747229}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-fr", "task": "linguistic-acceptability", "dataset_languages": ["fr"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"mcc": 0.21019141071563952, "macro_f1": 0.6015495296070836}, {"mcc": 0.31027699936388964, "macro_f1": 0.6444330321059557}, {"mcc": 0.30635226269454735, "macro_f1": 0.6514491475257818}, {"mcc": 0.29828958173465653, "macro_f1": 0.6484291178969835}, {"mcc": 0.32625282304828174, "macro_f1": 0.6630859375}, {"mcc": 0.3680711744789479, "macro_f1": 0.6838463928119101}, {"mcc": 0.37263128484503594, "macro_f1": 0.6854411075145298}, {"mcc": 0.296228132722125, "macro_f1": 0.6477266211799781}, {"mcc": 0.3250327666534242, "macro_f1": 0.655099487474531}, {"mcc": 0.3305272875083821, "macro_f1": 0.6604040200135539}], "total": {"test_mcc": 31.4385372376493, "test_mcc_se": 2.7960886380420584, "test_macro_f1": 65.41464393630308, "test_macro_f1_se": 1.4496620374707891}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-it", "task": "linguistic-acceptability", "dataset_languages": ["it"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"mcc": 0.26617543127196225, "macro_f1": 0.6217600113087808}, {"mcc": 0.3016305556555379, "macro_f1": 0.626485379768223}, {"mcc": 0.28207390346528344, "macro_f1": 0.6360991674657739}, {"mcc": 0.2661869813604726, "macro_f1": 0.6293171954275933}, {"mcc": 0.18050249766008014, "macro_f1": 0.5900144474362661}, {"mcc": 0.26853097505188406, "macro_f1": 0.6129987742848024}, {"mcc": 0.2831058890559526, "macro_f1": 0.6414647921723069}, {"mcc": 0.2631617250789258, "macro_f1": 0.6284909812210646}, {"mcc": 0.2530425623151979, "macro_f1": 0.6231368459792677}, {"mcc": 0.2929457117353015, "macro_f1": 0.6464506579072864}], "total": {"test_mcc": 26.57356232650598, "test_mcc_se": 2.069957167639086, "test_macro_f1": 62.56218252971366, "test_macro_f1_se": 0.9833835679974354}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"f1": 17.367721799558346, "em": 0.0}, {"f1": 15.023889107341766, "em": 0.0}, {"f1": 27.504401260414163, "em": 0.0}, {"f1": 17.827060331016977, "em": 0.0}, {"f1": 15.063664544747287, "em": 0.0}, {"f1": 15.16860318483947, "em": 0.0}, {"f1": 14.225586355671197, "em": 0.0}, {"f1": 15.752311090373956, "em": 0.0}, {"f1": 14.702362728754549, "em": 0.0}, {"f1": 15.78869698186514, "em": 0.0}], "total": {"test_f1": 16.842429738458286, "test_f1_se": 2.4263424719559525, "test_em": 0.0, "test_em_se": 0.0}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "norquad", "task": "reading-comprehension", "dataset_languages": ["nb", "nn", "no"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"f1": 23.825599013111596, "em": 0.08169934640522876}, {"f1": 19.81011451401089, "em": 0.0}, {"f1": 23.429124082006727, "em": 0.08326394671107411}, {"f1": 22.1861129173498, "em": 0.0}, {"f1": 27.474565335206332, "em": 0.0827129859387924}, {"f1": 17.393844404356088, "em": 0.0823045267489712}, {"f1": 22.020296235394312, "em": 0.16806722689075632}, {"f1": 18.627018771280508, "em": 0.16891891891891891}, {"f1": 26.826906708036574, "em": 0.08305647840531562}, {"f1": 21.01276728557099, "em": 0.16556291390728478}], "total": {"test_f1": 22.260634926632385, "test_f1_se": 2.0281211932008016, "test_em": 0.0915586343926342, "test_em_se": 0.038365164622245}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scandiqa-sv", "task": "reading-comprehension", "dataset_languages": ["sv"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"f1": 16.09489019405249, "em": 0.0}, {"f1": 16.47228744984551, "em": 0.0}, {"f1": 16.279501549292462, "em": 0.0}, {"f1": 16.463899389170418, "em": 0.0}, {"f1": 16.068522058139322, "em": 0.0}, {"f1": 15.96387799568119, "em": 0.0}, {"f1": 16.169927745913004, "em": 0.0}, {"f1": 16.389337552472384, "em": 0.0}, {"f1": 16.4984492896902, "em": 0.0}, {"f1": 16.1099972291893, "em": 0.0}], "total": {"test_f1": 16.25106904534463, "test_f1_se": 0.12082905235175045, "test_em": 0.0, "test_em_se": 0.0}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "nqii", "task": "reading-comprehension", "dataset_languages": ["is"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"f1": 29.494989974029394, "em": 0.6153846153846154}, {"f1": 26.376694766034515, "em": 0.3115264797507788}, {"f1": 26.319249319630746, "em": 0.3115264797507788}, {"f1": 26.520686451649542, "em": 0.14947683109118087}, {"f1": 27.953010740551797, "em": 0.4702194357366771}, {"f1": 28.30165901122753, "em": 0.9244992295839753}, {"f1": 28.16107648472873, "em": 0.6153846153846154}, {"f1": 24.80172915548363, "em": 0.6134969325153374}, {"f1": 26.80916018031384, "em": 0.6024096385542169}, {"f1": 26.545395064955162, "em": 0.7530120481927711}], "total": {"test_f1": 27.128365114860493, "test_f1_se": 0.8307357686106664, "test_em": 0.5366936305944947, "test_em_se": 0.14247234524847727}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "foqa", "task": "reading-comprehension", "dataset_languages": ["fo"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"f1": 19.276366535447046, "em": 0.0}, {"f1": 18.292097671075474, "em": 0.0}, {"f1": 23.998505415200132, "em": 0.24691358024691357}, {"f1": 22.897155571678727, "em": 0.0}, {"f1": 24.979617673022286, "em": 0.0}, {"f1": 21.1592350269653, "em": 0.0}, {"f1": 22.914461898519782, "em": 0.0}, {"f1": 25.417914649855323, "em": 0.2544529262086514}, {"f1": 23.174734016961427, "em": 0.24937655860349128}, {"f1": 24.909294949752532, "em": 0.2457002457002457}], "total": {"test_f1": 22.7019383408478, "test_f1_se": 1.5038853784009587, "test_em": 0.09964433107593021, "test_em_se": 0.0797441333096622}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "germanquad", "task": "reading-comprehension", "dataset_languages": ["de"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"f1": 21.521097246957794, "em": 0.0}, {"f1": 21.070463258602192, "em": 0.0}, {"f1": 21.107608802529295, "em": 0.0}, {"f1": 20.211155837165006, "em": 0.0}, {"f1": 20.62500203474434, "em": 0.0}, {"f1": 21.462761917222746, "em": 0.0}, {"f1": 20.627235548739215, "em": 0.0}, {"f1": 21.1830568410223, "em": 0.0}, {"f1": 20.579942627249924, "em": 0.0}, {"f1": 20.172239926336438, "em": 0.0}], "total": {"test_f1": 20.856056404056925, "test_f1_se": 0.2988266060322304, "test_em": 0.0, "test_em_se": 0.0}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "squad", "task": "reading-comprehension", "dataset_languages": ["en"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"f1": 20.168200222805456, "em": 0.0}, {"f1": 21.085824107002857, "em": 0.0}, {"f1": 21.109844888838907, "em": 0.0}, {"f1": 21.16362930133499, "em": 0.0}, {"f1": 20.60700376766989, "em": 0.0}, {"f1": 21.087043380416045, "em": 0.0}, {"f1": 20.770894812632346, "em": 0.0}, {"f1": 21.37674546309272, "em": 0.0}, {"f1": 20.784575041037325, "em": 0.0}, {"f1": 21.066788325071094, "em": 0.0}], "total": {"test_f1": 20.922054930990164, "test_f1_se": 0.21516998168254048, "test_em": 0.0, "test_em_se": 0.0}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "squad-nl", "task": "reading-comprehension", "dataset_languages": ["nl"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"f1": 20.408818325889353, "em": 0.07575757575757576}, {"f1": 20.509441310787793, "em": 0.0}, {"f1": 18.789038866532255, "em": 0.0}, {"f1": 20.15848468500911, "em": 0.0}, {"f1": 21.917293223706, "em": 0.07710100231303008}, {"f1": 23.4868697150559, "em": 0.15267175572519084}, {"f1": 20.608524832090236, "em": 0.078003120124805}, {"f1": 19.996012862661416, "em": 0.0}, {"f1": 19.81260619420641, "em": 0.07739938080495357}, {"f1": 21.639340038333373, "em": 0.07716049382716049}], "total": {"test_f1": 20.732643005427185, "test_f1_se": 0.8128079454625762, "test_em": 0.05380933285527158, "test_em_se": 0.03205123724154984}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "squad-it", "task": "reading-comprehension", "dataset_languages": ["it"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"f1": 15.39564740746139, "em": 0.0}, {"f1": 16.827374623932716, "em": 0.0}, {"f1": 14.62651061016539, "em": 0.0}, {"f1": 16.817991197966776, "em": 0.0}, {"f1": 14.655513386843152, "em": 0.0}, {"f1": 15.337175731912819, "em": 0.0}, {"f1": 14.82421217968341, "em": 0.0}, {"f1": 17.61913669909021, "em": 0.0}, {"f1": 14.913403920156647, "em": 0.0}, {"f1": 14.988039593137128, "em": 0.0}], "total": {"test_f1": 15.600500535034964, "test_f1_se": 0.6683568425711175, "test_em": 0.0, "test_em_se": 0.0}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fquad", "task": "reading-comprehension", "dataset_languages": ["fr"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"f1": 23.500915917808307, "em": 0.0}, {"f1": 23.381478490899273, "em": 0.0758150113722517}, {"f1": 23.467397202194064, "em": 0.0}, {"f1": 23.860229188283967, "em": 0.0}, {"f1": 23.08687194688292, "em": 0.0}, {"f1": 23.713599749995023, "em": 0.0}, {"f1": 23.204706634929444, "em": 0.0}, {"f1": 24.07595581195712, "em": 0.07861635220125786}, {"f1": 23.134642247847164, "em": 0.0}, {"f1": 23.91015900706037, "em": 0.07716049382716049}], "total": {"test_f1": 23.533595619785764, "test_f1_se": 0.21376375276633702, "test_em": 0.023159185740067006, "test_em_se": 0.023116101981055327}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "nordjylland-news", "task": "summarization", "dataset_languages": ["da"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"bertscore": 0.6160504544095602, "rouge_l": 0.09581964632076254}, {"bertscore": 0.6126691388926702, "rouge_l": 0.09018440770373932}, {"bertscore": 0.6117188252683263, "rouge_l": 0.09137026616907201}, {"bertscore": 0.6100151785067283, "rouge_l": 0.09335382301051037}, {"bertscore": 0.6159788803197443, "rouge_l": 0.09360050694032249}, {"bertscore": 0.6098259813006734, "rouge_l": 0.0930661102541514}, {"bertscore": 0.6124088669603225, "rouge_l": 0.09537128123195357}, {"bertscore": 0.6139498425909551, "rouge_l": 0.09555296812461142}, {"bertscore": 0.6084859510592651, "rouge_l": 0.09299201558477015}, {"bertscore": 0.6095361772895558, "rouge_l": 0.09634191227162456}], "total": {"test_bertscore": 61.206392965978004, "test_bertscore_se": 0.16499415598123657, "test_rouge_l": 9.376529376115178, "test_rouge_l_se": 0.12477894355958144}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mlsum", "task": "summarization", "dataset_languages": ["de"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"bertscore": 0.6239719609875465, "rouge_l": 0.10484425333502016}, {"bertscore": 0.6230208694760222, "rouge_l": 0.10265049518859766}, {"bertscore": 0.6239722429891117, "rouge_l": 0.10628211734168067}, {"bertscore": 0.621578237216454, "rouge_l": 0.09959936117443316}, {"bertscore": 0.6246807452407666, "rouge_l": 0.10989799512030313}, {"bertscore": 0.6210998783499235, "rouge_l": 0.0992331534320576}, {"bertscore": 0.6230415104364511, "rouge_l": 0.1069131778374687}, {"bertscore": 0.6215848083811579, "rouge_l": 0.09969865117375504}, {"bertscore": 0.624920908070635, "rouge_l": 0.10943505453882016}, {"bertscore": 0.6243180456076516, "rouge_l": 0.10956394640880582}], "total": {"test_bertscore": 62.32189206755721, "test_bertscore_se": 0.08593143159351045, "test_rouge_l": 10.481182055509422, "test_rouge_l_se": 0.26579650772953123}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "rrn", "task": "summarization", "dataset_languages": ["is"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"bertscore": 0.638884166313801, "rouge_l": 0.13562378530728247}, {"bertscore": 0.6332619597960729, "rouge_l": 0.1335953328398594}, {"bertscore": 0.6348103216441814, "rouge_l": 0.1338568081640522}, {"bertscore": 0.6347858873195946, "rouge_l": 0.13539675372857513}, {"bertscore": 0.6333183187234681, "rouge_l": 0.12952761518802908}, {"bertscore": 0.6309191700129304, "rouge_l": 0.12983829563707472}, {"bertscore": 0.6374096923682373, "rouge_l": 0.13827075710007405}, {"bertscore": 0.6287043156626169, "rouge_l": 0.12167927585180828}, {"bertscore": 0.6317375852668192, "rouge_l": 0.12755716116292165}, {"bertscore": 0.6266075938474387, "rouge_l": 0.13171723993099715}], "total": {"test_bertscore": 63.3043901095516, "test_bertscore_se": 0.2318417007710284, "test_rouge_l": 13.17063024910674, "test_rouge_l_se": 0.2967055348139284}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "no-sammendrag", "task": "summarization", "dataset_languages": ["nb", "nn", "no"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"bertscore": 0.5969991016172571, "rouge_l": 0.05960239071859811}, {"bertscore": 0.6033886406803504, "rouge_l": 0.07401531706686856}, {"bertscore": 0.6014540181931807, "rouge_l": 0.069462540340723}, {"bertscore": 0.5993441937753232, "rouge_l": 0.07000826079783479}, {"bertscore": 0.6023924407782033, "rouge_l": 0.07134354096495929}, {"bertscore": 0.5982823541999096, "rouge_l": 0.06872087584001839}, {"bertscore": 0.5987572247395292, "rouge_l": 0.06720867016928428}, {"bertscore": 0.6002520217880374, "rouge_l": 0.06613393748460421}, {"bertscore": 0.601433309566346, "rouge_l": 0.07314736726321269}, {"bertscore": 0.5995282595977187, "rouge_l": 0.06868617769873359}], "total": {"test_bertscore": 60.018315649358556, "test_bertscore_se": 0.12267377679992827, "test_rouge_l": 6.88329078344837, "test_rouge_l_se": 0.2519463862567411}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "wiki-lingua-nl", "task": "summarization", "dataset_languages": ["nl"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"bertscore": 0.635152467264561, "rouge_l": 0.11785109095528712}, {"bertscore": 0.6341657289303839, "rouge_l": 0.11393127674822345}, {"bertscore": 0.6330996365722967, "rouge_l": 0.11139641045882265}, {"bertscore": 0.6403320856625214, "rouge_l": 0.11641010281657044}, {"bertscore": 0.6377756412111921, "rouge_l": 0.1168479944792788}, {"bertscore": 0.6354387756873621, "rouge_l": 0.11655655032556486}, {"bertscore": 0.633667166417581, "rouge_l": 0.11439957616751825}, {"bertscore": 0.6346459207416046, "rouge_l": 0.11585400966341586}, {"bertscore": 0.6378301756631117, "rouge_l": 0.11677688367989955}, {"bertscore": 0.637918990541948, "rouge_l": 0.12241353009626006}], "total": {"test_bertscore": 63.60026588692562, "test_bertscore_se": 0.1446767551075469, "test_rouge_l": 11.62437425390841, "test_rouge_l_se": 0.17751163983350918}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "swedn", "task": "summarization", "dataset_languages": ["sv"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"bertscore": 0.6460641868616221, "rouge_l": 0.15740565504822995}, {"bertscore": 0.6495993910502875, "rouge_l": 0.15515511688656752}, {"bertscore": 0.6445479722169694, "rouge_l": 0.15220053873810008}, {"bertscore": 0.6424194782593986, "rouge_l": 0.15271020711533806}, {"bertscore": 0.6461919205466984, "rouge_l": 0.15662819822990037}, {"bertscore": 0.6432196423556888, "rouge_l": 0.14537240767752174}, {"bertscore": 0.6485636528668692, "rouge_l": 0.15930062199570028}, {"bertscore": 0.648188675797428, "rouge_l": 0.1593961853120529}, {"bertscore": 0.6488922308490146, "rouge_l": 0.1602792175101912}, {"bertscore": 0.6485848822339904, "rouge_l": 0.15855267437952114}], "total": {"test_bertscore": 64.66272033037967, "test_bertscore_se": 0.1575209185002629, "test_rouge_l": 15.570008228931231, "test_rouge_l_se": 0.2830111586405862}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "cnn-dailymail", "task": "summarization", "dataset_languages": ["en"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"bertscore": 0.6668203036824707, "rouge_l": 0.1699572330887234}, {"bertscore": 0.6669409456080757, "rouge_l": 0.17051483399280853}, {"bertscore": 0.6675281118077692, "rouge_l": 0.16949741639237254}, {"bertscore": 0.6659265918715391, "rouge_l": 0.16831461497711508}, {"bertscore": 0.6661910782568157, "rouge_l": 0.1692809001866701}, {"bertscore": 0.6670413201209158, "rouge_l": 0.16987735060123157}, {"bertscore": 0.6680807508819271, "rouge_l": 0.17200358303147567}, {"bertscore": 0.6663244712690357, "rouge_l": 0.1692929248804301}, {"bertscore": 0.6683835381700192, "rouge_l": 0.1691574906477163}, {"bertscore": 0.6664855761919171, "rouge_l": 0.16875090851937008}], "total": {"test_bertscore": 66.69722687860485, "test_bertscore_se": 0.050261796058726424, "test_rouge_l": 16.96647256317913, "test_rouge_l_se": 0.06385445549711159}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "orange-sum", "task": "summarization", "dataset_languages": ["fr"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"bertscore": 0.6373796452244278, "rouge_l": 0.12700133754230722}, {"bertscore": 0.6383320771274157, "rouge_l": 0.12434486241575358}, {"bertscore": 0.6398036068421789, "rouge_l": 0.13014507046843132}, {"bertscore": 0.6372131946263835, "rouge_l": 0.12630964621594226}, {"bertscore": 0.6378686709504109, "rouge_l": 0.12558422513721185}, {"bertscore": 0.6382453391561285, "rouge_l": 0.12741900130950956}, {"bertscore": 0.6362327440292574, "rouge_l": 0.12277552940383779}, {"bertscore": 0.6346148440497927, "rouge_l": 0.12148152079053388}, {"bertscore": 0.6402388568967581, "rouge_l": 0.1267122542979328}, {"bertscore": 0.6399041283002589, "rouge_l": 0.1278163520816913}], "total": {"test_bertscore": 63.79833107203012, "test_bertscore_se": 0.108662686294159, "test_rouge_l": 12.595897996631516, "test_rouge_l_se": 0.15708497595147286}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "ilpost-sum", "task": "summarization", "dataset_languages": ["it"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"bertscore": 0.6138126880250638, "rouge_l": 0.1092304231983305}, {"bertscore": 0.6142012116033584, "rouge_l": 0.10840203454364897}, {"bertscore": 0.6168546920089284, "rouge_l": 0.1089791517826923}, {"bertscore": 0.6141403877700213, "rouge_l": 0.10962057396524412}, {"bertscore": 0.6164413565566065, "rouge_l": 0.11046701756813222}, {"bertscore": 0.6154010530735832, "rouge_l": 0.10888875220520376}, {"bertscore": 0.6136599024321185, "rouge_l": 0.10783561519500925}, {"bertscore": 0.6143512966082199, "rouge_l": 0.10927065863135807}, {"bertscore": 0.6164240507350769, "rouge_l": 0.10996932224498802}, {"bertscore": 0.6198922913899878, "rouge_l": 0.10903860078253788}], "total": {"test_bertscore": 61.55178930202965, "test_bertscore_se": 0.12040123617896197, "test_rouge_l": 10.917021501171451, "test_rouge_l_se": 0.046387186802710506}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "danske-talemaader", "task": "knowledge", "dataset_languages": ["da"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"mcc": 0.24569470906585003, "accuracy": 0.43564356435643564}, {"mcc": 0.30880941845499216, "accuracy": 0.48267326732673266}, {"mcc": 0.3311654961528168, "accuracy": 0.49504950495049505}, {"mcc": 0.30636035664102823, "accuracy": 0.4777227722772277}, {"mcc": 0.3220656956496967, "accuracy": 0.49257425742574257}, {"mcc": 0.2786795789521202, "accuracy": 0.45792079207920794}, {"mcc": 0.2916814893711108, "accuracy": 0.47029702970297027}, {"mcc": 0.32500847841128033, "accuracy": 0.49257425742574257}, {"mcc": 0.3054626535780593, "accuracy": 0.4814356435643564}, {"mcc": 0.28397111531605607, "accuracy": 0.46410891089108913}], "total": {"test_mcc": 29.9889899159301, "test_mcc_se": 1.5951416861261645, "test_accuracy": 47.5, "test_accuracy_se": 1.151936880669172}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "danish-citizen-tests", "task": "knowledge", "dataset_languages": ["da"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"mcc": 0.5499784032292598, "accuracy": 0.699047619047619}, {"mcc": 0.5849503271650144, "accuracy": 0.7180952380952381}, {"mcc": 0.46793268951683165, "accuracy": 0.6476190476190476}, {"mcc": 0.5749943078663141, "accuracy": 0.7123809523809523}, {"mcc": 0.5589544758962648, "accuracy": 0.7047619047619048}, {"mcc": 0.590613816428365, "accuracy": 0.7219047619047619}, {"mcc": 0.5492840655363955, "accuracy": 0.6971428571428572}, {"mcc": 0.5856281550651595, "accuracy": 0.7238095238095238}, {"mcc": 0.5208343722882349, "accuracy": 0.68}, {"mcc": 0.5660914024058263, "accuracy": 0.7161904761904762}], "total": {"test_mcc": 55.49262015397666, "test_mcc_se": 2.3050466543969916, "test_accuracy": 70.20952380952382, "test_accuracy_se": 1.4493370813267141}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "nrk-quiz-qa", "task": "knowledge", "dataset_languages": ["nb", "nn", "no"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"mcc": 0.2819564862420912, "accuracy": 0.4599609375}, {"mcc": 0.27343544463756697, "accuracy": 0.451171875}, {"mcc": 0.2908375793565122, "accuracy": 0.45751953125}, {"mcc": 0.309849950800492, "accuracy": 0.48046875}, {"mcc": 0.3180825167786068, "accuracy": 0.4833984375}, {"mcc": 0.3144784171359876, "accuracy": 0.48486328125}, {"mcc": 0.2881354770417545, "accuracy": 0.4638671875}, {"mcc": 0.3066949594099542, "accuracy": 0.4755859375}, {"mcc": 0.2705552202255559, "accuracy": 0.44775390625}, {"mcc": 0.31848728792678294, "accuracy": 0.4833984375}], "total": {"test_mcc": 29.72513339555305, "test_mcc_se": 1.1442905431143373, "test_accuracy": 46.8798828125, "test_accuracy_se": 0.8885972023915811}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-sv", "task": "knowledge", "dataset_languages": ["sv"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"mcc": 0.36535288247184344, "accuracy": 0.52294921875}, {"mcc": 0.3855743588660129, "accuracy": 0.53759765625}, {"mcc": 0.34986849348939875, "accuracy": 0.5068359375}, {"mcc": 0.34703111845281437, "accuracy": 0.5087890625}, {"mcc": 0.397650684091037, "accuracy": 0.5439453125}, {"mcc": 0.3934917145696391, "accuracy": 0.54052734375}, {"mcc": 0.37350884362196246, "accuracy": 0.5283203125}, {"mcc": 0.37024663488990067, "accuracy": 0.52587890625}, {"mcc": 0.37170865189180113, "accuracy": 0.5263671875}, {"mcc": 0.388760707304255, "accuracy": 0.54150390625}], "total": {"test_mcc": 37.43194089648665, "test_mcc_se": 1.0723642092806884, "test_accuracy": 52.82714843750001, "test_accuracy_se": 0.8093628112058472}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "icelandic-knowledge", "task": "knowledge", "dataset_languages": ["is"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"mcc": 0.13085807870999766, "accuracy": 0.34375}, {"mcc": 0.09122290919903817, "accuracy": 0.3193359375}, {"mcc": 0.08603723781115453, "accuracy": 0.310546875}, {"mcc": 0.09592797353578539, "accuracy": 0.3251953125}, {"mcc": 0.08525425255541919, "accuracy": 0.3193359375}, {"mcc": 0.0649835862183791, "accuracy": 0.2939453125}, {"mcc": 0.1143806726555168, "accuracy": 0.341796875}, {"mcc": 0.09856307627412876, "accuracy": 0.3291015625}, {"mcc": 0.08789159140287177, "accuracy": 0.3203125}, {"mcc": 0.09585568314591926, "accuracy": 0.3203125}], "total": {"test_mcc": 9.509750615082107, "test_mcc_se": 1.0964906649214778, "test_accuracy": 32.236328125, "test_accuracy_se": 0.8916098603308957}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-de", "task": "knowledge", "dataset_languages": ["de"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"mcc": 0.3802137716257701, "accuracy": 0.52880859375}, {"mcc": 0.384773788388192, "accuracy": 0.53515625}, {"mcc": 0.36503725269324433, "accuracy": 0.521484375}, {"mcc": 0.38257562571449477, "accuracy": 0.533203125}, {"mcc": 0.39124080948118456, "accuracy": 0.53759765625}, {"mcc": 0.39171663789160516, "accuracy": 0.54150390625}, {"mcc": 0.3601505879304361, "accuracy": 0.51806640625}, {"mcc": 0.4058213431662101, "accuracy": 0.5517578125}, {"mcc": 0.40695363441824545, "accuracy": 0.552734375}, {"mcc": 0.3838795306595663, "accuracy": 0.53515625}], "total": {"test_mcc": 38.52362981968949, "test_mcc_se": 0.9343100028567543, "test_accuracy": 53.5546875, "test_accuracy_se": 0.7013587480202119}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-nl", "task": "knowledge", "dataset_languages": ["nl"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"mcc": 0.40647867256599046, "accuracy": 0.5537109375}, {"mcc": 0.42888214076112213, "accuracy": 0.56982421875}, {"mcc": 0.37055910497978156, "accuracy": 0.52880859375}, {"mcc": 0.40015733932071096, "accuracy": 0.54931640625}, {"mcc": 0.43766693800515494, "accuracy": 0.578125}, {"mcc": 0.42027705304046353, "accuracy": 0.56396484375}, {"mcc": 0.42590904980044103, "accuracy": 0.56787109375}, {"mcc": 0.4260813964197624, "accuracy": 0.56787109375}, {"mcc": 0.4075994937467944, "accuracy": 0.5537109375}, {"mcc": 0.40158872611599544, "accuracy": 0.548828125}], "total": {"test_mcc": 41.25199914756217, "test_mcc_se": 1.2077290342023188, "test_accuracy": 55.8203125, "test_accuracy_se": 0.8822007986307979}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu", "task": "knowledge", "dataset_languages": ["en"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"mcc": 0.49152848747898986, "accuracy": 0.615234375}, {"mcc": 0.5121062307872907, "accuracy": 0.6298828125}, {"mcc": 0.5288436401506241, "accuracy": 0.64453125}, {"mcc": 0.5051343261003782, "accuracy": 0.6259765625}, {"mcc": 0.5101880391834788, "accuracy": 0.630859375}, {"mcc": 0.4820865056628906, "accuracy": 0.609375}, {"mcc": 0.5141143432662453, "accuracy": 0.63232421875}, {"mcc": 0.518753462703629, "accuracy": 0.6337890625}, {"mcc": 0.5097600760182038, "accuracy": 0.62744140625}, {"mcc": 0.5115739290196837, "accuracy": 0.62890625}], "total": {"test_mcc": 50.840890403714155, "test_mcc_se": 0.8197599465380456, "test_accuracy": 62.783203125, "test_accuracy_se": 0.6030562656483723}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-fr", "task": "knowledge", "dataset_languages": ["fr"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"mcc": 0.4410589335471724, "accuracy": 0.57568359375}, {"mcc": 0.4274885540105205, "accuracy": 0.5693359375}, {"mcc": 0.4159104116427836, "accuracy": 0.5615234375}, {"mcc": 0.4295846043025426, "accuracy": 0.56787109375}, {"mcc": 0.4271462355393281, "accuracy": 0.56640625}, {"mcc": 0.42323976884313697, "accuracy": 0.56494140625}, {"mcc": 0.4473601459685366, "accuracy": 0.583984375}, {"mcc": 0.422795039904154, "accuracy": 0.56298828125}, {"mcc": 0.41573387370003945, "accuracy": 0.560546875}, {"mcc": 0.40034401116237156, "accuracy": 0.548828125}], "total": {"test_mcc": 42.506615786205856, "test_mcc_se": 0.8196663060614074, "test_accuracy": 56.62109374999999, "test_accuracy_se": 0.5793706959922688}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-it", "task": "knowledge", "dataset_languages": ["it"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"mcc": 0.4485240159939419, "accuracy": 0.5849609375}, {"mcc": 0.4437911472900659, "accuracy": 0.58251953125}, {"mcc": 0.43760015404900365, "accuracy": 0.57470703125}, {"mcc": 0.418251412781163, "accuracy": 0.56201171875}, {"mcc": 0.41212317904718726, "accuracy": 0.55615234375}, {"mcc": 0.4145089334925646, "accuracy": 0.55859375}, {"mcc": 0.4287452515922635, "accuracy": 0.56982421875}, {"mcc": 0.44099267047873214, "accuracy": 0.57958984375}, {"mcc": 0.42252594829269413, "accuracy": 0.5654296875}, {"mcc": 0.4346280300307214, "accuracy": 0.57421875}], "total": {"test_mcc": 43.01690743048338, "test_mcc_se": 0.7969830869602929, "test_accuracy": 57.080078125, "test_accuracy_se": 0.6241526645504003}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-da", "task": "common-sense-reasoning", "dataset_languages": ["da"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"mcc": 0.3146438086647696, "accuracy": 0.47216796875}, {"mcc": 0.3062847849362624, "accuracy": 0.45751953125}, {"mcc": 0.30439394734873615, "accuracy": 0.4453125}, {"mcc": 0.326301311764675, "accuracy": 0.486328125}, {"mcc": 0.27113270497154907, "accuracy": 0.43017578125}, {"mcc": 0.316773654455768, "accuracy": 0.47412109375}, {"mcc": 0.23269876676957885, "accuracy": 0.40771484375}, {"mcc": 0.29029142999068835, "accuracy": 0.4482421875}, {"mcc": 0.3192750386403957, "accuracy": 0.4755859375}, {"mcc": 0.3298714728784288, "accuracy": 0.4892578125}], "total": {"test_mcc": 30.116669204208517, "test_mcc_se": 1.8425406539474918, "test_accuracy": 45.8642578125, "test_accuracy_se": 1.6142084696406076}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "nor-common-sense-qa", "task": "common-sense-reasoning", "dataset_languages": ["nb", "nn", "no"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"mcc": 0.6025697751668971, "accuracy": 0.681551116333725}, {"mcc": 0.6317524439759468, "accuracy": 0.7038777908343126}, {"mcc": 0.5776689475144607, "accuracy": 0.6603995299647474}, {"mcc": 0.6006003604853691, "accuracy": 0.6780258519388954}, {"mcc": 0.5934126146599963, "accuracy": 0.6745005875440658}, {"mcc": 0.6065311509620174, "accuracy": 0.6850763807285546}, {"mcc": 0.5918173491463754, "accuracy": 0.6733254994124559}, {"mcc": 0.5836392734385684, "accuracy": 0.6650998824911868}, {"mcc": 0.6102029292205335, "accuracy": 0.6862514688601645}, {"mcc": 0.5326156005751607, "accuracy": 0.6251468860164512}], "total": {"test_mcc": 59.30810445145325, "test_mcc_se": 1.6136902323606088, "test_accuracy": 67.33254994124559, "test_accuracy_se": 1.2878559924524902}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-sv", "task": "common-sense-reasoning", "dataset_languages": ["sv"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"mcc": 0.29974042227256764, "accuracy": 0.46826171875}, {"mcc": 0.3525324602113035, "accuracy": 0.50830078125}, {"mcc": 0.3299117451523986, "accuracy": 0.4921875}, {"mcc": 0.33042774742309994, "accuracy": 0.48779296875}, {"mcc": 0.3238794799961832, "accuracy": 0.4853515625}, {"mcc": 0.32270456390395025, "accuracy": 0.4853515625}, {"mcc": 0.3335500294833021, "accuracy": 0.47998046875}, {"mcc": 0.2657122277394391, "accuracy": 0.4404296875}, {"mcc": 0.3100296764103844, "accuracy": 0.47412109375}, {"mcc": 0.34799545342074567, "accuracy": 0.5029296875}], "total": {"test_mcc": 32.16483806013375, "test_mcc_se": 1.5577450370280064, "test_accuracy": 48.2470703125, "test_accuracy_se": 1.1814976185416664}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "winogrande-is", "task": "common-sense-reasoning", "dataset_languages": ["is"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"mcc": -0.014558023463488485, "accuracy": 0.49888392857142855}, {"mcc": 0.015873015873015872, "accuracy": 0.5}, {"mcc": 0.022962161053222324, "accuracy": 0.53125}, {"mcc": 0.052582011073123915, "accuracy": 0.5267857142857143}, {"mcc": -0.03235018222896608, "accuracy": 0.4921875}, {"mcc": 0.1077548822887049, "accuracy": 0.5569196428571429}, {"mcc": -0.03216692455769038, "accuracy": 0.4888392857142857}, {"mcc": -0.025767897202232097, "accuracy": 0.4921875}, {"mcc": 0.04559077790828707, "accuracy": 0.5290178571428571}, {"mcc": -0.007142646597374868, "accuracy": 0.4921875}], "total": {"test_mcc": 1.3277717414660213, "test_mcc_se": 2.8084915126381174, "test_accuracy": 51.08258928571428, "test_accuracy_se": 1.4481073765292183}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-de", "task": "common-sense-reasoning", "dataset_languages": ["de"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"mcc": 0.35709835547271757, "accuracy": 0.50732421875}, {"mcc": 0.4377665804520528, "accuracy": 0.5625}, {"mcc": 0.3861114517293943, "accuracy": 0.529296875}, {"mcc": 0.3932710714483462, "accuracy": 0.53662109375}, {"mcc": 0.3709237873068097, "accuracy": 0.51708984375}, {"mcc": 0.3868959465832769, "accuracy": 0.53271484375}, {"mcc": 0.3757178320293929, "accuracy": 0.5224609375}, {"mcc": 0.4127779654302969, "accuracy": 0.54931640625}, {"mcc": 0.4688724437196778, "accuracy": 0.595703125}, {"mcc": 0.3994593796026815, "accuracy": 0.53759765625}], "total": {"test_mcc": 39.88894813774647, "test_mcc_se": 2.0666877600854656, "test_accuracy": 53.90625, "test_accuracy_se": 1.5696479567890225}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-nl", "task": "common-sense-reasoning", "dataset_languages": ["nl"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"mcc": 0.3733740745762763, "accuracy": 0.52392578125}, {"mcc": 0.3789924743389098, "accuracy": 0.52001953125}, {"mcc": 0.40715650591585306, "accuracy": 0.55078125}, {"mcc": 0.39895028298836716, "accuracy": 0.54248046875}, {"mcc": 0.34683906434921846, "accuracy": 0.50048828125}, {"mcc": 0.3476756294481161, "accuracy": 0.5029296875}, {"mcc": 0.34361352059520783, "accuracy": 0.49951171875}, {"mcc": 0.3918897984703488, "accuracy": 0.529296875}, {"mcc": 0.36140081727902423, "accuracy": 0.51123046875}, {"mcc": 0.35597163238524976, "accuracy": 0.505859375}], "total": {"test_mcc": 37.05863800346571, "test_mcc_se": 1.4296647857453908, "test_accuracy": 51.865234375, "test_accuracy_se": 1.113597695035729}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag", "task": "common-sense-reasoning", "dataset_languages": ["en"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"mcc": 0.6274385400155363, "accuracy": 0.70654296875}, {"mcc": 0.6492027956685646, "accuracy": 0.7255859375}, {"mcc": 0.6386305390847437, "accuracy": 0.71533203125}, {"mcc": 0.6451037896969507, "accuracy": 0.7216796875}, {"mcc": 0.6657190714909939, "accuracy": 0.74267578125}, {"mcc": 0.6504153107647268, "accuracy": 0.72509765625}, {"mcc": 0.6560576559835836, "accuracy": 0.73193359375}, {"mcc": 0.6410270764307714, "accuracy": 0.720703125}, {"mcc": 0.6213295113380491, "accuracy": 0.701171875}, {"mcc": 0.6308450613598862, "accuracy": 0.7099609375}], "total": {"test_mcc": 64.25769351833807, "test_mcc_se": 0.8430917856102279, "test_accuracy": 72.0068359375, "test_accuracy_se": 0.7661631409544111}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-fr", "task": "common-sense-reasoning", "dataset_languages": ["fr"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"mcc": 0.3380697775511611, "accuracy": 0.482421875}, {"mcc": 0.2872530352972745, "accuracy": 0.431640625}, {"mcc": 0.28620041957997944, "accuracy": 0.435546875}, {"mcc": 0.3413741233989033, "accuracy": 0.47705078125}, {"mcc": 0.3428682262980513, "accuracy": 0.478515625}, {"mcc": 0.3339850450429944, "accuracy": 0.4716796875}, {"mcc": 0.29771523026540825, "accuracy": 0.4375}, {"mcc": 0.30141944145363025, "accuracy": 0.44775390625}, {"mcc": 0.3074671540073499, "accuracy": 0.451171875}, {"mcc": 0.28827872729361037, "accuracy": 0.4375}], "total": {"test_mcc": 31.246311801883632, "test_mcc_se": 1.4831956286756973, "test_accuracy": 45.5078125, "test_accuracy_se": 1.2521311246750442}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-it", "task": "common-sense-reasoning", "dataset_languages": ["it"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"mcc": 0.34956536397272714, "accuracy": 0.49755859375}, {"mcc": 0.2800773696119513, "accuracy": 0.42919921875}, {"mcc": 0.33956347857920594, "accuracy": 0.49267578125}, {"mcc": 0.30880328056024886, "accuracy": 0.46923828125}, {"mcc": 0.3657701983701746, "accuracy": 0.50537109375}, {"mcc": 0.2845131978342792, "accuracy": 0.453125}, {"mcc": 0.3422125939018012, "accuracy": 0.4873046875}, {"mcc": 0.2909863170720869, "accuracy": 0.44873046875}, {"mcc": 0.3933095376762711, "accuracy": 0.5283203125}, {"mcc": 0.3618942972718582, "accuracy": 0.5087890625}], "total": {"test_mcc": 33.16695634850604, "test_mcc_se": 2.394542341155266, "test_accuracy": 48.203125, "test_accuracy_se": 1.9253963486282457}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "Qwen/Qwen1.5-14B", "results": {"raw": [{"test_speed": 485.64, "test_speed_short": 54.8}, {"test_speed": 944.7, "test_speed_short": 102.75}, {"test_speed": 1413.7, "test_speed_short": 196.91}, {"test_speed": 1860.22, "test_speed_short": 244.79999999999998}, {"test_speed": 2330.64, "test_speed_short": 294.98}, {"test_speed": 2791.23, "test_speed_short": 387.59999999999997}, {"test_speed": 3250.42, "test_speed_short": 438.4}, {"test_speed": 3725.04, "test_speed_short": 487.06}, {"test_speed": 4189.84, "test_speed_short": 533.52}, {"test_speed": 4605.570000000001, "test_speed_short": 578.85}], "total": {"test_speed": 2559.7, "test_speed_se": 864.1963965908984, "test_speed_short": 331.967, "test_speed_short_se": 112.52140885009767}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "swerec", "task": "sentiment-classification", "dataset_languages": ["sv"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"mcc": 0.7926813681198317, "macro_f1": 0.7924339568500908}, {"mcc": 0.7715563044410918, "macro_f1": 0.7637197911852848}, {"mcc": 0.7866453717525971, "macro_f1": 0.7801120996818837}, {"mcc": 0.7716759171263221, "macro_f1": 0.7783445248769253}, {"mcc": 0.7729907661735143, "macro_f1": 0.7900218565288132}, {"mcc": 0.7895588252350716, "macro_f1": 0.7856319359682681}, {"mcc": 0.7918941766277875, "macro_f1": 0.7791338231077645}, {"mcc": 0.7677671089678175, "macro_f1": 0.7815769741324575}, {"mcc": 0.7635036540673134, "macro_f1": 0.7459773430584504}, {"mcc": 0.7582155230982248, "macro_f1": 0.7655534872726735}], "total": {"test_mcc": 77.66489015609572, "test_mcc_se": 0.7762233037688605, "test_macro_f1": 77.62505792662613, "test_macro_f1_se": 0.8725012702049211}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "angry-tweets", "task": "sentiment-classification", "dataset_languages": ["da"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"mcc": 0.5168835608766348, "macro_f1": 0.6812062993339888}, {"mcc": 0.5083497402242507, "macro_f1": 0.6734211782133035}, {"mcc": 0.5518523806686682, "macro_f1": 0.7047425071693626}, {"mcc": 0.5152012951638145, "macro_f1": 0.6793975198324499}, {"mcc": 0.49152945001512977, "macro_f1": 0.6648889306277548}, {"mcc": 0.5162721149460026, "macro_f1": 0.6789690453890476}, {"mcc": 0.49198584816208096, "macro_f1": 0.6603629561784295}, {"mcc": 0.5111329191032269, "macro_f1": 0.6695490846523656}, {"mcc": 0.52992190513491, "macro_f1": 0.6898274438484693}, {"mcc": 0.4674698118609992, "macro_f1": 0.6449014656742247}], "total": {"test_mcc": 51.00599026155718, "test_mcc_se": 1.42251824833515, "test_macro_f1": 67.47266430919396, "test_macro_f1_se": 1.018851482945608}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "norec", "task": "sentiment-classification", "dataset_languages": ["nb", "nn", "no"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"mcc": 0.3320568258399906, "macro_f1": 0.3546463976749594}, {"mcc": 0.3266395630424899, "macro_f1": 0.3499706948335543}, {"mcc": 0.34930841962310777, "macro_f1": 0.36817374612675907}, {"mcc": 0.34591446766595235, "macro_f1": 0.36769257165382685}, {"mcc": 0.3313819607374014, "macro_f1": 0.3519149032053015}, {"mcc": 0.35717437037169625, "macro_f1": 0.3647894872058148}, {"mcc": 0.33238722541620236, "macro_f1": 0.3435459425770903}, {"mcc": 0.3372345751881353, "macro_f1": 0.3508821316444264}, {"mcc": 0.35020334861933056, "macro_f1": 0.36503747787228513}, {"mcc": 0.3468135486021664, "macro_f1": 0.3685723362541309}], "total": {"test_mcc": 34.09114305106473, "test_mcc_se": 0.6335245409473744, "test_macro_f1": 35.852256890481485, "test_macro_f1_se": 0.5749505636385979}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hotter-and-colder-sentiment", "task": "sentiment-classification", "dataset_languages": ["is"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"mcc": 0.25789096390153493, "macro_f1": 0.5033891921132813}, {"mcc": 0.16067684964961693, "macro_f1": 0.4159723208295403}, {"mcc": 0.18053690642201853, "macro_f1": 0.4196705572842918}, {"mcc": 0.22625796425857486, "macro_f1": 0.4692065311143305}, {"mcc": 0.17915342968303466, "macro_f1": 0.4378233648270337}, {"mcc": 0.15432787145107826, "macro_f1": 0.4264432716039124}, {"mcc": 0.20933419229872083, "macro_f1": 0.4665346880780447}, {"mcc": 0.18024242108435037, "macro_f1": 0.40832939568339244}, {"mcc": 0.19963566206014596, "macro_f1": 0.4341388510325685}, {"mcc": 0.13539961387770086, "macro_f1": 0.42295861277636176}], "total": {"test_mcc": 18.834558746867764, "test_mcc_se": 2.2422822377341634, "test_macro_f1": 44.04466785342757, "test_macro_f1_se": 1.8571490363818288}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "sb10k", "task": "sentiment-classification", "dataset_languages": ["de"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"mcc": 0.594549663614684, "macro_f1": 0.7258910378629261}, {"mcc": 0.5644971092445072, "macro_f1": 0.7075938174635984}, {"mcc": 0.5402963060054768, "macro_f1": 0.691290544568235}, {"mcc": 0.5306230797799839, "macro_f1": 0.6890987450596943}, {"mcc": 0.5369026802845698, "macro_f1": 0.691598065523567}, {"mcc": 0.5624977462562705, "macro_f1": 0.7071322351602278}, {"mcc": 0.5448060095740512, "macro_f1": 0.6933026504548957}, {"mcc": 0.507782891813172, "macro_f1": 0.6692886966117001}, {"mcc": 0.6128132100914703, "macro_f1": 0.7417958610797193}, {"mcc": 0.6024332404680384, "macro_f1": 0.7345924505451459}], "total": {"test_mcc": 55.97201937132225, "test_mcc_se": 2.1222156415715685, "test_macro_f1": 70.5158410432971, "test_macro_f1_se": 1.4178452480336894}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "dutch-social", "task": "sentiment-classification", "dataset_languages": ["nl"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"mcc": 0.11548821727211568, "macro_f1": 0.21284836735739252}, {"mcc": 0.09071167467065505, "macro_f1": 0.19061873102897775}, {"mcc": 0.06681545942138101, "macro_f1": 0.18814404543310959}, {"mcc": 0.12054701260944575, "macro_f1": 0.20898187960799627}, {"mcc": 0.0955989246244982, "macro_f1": 0.2072893808403322}, {"mcc": 0.10028188036234234, "macro_f1": 0.19940415927308885}, {"mcc": 0.0994265888451076, "macro_f1": 0.20021100756110424}, {"mcc": 0.1041102109333337, "macro_f1": 0.2066692024851021}, {"mcc": 0.09103201332159148, "macro_f1": 0.2037079947020474}, {"mcc": 0.11922806969436561, "macro_f1": 0.2151123134621299}], "total": {"test_mcc": 10.032400517548364, "test_mcc_se": 0.9984071755727039, "test_macro_f1": 20.329870817512813, "test_macro_f1_se": 0.5488217552066561}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "sst5", "task": "sentiment-classification", "dataset_languages": ["en"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"mcc": 0.655420193310875, "macro_f1": 0.7289491153616735}, {"mcc": 0.6341772274097295, "macro_f1": 0.7047023637397253}, {"mcc": 0.6457409361223615, "macro_f1": 0.7172530570469172}, {"mcc": 0.6661437624826728, "macro_f1": 0.7333518977181138}, {"mcc": 0.6519900660620463, "macro_f1": 0.7222896934421735}, {"mcc": 0.6265822650823288, "macro_f1": 0.7030630983285587}, {"mcc": 0.618458715902594, "macro_f1": 0.7026136938795343}, {"mcc": 0.64645059613286, "macro_f1": 0.7192209101689028}, {"mcc": 0.6756018693880759, "macro_f1": 0.7224392609438075}, {"mcc": 0.6387407547367859, "macro_f1": 0.7127543957631378}], "total": {"test_mcc": 64.5930638663033, "test_mcc_se": 1.08045811903432, "test_macro_f1": 71.66637486392544, "test_macro_f1_se": 0.6669308487159016}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fosent", "task": "sentiment-classification", "dataset_languages": ["fo"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"mcc": 0.38057099719504095, "macro_f1": 0.433680220291099}, {"mcc": 0.34063176162585707, "macro_f1": 0.38326608621788066}, {"mcc": 0.3813697394436712, "macro_f1": 0.43315813670307274}, {"mcc": 0.38199935817720476, "macro_f1": 0.42568542568542567}, {"mcc": 0.28891743187760077, "macro_f1": 0.34448160535117056}, {"mcc": 0.3615419110823245, "macro_f1": 0.40216941079010043}, {"mcc": 0.30186079689834006, "macro_f1": 0.3884249855813837}, {"mcc": 0.37679084332404433, "macro_f1": 0.42270772491466735}, {"mcc": 0.37220616311143545, "macro_f1": 0.4147921698872786}, {"mcc": 0.37255663108698556, "macro_f1": 0.41758271149268505}], "total": {"test_mcc": 35.584456338225046, "test_mcc_se": 2.126108017435473, "test_macro_f1": 40.65948476914764, "test_macro_f1_se": 1.728958047981494}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "allocine", "task": "sentiment-classification", "dataset_languages": ["fr"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"mcc": 0.9445494503490776, "macro_f1": 0.9720500931974817}, {"mcc": 0.9105979076153871, "macro_f1": 0.9544462087418104}, {"mcc": 0.9462848327726914, "macro_f1": 0.97278000838542}, {"mcc": 0.9248710025299569, "macro_f1": 0.9618117611284349}, {"mcc": 0.9316212287828143, "macro_f1": 0.9651573738539887}, {"mcc": 0.9305878445333238, "macro_f1": 0.9647143956536222}, {"mcc": 0.938950569876134, "macro_f1": 0.9691572089741086}, {"mcc": 0.929348879145385, "macro_f1": 0.9642466223527963}, {"mcc": 0.9388667146098095, "macro_f1": 0.9689568481807886}, {"mcc": 0.9283053208039058, "macro_f1": 0.9635192802452589}], "total": {"test_mcc": 93.23983751018487, "test_mcc_se": 0.6491086881106132, "test_macro_f1": 96.5683980071371, "test_macro_f1_se": 0.33499306769988885}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "sentipolc16", "task": "sentiment-classification", "dataset_languages": ["it"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"mcc": 0.4514545871767652, "macro_f1": 0.4772230903491698}, {"mcc": 0.4053558936671472, "macro_f1": 0.44247948815576316}, {"mcc": 0.4384591023412147, "macro_f1": 0.4656007731179647}, {"mcc": 0.4118800654027321, "macro_f1": 0.44585878098917703}, {"mcc": 0.4227692976936877, "macro_f1": 0.4548214948203533}, {"mcc": 0.44051514528540725, "macro_f1": 0.4666447739673157}, {"mcc": 0.4356987435276659, "macro_f1": 0.46352894966925184}, {"mcc": 0.41904991462270413, "macro_f1": 0.44847791839703993}, {"mcc": 0.40824095032762403, "macro_f1": 0.4514524986037756}, {"mcc": 0.4213450670615143, "macro_f1": 0.4455968271651876}], "total": {"test_mcc": 42.54768767106462, "test_mcc_se": 0.9527313339810674, "test_macro_f1": 45.61684595234999, "test_macro_f1_se": 0.71115321085771}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "suc3", "task": "named-entity-recognition", "dataset_languages": ["sv"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"micro_f1_no_misc": 0.4278754501165008, "micro_f1": 0.22818489289740698}, {"micro_f1_no_misc": 0.4631724774878781, "micro_f1": 0.31055718475073313}, {"micro_f1_no_misc": 0.44660596026490074, "micro_f1": 0.2753744757339724}, {"micro_f1_no_misc": 0.43169181890389197, "micro_f1": 0.27781192378610936}, {"micro_f1_no_misc": 0.4414563508481589, "micro_f1": 0.2770584538026398}, {"micro_f1_no_misc": 0.4350940017905103, "micro_f1": 0.27922971114167816}, {"micro_f1_no_misc": 0.4197530864197531, "micro_f1": 0.24953917050691243}, {"micro_f1_no_misc": 0.4026444870974621, "micro_f1": 0.24925149700598806}, {"micro_f1_no_misc": 0.39391993149218585, "micro_f1": 0.22482209624894464}, {"micro_f1_no_misc": 0.44472520530638027, "micro_f1": 0.28768549856808123}], "total": {"test_micro_f1_no_misc": 43.06938769727622, "test_micro_f1_no_misc_se": 1.2915287365074757, "test_micro_f1": 26.59514904442466, "test_micro_f1_se": 1.6857228586737378}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "dansk", "task": "named-entity-recognition", "dataset_languages": ["da"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"micro_f1_no_misc": 0.48487229862475434, "micro_f1": 0.2613852997246346}, {"micro_f1_no_misc": 0.5137180700094607, "micro_f1": 0.24542372881355934}, {"micro_f1_no_misc": 0.5116279069767442, "micro_f1": 0.28990551964196914}, {"micro_f1_no_misc": 0.4758842443729903, "micro_f1": 0.2885231887155375}, {"micro_f1_no_misc": 0.46921797004991683, "micro_f1": 0.23921488448170108}, {"micro_f1_no_misc": 0.4604985213350232, "micro_f1": 0.2620751341681574}, {"micro_f1_no_misc": 0.5065738592420728, "micro_f1": 0.3050385837494326}, {"micro_f1_no_misc": 0.5211097708082025, "micro_f1": 0.30589914367269266}, {"micro_f1_no_misc": 0.49467858663260966, "micro_f1": 0.29056347589952475}, {"micro_f1_no_misc": 0.4737070838765754, "micro_f1": 0.25113073443894035}], "total": {"test_micro_f1_no_misc": 49.11888311928349, "test_micro_f1_no_misc_se": 1.3186682998812447, "test_micro_f1": 27.39159693306149, "test_micro_f1_se": 1.5419330599741294}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "norne-nb", "task": "named-entity-recognition", "dataset_languages": ["nb", "no"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"micro_f1_no_misc": 0.5673523556735235, "micro_f1": 0.3699510070767556}, {"micro_f1_no_misc": 0.493198354950965, "micro_f1": 0.30372720063441716}, {"micro_f1_no_misc": 0.5669520859901621, "micro_f1": 0.37572792918704867}, {"micro_f1_no_misc": 0.5932960893854748, "micro_f1": 0.4622284512964261}, {"micro_f1_no_misc": 0.5546847167794798, "micro_f1": 0.34834229390681}, {"micro_f1_no_misc": 0.5077564637197665, "micro_f1": 0.333811362382791}, {"micro_f1_no_misc": 0.5495278652101463, "micro_f1": 0.3600338286818896}, {"micro_f1_no_misc": 0.5214119333452786, "micro_f1": 0.34524929444967073}, {"micro_f1_no_misc": 0.5672753057968446, "micro_f1": 0.39371777964272864}, {"micro_f1_no_misc": 0.5196242871519624, "micro_f1": 0.34400087979764654}], "total": {"test_micro_f1_no_misc": 54.410794580036026, "test_micro_f1_no_misc_se": 1.9802980803147119, "test_micro_f1": 36.36790027056184, "test_micro_f1_se": 2.6345228782807992}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "norne-nn", "task": "named-entity-recognition", "dataset_languages": ["nn"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"micro_f1_no_misc": 0.5503826530612245, "micro_f1": 0.3991670320035072}, {"micro_f1_no_misc": 0.5566133968038083, "micro_f1": 0.3552331606217617}, {"micro_f1_no_misc": 0.5485026041666666, "micro_f1": 0.34550311665182554}, {"micro_f1_no_misc": 0.5682277890150926, "micro_f1": 0.3869628399418726}, {"micro_f1_no_misc": 0.5581537521014825, "micro_f1": 0.45734265734265733}, {"micro_f1_no_misc": 0.5403864428254672, "micro_f1": 0.3417184571372201}, {"micro_f1_no_misc": 0.5405152224824357, "micro_f1": 0.4270749395648671}, {"micro_f1_no_misc": 0.5552188552188552, "micro_f1": 0.41216925049539577}, {"micro_f1_no_misc": 0.5460555972952668, "micro_f1": 0.3357269420871025}, {"micro_f1_no_misc": 0.5438624939192477, "micro_f1": 0.4057070532735013}], "total": {"test_micro_f1_no_misc": 55.07918806889547, "test_micro_f1_no_misc_se": 0.5471783016789857, "test_micro_f1": 38.6660544911971, "test_micro_f1_se": 2.5383399112505933}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mim-gold-ner", "task": "named-entity-recognition", "dataset_languages": ["is"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"micro_f1_no_misc": 0.4190268700072622, "micro_f1": 0.22978009690644804}, {"micro_f1_no_misc": 0.3902439024390244, "micro_f1": 0.14947683109118087}, {"micro_f1_no_misc": 0.46253391776247127, "micro_f1": 0.29306542514089684}, {"micro_f1_no_misc": 0.3559687863191101, "micro_f1": 0.19003909569947305}, {"micro_f1_no_misc": 0.3532320734722713, "micro_f1": 0.19433198380566805}, {"micro_f1_no_misc": 0.3446617505610773, "micro_f1": 0.2077099102270727}, {"micro_f1_no_misc": 0.432851029142042, "micro_f1": 0.2305579236525528}, {"micro_f1_no_misc": 0.4928171414657901, "micro_f1": 0.22875412541254128}, {"micro_f1_no_misc": 0.41004497751124436, "micro_f1": 0.2136720891975873}, {"micro_f1_no_misc": 0.3623853211009175, "micro_f1": 0.201980198019802}], "total": {"test_micro_f1_no_misc": 40.237657697812104, "test_micro_f1_no_misc_se": 3.113426325466001, "test_micro_f1": 21.39367679153223, "test_micro_f1_se": 2.291375664361263}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fone", "task": "named-entity-recognition", "dataset_languages": ["fo"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"micro_f1_no_misc": 0.6147747747747748, "micro_f1": 0.4177569485126603}, {"micro_f1_no_misc": 0.6221522504167438, "micro_f1": 0.4378411996406606}, {"micro_f1_no_misc": 0.6164574616457462, "micro_f1": 0.4661819179585678}, {"micro_f1_no_misc": 0.6288267731507892, "micro_f1": 0.37966529838964314}, {"micro_f1_no_misc": 0.6398069067953954, "micro_f1": 0.45355577548423576}, {"micro_f1_no_misc": 0.6651962135833104, "micro_f1": 0.47164966340481645}, {"micro_f1_no_misc": 0.6408902301018484, "micro_f1": 0.47577028786344566}, {"micro_f1_no_misc": 0.6465278398427027, "micro_f1": 0.5309640831758033}, {"micro_f1_no_misc": 0.6426661914097309, "micro_f1": 0.41949588477366256}, {"micro_f1_no_misc": 0.6787550238340032, "micro_f1": 0.5638588015273124}], "total": {"test_micro_f1_no_misc": 63.96053665555045, "test_micro_f1_no_misc_se": 1.276884973281895, "test_micro_f1": 46.16739860730809, "test_micro_f1_se": 3.3713788834513188}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "germeval", "task": "named-entity-recognition", "dataset_languages": ["de"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"micro_f1_no_misc": 0.6057763515181437, "micro_f1": 0.4704174228675136}, {"micro_f1_no_misc": 0.6109279321714555, "micro_f1": 0.5027932960893855}, {"micro_f1_no_misc": 0.5947575968452795, "micro_f1": 0.42489679263258184}, {"micro_f1_no_misc": 0.5932033983008496, "micro_f1": 0.4173650687227623}, {"micro_f1_no_misc": 0.5940108001963672, "micro_f1": 0.4908479641389615}, {"micro_f1_no_misc": 0.602375296912114, "micro_f1": 0.4474439007929813}, {"micro_f1_no_misc": 0.6017838405036726, "micro_f1": 0.45461365341335336}, {"micro_f1_no_misc": 0.6144957983193278, "micro_f1": 0.5038744287701172}, {"micro_f1_no_misc": 0.6084280303030303, "micro_f1": 0.44373530399731276}, {"micro_f1_no_misc": 0.5723734418722971, "micro_f1": 0.3903862660944206}], "total": {"test_micro_f1_no_misc": 59.981324869425364, "test_micro_f1_no_misc_se": 0.7492554101287464, "test_micro_f1": 45.4637409751939, "test_micro_f1_se": 2.344368182743319}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "conll-nl", "task": "named-entity-recognition", "dataset_languages": ["nl"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"micro_f1_no_misc": 0.6393982134461683, "micro_f1": 0.3695856873822975}, {"micro_f1_no_misc": 0.5781333333333333, "micro_f1": 0.42016806722689076}, {"micro_f1_no_misc": 0.6223021582733813, "micro_f1": 0.37575476079888526}, {"micro_f1_no_misc": 0.5715686274509804, "micro_f1": 0.34623707622024524}, {"micro_f1_no_misc": 0.6032183908045977, "micro_f1": 0.36185133239831696}, {"micro_f1_no_misc": 0.5893946503988737, "micro_f1": 0.34484401047868535}, {"micro_f1_no_misc": 0.6129193790686029, "micro_f1": 0.36649214659685864}, {"micro_f1_no_misc": 0.5662525879917184, "micro_f1": 0.37341259119156983}, {"micro_f1_no_misc": 0.6136842105263158, "micro_f1": 0.33130328867235076}, {"micro_f1_no_misc": 0.5842696629213483, "micro_f1": 0.4122833688051079}], "total": {"test_micro_f1_no_misc": 59.811412142153195, "test_micro_f1_no_misc_se": 1.4846437485947535, "test_micro_f1": 37.019323297712084, "test_micro_f1_se": 1.7436683584788712}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "conll-en", "task": "named-entity-recognition", "dataset_languages": ["en"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"micro_f1_no_misc": 0.7095540118795933, "micro_f1": 0.6298565075102796}, {"micro_f1_no_misc": 0.7336587233228059, "micro_f1": 0.6535864643736595}, {"micro_f1_no_misc": 0.7423606950269622, "micro_f1": 0.648737022807161}, {"micro_f1_no_misc": 0.6855001441337562, "micro_f1": 0.6162958219945575}, {"micro_f1_no_misc": 0.707070707070707, "micro_f1": 0.6184178099071717}, {"micro_f1_no_misc": 0.727366787377134, "micro_f1": 0.6590223295111647}, {"micro_f1_no_misc": 0.6890302679776363, "micro_f1": 0.6327308097592608}, {"micro_f1_no_misc": 0.7256940938919738, "micro_f1": 0.6351250928907605}, {"micro_f1_no_misc": 0.7113575601544707, "micro_f1": 0.6422203532380151}, {"micro_f1_no_misc": 0.7321156773211568, "micro_f1": 0.6416706217370669}], "total": {"test_micro_f1_no_misc": 71.63708668156197, "test_micro_f1_no_misc_se": 1.1848013352588558, "test_micro_f1": 63.77662833729097, "test_micro_f1_se": 0.8735577096187613}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "eltec", "task": "named-entity-recognition", "dataset_languages": ["fr"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"micro_f1_no_misc": 0.5641784732501586, "micro_f1": 0.4989858012170386}, {"micro_f1_no_misc": 0.5511811023622047, "micro_f1": 0.481025818748917}, {"micro_f1_no_misc": 0.5690200210748156, "micro_f1": 0.48991354466858783}, {"micro_f1_no_misc": 0.5764421137555467, "micro_f1": 0.5121341898643825}, {"micro_f1_no_misc": 0.5637834474175483, "micro_f1": 0.5111358574610244}, {"micro_f1_no_misc": 0.5252854812398043, "micro_f1": 0.44284490145672667}, {"micro_f1_no_misc": 0.5648979591836735, "micro_f1": 0.460328317373461}, {"micro_f1_no_misc": 0.5536793829476756, "micro_f1": 0.4667143879742304}, {"micro_f1_no_misc": 0.5084409136047666, "micro_f1": 0.40858794384805946}, {"micro_f1_no_misc": 0.513160538476994, "micro_f1": 0.40558532229258}], "total": {"test_micro_f1_no_misc": 54.900694333131874, "test_micro_f1_no_misc_se": 1.5145628750250673, "test_micro_f1": 46.77256084905007, "test_micro_f1_se": 2.4037377442798555}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "multinerd-it", "task": "named-entity-recognition", "dataset_languages": ["it"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"micro_f1_no_misc": 0.48487240199947385, "micro_f1": 0.321652719665272}, {"micro_f1_no_misc": 0.5534634257674313, "micro_f1": 0.3892905999609146}, {"micro_f1_no_misc": 0.5431613052534136, "micro_f1": 0.3640579710144927}, {"micro_f1_no_misc": 0.4931890515595162, "micro_f1": 0.4281793229643184}, {"micro_f1_no_misc": 0.5581672831404366, "micro_f1": 0.3668483614631904}, {"micro_f1_no_misc": 0.5516723036452462, "micro_f1": 0.4395571955719557}, {"micro_f1_no_misc": 0.4762024996843833, "micro_f1": 0.33028929050386363}, {"micro_f1_no_misc": 0.556033920417482, "micro_f1": 0.45144026444199586}, {"micro_f1_no_misc": 0.5259786476868328, "micro_f1": 0.36353916065223196}, {"micro_f1_no_misc": 0.5342756183745583, "micro_f1": 0.34669870984062734}], "total": {"test_micro_f1_no_misc": 52.77016457528774, "test_micro_f1_no_misc_se": 1.9525757049824086, "test_micro_f1": 38.015535960788625, "test_micro_f1_se": 2.8283146944096442}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-sv", "task": "linguistic-acceptability", "dataset_languages": ["sv"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"mcc": 0.3761414104761431, "macro_f1": 0.6877946743841346}, {"mcc": 0.40926136734612045, "macro_f1": 0.702406196053261}, {"mcc": 0.42387890842839343, "macro_f1": 0.7001517453640196}, {"mcc": 0.4086379779563115, "macro_f1": 0.6997688937946259}, {"mcc": 0.46474928908626356, "macro_f1": 0.7323718499017532}, {"mcc": 0.41748745443013646, "macro_f1": 0.698067818260727}, {"mcc": 0.40960373747188833, "macro_f1": 0.702032674408365}, {"mcc": 0.4314751917426263, "macro_f1": 0.7123834581509791}, {"mcc": 0.45681408506637006, "macro_f1": 0.7237452812674052}, {"mcc": 0.4142850863136174, "macro_f1": 0.7054806271572739}], "total": {"test_mcc": 42.12334508317871, "test_mcc_se": 1.574022610732255, "test_macro_f1": 70.64203218742544, "test_macro_f1_se": 0.812253447222622}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-da", "task": "linguistic-acceptability", "dataset_languages": ["da"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"mcc": 0.372048953123843, "macro_f1": 0.6588679245283019}, {"mcc": 0.3774335050294566, "macro_f1": 0.6502260129676218}, {"mcc": 0.3607249995155614, "macro_f1": 0.6736454530048837}, {"mcc": 0.3584480201899263, "macro_f1": 0.6380843508605025}, {"mcc": 0.3501076744699547, "macro_f1": 0.6684321070314809}, {"mcc": 0.3488571593432489, "macro_f1": 0.6410721174411969}, {"mcc": 0.4070576228167507, "macro_f1": 0.6970267243191377}, {"mcc": 0.35262117627900386, "macro_f1": 0.6543272208507378}, {"mcc": 0.395928679809409, "macro_f1": 0.6961234842408356}, {"mcc": 0.3446073223003799, "macro_f1": 0.6494547770262644}], "total": {"test_mcc": 36.67835112877535, "test_mcc_se": 1.3083920991867986, "test_macro_f1": 66.27260172270964, "test_macro_f1_se": 1.2964389719598706}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-nb", "task": "linguistic-acceptability", "dataset_languages": ["nb", "no"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"mcc": 0.3192910936188034, "macro_f1": 0.5820080414468589}, {"mcc": 0.3171157676385038, "macro_f1": 0.579288456562264}, {"mcc": 0.37704427455265704, "macro_f1": 0.6520620214887731}, {"mcc": 0.30795933216444854, "macro_f1": 0.5908312458231442}, {"mcc": 0.35571913302597963, "macro_f1": 0.6185062431443641}, {"mcc": 0.3168618754567928, "macro_f1": 0.6043728001196321}, {"mcc": 0.28056281895133617, "macro_f1": 0.5623773271036875}, {"mcc": 0.3045339357009835, "macro_f1": 0.5634137084248206}, {"mcc": 0.33101485417770016, "macro_f1": 0.6022008065174242}, {"mcc": 0.3210868425999893, "macro_f1": 0.5835665879796281}], "total": {"test_mcc": 32.31189927887194, "test_mcc_se": 1.6673791216934721, "test_macro_f1": 59.386272386105965, "test_macro_f1_se": 1.6734852954952633}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-nn", "task": "linguistic-acceptability", "dataset_languages": ["nn"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"mcc": 0.19026947865809732, "macro_f1": 0.5595079668481834}, {"mcc": 0.20468472817913058, "macro_f1": 0.5767203281107234}, {"mcc": 0.17354306887966547, "macro_f1": 0.58497706633286}, {"mcc": 0.16559905797622987, "macro_f1": 0.5563949489460505}, {"mcc": 0.17505825157546415, "macro_f1": 0.5845667680907956}, {"mcc": 0.14917625601553167, "macro_f1": 0.5704114997840706}, {"mcc": 0.2456162672195668, "macro_f1": 0.6217044470285351}, {"mcc": 0.1817448704223154, "macro_f1": 0.5700008398421097}, {"mcc": 0.1723871254745117, "macro_f1": 0.5802922523801441}, {"mcc": 0.16687196107406305, "macro_f1": 0.5831015897296994}], "total": {"test_mcc": 18.24951065474576, "test_mcc_se": 1.6547510734846782, "test_macro_f1": 57.876777070931716, "test_macro_f1_se": 1.1238870627869761}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-is", "task": "linguistic-acceptability", "dataset_languages": ["is"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"mcc": 0.009059543767237214, "macro_f1": 0.41156561799790464}, {"mcc": 0.08750795159526945, "macro_f1": 0.467847450083239}, {"mcc": 0.0851339307892741, "macro_f1": 0.5417986787686814}, {"mcc": 0.0563229439578128, "macro_f1": 0.4595522135906005}, {"mcc": 0.06921382481205639, "macro_f1": 0.5273893441392766}, {"mcc": 0.08551330622742838, "macro_f1": 0.5078623115772865}, {"mcc": 0.04588956073574698, "macro_f1": 0.4536234444957049}, {"mcc": 0.06605730066475209, "macro_f1": 0.5236012032556702}, {"mcc": 0.06787369011152368, "macro_f1": 0.5138360163916774}, {"mcc": 0.03977182136216832, "macro_f1": 0.5074089923121191}], "total": {"test_mcc": 6.123438740232693, "test_mcc_se": 1.5216368316230844, "test_macro_f1": 49.14485272612161, "test_macro_f1_se": 2.5532233921736798}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-fo", "task": "linguistic-acceptability", "dataset_languages": ["fo"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"mcc": 0.12393779461813234, "macro_f1": 0.4613409929317676}, {"mcc": 0.058460885504074155, "macro_f1": 0.45347565738089035}, {"mcc": 0.08706378053030897, "macro_f1": 0.4985780536944693}, {"mcc": 0.05303982166692776, "macro_f1": 0.4254744024822937}, {"mcc": 0.09253507905410216, "macro_f1": 0.5321465883263636}, {"mcc": 0.10711421215468767, "macro_f1": 0.5269952488362049}, {"mcc": 0.04363699989662668, "macro_f1": 0.3823105733218093}, {"mcc": 0.13578090752936633, "macro_f1": 0.45440034825509684}, {"mcc": 0.08488980082308481, "macro_f1": 0.44886076362272276}, {"mcc": 0.09818442911605815, "macro_f1": 0.543256006854916}], "total": {"test_mcc": 8.846437108933692, "test_mcc_se": 1.8634809217006376, "test_macro_f1": 47.268386357065346, "test_macro_f1_se": 3.1961281966904433}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-de", "task": "linguistic-acceptability", "dataset_languages": ["de"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"mcc": 0.46378911857557387, "macro_f1": 0.7318552786627679}, {"mcc": 0.43146699586813775, "macro_f1": 0.7156508271856222}, {"mcc": 0.41157301653014855, "macro_f1": 0.6991332451887762}, {"mcc": 0.40943844441434596, "macro_f1": 0.7047131617141547}, {"mcc": 0.3607233864786451, "macro_f1": 0.6796747078318799}, {"mcc": 0.4561303087619599, "macro_f1": 0.7279385442416828}, {"mcc": 0.39486896434172014, "macro_f1": 0.6972422376835369}, {"mcc": 0.4060675635597723, "macro_f1": 0.6977471676819123}, {"mcc": 0.38639113860912805, "macro_f1": 0.6926951792539093}, {"mcc": 0.4254866399355863, "macro_f1": 0.7083544303797469}], "total": {"test_mcc": 41.45935577075018, "test_mcc_se": 1.9318901914189808, "test_macro_f1": 70.55004779823989, "test_macro_f1_se": 0.9929026584596103}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-nl", "task": "linguistic-acceptability", "dataset_languages": ["nl"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"mcc": 0.4844381030029335, "macro_f1": 0.7176189803256552}, {"mcc": 0.43077986906953997, "macro_f1": 0.6725518540776715}, {"mcc": 0.502374484579543, "macro_f1": 0.7219299566328737}, {"mcc": 0.4438816535233403, "macro_f1": 0.6803299549956519}, {"mcc": 0.4736041874808159, "macro_f1": 0.7116481701945723}, {"mcc": 0.4203191778391337, "macro_f1": 0.657159802369462}, {"mcc": 0.4253602017110689, "macro_f1": 0.6571300721744531}, {"mcc": 0.4482545302817099, "macro_f1": 0.6828616975993844}, {"mcc": 0.43218236319080994, "macro_f1": 0.6757195724667542}, {"mcc": 0.5104001147756332, "macro_f1": 0.7290510487680862}], "total": {"test_mcc": 45.71594685454528, "test_mcc_se": 2.04914065827797, "test_macro_f1": 69.06001109604564, "test_macro_f1_se": 1.6764208483604537}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-en", "task": "linguistic-acceptability", "dataset_languages": ["en"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"mcc": 0.37994994063799875, "macro_f1": 0.6837787182950623}, {"mcc": 0.38652436697082515, "macro_f1": 0.6931132395118995}, {"mcc": 0.39781404338668563, "macro_f1": 0.6980898757403056}, {"mcc": 0.3773116923095867, "macro_f1": 0.6874973177679633}, {"mcc": 0.3195933540368825, "macro_f1": 0.6588869077844677}, {"mcc": 0.4354740472771271, "macro_f1": 0.7057175928098831}, {"mcc": 0.3326797747519108, "macro_f1": 0.6607622487059106}, {"mcc": 0.38445615462331445, "macro_f1": 0.6918938701259687}, {"mcc": 0.3295994709583128, "macro_f1": 0.6641695764908011}, {"mcc": 0.3622926969564304, "macro_f1": 0.6811461860754662}], "total": {"test_mcc": 37.05695541909074, "test_mcc_se": 2.1978583579954787, "test_macro_f1": 68.25055533307727, "test_macro_f1_se": 1.007392477732901}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-fr", "task": "linguistic-acceptability", "dataset_languages": ["fr"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"mcc": 0.35977618552209517, "macro_f1": 0.6415166254575875}, {"mcc": 0.36845591693877494, "macro_f1": 0.6556889575854619}, {"mcc": 0.39191168471605803, "macro_f1": 0.6767562337003339}, {"mcc": 0.3850780824789281, "macro_f1": 0.6562587932132438}, {"mcc": 0.38851650481868283, "macro_f1": 0.6459927360002042}, {"mcc": 0.4089385703593249, "macro_f1": 0.6772179128382725}, {"mcc": 0.4691536729746736, "macro_f1": 0.7294070732437805}, {"mcc": 0.32325315304380686, "macro_f1": 0.6506056396383222}, {"mcc": 0.368746471302885, "macro_f1": 0.660484060680631}, {"mcc": 0.4057374705843154, "macro_f1": 0.6842334949199067}], "total": {"test_mcc": 38.69567712739544, "test_mcc_se": 2.3640909835147195, "test_macro_f1": 66.78161527277744, "test_macro_f1_se": 1.6043049535848304}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-it", "task": "linguistic-acceptability", "dataset_languages": ["it"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"mcc": 0.32265390313448156, "macro_f1": 0.6551389330947219}, {"mcc": 0.3331900385323533, "macro_f1": 0.6583604803046756}, {"mcc": 0.2601869782963701, "macro_f1": 0.607511386504485}, {"mcc": 0.26519927898563117, "macro_f1": 0.6192606363547586}, {"mcc": 0.20422126923014663, "macro_f1": 0.5973485709885344}, {"mcc": 0.26108293495138896, "macro_f1": 0.6283147507535314}, {"mcc": 0.2639505160875011, "macro_f1": 0.6279922204425222}, {"mcc": 0.3370940629957842, "macro_f1": 0.6643942563687415}, {"mcc": 0.27645587768494184, "macro_f1": 0.6024132907293039}, {"mcc": 0.3098329414282891, "macro_f1": 0.6547251450135028}], "total": {"test_mcc": 28.338678013268883, "test_mcc_se": 2.585880175367587, "test_macro_f1": 63.154596705547775, "test_macro_f1_se": 1.5545717046469216}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"f1": 63.740098798429685, "em": 53.78787878787879}, {"f1": 62.65904812102669, "em": 53.449583017437455}, {"f1": 63.13457681911203, "em": 54.15695415695416}, {"f1": 62.71633737959694, "em": 52.96803652968037}, {"f1": 64.23012617461102, "em": 55.43562066306862}, {"f1": 62.55719761637556, "em": 52.595419847328245}, {"f1": 61.70306202903896, "em": 51.87207488299532}, {"f1": 62.204350383986224, "em": 52.20125786163522}, {"f1": 63.134690323175725, "em": 55.3405572755418}, {"f1": 64.67038413252995, "em": 54.01234567901235}], "total": {"test_f1": 63.07498717778829, "test_f1_se": 0.5672744934169895, "test_em": 53.58197287015323, "test_em_se": 0.7530274642942372}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "norquad", "task": "reading-comprehension", "dataset_languages": ["nb", "nn", "no"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"f1": 69.8017072161618, "em": 38.970588235294116}, {"f1": 64.17171481887158, "em": 35.52845528455285}, {"f1": 67.79039952679452, "em": 38.55120732722731}, {"f1": 64.9194671907821, "em": 37.69419460343418}, {"f1": 68.6744143732362, "em": 37.13813068651778}, {"f1": 62.36305644068215, "em": 34.5679012345679}, {"f1": 63.792032827617746, "em": 37.47899159663866}, {"f1": 66.1841233023756, "em": 38.682432432432435}, {"f1": 61.10063597478026, "em": 33.970099667774086}, {"f1": 62.95255599435626, "em": 33.36092715231788}], "total": {"test_f1": 65.17501076656583, "test_f1_se": 1.7765641974628499, "test_em": 36.59429282207572, "test_em_se": 1.2853936523027487}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scandiqa-sv", "task": "reading-comprehension", "dataset_languages": ["sv"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"f1": 63.726971114618784, "em": 55.0}, {"f1": 62.70674395581377, "em": 52.23654283548142}, {"f1": 63.673193144279274, "em": 53.69075369075369}, {"f1": 63.35732409086869, "em": 53.729071537290714}, {"f1": 63.05013037614576, "em": 54.04780262143408}, {"f1": 62.35701826828659, "em": 53.969465648854964}, {"f1": 62.09495582120463, "em": 52.418096723868956}, {"f1": 64.19188348652676, "em": 55.11006289308176}, {"f1": 63.2427463402671, "em": 53.328173374613}, {"f1": 63.390653201669636, "em": 54.089506172839506}], "total": {"test_f1": 63.1791619799681, "test_f1_se": 0.3996312928729023, "test_em": 53.76194754982181, "test_em_se": 0.5810394352863227}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "nqii", "task": "reading-comprehension", "dataset_languages": ["is"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"f1": 52.31999650997804, "em": 18.76923076923077}, {"f1": 54.42549167137479, "em": 22.274143302180686}, {"f1": 46.74084449825807, "em": 23.67601246105919}, {"f1": 48.129728417005104, "em": 24.364723467862483}, {"f1": 49.941139425934715, "em": 19.435736677115987}, {"f1": 52.65168492267859, "em": 27.5808936825886}, {"f1": 52.53302788029689, "em": 25.692307692307693}, {"f1": 46.31893646181303, "em": 22.85276073619632}, {"f1": 50.29482531369545, "em": 20.180722891566266}, {"f1": 52.83045088186003, "em": 27.710843373493976}], "total": {"test_f1": 50.618612598289474, "test_f1_se": 1.7330002064418564, "test_em": 23.2537375053602, "test_em_se": 1.9730346572424946}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "foqa", "task": "reading-comprehension", "dataset_languages": ["fo"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"f1": 55.24700285864507, "em": 35.78431372549019}, {"f1": 42.68158813855914, "em": 19.41031941031941}, {"f1": 66.82334104224084, "em": 48.148148148148145}, {"f1": 63.72824740768927, "em": 45.38834951456311}, {"f1": 65.59727749845153, "em": 44.801980198019805}, {"f1": 55.41479693507077, "em": 35.30864197530864}, {"f1": 55.72266236754215, "em": 35.37735849056604}, {"f1": 65.59318550655173, "em": 44.020356234096695}, {"f1": 65.96737545667553, "em": 46.88279301745636}, {"f1": 64.74079112157959, "em": 44.22604422604422}], "total": {"test_f1": 60.15162683330057, "test_f1_se": 4.813270729499215, "test_em": 39.93483049400127, "test_em_se": 5.406992894777495}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "germanquad", "task": "reading-comprehension", "dataset_languages": ["de"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"f1": 59.10875425957474, "em": 30.454545454545453}, {"f1": 55.619274038416975, "em": 27.824109173616375}, {"f1": 60.22478283188871, "em": 29.13752913752914}, {"f1": 59.36425632710156, "em": 31.735159817351597}, {"f1": 59.55232131122961, "em": 30.06939090208173}, {"f1": 60.43408326047825, "em": 29.770992366412212}, {"f1": 54.41658471555465, "em": 27.067082683307333}, {"f1": 59.86483332506186, "em": 30.50314465408805}, {"f1": 58.997531911063874, "em": 28.86996904024768}, {"f1": 52.311826751705645, "em": 27.006172839506174}], "total": {"test_f1": 57.98942487320759, "test_f1_se": 1.7495462030083657, "test_em": 29.243809606868574, "test_em_se": 0.9719065827946703}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "squad", "task": "reading-comprehension", "dataset_languages": ["en"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"f1": 83.09671328945011, "em": 66.66666666666667}, {"f1": 79.49569288509184, "em": 58.605003790750565}, {"f1": 81.75556088608803, "em": 62.16006216006216}, {"f1": 82.29770573946875, "em": 64.53576864535769}, {"f1": 85.40200962435605, "em": 68.38858905165768}, {"f1": 80.2787334614805, "em": 61.908396946564885}, {"f1": 81.00498571877797, "em": 61.93447737909516}, {"f1": 80.8396147926319, "em": 59.5125786163522}, {"f1": 81.37615039837857, "em": 62.77089783281734}, {"f1": 79.78216620074457, "em": 61.03395061728395}], "total": {"test_f1": 81.53293329964683, "test_f1_se": 1.085834312676897, "test_em": 62.75163917066082, "test_em_se": 1.8782789019274146}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "squad-nl", "task": "reading-comprehension", "dataset_languages": ["nl"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"f1": 74.68715124155372, "em": 55.45454545454545}, {"f1": 74.42511757753529, "em": 55.799848369977255}, {"f1": 72.84694403198029, "em": 56.56565656565657}, {"f1": 72.52382600046808, "em": 56.0882800608828}, {"f1": 72.65438179280241, "em": 54.81881264456438}, {"f1": 75.32401806417052, "em": 56.030534351145036}, {"f1": 71.55953719549892, "em": 54.29017160686428}, {"f1": 73.43380949971998, "em": 56.44654088050314}, {"f1": 72.76368444623604, "em": 57.6625386996904}, {"f1": 74.37060997333268, "em": 56.25}], "total": {"test_f1": 73.4589079823298, "test_f1_se": 0.737418994225499, "test_em": 55.94069286338292, "test_em_se": 0.5834728088382651}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "squad-it", "task": "reading-comprehension", "dataset_languages": ["it"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"f1": 68.5828498879579, "em": 49.24242424242424}, {"f1": 69.26281006154333, "em": 45.86808188021228}, {"f1": 69.52412422118401, "em": 50.427350427350426}, {"f1": 67.91650508344021, "em": 46.04261796042618}, {"f1": 67.96069877015955, "em": 46.18350038550501}, {"f1": 68.68206055309628, "em": 48.16793893129771}, {"f1": 69.59814960555727, "em": 50.15600624024961}, {"f1": 68.21003886865954, "em": 46.776729559748425}, {"f1": 68.8660984773941, "em": 50.309597523219814}, {"f1": 69.27235509487964, "em": 47.76234567901235}], "total": {"test_f1": 68.78756906238718, "test_f1_se": 0.3862765437829778, "test_em": 48.09365928294461, "test_em_se": 1.1426367594315696}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fquad", "task": "reading-comprehension", "dataset_languages": ["fr"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"f1": 67.98024956269894, "em": 36.666666666666664}, {"f1": 69.15758479638802, "em": 39.8786959818044}, {"f1": 70.3561295778729, "em": 40.48174048174048}, {"f1": 71.104497025969, "em": 40.25875190258752}, {"f1": 65.30741010135846, "em": 37.7023901310717}, {"f1": 70.99061448970254, "em": 39.541984732824424}, {"f1": 67.11796775984851, "em": 37.597503900156006}, {"f1": 67.38879040229452, "em": 39.62264150943396}, {"f1": 66.58183981251821, "em": 39.31888544891641}, {"f1": 70.23759882134193, "em": 39.96913580246913}], "total": {"test_f1": 68.62226823499931, "test_f1_se": 1.2570660282074364, "test_em": 39.10383965576707, "test_em_se": 0.8071209307675844}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "nordjylland-news", "task": "summarization", "dataset_languages": ["da"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"bertscore": 0.6462992292363197, "rouge_l": 0.14748115796457123}, {"bertscore": 0.6452838438999606, "rouge_l": 0.1478589941278839}, {"bertscore": 0.6421404247666942, "rouge_l": 0.13747483009837294}, {"bertscore": 0.6434881862805923, "rouge_l": 0.14137515985523183}, {"bertscore": 0.6474899801978609, "rouge_l": 0.14893260785579993}, {"bertscore": 0.6451741092896555, "rouge_l": 0.14461044610438673}, {"bertscore": 0.6469040165102342, "rouge_l": 0.14613285477433896}, {"bertscore": 0.6435038053459721, "rouge_l": 0.1388007260700792}, {"bertscore": 0.6439872249175096, "rouge_l": 0.13947501828800707}, {"bertscore": 0.6474219494994031, "rouge_l": 0.14811242617895265}], "total": {"test_bertscore": 64.51692769944202, "test_bertscore_se": 0.11490221029617695, "test_rouge_l": 14.402542213176245, "test_rouge_l_se": 0.2694901038374075}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mlsum", "task": "summarization", "dataset_languages": ["de"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"bertscore": 0.637860599468695, "rouge_l": 0.1298571432423006}, {"bertscore": 0.6379262570699211, "rouge_l": 0.13233336190892364}, {"bertscore": 0.6362492429034319, "rouge_l": 0.1267843931865959}, {"bertscore": 0.6371789742261171, "rouge_l": 0.13450695873899837}, {"bertscore": 0.637028075667331, "rouge_l": 0.12492736890704545}, {"bertscore": 0.6359008524450473, "rouge_l": 0.13066834169417274}, {"bertscore": 0.6387121574953198, "rouge_l": 0.13373554888174885}, {"bertscore": 0.6385723138519097, "rouge_l": 0.13130844277002957}, {"bertscore": 0.6384117885609157, "rouge_l": 0.12864086206619424}, {"bertscore": 0.6362702340411488, "rouge_l": 0.13159420704799937}], "total": {"test_bertscore": 63.741104957298376, "test_bertscore_se": 0.06415622593718015, "test_rouge_l": 13.043566284440086, "test_rouge_l_se": 0.18531317825610766}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "rrn", "task": "summarization", "dataset_languages": ["is"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"bertscore": 0.6331175101804547, "rouge_l": 0.14036570371928275}, {"bertscore": 0.6281554188462906, "rouge_l": 0.1368783236258222}, {"bertscore": 0.6302590199920814, "rouge_l": 0.14004400080887758}, {"bertscore": 0.627529741905164, "rouge_l": 0.13717335882503318}, {"bertscore": 0.6314399475522805, "rouge_l": 0.13666649865158692}, {"bertscore": 0.6324098229233641, "rouge_l": 0.14317043908924193}, {"bertscore": 0.6281575529428665, "rouge_l": 0.13689886909440835}, {"bertscore": 0.627402212965535, "rouge_l": 0.13399011001741024}, {"bertscore": 0.6266159455990419, "rouge_l": 0.13275099153834136}, {"bertscore": 0.6264051158213988, "rouge_l": 0.13645003719644944}], "total": {"test_bertscore": 62.914922887284774, "test_bertscore_se": 0.15245407162282645, "test_rouge_l": 13.743883325664541, "test_rouge_l_se": 0.18971966831494083}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "no-sammendrag", "task": "summarization", "dataset_languages": ["nb", "nn", "no"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"bertscore": 0.6258150516659953, "rouge_l": 0.1084442095036203}, {"bertscore": 0.6258856837230269, "rouge_l": 0.10767866459574604}, {"bertscore": 0.6262480283039622, "rouge_l": 0.10823625930938652}, {"bertscore": 0.6256152082642075, "rouge_l": 0.11157087617337308}, {"bertscore": 0.62591573264217, "rouge_l": 0.10849216879917387}, {"bertscore": 0.6238673801562982, "rouge_l": 0.10650220310050224}, {"bertscore": 0.6238502449559746, "rouge_l": 0.10326740559352846}, {"bertscore": 0.6267618454294279, "rouge_l": 0.11047364535000156}, {"bertscore": 0.6260831207036972, "rouge_l": 0.11184257088704568}, {"bertscore": 0.6219231088471133, "rouge_l": 0.10292726706562422}], "total": {"test_bertscore": 62.519654046918724, "test_bertscore_se": 0.09282475472146566, "test_rouge_l": 10.794352703780019, "test_rouge_l_se": 0.1900016772203977}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "wiki-lingua-nl", "task": "summarization", "dataset_languages": ["nl"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"bertscore": 0.6634691519575426, "rouge_l": 0.16264731528637527}, {"bertscore": 0.6600964888057206, "rouge_l": 0.15551538557281697}, {"bertscore": 0.6528107980411733, "rouge_l": 0.14917658714403892}, {"bertscore": 0.6673954397701891, "rouge_l": 0.16178014889250386}, {"bertscore": 0.6615231700416189, "rouge_l": 0.1578310843055128}, {"bertscore": 0.6653650776133873, "rouge_l": 0.161019845034487}, {"bertscore": 0.6658757641853299, "rouge_l": 0.165426196284103}, {"bertscore": 0.6659749361569993, "rouge_l": 0.1616924882217378}, {"bertscore": 0.6641857030481333, "rouge_l": 0.1597797229566867}, {"bertscore": 0.6666505243483698, "rouge_l": 0.16522603807412484}], "total": {"test_bertscore": 66.33347053968464, "test_bertscore_se": 0.26961146688505866, "test_rouge_l": 16.000948117723873, "test_rouge_l_se": 0.30141739868330647}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "swedn", "task": "summarization", "dataset_languages": ["sv"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"bertscore": 0.6531483606959227, "rouge_l": 0.15937681770990436}, {"bertscore": 0.6532156270113774, "rouge_l": 0.15820817928005382}, {"bertscore": 0.6522458010003902, "rouge_l": 0.16073417670479}, {"bertscore": 0.6517224805429578, "rouge_l": 0.15791399835561748}, {"bertscore": 0.6523350937350187, "rouge_l": 0.1551722312914764}, {"bertscore": 0.6505243561405223, "rouge_l": 0.15722124013899588}, {"bertscore": 0.6517775553511456, "rouge_l": 0.15712763411919212}, {"bertscore": 0.6527121344697662, "rouge_l": 0.1592268629383549}, {"bertscore": 0.6528482708672527, "rouge_l": 0.15686297899513074}, {"bertscore": 0.6510106977657415, "rouge_l": 0.1557128262406444}], "total": {"test_bertscore": 65.21540377580095, "test_bertscore_se": 0.05559614195456763, "test_rouge_l": 15.775569457741604, "test_rouge_l_se": 0.10578790405099746}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "cnn-dailymail", "task": "summarization", "dataset_languages": ["en"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"bertscore": 0.6836917940527201, "rouge_l": 0.21954025502360353}, {"bertscore": 0.6834396312187891, "rouge_l": 0.21451674941520443}, {"bertscore": 0.6836795207927935, "rouge_l": 0.21142734729863633}, {"bertscore": 0.6836533969617449, "rouge_l": 0.21185533170058898}, {"bertscore": 0.6830472582078073, "rouge_l": 0.2161781034696839}, {"bertscore": 0.6850741290254518, "rouge_l": 0.21491809102581072}, {"bertscore": 0.6849311572150327, "rouge_l": 0.22013497436578627}, {"bertscore": 0.6839756737754215, "rouge_l": 0.2162255947004264}, {"bertscore": 0.6843701374600641, "rouge_l": 0.21627639278829391}, {"bertscore": 0.6845000963658094, "rouge_l": 0.21548146622237702}], "total": {"test_bertscore": 68.40362795075634, "test_bertscore_se": 0.04098278482623873, "test_rouge_l": 21.56554306010412, "test_rouge_l_se": 0.17326273048240956}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "orange-sum", "task": "summarization", "dataset_languages": ["fr"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"bertscore": 0.6584073767880909, "rouge_l": 0.16501966725571185}, {"bertscore": 0.659017197729554, "rouge_l": 0.16515482733953807}, {"bertscore": 0.6600282039144076, "rouge_l": 0.1708777580770624}, {"bertscore": 0.6593376019154675, "rouge_l": 0.1728031596306569}, {"bertscore": 0.6595553982187994, "rouge_l": 0.1755881567290965}, {"bertscore": 0.6614867276512086, "rouge_l": 0.16959905238730424}, {"bertscore": 0.6588192362105474, "rouge_l": 0.16168916909060024}, {"bertscore": 0.6588357787695713, "rouge_l": 0.17248567126691317}, {"bertscore": 0.6617776681378018, "rouge_l": 0.1682217390588418}, {"bertscore": 0.6592792547889985, "rouge_l": 0.16304942817357915}], "total": {"test_bertscore": 65.96544444124447, "test_bertscore_se": 0.07032079746101622, "test_rouge_l": 16.844886290093044, "test_rouge_l_se": 0.28549747191129726}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "ilpost-sum", "task": "summarization", "dataset_languages": ["it"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"bertscore": 0.6506075346842408, "rouge_l": 0.1624662204274676}, {"bertscore": 0.647171173884999, "rouge_l": 0.15753231548219687}, {"bertscore": 0.6500357421609806, "rouge_l": 0.16881502431695344}, {"bertscore": 0.651026456602267, "rouge_l": 0.17402699299950453}, {"bertscore": 0.6484120447275927, "rouge_l": 0.1614695547174863}, {"bertscore": 0.6500553353398573, "rouge_l": 0.16865996296136335}, {"bertscore": 0.6525299346249085, "rouge_l": 0.16726318824665426}, {"bertscore": 0.6533547629369423, "rouge_l": 0.17076292964273082}, {"bertscore": 0.6505551352165639, "rouge_l": 0.16511722087117933}, {"bertscore": 0.6523166031693108, "rouge_l": 0.1679185430605043}], "total": {"test_bertscore": 65.06064723347663, "test_bertscore_se": 0.116143947380023, "test_rouge_l": 16.64031952726041, "test_rouge_l_se": 0.3002933367787797}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "danske-talemaader", "task": "knowledge", "dataset_languages": ["da"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"mcc": 0.3245334200190946, "accuracy": 0.49257425742574257}, {"mcc": 0.24753267269771154, "accuracy": 0.43564356435643564}, {"mcc": 0.35092003362350316, "accuracy": 0.5111386138613861}, {"mcc": 0.3012098836774983, "accuracy": 0.47648514851485146}, {"mcc": 0.38417662909139616, "accuracy": 0.5383663366336634}, {"mcc": 0.2627223102912003, "accuracy": 0.4467821782178218}, {"mcc": 0.3338580595915238, "accuracy": 0.49876237623762376}, {"mcc": 0.34284662252088033, "accuracy": 0.5061881188118812}, {"mcc": 0.3403552438174415, "accuracy": 0.504950495049505}, {"mcc": 0.3118631691642819, "accuracy": 0.47896039603960394}], "total": {"test_mcc": 32.000180444945315, "test_mcc_se": 2.5450762259122546, "test_accuracy": 48.89851485148514, "test_accuracy_se": 1.9016095906286457}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "danish-citizen-tests", "task": "knowledge", "dataset_languages": ["da"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"mcc": 0.4931011048577625, "accuracy": 0.659047619047619}, {"mcc": 0.6055856263248037, "accuracy": 0.7333333333333333}, {"mcc": 0.44477928520352294, "accuracy": 0.6323809523809524}, {"mcc": 0.528293094721737, "accuracy": 0.68}, {"mcc": 0.5990996094767672, "accuracy": 0.7295238095238096}, {"mcc": 0.5578060767541638, "accuracy": 0.699047619047619}, {"mcc": 0.5014350957340598, "accuracy": 0.6647619047619048}, {"mcc": 0.5766156018055459, "accuracy": 0.7180952380952381}, {"mcc": 0.46934721902459653, "accuracy": 0.64}, {"mcc": 0.48245026486885445, "accuracy": 0.6571428571428571}], "total": {"test_mcc": 52.58512978771813, "test_mcc_se": 3.4966898666936466, "test_accuracy": 68.13333333333334, "test_accuracy_se": 2.2777413113612095}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "nrk-quiz-qa", "task": "knowledge", "dataset_languages": ["nb", "nn", "no"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"mcc": 0.2663877243360128, "accuracy": 0.453125}, {"mcc": 0.29719659005359345, "accuracy": 0.47705078125}, {"mcc": 0.3007574336947952, "accuracy": 0.4765625}, {"mcc": 0.29381532296457397, "accuracy": 0.4755859375}, {"mcc": 0.30417079806449493, "accuracy": 0.478515625}, {"mcc": 0.29509826251420634, "accuracy": 0.470703125}, {"mcc": 0.26291014588107464, "accuracy": 0.45068359375}, {"mcc": 0.291163875685982, "accuracy": 0.46630859375}, {"mcc": 0.24817900114624955, "accuracy": 0.44091796875}, {"mcc": 0.28788986711229275, "accuracy": 0.47265625}], "total": {"test_mcc": 28.475690214532754, "test_mcc_se": 1.165092466730061, "test_accuracy": 46.62109375, "test_accuracy_se": 0.8206446086977739}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-sv", "task": "knowledge", "dataset_languages": ["sv"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"mcc": 0.4103463257836698, "accuracy": 0.55908203125}, {"mcc": 0.39738796980122243, "accuracy": 0.5498046875}, {"mcc": 0.40768726977238134, "accuracy": 0.55615234375}, {"mcc": 0.38600043194222344, "accuracy": 0.541015625}, {"mcc": 0.39824653415516026, "accuracy": 0.5478515625}, {"mcc": 0.4077306412532237, "accuracy": 0.55615234375}, {"mcc": 0.3928150501166826, "accuracy": 0.54541015625}, {"mcc": 0.4061053025486061, "accuracy": 0.55517578125}, {"mcc": 0.3820982267145568, "accuracy": 0.53662109375}, {"mcc": 0.3992285367762818, "accuracy": 0.54931640625}], "total": {"test_mcc": 39.87646288864008, "test_mcc_se": 0.5939565717732629, "test_accuracy": 54.9658203125, "test_accuracy_se": 0.4477638208587419}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "icelandic-knowledge", "task": "knowledge", "dataset_languages": ["is"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"mcc": 0.1258349611017612, "accuracy": 0.33984375}, {"mcc": 0.1273705025818846, "accuracy": 0.341796875}, {"mcc": 0.08612718708302207, "accuracy": 0.310546875}, {"mcc": 0.13667746852137436, "accuracy": 0.349609375}, {"mcc": 0.1216146464635608, "accuracy": 0.34375}, {"mcc": 0.12325134952003894, "accuracy": 0.337890625}, {"mcc": 0.09730283890322805, "accuracy": 0.322265625}, {"mcc": 0.11043282481078205, "accuracy": 0.3369140625}, {"mcc": 0.0903909735127111, "accuracy": 0.3154296875}, {"mcc": 0.12503282901423382, "accuracy": 0.33984375}], "total": {"test_mcc": 11.440355815125969, "test_mcc_se": 1.0780136633237716, "test_accuracy": 33.37890625, "test_accuracy_se": 0.806434104330128}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-de", "task": "knowledge", "dataset_languages": ["de"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"mcc": 0.40426784425467743, "accuracy": 0.5537109375}, {"mcc": 0.4049118622116769, "accuracy": 0.5546875}, {"mcc": 0.39015410145841656, "accuracy": 0.54443359375}, {"mcc": 0.41162640929333366, "accuracy": 0.5595703125}, {"mcc": 0.3953515870999816, "accuracy": 0.54443359375}, {"mcc": 0.4098395124864473, "accuracy": 0.556640625}, {"mcc": 0.39488825641661796, "accuracy": 0.54736328125}, {"mcc": 0.41086576460965374, "accuracy": 0.55859375}, {"mcc": 0.3984825538806144, "accuracy": 0.55029296875}, {"mcc": 0.4012700043510979, "accuracy": 0.55126953125}], "total": {"test_mcc": 40.21657896062517, "test_mcc_se": 0.45923506642191425, "test_accuracy": 55.2099609375, "test_accuracy_se": 0.34062485690021516}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-nl", "task": "knowledge", "dataset_languages": ["nl"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"mcc": 0.460797975444921, "accuracy": 0.59423828125}, {"mcc": 0.43559615045214817, "accuracy": 0.576171875}, {"mcc": 0.40467237945239076, "accuracy": 0.55419921875}, {"mcc": 0.41776762293939607, "accuracy": 0.5634765625}, {"mcc": 0.4126572243591647, "accuracy": 0.5595703125}, {"mcc": 0.4156673864841755, "accuracy": 0.56103515625}, {"mcc": 0.4167578877723013, "accuracy": 0.56201171875}, {"mcc": 0.4341428605918437, "accuracy": 0.5751953125}, {"mcc": 0.42835324785101314, "accuracy": 0.5712890625}, {"mcc": 0.41232847367064673, "accuracy": 0.55859375}], "total": {"test_mcc": 42.38741209018001, "test_mcc_se": 1.0141187228313187, "test_accuracy": 56.75781250000001, "test_accuracy_se": 0.7365203730572463}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu", "task": "knowledge", "dataset_languages": ["en"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"mcc": 0.5298762648859262, "accuracy": 0.6484375}, {"mcc": 0.5596617444614064, "accuracy": 0.67138671875}, {"mcc": 0.5726730044424704, "accuracy": 0.6806640625}, {"mcc": 0.5300693285685866, "accuracy": 0.64794921875}, {"mcc": 0.5185341944260677, "accuracy": 0.64013671875}, {"mcc": 0.532363244696779, "accuracy": 0.64892578125}, {"mcc": 0.5304421634465833, "accuracy": 0.6474609375}, {"mcc": 0.5606951136668267, "accuracy": 0.67138671875}, {"mcc": 0.5338414820673614, "accuracy": 0.65087890625}, {"mcc": 0.5715114002667999, "accuracy": 0.68017578125}], "total": {"test_mcc": 54.39667940928808, "test_mcc_se": 1.2340046143851238, "test_accuracy": 65.8740234375, "test_accuracy_se": 0.9499281761279131}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-fr", "task": "knowledge", "dataset_languages": ["fr"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"mcc": 0.46307294221397954, "accuracy": 0.5947265625}, {"mcc": 0.43997045918249605, "accuracy": 0.578125}, {"mcc": 0.4387997804254575, "accuracy": 0.57958984375}, {"mcc": 0.4516727669189532, "accuracy": 0.5869140625}, {"mcc": 0.4340100539832342, "accuracy": 0.57421875}, {"mcc": 0.44080444159534443, "accuracy": 0.580078125}, {"mcc": 0.45941140793026386, "accuracy": 0.59423828125}, {"mcc": 0.46054397988546697, "accuracy": 0.5947265625}, {"mcc": 0.46958460147646025, "accuracy": 0.6025390625}, {"mcc": 0.4500498251847668, "accuracy": 0.587890625}], "total": {"test_mcc": 45.07920258796422, "test_mcc_se": 0.7497720628958383, "test_accuracy": 58.73046875, "test_accuracy_se": 0.5688412845747708}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-it", "task": "knowledge", "dataset_languages": ["it"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"mcc": 0.44420857136505343, "accuracy": 0.5830078125}, {"mcc": 0.44016492815864644, "accuracy": 0.5791015625}, {"mcc": 0.4376073680372133, "accuracy": 0.57861328125}, {"mcc": 0.4015505698205508, "accuracy": 0.55078125}, {"mcc": 0.4040004224492349, "accuracy": 0.55029296875}, {"mcc": 0.4116607694201666, "accuracy": 0.5576171875}, {"mcc": 0.4096504064758927, "accuracy": 0.556640625}, {"mcc": 0.4183090315106329, "accuracy": 0.5634765625}, {"mcc": 0.432404621850998, "accuracy": 0.57373046875}, {"mcc": 0.43197345559234074, "accuracy": 0.5732421875}], "total": {"test_mcc": 42.31530144680729, "test_mcc_se": 0.9857338934203643, "test_accuracy": 56.6650390625, "test_accuracy_se": 0.7648603223443274}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-da", "task": "common-sense-reasoning", "dataset_languages": ["da"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"mcc": 0.4276867354363546, "accuracy": 0.56982421875}, {"mcc": 0.42165551514806904, "accuracy": 0.56591796875}, {"mcc": 0.4172435172030677, "accuracy": 0.5576171875}, {"mcc": 0.4269869399704682, "accuracy": 0.5703125}, {"mcc": 0.43511476207443706, "accuracy": 0.57470703125}, {"mcc": 0.4257968063627398, "accuracy": 0.56787109375}, {"mcc": 0.3895933363109185, "accuracy": 0.53955078125}, {"mcc": 0.4354213277335622, "accuracy": 0.57568359375}, {"mcc": 0.42785039822301396, "accuracy": 0.57080078125}, {"mcc": 0.4100746735235949, "accuracy": 0.55712890625}], "total": {"test_mcc": 42.174240119862255, "test_mcc_se": 0.8449155983841892, "test_accuracy": 56.494140625, "test_accuracy_se": 0.6747654762536338}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "nor-common-sense-qa", "task": "common-sense-reasoning", "dataset_languages": ["nb", "nn", "no"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"mcc": 0.6134106107735131, "accuracy": 0.690951821386604}, {"mcc": 0.6191966812801828, "accuracy": 0.6956521739130435}, {"mcc": 0.6270422613358372, "accuracy": 0.7015276145710928}, {"mcc": 0.6164735672685326, "accuracy": 0.6933019976498237}, {"mcc": 0.5873956400835872, "accuracy": 0.6698002350176263}, {"mcc": 0.6219934662617747, "accuracy": 0.6968272620446534}, {"mcc": 0.6385534122815296, "accuracy": 0.7109283196239718}, {"mcc": 0.6205296923141123, "accuracy": 0.6956521739130435}, {"mcc": 0.6284933718602063, "accuracy": 0.7027027027027027}, {"mcc": 0.6047704028401867, "accuracy": 0.6839012925969448}], "total": {"test_mcc": 61.77859106299463, "test_mcc_se": 0.8698608899708251, "test_accuracy": 69.41245593419507, "test_accuracy_se": 0.6943979865122314}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-sv", "task": "common-sense-reasoning", "dataset_languages": ["sv"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"mcc": 0.3993779538860692, "accuracy": 0.548828125}, {"mcc": 0.4302877176290668, "accuracy": 0.572265625}, {"mcc": 0.3987235174534973, "accuracy": 0.548828125}, {"mcc": 0.44413080699532176, "accuracy": 0.5810546875}, {"mcc": 0.4089058149594883, "accuracy": 0.55419921875}, {"mcc": 0.3780416195073994, "accuracy": 0.53271484375}, {"mcc": 0.41951571033748913, "accuracy": 0.564453125}, {"mcc": 0.42967367514770954, "accuracy": 0.57177734375}, {"mcc": 0.4166015168599969, "accuracy": 0.560546875}, {"mcc": 0.4202076439746141, "accuracy": 0.5634765625}], "total": {"test_mcc": 41.45465976750653, "test_mcc_se": 1.1792057608266793, "test_accuracy": 55.9814453125, "test_accuracy_se": 0.874606489069137}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "winogrande-is", "task": "common-sense-reasoning", "dataset_languages": ["is"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"mcc": 0.04668581707236874, "accuracy": 0.5167410714285714}, {"mcc": 0.10959564960390658, "accuracy": 0.5279017857142857}, {"mcc": 0.09619363221512678, "accuracy": 0.5412946428571429}, {"mcc": 0.049513127729750465, "accuracy": 0.5100446428571429}, {"mcc": 0.06512789374606799, "accuracy": 0.5133928571428571}, {"mcc": 0.15428199124132147, "accuracy": 0.5625}, {"mcc": 0.06097233674881534, "accuracy": 0.5256696428571429}, {"mcc": 0.08262753611475511, "accuracy": 0.5435267857142857}, {"mcc": 0.07163471957839918, "accuracy": 0.5323660714285714}, {"mcc": 0.09006046079756222, "accuracy": 0.5133928571428571}], "total": {"test_mcc": 8.266931648480739, "test_mcc_se": 2.0015161053519916, "test_accuracy": 52.86830357142856, "test_accuracy_se": 1.0337978836484971}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-de", "task": "common-sense-reasoning", "dataset_languages": ["de"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"mcc": 0.4722489709734715, "accuracy": 0.60205078125}, {"mcc": 0.516462022288745, "accuracy": 0.63525390625}, {"mcc": 0.4971464243854751, "accuracy": 0.62109375}, {"mcc": 0.46191273051271864, "accuracy": 0.5947265625}, {"mcc": 0.4688228874989792, "accuracy": 0.59521484375}, {"mcc": 0.4802685823944663, "accuracy": 0.6064453125}, {"mcc": 0.5018107125684957, "accuracy": 0.6240234375}, {"mcc": 0.4862422658288297, "accuracy": 0.611328125}, {"mcc": 0.5276028955283076, "accuracy": 0.64404296875}, {"mcc": 0.498246225659895, "accuracy": 0.62158203125}], "total": {"test_mcc": 49.10763717639384, "test_mcc_se": 1.3103332641028036, "test_accuracy": 61.5576171875, "test_accuracy_se": 1.0266040707781614}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-nl", "task": "common-sense-reasoning", "dataset_languages": ["nl"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"mcc": 0.4705083620920702, "accuracy": 0.60107421875}, {"mcc": 0.46400805364827363, "accuracy": 0.59716796875}, {"mcc": 0.45267262028572114, "accuracy": 0.58837890625}, {"mcc": 0.47597244207753225, "accuracy": 0.6044921875}, {"mcc": 0.4547871864205294, "accuracy": 0.59033203125}, {"mcc": 0.44865260153560055, "accuracy": 0.58349609375}, {"mcc": 0.45159192631140255, "accuracy": 0.58642578125}, {"mcc": 0.46381634817255135, "accuracy": 0.59423828125}, {"mcc": 0.4234509329232689, "accuracy": 0.56640625}, {"mcc": 0.4335788508630051, "accuracy": 0.57421875}], "total": {"test_mcc": 45.39039324329955, "test_mcc_se": 0.9991489037607262, "test_accuracy": 58.8623046875, "test_accuracy_se": 0.7294464591777191}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag", "task": "common-sense-reasoning", "dataset_languages": ["en"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"mcc": 0.7091436466273378, "accuracy": 0.78125}, {"mcc": 0.740513952168436, "accuracy": 0.80517578125}, {"mcc": 0.7478728657746544, "accuracy": 0.810546875}, {"mcc": 0.7199841455841238, "accuracy": 0.78955078125}, {"mcc": 0.7251439792771089, "accuracy": 0.79443359375}, {"mcc": 0.7446068624314665, "accuracy": 0.80859375}, {"mcc": 0.7367905805576723, "accuracy": 0.80224609375}, {"mcc": 0.7094293161783565, "accuracy": 0.78173828125}, {"mcc": 0.7265436303027711, "accuracy": 0.79443359375}, {"mcc": 0.7246684744282967, "accuracy": 0.79345703125}], "total": {"test_mcc": 72.84697453330224, "test_mcc_se": 0.8489434009254573, "test_accuracy": 79.6142578125, "test_accuracy_se": 0.6425110549734339}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-fr", "task": "common-sense-reasoning", "dataset_languages": ["fr"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"mcc": 0.5045873011614825, "accuracy": 0.6279296875}, {"mcc": 0.4986652601067407, "accuracy": 0.62255859375}, {"mcc": 0.4956679079265073, "accuracy": 0.62158203125}, {"mcc": 0.5207082422946986, "accuracy": 0.6376953125}, {"mcc": 0.501900245419223, "accuracy": 0.62548828125}, {"mcc": 0.5329656169836782, "accuracy": 0.6474609375}, {"mcc": 0.5204149574858726, "accuracy": 0.63818359375}, {"mcc": 0.4985016186134311, "accuracy": 0.623046875}, {"mcc": 0.5210852423474741, "accuracy": 0.63818359375}, {"mcc": 0.5152909513117323, "accuracy": 0.634765625}], "total": {"test_mcc": 51.0978734365084, "test_mcc_se": 0.7874919691416472, "test_accuracy": 63.1689453125, "test_accuracy_se": 0.5432645304047843}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-it", "task": "common-sense-reasoning", "dataset_languages": ["it"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"mcc": 0.4799719769240729, "accuracy": 0.60791015625}, {"mcc": 0.5187496337108571, "accuracy": 0.638671875}, {"mcc": 0.5122388865838836, "accuracy": 0.63232421875}, {"mcc": 0.5080598093942792, "accuracy": 0.63037109375}, {"mcc": 0.49034108453702435, "accuracy": 0.61328125}, {"mcc": 0.5150667266620818, "accuracy": 0.6357421875}, {"mcc": 0.48875998529545095, "accuracy": 0.6142578125}, {"mcc": 0.5149793719166987, "accuracy": 0.63427734375}, {"mcc": 0.5123205963319896, "accuracy": 0.6328125}, {"mcc": 0.49492908588052753, "accuracy": 0.61962890625}], "total": {"test_mcc": 50.35417157236866, "test_mcc_se": 0.8494628643849405, "test_accuracy": 62.59277343750001, "test_accuracy_se": 0.6842680485115558}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "Qwen/Qwen1.5-14B-Chat", "results": {"raw": [{"test_speed": 482.8, "test_speed_short": 54.8}, {"test_speed": 940.47, "test_speed_short": 100.35000000000001}, {"test_speed": 1388.38, "test_speed_short": 198.35999999999999}, {"test_speed": 1868.65, "test_speed_short": 245.88}, {"test_speed": 2327.13, "test_speed_short": 291.10999999999996}, {"test_speed": 2778.6, "test_speed_short": 390.45}, {"test_speed": 3211.14, "test_speed_short": 435.84}, {"test_speed": 3674.5499999999997, "test_speed_short": 483.51}, {"test_speed": 4133.05, "test_speed_short": 530.4}, {"test_speed": 4521.45, "test_speed_short": 575.4499999999999}], "total": {"test_speed": 2532.622, "test_speed_se": 848.599065365887, "test_speed_short": 330.61499999999995, "test_speed_short_se": 111.87378415872055}}, "num_model_parameters": 14167290880, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "allocine", "task": "sentiment-classification", "dataset_languages": ["fr"], "model": "meta-llama/Llama-2-13b-chat-hf", "results": {"raw": [{"mcc": 0.8912627424226665, "macro_f1": 0.943845393627053}, {"mcc": 0.898732528593528, "macro_f1": 0.9487304565263883}, {"mcc": 0.9048455238846916, "macro_f1": 0.951455425027205}, {"mcc": 0.9040056565901153, "macro_f1": 0.9516575629595914}, {"mcc": 0.8941217512402704, "macro_f1": 0.9453122913829475}, {"mcc": 0.926051216332801, "macro_f1": 0.9628702290076336}, {"mcc": 0.9144752759093849, "macro_f1": 0.9560545198612971}, {"mcc": 0.9110241327088267, "macro_f1": 0.9550708837312227}, {"mcc": 0.901393304751934, "macro_f1": 0.9500810093913543}, {"mcc": 0.9386003211594973, "macro_f1": 0.9691305255950061}], "total": {"test_mcc": 90.84512453593717, "test_mcc_se": 0.9098580809343112, "test_macro_f1": 95.34208297109699, "test_macro_f1_se": 0.48156826820953746}}, "num_model_parameters": 13015866880, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "sentipolc16", "task": "sentiment-classification", "dataset_languages": ["it"], "model": "meta-llama/Llama-2-13b-chat-hf", "results": {"raw": [{"mcc": 0.5038754182728822, "macro_f1": 0.5888888888888889}, {"mcc": 0.5002005293911871, "macro_f1": 0.5591097353824974}, {"mcc": 0.4473715900925903, "macro_f1": 0.5333778044785163}, {"mcc": 0.45885412812253334, "macro_f1": 0.5506369056837825}, {"mcc": 0.502347313848682, "macro_f1": 0.591342924899085}, {"mcc": 0.4747975985462705, "macro_f1": 0.5502705000386429}, {"mcc": 0.5039790089522113, "macro_f1": 0.5903149958786206}, {"mcc": 0.4838597676403797, "macro_f1": 0.5697335052654201}, {"mcc": 0.43636016378927417, "macro_f1": 0.5058656888760891}, {"mcc": 0.4542745657804928, "macro_f1": 0.5394132124764398}], "total": {"test_mcc": 47.659200844365024, "test_mcc_se": 1.607824703677058, "test_macro_f1": 55.78954161867984, "test_macro_f1_se": 1.7325777010850023}}, "num_model_parameters": 13015866880, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "eltec", "task": "named-entity-recognition", "dataset_languages": ["fr"], "model": "meta-llama/Llama-2-13b-chat-hf", "results": {"raw": [{"micro_f1_no_misc": 0.35651586765698845, "micro_f1": 0.28287581699346404}, {"micro_f1_no_misc": 0.42704626334519574, "micro_f1": 0.33887927592173567}, {"micro_f1_no_misc": 0.40608259892863313, "micro_f1": 0.31041833200106583}, {"micro_f1_no_misc": 0.44429137248148787, "micro_f1": 0.3452822414503502}, {"micro_f1_no_misc": 0.42602948652770717, "micro_f1": 0.3578410539736506}, {"micro_f1_no_misc": 0.380178716490658, "micro_f1": 0.2999351911860013}, {"micro_f1_no_misc": 0.4116575591985428, "micro_f1": 0.29841191677951495}, {"micro_f1_no_misc": 0.40013833650354486, "micro_f1": 0.2839756592292089}, {"micro_f1_no_misc": 0.3505257886830245, "micro_f1": 0.2536079930923893}, {"micro_f1_no_misc": 0.39512764892374436, "micro_f1": 0.29970440817375654}], "total": {"test_micro_f1_no_misc": 39.975936387395265, "test_micro_f1_no_misc_se": 1.8820408823212658, "test_micro_f1": 30.70931888801137, "test_micro_f1_se": 1.983587612958227}}, "num_model_parameters": 13015866880, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "multinerd-it", "task": "named-entity-recognition", "dataset_languages": ["it"], "model": "meta-llama/Llama-2-13b-chat-hf", "results": {"raw": [{"micro_f1_no_misc": 0.5663474692202461, "micro_f1": 0.3540119760479042}, {"micro_f1_no_misc": 0.5378333169339762, "micro_f1": 0.3561959654178674}, {"micro_f1_no_misc": 0.4750689972401104, "micro_f1": 0.34540031580794195}, {"micro_f1_no_misc": 0.5200431245712045, "micro_f1": 0.37464306110793827}, {"micro_f1_no_misc": 0.5133720930232558, "micro_f1": 0.36810739602283427}, {"micro_f1_no_misc": 0.5629542329601257, "micro_f1": 0.4070864134590874}, {"micro_f1_no_misc": 0.4995037444735179, "micro_f1": 0.35200091996320143}, {"micro_f1_no_misc": 0.552766858179619, "micro_f1": 0.3894436519258202}, {"micro_f1_no_misc": 0.513699906803355, "micro_f1": 0.347959243771718}, {"micro_f1_no_misc": 0.4375770526834881, "micro_f1": 0.31900637359045597}], "total": {"test_micro_f1_no_misc": 51.79166796088899, "test_micro_f1_no_misc_se": 2.5012943169339588, "test_micro_f1": 36.138553171147684, "test_micro_f1_se": 1.5319143287100894}}, "num_model_parameters": 13015866880, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-fr", "task": "linguistic-acceptability", "dataset_languages": ["fr"], "model": "meta-llama/Llama-2-13b-chat-hf", "results": {"raw": [{"mcc": 0.16967904477600904, "macro_f1": 0.5841932278529389}, {"mcc": 0.19576799780836796, "macro_f1": 0.5801456453847889}, {"mcc": 0.21792845669580718, "macro_f1": 0.536739297138952}, {"mcc": 0.17749448111320043, "macro_f1": 0.5520509771338948}, {"mcc": 0.20342220229338537, "macro_f1": 0.5754347652002236}, {"mcc": 0.254196981832936, "macro_f1": 0.6054934525710636}, {"mcc": 0.23920723480198056, "macro_f1": 0.6029716962244379}, {"mcc": 0.16043784245158355, "macro_f1": 0.5353592396485843}, {"mcc": 0.19234537426553433, "macro_f1": 0.5457474490722529}, {"mcc": 0.19488372563410325, "macro_f1": 0.565347491532618}], "total": {"test_mcc": 20.053633416729078, "test_mcc_se": 1.8361441382520838, "test_macro_f1": 56.834832417597546, "test_macro_f1_se": 1.5858396246352924}}, "num_model_parameters": 13015866880, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-it", "task": "linguistic-acceptability", "dataset_languages": ["it"], "model": "meta-llama/Llama-2-13b-chat-hf", "results": {"raw": [{"mcc": 0.09612939596066941, "macro_f1": 0.4381585654312927}, {"mcc": 0.11799698345092166, "macro_f1": 0.46203264273072003}, {"mcc": 0.10168110530729504, "macro_f1": 0.3856133452838934}, {"mcc": 0.12594993810905544, "macro_f1": 0.40509830501377814}, {"mcc": 0.12686781909170303, "macro_f1": 0.4023095404169573}, {"mcc": 0.0808971746331739, "macro_f1": 0.4307890636804384}, {"mcc": 0.0943498829440065, "macro_f1": 0.40176418622064314}, {"mcc": 0.14740805802225257, "macro_f1": 0.4411018427339407}, {"mcc": 0.14224820596237467, "macro_f1": 0.41501071667524175}, {"mcc": 0.10828857734131511, "macro_f1": 0.38460340632603407}], "total": {"test_mcc": 11.418171408227673, "test_mcc_se": 1.3427144632657184, "test_macro_f1": 41.6648161451294, "test_macro_f1_se": 1.5833909856232602}}, "num_model_parameters": 13015866880, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "squad-it", "task": "reading-comprehension", "dataset_languages": ["it"], "model": "meta-llama/Llama-2-13b-chat-hf", "results": {"raw": [{"f1": 60.409321131785326, "em": 44.54545454545455}, {"f1": 63.91230558094575, "em": 45.564821834723276}, {"f1": 57.963150570856435, "em": 40.63714063714064}, {"f1": 63.245149136714, "em": 45.89041095890411}, {"f1": 62.76328713347509, "em": 41.32613723978412}, {"f1": 66.17240497351379, "em": 50.916030534351144}, {"f1": 62.51361871867097, "em": 46.8798751950078}, {"f1": 61.84077861812206, "em": 41.588050314465406}, {"f1": 60.76038316021985, "em": 41.873065015479874}, {"f1": 64.41357639786177, "em": 47.68518518518518}], "total": {"test_f1": 62.399397542216505, "test_f1_se": 1.4325927795281486, "test_em": 44.690617146049604, "test_em_se": 2.0665976349002144}}, "num_model_parameters": 13015866880, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fquad", "task": "reading-comprehension", "dataset_languages": ["fr"], "model": "meta-llama/Llama-2-13b-chat-hf", "results": {"raw": [{"f1": 57.80726486508449, "em": 32.34848484848485}, {"f1": 61.866877370441614, "em": 36.08794541319181}, {"f1": 62.050664596100845, "em": 35.58663558663559}, {"f1": 63.54754664743195, "em": 36.757990867579906}, {"f1": 57.81734306517905, "em": 32.99922898997687}, {"f1": 62.48271158587585, "em": 35.64885496183206}, {"f1": 57.91324254514388, "em": 33.93135725429017}, {"f1": 59.43354877815677, "em": 35.613207547169814}, {"f1": 53.23080287755409, "em": 32.04334365325077}, {"f1": 60.61967353014941, "em": 35.18518518518518}], "total": {"test_f1": 59.676967586111786, "test_f1_se": 1.9142714421007017, "test_em": 34.6202234307597, "test_em_se": 1.0314549967278626}}, "num_model_parameters": 13015866880, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "orange-sum", "task": "summarization", "dataset_languages": ["fr"], "model": "meta-llama/Llama-2-13b-chat-hf", "results": {"raw": [{"bertscore": 0.6601682982291095, "rouge_l": 0.16663446989343506}, {"bertscore": 0.6604724612552673, "rouge_l": 0.1641232217810239}, {"bertscore": 0.6627455437555909, "rouge_l": 0.1670862467317401}, {"bertscore": 0.6550396967504639, "rouge_l": 0.155197309353308}, {"bertscore": 0.6604144040902611, "rouge_l": 0.16650133439286596}, {"bertscore": 0.6626196171273477, "rouge_l": 0.16867303871133454}, {"bertscore": 0.6605239165946841, "rouge_l": 0.16749066079293184}, {"bertscore": 0.6563865566859022, "rouge_l": 0.15645536478130412}, {"bertscore": 0.6634758296131622, "rouge_l": 0.1668856039636738}, {"bertscore": 0.6559044231835287, "rouge_l": 0.14999261996896882}], "total": {"test_bertscore": 65.97750747285318, "test_bertscore_se": 0.18606696565143324, "test_rouge_l": 16.29039870370586, "test_rouge_l_se": 0.4046919099398097}}, "num_model_parameters": 13015866880, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "ilpost-sum", "task": "summarization", "dataset_languages": ["it"], "model": "meta-llama/Llama-2-13b-chat-hf", "results": {"raw": [{"bertscore": 0.6578202575910836, "rouge_l": 0.17928635990264064}, {"bertscore": 0.6475888100248994, "rouge_l": 0.15483229077715777}, {"bertscore": 0.652403716405388, "rouge_l": 0.16601988026463577}, {"bertscore": 0.6543169335636776, "rouge_l": 0.17397827916163633}, {"bertscore": 0.6526868253713474, "rouge_l": 0.16539193288427195}, {"bertscore": 0.652845559787238, "rouge_l": 0.1687686888132423}, {"bertscore": 0.6559846739983186, "rouge_l": 0.17314724673207524}, {"bertscore": 0.6586321929353289, "rouge_l": 0.18187931642839}, {"bertscore": 0.6543452005571453, "rouge_l": 0.17098489278542212}, {"bertscore": 0.6569596767076291, "rouge_l": 0.17799705059501228}], "total": {"test_bertscore": 65.43583846942056, "test_bertscore_se": 0.20072761493602315, "test_rouge_l": 17.122859383444844, "test_rouge_l_se": 0.49393299563296855}}, "num_model_parameters": 13015866880, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-fr", "task": "knowledge", "dataset_languages": ["fr"], "model": "meta-llama/Llama-2-13b-chat-hf", "results": {"raw": [{"mcc": 0.08757996782720974, "accuracy": 0.2958984375}, {"mcc": 0.1195422231220653, "accuracy": 0.314453125}, {"mcc": 0.1172805295164854, "accuracy": 0.29150390625}, {"mcc": 0.08025572531489222, "accuracy": 0.30517578125}, {"mcc": 0.08758178212055986, "accuracy": 0.30859375}, {"mcc": 0.037429068140816764, "accuracy": 0.26953125}, {"mcc": 0.12024235983261673, "accuracy": 0.3173828125}, {"mcc": 0.11196883062637449, "accuracy": 0.27392578125}, {"mcc": 0.10133140005241788, "accuracy": 0.30908203125}, {"mcc": 0.07206497869838292, "accuracy": 0.2724609375}], "total": {"test_mcc": 9.352768652518213, "test_mcc_se": 1.6281216206663864, "test_accuracy": 29.580078124999996, "test_accuracy_se": 1.1265901531294575}}, "num_model_parameters": 13015866880, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-it", "task": "knowledge", "dataset_languages": ["it"], "model": "meta-llama/Llama-2-13b-chat-hf", "results": {"raw": [{"mcc": 0.09287066357249181, "accuracy": 0.27392578125}, {"mcc": 0.12309255372995412, "accuracy": 0.3212890625}, {"mcc": 0.07150016189165545, "accuracy": 0.26806640625}, {"mcc": 0.06836082022175093, "accuracy": 0.28076171875}, {"mcc": 0.09226044441702404, "accuracy": 0.3212890625}, {"mcc": 0.07411275763145472, "accuracy": 0.29736328125}, {"mcc": 0.08630109131237408, "accuracy": 0.3017578125}, {"mcc": 0.10817539453333287, "accuracy": 0.28076171875}, {"mcc": 0.1106125629211359, "accuracy": 0.30224609375}, {"mcc": 0.13008401376544795, "accuracy": 0.32080078125}], "total": {"test_mcc": 9.573704639966218, "test_mcc_se": 1.3417796332260543, "test_accuracy": 29.6826171875, "test_accuracy_se": 1.2555765315266343}}, "num_model_parameters": 13015866880, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-fr", "task": "common-sense-reasoning", "dataset_languages": ["fr"], "model": "meta-llama/Llama-2-13b-chat-hf", "results": {"raw": [{"mcc": 0.011682094760552143, "accuracy": 0.24853515625}, {"mcc": 0.026384136899163652, "accuracy": 0.26513671875}, {"mcc": 0.03169429423889304, "accuracy": 0.2666015625}, {"mcc": 0.007443185138450587, "accuracy": 0.23974609375}, {"mcc": 0.027366549406729033, "accuracy": 0.267578125}, {"mcc": -0.0007433740188654153, "accuracy": 0.26416015625}, {"mcc": 0.04182969562103371, "accuracy": 0.2646484375}, {"mcc": 0.0, "accuracy": 0.2421875}, {"mcc": 0.004304684025545499, "accuracy": 0.24267578125}, {"mcc": 0.0009265133724885342, "accuracy": 0.26171875}], "total": {"test_mcc": 1.5088777944399077, "test_mcc_se": 0.9544938194343464, "test_accuracy": 25.6298828125, "test_accuracy_se": 0.7131078109281805}}, "num_model_parameters": 13015866880, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-it", "task": "common-sense-reasoning", "dataset_languages": ["it"], "model": "meta-llama/Llama-2-13b-chat-hf", "results": {"raw": [{"mcc": -0.0032183908970622945, "accuracy": 0.265625}, {"mcc": 0.05916591314210017, "accuracy": 0.29638671875}, {"mcc": -0.01936017479041051, "accuracy": 0.2724609375}, {"mcc": 0.012102138702562038, "accuracy": 0.25927734375}, {"mcc": 0.014788268942612875, "accuracy": 0.26708984375}, {"mcc": 0.01106406616823554, "accuracy": 0.25634765625}, {"mcc": 0.005286602072720532, "accuracy": 0.255859375}, {"mcc": -0.005589835954568506, "accuracy": 0.26611328125}, {"mcc": -0.0043675861977114944, "accuracy": 0.26220703125}, {"mcc": 0.01955827797025145, "accuracy": 0.27734375}], "total": {"test_mcc": 0.8942927915872981, "test_mcc_se": 1.3122708299160206, "test_accuracy": 26.787109375, "test_accuracy_se": 0.7493961418138282}}, "num_model_parameters": 13015866880, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "meta-llama/Llama-2-13b-chat-hf", "results": {"raw": [{"test_speed": 460.79, "test_speed_short": 53.36}, {"test_speed": 906.63, "test_speed_short": 97.95}, {"test_speed": 1350.4, "test_speed_short": 189.66}, {"test_speed": 7648.82, "test_speed_short": 234.35999999999999}, {"test_speed": 9477.0, "test_speed_short": 283.37}, {"test_speed": 11497.51, "test_speed_short": 376.2}, {"test_speed": 13266.82, "test_speed_short": 420.48}, {"test_speed": 15169.439999999999, "test_speed_short": 462.21}, {"test_speed": 16677.329999999998, "test_speed_short": 510.12}, {"test_speed": 18583.510000000002, "test_speed_short": 565.25}], "total": {"test_speed": 9503.825, "test_speed_se": 4182.059616153855, "test_speed_short": 319.296, "test_speed_short_se": 108.50726900835845}}, "num_model_parameters": 13015866880, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "sb10k", "task": "sentiment-classification", "dataset_languages": ["de"], "model": "seedboxai/KafkaLM-13B-German-V0.1", "results": {"raw": [{"mcc": 0.3514280722472151, "macro_f1": 0.5498510873439173}, {"mcc": 0.45993536351228714, "macro_f1": 0.5866702765934908}, {"mcc": 0.5123839545808399, "macro_f1": 0.6734988017780594}, {"mcc": 0.38961578080626197, "macro_f1": 0.5974458078125524}, {"mcc": 0.4674726029314247, "macro_f1": 0.64119335844409}, {"mcc": 0.44367870552802396, "macro_f1": 0.607040240583687}, {"mcc": 0.47931678020618795, "macro_f1": 0.6494515163156122}, {"mcc": 0.43663491874897475, "macro_f1": 0.6254415643549884}, {"mcc": 0.5297370118316658, "macro_f1": 0.6706707512042973}, {"mcc": 0.5184412057883047, "macro_f1": 0.671257878816748}], "total": {"test_mcc": 45.886443961811864, "test_mcc_se": 3.517877233036274, "test_macro_f1": 62.72521283247443, "test_macro_f1_se": 2.5792712610045005}}, "num_model_parameters": 13015864320, "max_sequence_length": 8192, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "germeval", "task": "named-entity-recognition", "dataset_languages": ["de"], "model": "seedboxai/KafkaLM-13B-German-V0.1", "results": {"raw": [{"micro_f1_no_misc": 0.5278698403133475, "micro_f1": 0.45358980093938717}, {"micro_f1_no_misc": 0.5289488476672287, "micro_f1": 0.453197610090728}, {"micro_f1_no_misc": 0.555956678700361, "micro_f1": 0.4867218110579016}, {"micro_f1_no_misc": 0.5057263411693791, "micro_f1": 0.4478056037689065}, {"micro_f1_no_misc": 0.4251139950894423, "micro_f1": 0.37254448809798935}, {"micro_f1_no_misc": 0.5285592497868713, "micro_f1": 0.49597585513078474}, {"micro_f1_no_misc": 0.5060398302317989, "micro_f1": 0.4730878186968839}, {"micro_f1_no_misc": 0.48615800135043885, "micro_f1": 0.43951726073533537}, {"micro_f1_no_misc": 0.4761904761904762, "micro_f1": 0.4343334972946385}, {"micro_f1_no_misc": 0.5213032581453635, "micro_f1": 0.4861394805952728}], "total": {"test_micro_f1_no_misc": 50.61866518644707, "test_micro_f1_no_misc_se": 2.269645135459707, "test_micro_f1": 45.429132264078284, "test_micro_f1_se": 2.215545310737631}}, "num_model_parameters": 13015864320, "max_sequence_length": 8192, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-de", "task": "linguistic-acceptability", "dataset_languages": ["de"], "model": "seedboxai/KafkaLM-13B-German-V0.1", "results": {"raw": [{"mcc": 0.059104591082381634, "macro_f1": 0.5052390852911319}, {"mcc": 0.037982508242218015, "macro_f1": 0.5167962739277012}, {"mcc": 0.020838415035039522, "macro_f1": 0.36057760944077605}, {"mcc": 0.03628744365921153, "macro_f1": 0.4818182455852041}, {"mcc": 0.17919413829666395, "macro_f1": 0.4713415767348127}, {"mcc": 0.045904301981156, "macro_f1": 0.42947206866921234}, {"mcc": 0.07050778371350466, "macro_f1": 0.5290644687877354}, {"mcc": 0.03238369429283648, "macro_f1": 0.40119136627521096}, {"mcc": 0.06076317603217785, "macro_f1": 0.35700583981727474}, {"mcc": 0.12911180264602926, "macro_f1": 0.5572028663207313}], "total": {"test_mcc": 6.72077854981219, "test_mcc_se": 3.072611727120477, "test_macro_f1": 46.09709400849791, "test_macro_f1_se": 4.381180866165792}}, "num_model_parameters": 13015864320, "max_sequence_length": 8192, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "germanquad", "task": "reading-comprehension", "dataset_languages": ["de"], "model": "seedboxai/KafkaLM-13B-German-V0.1", "results": {"raw": [{"f1": 47.334248911791846, "em": 22.87878787878788}, {"f1": 45.59383030112568, "em": 22.21379833206975}, {"f1": 44.14839104117032, "em": 17.171717171717173}, {"f1": 43.953166485397404, "em": 18.72146118721461}, {"f1": 47.696408834810725, "em": 22.51349267540478}, {"f1": 48.19421015229093, "em": 19.84732824427481}, {"f1": 28.950502311502714, "em": 15.132605304212168}, {"f1": 39.41911097148362, "em": 18.946540880503143}, {"f1": 44.10690684036964, "em": 20.356037151702786}, {"f1": 41.23622997711207, "em": 20.833333333333332}], "total": {"test_f1": 43.06330058270549, "test_f1_se": 3.5260895836529795, "test_em": 19.861510215922046, "test_em_se": 1.5265345771993815}}, "num_model_parameters": 13015864320, "max_sequence_length": 8192, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mlsum", "task": "summarization", "dataset_languages": ["de"], "model": "seedboxai/KafkaLM-13B-German-V0.1", "results": {"raw": [{"bertscore": 0.5727170314639807, "rouge_l": 0.18349028336704692}, {"bertscore": 0.5351344065275043, "rouge_l": 0.1592874493939163}, {"bertscore": 0.5888353566551814, "rouge_l": 0.17835850029777583}, {"bertscore": 0.503649599457276, "rouge_l": 0.14784947454470168}, {"bertscore": 0.4731303163134726, "rouge_l": 0.15843343360585382}, {"bertscore": 0.5764552860637195, "rouge_l": 0.1535270969157982}, {"bertscore": 0.5589747780613834, "rouge_l": 0.0789331431452176}, {"bertscore": 0.5022366826451616, "rouge_l": 0.13855212794725894}, {"bertscore": 0.46078655813471414, "rouge_l": 0.13345505029418436}, {"bertscore": 0.5939348390529631, "rouge_l": 0.14628319992382666}], "total": {"test_bertscore": 53.65854854375357, "test_bertscore_se": 3.0261481612800454, "test_rouge_l": 14.781697594355805, "test_rouge_l_se": 1.7918128819509191}}, "num_model_parameters": 13015864320, "max_sequence_length": 8192, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-de", "task": "knowledge", "dataset_languages": ["de"], "model": "seedboxai/KafkaLM-13B-German-V0.1", "results": {"raw": [{"mcc": 0.1977305633680539, "accuracy": 0.3935546875}, {"mcc": 0.14709507766643437, "accuracy": 0.361328125}, {"mcc": 0.10999594213498894, "accuracy": 0.31591796875}, {"mcc": 0.1703622628625652, "accuracy": 0.38134765625}, {"mcc": 0.21463982927692107, "accuracy": 0.40869140625}, {"mcc": 0.20290192425888706, "accuracy": 0.404296875}, {"mcc": 0.153723681152788, "accuracy": 0.361328125}, {"mcc": 0.18672975983232085, "accuracy": 0.3896484375}, {"mcc": 0.14836797907104896, "accuracy": 0.34521484375}, {"mcc": 0.18461134692211817, "accuracy": 0.3818359375}], "total": {"test_mcc": 17.161583665461265, "test_mcc_se": 1.978383381561908, "test_accuracy": 37.431640625, "test_accuracy_se": 1.7756132400769569}}, "num_model_parameters": 13015864320, "max_sequence_length": 8192, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-de", "task": "common-sense-reasoning", "dataset_languages": ["de"], "model": "seedboxai/KafkaLM-13B-German-V0.1", "results": {"raw": [{"mcc": 0.11927514219632115, "accuracy": 0.33251953125}, {"mcc": 0.04304668326783879, "accuracy": 0.2841796875}, {"mcc": 0.10143404458271607, "accuracy": 0.3212890625}, {"mcc": 0.13640053940893349, "accuracy": 0.349609375}, {"mcc": 0.09293315229862432, "accuracy": 0.28271484375}, {"mcc": 0.10966576942740915, "accuracy": 0.31396484375}, {"mcc": 0.07304918018900035, "accuracy": 0.29296875}, {"mcc": 0.07480545565535174, "accuracy": 0.28369140625}, {"mcc": 0.11016017468854451, "accuracy": 0.32958984375}, {"mcc": 0.08147635837808312, "accuracy": 0.30078125}], "total": {"test_mcc": 9.422465000928227, "test_mcc_se": 1.6732929298815515, "test_accuracy": 30.9130859375, "test_accuracy_se": 1.4716341585202852}}, "num_model_parameters": 13015864320, "max_sequence_length": 8192, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "seedboxai/KafkaLM-13B-German-V0.1", "results": {"raw": [{"test_speed": 467.18, "test_speed_short": 52.8}, {"test_speed": 905.22, "test_speed_short": 96.60000000000001}, {"test_speed": 1341.96, "test_speed_short": 190.53}, {"test_speed": 1801.21, "test_speed_short": 236.16}, {"test_speed": 2256.93, "test_speed_short": 280.35999999999996}, {"test_speed": 2664.93, "test_speed_short": 373.91999999999996}, {"test_speed": 3147.31, "test_speed_short": 421.12}, {"test_speed": 3584.79, "test_speed_short": 459.37}, {"test_speed": 4032.0899999999997, "test_speed_short": 517.14}, {"test_speed": 4437.33, "test_speed_short": 558.45}], "total": {"test_speed": 2463.8949999999995, "test_speed_se": 833.4705387677691, "test_speed_short": 318.645, "test_speed_short_se": 108.32873399513784}}, "num_model_parameters": 13015864320, "max_sequence_length": 8192, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hotter-and-colder-sentiment", "task": "sentiment-classification", "dataset_languages": ["is"], "model": "mistralai/Mistral-Nemo-Instruct-2407", "results": {"raw": [{"mcc": 0.5026826992846664, "macro_f1": 0.6650331067351458}, {"mcc": 0.45295289604992695, "macro_f1": 0.6191297196817196}, {"mcc": 0.4980971399638035, "macro_f1": 0.6413304431158188}, {"mcc": 0.44492634342878407, "macro_f1": 0.6002211344970577}, {"mcc": 0.469211660021771, "macro_f1": 0.6293374678717086}, {"mcc": 0.4517285941852297, "macro_f1": 0.5989373372141965}, {"mcc": 0.4776282813475113, "macro_f1": 0.6439523266826918}, {"mcc": 0.41728328465522174, "macro_f1": 0.5782929893149166}, {"mcc": 0.46452895286927176, "macro_f1": 0.57619964067744}, {"mcc": 0.47721480034282554, "macro_f1": 0.634036154748665}], "total": {"test_mcc": 46.56254652149013, "test_mcc_se": 1.5787470858611166, "test_macro_f1": 61.8647032053936, "test_macro_f1_se": 1.8265454507302956}}, "num_model_parameters": 12247782400, "max_sequence_length": 131072, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fosent", "task": "sentiment-classification", "dataset_languages": ["fo"], "model": "mistralai/Mistral-Nemo-Instruct-2407", "results": {"raw": [{"mcc": 0.5125315493181453, "macro_f1": 0.6692977944453737}, {"mcc": 0.5029082201957638, "macro_f1": 0.6659588299024919}, {"mcc": 0.48578429936254813, "macro_f1": 0.6194075679953179}, {"mcc": 0.5481753684928743, "macro_f1": 0.6931763536563847}, {"mcc": 0.35964768241255735, "macro_f1": 0.5796036959787865}, {"mcc": 0.4951052891281203, "macro_f1": 0.658912846594006}, {"mcc": 0.4811694859352275, "macro_f1": 0.6467068070652736}, {"mcc": 0.5022560079446801, "macro_f1": 0.6471239016657785}, {"mcc": 0.46149678665584304, "macro_f1": 0.6273172987458702}, {"mcc": 0.5266556611664053, "macro_f1": 0.654171125869239}], "total": {"test_mcc": 48.757303506121644, "test_mcc_se": 3.161727400975383, "test_macro_f1": 64.61676221918522, "test_macro_f1_se": 1.945399267872059}}, "num_model_parameters": 12247782400, "max_sequence_length": 131072, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "allocine", "task": "sentiment-classification", "dataset_languages": ["fr"], "model": "mistralai/Mistral-Nemo-Instruct-2407", "results": {"raw": [{"mcc": 0.9172029696602149, "macro_f1": 0.9579827682782878}, {"mcc": 0.932572491157081, "macro_f1": 0.966286014894447}, {"mcc": 0.9457879177940841, "macro_f1": 0.972893722649167}, {"mcc": 0.9335738658404955, "macro_f1": 0.9667786259541984}, {"mcc": 0.9317008624858308, "macro_f1": 0.9657756298420466}, {"mcc": 0.9306167796423817, "macro_f1": 0.965269356670532}, {"mcc": 0.9238568902445952, "macro_f1": 0.9614217250301214}, {"mcc": 0.9278019233246729, "macro_f1": 0.9638420494595108}, {"mcc": 0.940125398277017, "macro_f1": 0.9700605963292308}, {"mcc": 0.9461337518479958, "macro_f1": 0.9730273301822117}], "total": {"test_mcc": 93.29372850274368, "test_mcc_se": 0.5676111180594742, "test_macro_f1": 96.63337819289754, "test_macro_f1_se": 0.29468644829817303}}, "num_model_parameters": 12247782400, "max_sequence_length": 131072, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "sentipolc16", "task": "sentiment-classification", "dataset_languages": ["it"], "model": "mistralai/Mistral-Nemo-Instruct-2407", "results": {"raw": [{"mcc": 0.6322363064415982, "macro_f1": 0.753032050586132}, {"mcc": 0.6186732955781659, "macro_f1": 0.7420551879433099}, {"mcc": 0.6301727503725084, "macro_f1": 0.7359421139494539}, {"mcc": 0.6225295080827065, "macro_f1": 0.7382893676483477}, {"mcc": 0.6341524482651364, "macro_f1": 0.7478594811931631}, {"mcc": 0.5967970267698466, "macro_f1": 0.706540492369106}, {"mcc": 0.5781026407680365, "macro_f1": 0.6853047571615125}, {"mcc": 0.6252790497241459, "macro_f1": 0.7428075294149507}, {"mcc": 0.5809582023612526, "macro_f1": 0.6770244156542488}, {"mcc": 0.6012582861713369, "macro_f1": 0.7297928558488893}], "total": {"test_mcc": 61.20159514534734, "test_mcc_se": 1.3101509721332127, "test_macro_f1": 72.58648251769114, "test_macro_f1_se": 1.6587837530652765}}, "num_model_parameters": 12247782400, "max_sequence_length": 131072, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mim-gold-ner", "task": "named-entity-recognition", "dataset_languages": ["is"], "model": "mistralai/Mistral-Nemo-Instruct-2407", "results": {"raw": [{"micro_f1_no_misc": 0.4636299435028248, "micro_f1": 0.2479663895593099}, {"micro_f1_no_misc": 0.4792685519880927, "micro_f1": 0.19316750629722923}, {"micro_f1_no_misc": 0.5590089111062813, "micro_f1": 0.3173764718829096}, {"micro_f1_no_misc": 0.46448087431693985, "micro_f1": 0.23084368446523634}, {"micro_f1_no_misc": 0.4484944532488114, "micro_f1": 0.2523252601528686}, {"micro_f1_no_misc": 0.48314190440904037, "micro_f1": 0.26773861830419743}, {"micro_f1_no_misc": 0.5211049723756906, "micro_f1": 0.29266229646859804}, {"micro_f1_no_misc": 0.46431014823261113, "micro_f1": 0.22309343787921215}, {"micro_f1_no_misc": 0.4842814371257485, "micro_f1": 0.2625876354365838}, {"micro_f1_no_misc": 0.5243017374092809, "micro_f1": 0.2505809090064132}], "total": {"test_micro_f1_no_misc": 48.92022933715321, "test_micro_f1_no_misc_se": 2.1494745019224393, "test_micro_f1": 25.38342209452558, "test_micro_f1_se": 2.1684855607975444}}, "num_model_parameters": 12247782400, "max_sequence_length": 131072, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fone", "task": "named-entity-recognition", "dataset_languages": ["fo"], "model": "mistralai/Mistral-Nemo-Instruct-2407", "results": {"raw": [{"micro_f1_no_misc": 0.6206006367782463, "micro_f1": 0.4462734339277549}, {"micro_f1_no_misc": 0.6717680950666186, "micro_f1": 0.5373631948974414}, {"micro_f1_no_misc": 0.6626938918587328, "micro_f1": 0.4603493779062461}, {"micro_f1_no_misc": 0.6537583340944378, "micro_f1": 0.47192394501060125}, {"micro_f1_no_misc": 0.7100256128796194, "micro_f1": 0.5497187111857967}, {"micro_f1_no_misc": 0.6994698167612317, "micro_f1": 0.5464253930584395}, {"micro_f1_no_misc": 0.7024702653247942, "micro_f1": 0.48379955725498097}, {"micro_f1_no_misc": 0.66518886327363, "micro_f1": 0.5134001563943982}, {"micro_f1_no_misc": 0.6632780453005171, "micro_f1": 0.48671726755218214}, {"micro_f1_no_misc": 0.682643427741467, "micro_f1": 0.5901664851408122}], "total": {"test_micro_f1_no_misc": 67.31896989079294, "test_micro_f1_no_misc_se": 1.6543502110345394, "test_micro_f1": 50.86137522328652, "test_micro_f1_se": 2.8684781043535588}}, "num_model_parameters": 12247782400, "max_sequence_length": 131072, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "eltec", "task": "named-entity-recognition", "dataset_languages": ["fr"], "model": "mistralai/Mistral-Nemo-Instruct-2407", "results": {"raw": [{"micro_f1_no_misc": 0.6117175317442638, "micro_f1": 0.5332844305886668}, {"micro_f1_no_misc": 0.5881121808287987, "micro_f1": 0.515431003902093}, {"micro_f1_no_misc": 0.618840247536099, "micro_f1": 0.5601098254559717}, {"micro_f1_no_misc": 0.6319246770308736, "micro_f1": 0.546669140842457}, {"micro_f1_no_misc": 0.6299212598425197, "micro_f1": 0.5851986287557974}, {"micro_f1_no_misc": 0.5768402927249247, "micro_f1": 0.5}, {"micro_f1_no_misc": 0.6084284460052678, "micro_f1": 0.546025878003697}, {"micro_f1_no_misc": 0.601932811780948, "micro_f1": 0.5331781140861467}, {"micro_f1_no_misc": 0.5801762114537445, "micro_f1": 0.4997221707723653}, {"micro_f1_no_misc": 0.6318192193563112, "micro_f1": 0.5148267238299392}], "total": {"test_micro_f1_no_misc": 60.7971287830375, "test_micro_f1_no_misc_se": 1.2950543400387593, "test_micro_f1": 53.34445916237135, "test_micro_f1_se": 1.6847446134846848}}, "num_model_parameters": 12247782400, "max_sequence_length": 131072, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "multinerd-it", "task": "named-entity-recognition", "dataset_languages": ["it"], "model": "mistralai/Mistral-Nemo-Instruct-2407", "results": {"raw": [{"micro_f1_no_misc": 0.6704559495860619, "micro_f1": 0.47582827470196737}, {"micro_f1_no_misc": 0.6503264230901387, "micro_f1": 0.5007246376811594}, {"micro_f1_no_misc": 0.5902818044764003, "micro_f1": 0.47486646667154464}, {"micro_f1_no_misc": 0.665083135391924, "micro_f1": 0.5203791469194312}, {"micro_f1_no_misc": 0.7094830702494277, "micro_f1": 0.531278748850046}, {"micro_f1_no_misc": 0.6441991545326445, "micro_f1": 0.49509269356597596}, {"micro_f1_no_misc": 0.6398131932282546, "micro_f1": 0.4697616751618681}, {"micro_f1_no_misc": 0.6751497005988023, "micro_f1": 0.5324964245987605}, {"micro_f1_no_misc": 0.6187163375224416, "micro_f1": 0.45821000704721637}, {"micro_f1_no_misc": 0.6853984339014279, "micro_f1": 0.518855269041586}], "total": {"test_micro_f1_no_misc": 65.48907202577523, "test_micro_f1_no_misc_se": 2.118352651576318, "test_micro_f1": 49.77493344239556, "test_micro_f1_se": 1.6855350380987688}}, "num_model_parameters": 12247782400, "max_sequence_length": 131072, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-is", "task": "linguistic-acceptability", "dataset_languages": ["is"], "model": "mistralai/Mistral-Nemo-Instruct-2407", "results": {"raw": [{"mcc": 0.09257387008765783, "macro_f1": 0.5170988361366882}, {"mcc": 0.08233719875497349, "macro_f1": 0.5169117173369013}, {"mcc": 0.10929339278914751, "macro_f1": 0.47884004735403785}, {"mcc": 0.16092037855263253, "macro_f1": 0.5766882516188714}, {"mcc": 0.11365139346934226, "macro_f1": 0.5407215124055837}, {"mcc": 0.11033530641716614, "macro_f1": 0.521460595171453}, {"mcc": 0.06279626802880536, "macro_f1": 0.5299668913836143}, {"mcc": 0.11571753398360975, "macro_f1": 0.5559743864401832}, {"mcc": 0.13871180705102598, "macro_f1": 0.569321151315162}, {"mcc": 0.09193746678641682, "macro_f1": 0.5333789017999544}], "total": {"test_mcc": 10.782746159207775, "test_mcc_se": 1.7334936772138871, "test_macro_f1": 53.403622909624495, "test_macro_f1_se": 1.7763071654433402}}, "num_model_parameters": 12247782400, "max_sequence_length": 131072, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-fo", "task": "linguistic-acceptability", "dataset_languages": ["fo"], "model": "mistralai/Mistral-Nemo-Instruct-2407", "results": {"raw": [{"mcc": 0.05282338793860441, "macro_f1": 0.39693757361601884}, {"mcc": 0.10270123613645063, "macro_f1": 0.5058316792663866}, {"mcc": 0.1003741370113377, "macro_f1": 0.5183047178210418}, {"mcc": 0.046884144998392684, "macro_f1": 0.5169881209034479}, {"mcc": 0.04989667003624297, "macro_f1": 0.44020963223999277}, {"mcc": 0.11462092712574468, "macro_f1": 0.5457036114570362}, {"mcc": 0.18587736979399244, "macro_f1": 0.5708530485331891}, {"mcc": 0.09072599084222782, "macro_f1": 0.536820776272293}, {"mcc": 0.16009025992783144, "macro_f1": 0.5020878885587504}, {"mcc": 0.10474075778445348, "macro_f1": 0.5380183931979872}], "total": {"test_mcc": 10.087348815952783, "test_mcc_se": 2.837685687467269, "test_macro_f1": 50.717554418661436, "test_macro_f1_se": 3.217561305598732}}, "num_model_parameters": 12247782400, "max_sequence_length": 131072, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-fr", "task": "linguistic-acceptability", "dataset_languages": ["fr"], "model": "mistralai/Mistral-Nemo-Instruct-2407", "results": {"raw": [{"mcc": 0.35229212743448807, "macro_f1": 0.602544591157387}, {"mcc": 0.38729225648339155, "macro_f1": 0.6310055020904963}, {"mcc": 0.35386883200150393, "macro_f1": 0.5724464593798514}, {"mcc": 0.3846100213919654, "macro_f1": 0.6252778040763227}, {"mcc": 0.2950217966349287, "macro_f1": 0.5183338121066924}, {"mcc": 0.3395709992178675, "macro_f1": 0.567812997375468}, {"mcc": 0.4445314027901211, "macro_f1": 0.6820512820512821}, {"mcc": 0.40809268940319987, "macro_f1": 0.6888916765978799}, {"mcc": 0.41780219934545926, "macro_f1": 0.6635120925341746}, {"mcc": 0.32619850790754484, "macro_f1": 0.5623361971708757}], "total": {"test_mcc": 37.092808326104695, "test_mcc_se": 2.835414266578456, "test_macro_f1": 61.142124145404296, "test_macro_f1_se": 3.509403209852461}}, "num_model_parameters": 12247782400, "max_sequence_length": 131072, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-it", "task": "linguistic-acceptability", "dataset_languages": ["it"], "model": "mistralai/Mistral-Nemo-Instruct-2407", "results": {"raw": [{"mcc": 0.3629572527782825, "macro_f1": 0.6810883987768581}, {"mcc": 0.3521859391043631, "macro_f1": 0.6455044339432714}, {"mcc": 0.3034628384509451, "macro_f1": 0.5758895268463164}, {"mcc": 0.3217104193522406, "macro_f1": 0.5997911507775359}, {"mcc": 0.24161796778102543, "macro_f1": 0.5065897620359814}, {"mcc": 0.31754618234245385, "macro_f1": 0.5789299178166352}, {"mcc": 0.3158082018136375, "macro_f1": 0.5879456544801399}, {"mcc": 0.34653067223597994, "macro_f1": 0.6667723186104811}, {"mcc": 0.33396545427688934, "macro_f1": 0.6488724108691339}, {"mcc": 0.2810493258116876, "macro_f1": 0.5262066713525899}], "total": {"test_mcc": 31.768342539475054, "test_mcc_se": 2.233540107074121, "test_macro_f1": 60.175902455089435, "test_macro_f1_se": 3.6260669442676354}}, "num_model_parameters": 12247782400, "max_sequence_length": 131072, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "nqii", "task": "reading-comprehension", "dataset_languages": ["is"], "model": "mistralai/Mistral-Nemo-Instruct-2407", "results": {"raw": [{"f1": 57.81936899697387, "em": 27.23076923076923}, {"f1": 57.445021205567244, "em": 30.062305295950157}, {"f1": 49.4516343062203, "em": 29.439252336448597}, {"f1": 50.102289771117675, "em": 26.00896860986547}, {"f1": 55.15439923730968, "em": 28.996865203761754}, {"f1": 54.03980017047364, "em": 28.35130970724191}, {"f1": 56.28854043614461, "em": 30.307692307692307}, {"f1": 51.084274584522845, "em": 26.533742331288344}, {"f1": 53.902559437625555, "em": 27.259036144578314}, {"f1": 53.79697998920799, "em": 27.86144578313253}], "total": {"test_f1": 53.90848681351633, "test_f1_se": 1.8173425265476362, "test_em": 28.20513869507286, "test_em_se": 0.9151423642641198}}, "num_model_parameters": 12247782400, "max_sequence_length": 131072, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "foqa", "task": "reading-comprehension", "dataset_languages": ["fo"], "model": "mistralai/Mistral-Nemo-Instruct-2407", "results": {"raw": [{"f1": 68.28598253048402, "em": 46.81372549019608}, {"f1": 67.21364671608978, "em": 47.42014742014742}, {"f1": 71.71696829040282, "em": 51.111111111111114}, {"f1": 67.84181860599318, "em": 48.786407766990294}, {"f1": 72.6733277821199, "em": 51.98019801980198}, {"f1": 68.32996635892695, "em": 49.629629629629626}, {"f1": 71.39245059542623, "em": 52.35849056603774}, {"f1": 70.9657656101211, "em": 49.61832061068702}, {"f1": 74.35259513132092, "em": 57.35660847880299}, {"f1": 70.76924224372746, "em": 47.91154791154791}], "total": {"test_f1": 70.35417638646123, "test_f1_se": 1.4520628703884277, "test_em": 50.29861870049522, "test_em_se": 1.9256323997417166}}, "num_model_parameters": 12247782400, "max_sequence_length": 131072, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "squad-it", "task": "reading-comprehension", "dataset_languages": ["it"], "model": "mistralai/Mistral-Nemo-Instruct-2407", "results": {"raw": [{"f1": 72.29478626031643, "em": 56.28787878787879}, {"f1": 74.38963353630302, "em": 55.420773313115994}, {"f1": 71.13705339112775, "em": 55.08935508935509}, {"f1": 73.78274804154464, "em": 56.39269406392694}, {"f1": 74.26273376758381, "em": 57.51734772552043}, {"f1": 72.57461104899231, "em": 55.19083969465649}, {"f1": 74.25740881265172, "em": 57.56630265210608}, {"f1": 73.61042225476696, "em": 55.26729559748428}, {"f1": 73.0075049756526, "em": 57.27554179566563}, {"f1": 73.78707789163758, "em": 55.16975308641975}], "total": {"test_f1": 73.3103979980577, "test_f1_se": 0.6499565324808768, "test_em": 56.117778180612945, "test_em_se": 0.6376849483770078}}, "num_model_parameters": 12247782400, "max_sequence_length": 131072, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fquad", "task": "reading-comprehension", "dataset_languages": ["fr"], "model": "mistralai/Mistral-Nemo-Instruct-2407", "results": {"raw": [{"f1": 72.84999740875477, "em": 41.515151515151516}, {"f1": 72.68065312577833, "em": 41.394996209249435}, {"f1": 73.45831542607156, "em": 43.27894327894328}, {"f1": 75.93434284308469, "em": 45.28158295281583}, {"f1": 69.15868985625684, "em": 39.784117193523514}, {"f1": 76.32761888755832, "em": 44.58015267175573}, {"f1": 71.73356360939125, "em": 41.57566302652106}, {"f1": 71.47428758175951, "em": 41.19496855345912}, {"f1": 70.43238134447793, "em": 41.640866873065015}, {"f1": 75.92713084281692, "em": 45.370370370370374}], "total": {"test_f1": 72.99769809259502, "test_f1_se": 1.5163243481004116, "test_em": 42.561681264485486, "test_em_se": 1.199975516598383}}, "num_model_parameters": 12247782400, "max_sequence_length": 131072, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "rrn", "task": "summarization", "dataset_languages": ["is"], "model": "mistralai/Mistral-Nemo-Instruct-2407", "results": {"raw": [{"bertscore": 0.6805384155595675, "rouge_l": 0.18273225809377364}, {"bertscore": 0.6768727792077698, "rouge_l": 0.17310469825687919}, {"bertscore": 0.6804200615733862, "rouge_l": 0.17841991771503235}, {"bertscore": 0.6755432783393189, "rouge_l": 0.17622853964435153}, {"bertscore": 0.6781171862385236, "rouge_l": 0.1790692028386991}, {"bertscore": 0.6783384847512934, "rouge_l": 0.17977444260188408}, {"bertscore": 0.6755523454339709, "rouge_l": 0.17258894003788777}, {"bertscore": 0.6729042346996721, "rouge_l": 0.1706298165416808}, {"bertscore": 0.6783566234516911, "rouge_l": 0.17864563495467628}, {"bertscore": 0.6761167429503985, "rouge_l": 0.17286747858929397}], "total": {"test_bertscore": 67.72760152205592, "test_bertscore_se": 0.14627197920499338, "test_rouge_l": 17.640609292741587, "test_rouge_l_se": 0.24350837405486078}}, "num_model_parameters": 12247782400, "max_sequence_length": 131072, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "orange-sum", "task": "summarization", "dataset_languages": ["fr"], "model": "mistralai/Mistral-Nemo-Instruct-2407", "results": {"raw": [{"bertscore": 0.666771347343456, "rouge_l": 0.17421568256624717}, {"bertscore": 0.6648202763171867, "rouge_l": 0.17116729304605188}, {"bertscore": 0.6676635106559843, "rouge_l": 0.1754027295106882}, {"bertscore": 0.6643514896277338, "rouge_l": 0.17277408171430872}, {"bertscore": 0.665819771529641, "rouge_l": 0.1793525600660843}, {"bertscore": 0.6662481081439182, "rouge_l": 0.17120989698743205}, {"bertscore": 0.6664638663060032, "rouge_l": 0.1729877224292276}, {"bertscore": 0.663412461464759, "rouge_l": 0.17116130473291014}, {"bertscore": 0.6676893582334742, "rouge_l": 0.1785108426315795}, {"bertscore": 0.6659705133643001, "rouge_l": 0.17064342912163988}], "total": {"test_bertscore": 66.59210702986456, "test_bertscore_se": 0.08585468258671304, "test_rouge_l": 17.374255428061694, "test_rouge_l_se": 0.1938212525418149}}, "num_model_parameters": 12247782400, "max_sequence_length": 131072, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "ilpost-sum", "task": "summarization", "dataset_languages": ["it"], "model": "mistralai/Mistral-Nemo-Instruct-2407", "results": {"raw": [{"bertscore": 0.6573593374923803, "rouge_l": 0.17251468168171852}, {"bertscore": 0.656622262133169, "rouge_l": 0.17546678059814513}, {"bertscore": 0.6585288058122387, "rouge_l": 0.1776252856875998}, {"bertscore": 0.6564934454945615, "rouge_l": 0.17438671660469063}, {"bertscore": 0.6562129807425663, "rouge_l": 0.16990899235513107}, {"bertscore": 0.6563431165996008, "rouge_l": 0.1756211486770513}, {"bertscore": 0.6593524891359266, "rouge_l": 0.17960962014261606}, {"bertscore": 0.6602904517494608, "rouge_l": 0.18140685834415016}, {"bertscore": 0.6612860184832243, "rouge_l": 0.184749546377228}, {"bertscore": 0.6619364893704187, "rouge_l": 0.1830123436519917}], "total": {"test_bertscore": 65.84425397013547, "test_bertscore_se": 0.1342238325950912, "test_rouge_l": 17.743019741203224, "test_rouge_l_se": 0.29396824606715366}}, "num_model_parameters": 12247782400, "max_sequence_length": 131072, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "icelandic-knowledge", "task": "knowledge", "dataset_languages": ["is"], "model": "mistralai/Mistral-Nemo-Instruct-2407", "results": {"raw": [{"mcc": 0.1384089933271547, "accuracy": 0.3486328125}, {"mcc": 0.1864221103792121, "accuracy": 0.3779296875}, {"mcc": 0.14621385483453625, "accuracy": 0.3486328125}, {"mcc": 0.2058083295465508, "accuracy": 0.390625}, {"mcc": 0.17893218904473582, "accuracy": 0.3740234375}, {"mcc": 0.18965623196060502, "accuracy": 0.3779296875}, {"mcc": 0.15119756804854043, "accuracy": 0.34765625}, {"mcc": 0.20439551304913084, "accuracy": 0.38671875}, {"mcc": 0.2017645526585827, "accuracy": 0.3916015625}, {"mcc": 0.19618098079497678, "accuracy": 0.3896484375}], "total": {"test_mcc": 17.989803236440256, "test_mcc_se": 1.5779249689252213, "test_accuracy": 37.333984375, "test_accuracy_se": 1.1316734350467432}}, "num_model_parameters": 12247782400, "max_sequence_length": 131072, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-fr", "task": "knowledge", "dataset_languages": ["fr"], "model": "mistralai/Mistral-Nemo-Instruct-2407", "results": {"raw": [{"mcc": 0.48201254625282863, "accuracy": 0.6064453125}, {"mcc": 0.4570028826532702, "accuracy": 0.5869140625}, {"mcc": 0.48469199616876524, "accuracy": 0.609375}, {"mcc": 0.4547963278637748, "accuracy": 0.58447265625}, {"mcc": 0.46799128891974484, "accuracy": 0.59619140625}, {"mcc": 0.4481936761332073, "accuracy": 0.58203125}, {"mcc": 0.44197266537449753, "accuracy": 0.57666015625}, {"mcc": 0.4724233310903533, "accuracy": 0.6005859375}, {"mcc": 0.469410910628177, "accuracy": 0.59716796875}, {"mcc": 0.4738972314921964, "accuracy": 0.60009765625}], "total": {"test_mcc": 46.523928565768145, "test_mcc_se": 0.8814139478234074, "test_accuracy": 59.3994140625, "test_accuracy_se": 0.6763096071220643}}, "num_model_parameters": 12247782400, "max_sequence_length": 131072, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-it", "task": "knowledge", "dataset_languages": ["it"], "model": "mistralai/Mistral-Nemo-Instruct-2407", "results": {"raw": [{"mcc": 0.4550308694610732, "accuracy": 0.58740234375}, {"mcc": 0.4520808474928057, "accuracy": 0.5830078125}, {"mcc": 0.4503055659088314, "accuracy": 0.58447265625}, {"mcc": 0.41526387650436203, "accuracy": 0.55908203125}, {"mcc": 0.43942180960646793, "accuracy": 0.57470703125}, {"mcc": 0.4608887871337538, "accuracy": 0.58837890625}, {"mcc": 0.4130049793699797, "accuracy": 0.55615234375}, {"mcc": 0.4260399840934806, "accuracy": 0.564453125}, {"mcc": 0.40805344744880356, "accuracy": 0.55029296875}, {"mcc": 0.437416120856163, "accuracy": 0.57666015625}], "total": {"test_mcc": 43.5750628787572, "test_mcc_se": 1.186286825350788, "test_accuracy": 57.24609375000001, "test_accuracy_se": 0.8655946606132715}}, "num_model_parameters": 12247782400, "max_sequence_length": 131072, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "winogrande-is", "task": "common-sense-reasoning", "dataset_languages": ["is"], "model": "mistralai/Mistral-Nemo-Instruct-2407", "results": {"raw": [{"mcc": 0.13647669364733447, "accuracy": 0.5825892857142857}, {"mcc": 0.1186472149242768, "accuracy": 0.53125}, {"mcc": 0.09909136635447272, "accuracy": 0.5747767857142857}, {"mcc": 0.17952014255905963, "accuracy": 0.6071428571428571}, {"mcc": 0.08759645168642846, "accuracy": 0.5290178571428571}, {"mcc": 0.07551515949080143, "accuracy": 0.5256696428571429}, {"mcc": 0.08917722019656193, "accuracy": 0.5747767857142857}, {"mcc": 0.12198716500680683, "accuracy": 0.5691964285714286}, {"mcc": 0.10036688851347544, "accuracy": 0.59375}, {"mcc": 0.20569204714019143, "accuracy": 0.5770089285714286}], "total": {"test_mcc": 12.14070349519409, "test_mcc_se": 2.608821592780645, "test_accuracy": 56.65178571428571, "test_accuracy_se": 1.7558236433841141}}, "num_model_parameters": 12247782400, "max_sequence_length": 131072, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-fr", "task": "common-sense-reasoning", "dataset_languages": ["fr"], "model": "mistralai/Mistral-Nemo-Instruct-2407", "results": {"raw": [{"mcc": 0.5085152710643537, "accuracy": 0.62109375}, {"mcc": 0.5025746429402782, "accuracy": 0.6181640625}, {"mcc": 0.4714651251911579, "accuracy": 0.59033203125}, {"mcc": 0.47016652335443704, "accuracy": 0.58203125}, {"mcc": 0.4751361861029781, "accuracy": 0.59228515625}, {"mcc": 0.48991773276105927, "accuracy": 0.599609375}, {"mcc": 0.47713805905859796, "accuracy": 0.60009765625}, {"mcc": 0.5023753878056004, "accuracy": 0.61279296875}, {"mcc": 0.5296825186744292, "accuracy": 0.63671875}, {"mcc": 0.521130499496677, "accuracy": 0.62646484375}], "total": {"test_mcc": 49.48101946449569, "test_mcc_se": 1.3212518364152792, "test_accuracy": 60.79589843749999, "test_accuracy_se": 1.0990313976038664}}, "num_model_parameters": 12247782400, "max_sequence_length": 131072, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-it", "task": "common-sense-reasoning", "dataset_languages": ["it"], "model": "mistralai/Mistral-Nemo-Instruct-2407", "results": {"raw": [{"mcc": 0.4806690368558487, "accuracy": 0.59521484375}, {"mcc": 0.526803707464236, "accuracy": 0.63525390625}, {"mcc": 0.4547807032384925, "accuracy": 0.5751953125}, {"mcc": 0.48157195501739547, "accuracy": 0.59765625}, {"mcc": 0.4870469991086749, "accuracy": 0.59765625}, {"mcc": 0.48850981245005826, "accuracy": 0.60546875}, {"mcc": 0.543140355669956, "accuracy": 0.6484375}, {"mcc": 0.5244589624411915, "accuracy": 0.6298828125}, {"mcc": 0.5212959990836158, "accuracy": 0.63134765625}, {"mcc": 0.4821947703410977, "accuracy": 0.60302734375}], "total": {"test_mcc": 49.90472301670567, "test_mcc_se": 1.7282002133471717, "test_accuracy": 61.19140625, "test_accuracy_se": 1.4214267956778215}}, "num_model_parameters": 12247782400, "max_sequence_length": 131072, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "mistralai/Mistral-Nemo-Instruct-2407", "results": {"raw": [{"test_speed": 497.71, "test_speed_short": 56.08}, {"test_speed": 970.08, "test_speed_short": 105.45}, {"test_speed": 1449.57, "test_speed_short": 203.29}, {"test_speed": 1938.9, "test_speed_short": 254.52}, {"test_speed": 2425.41, "test_speed_short": 303.58}, {"test_speed": 2888.06, "test_speed_short": 400.14}, {"test_speed": 3392.81, "test_speed_short": 451.2}, {"test_speed": 3842.85, "test_speed_short": 501.97}, {"test_speed": 4290.8, "test_speed_short": 546.0}, {"test_speed": 4724.74, "test_speed_short": 590.75}], "total": {"test_speed": 2642.093, "test_speed_se": 888.127198575027, "test_speed_short": 341.298, "test_speed_short_se": 115.11621998417347}}, "num_model_parameters": 12247782400, "max_sequence_length": 131072, "vocabulary_size": 131072, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "sb10k", "task": "sentiment-classification", "dataset_languages": ["de"], "model": "seedboxai/KafkaLM-70B-German-V0.1", "results": {"raw": [{"mcc": 0.05904818542077356, "macro_f1": 0.2790232490457481}, {"mcc": 0.3199506652694791, "macro_f1": 0.4197109148769225}, {"mcc": 0.009796665342258492, "macro_f1": 0.27635211181096775}, {"mcc": 0.07271839891991802, "macro_f1": 0.29732241953385125}, {"mcc": 0.18452876799286083, "macro_f1": 0.3292583241417718}, {"mcc": 0.31455382723247427, "macro_f1": 0.4104382431567677}, {"mcc": 0.1737003993475709, "macro_f1": 0.3423301632662024}, {"mcc": -0.05018136570994205, "macro_f1": 0.20528659416768358}, {"mcc": 0.326784759481596, "macro_f1": 0.40136875601948874}, {"mcc": 0.18357218310867504, "macro_f1": 0.24944868148751645}], "total": {"test_mcc": 15.944724864056642, "test_mcc_se": 8.345724232938382, "test_macro_f1": 32.1053945750692, "test_macro_f1_se": 4.505187919884934}}, "num_model_parameters": 68976648192, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "germeval", "task": "named-entity-recognition", "dataset_languages": ["de"], "model": "seedboxai/KafkaLM-70B-German-V0.1", "results": {"raw": [{"micro_f1_no_misc": 0.6192231358094697, "micro_f1": 0.5535754568012632}, {"micro_f1_no_misc": 0.6125609406366505, "micro_f1": 0.5595121951219513}, {"micro_f1_no_misc": 0.5704660303200448, "micro_f1": 0.5242320819112627}, {"micro_f1_no_misc": 0.5732595397611419, "micro_f1": 0.47096498719043545}, {"micro_f1_no_misc": 0.5106522451655194, "micro_f1": 0.47614025837068286}, {"micro_f1_no_misc": 0.6068326577880718, "micro_f1": 0.5070482699700983}, {"micro_f1_no_misc": 0.5845498783454988, "micro_f1": 0.5114662960389159}, {"micro_f1_no_misc": 0.5861325115562405, "micro_f1": 0.5363888189372955}, {"micro_f1_no_misc": 0.6432374866879659, "micro_f1": 0.5554118154376888}, {"micro_f1_no_misc": 0.5938650306748466, "micro_f1": 0.5381649961449498}], "total": {"test_micro_f1_no_misc": 59.007794567454496, "test_micro_f1_no_misc_se": 2.2153665586689497, "test_micro_f1": 52.329051759245424, "test_micro_f1_se": 1.962111121797098}}, "num_model_parameters": 68976648192, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-de", "task": "linguistic-acceptability", "dataset_languages": ["de"], "model": "seedboxai/KafkaLM-70B-German-V0.1", "results": {"raw": [{"mcc": 0.06369909691337881, "macro_f1": 0.3510231343736718}, {"mcc": -0.01660386312941578, "macro_f1": 0.33629514513780645}, {"mcc": 0.0778132857676499, "macro_f1": 0.42000753059706086}, {"mcc": 0.20333236811865213, "macro_f1": 0.551472812227189}, {"mcc": 0.0855410504425509, "macro_f1": 0.43096056379387976}, {"mcc": 0.02571970398243693, "macro_f1": 0.3544699510721119}, {"mcc": 0.09853443435549561, "macro_f1": 0.35278276032636663}, {"mcc": 0.0031092637406779076, "macro_f1": 0.35733752400820573}, {"mcc": 0.041823969742694755, "macro_f1": 0.3408299584229535}, {"mcc": 0.11510874319774424, "macro_f1": 0.38000847889535605}], "total": {"test_mcc": 6.980780531318653, "test_mcc_se": 3.9035050916816534, "test_macro_f1": 38.75187858854602, "test_macro_f1_se": 4.091838722945621}}, "num_model_parameters": 68976648192, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "germanquad", "task": "reading-comprehension", "dataset_languages": ["de"], "model": "seedboxai/KafkaLM-70B-German-V0.1", "results": {"raw": [{"f1": 60.30762572908252, "em": 22.575757575757574}, {"f1": 54.68068070855198, "em": 25.549658832448824}, {"f1": 51.61182706158933, "em": 13.752913752913752}, {"f1": 52.33528563686393, "em": 18.41704718417047}, {"f1": 59.559031426867115, "em": 26.522744795682343}, {"f1": 53.2478062840797, "em": 18.244274809160306}, {"f1": 50.38851386345316, "em": 23.868954758190327}, {"f1": 59.00622962684541, "em": 25.943396226415093}, {"f1": 50.69185576688583, "em": 22.445820433436534}, {"f1": 47.94608452820874, "em": 24.691358024691358}], "total": {"test_f1": 53.97749406324277, "test_f1_se": 2.6609726179579942, "test_em": 22.20119263928666, "test_em_se": 2.567400451592369}}, "num_model_parameters": 68976648192, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mlsum", "task": "summarization", "dataset_languages": ["de"], "model": "seedboxai/KafkaLM-70B-German-V0.1", "results": {"raw": [{"bertscore": 0.6043005461542634, "rouge_l": 0.20183426340552235}, {"bertscore": 0.6514157654892188, "rouge_l": 0.22605672243488215}, {"bertscore": 0.6364179228257854, "rouge_l": 0.23396780609247947}, {"bertscore": 0.6473969296785071, "rouge_l": 0.2185720608669962}, {"bertscore": 0.6458271281589987, "rouge_l": 0.2320244658838659}, {"bertscore": 0.6607728196977405, "rouge_l": 0.23714277907740233}, {"bertscore": 0.6622551557811676, "rouge_l": 0.2387531072587903}, {"bertscore": 0.6074403808743227, "rouge_l": 0.15059289483309696}, {"bertscore": 0.6598943769058678, "rouge_l": 0.2522298288820858}, {"bertscore": 0.5525630133633967, "rouge_l": 0.18325913895085316}], "total": {"test_bertscore": 63.28284038929268, "test_bertscore_se": 2.1688204450780235, "test_rouge_l": 21.744330676859747, "test_rouge_l_se": 1.9041713955273627}}, "num_model_parameters": 68976648192, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-de", "task": "knowledge", "dataset_languages": ["de"], "model": "seedboxai/KafkaLM-70B-German-V0.1", "results": {"raw": [{"mcc": 0.28183381116636436, "accuracy": 0.39599609375}, {"mcc": 0.2655667173275664, "accuracy": 0.375}, {"mcc": 0.2924265682526769, "accuracy": 0.39111328125}, {"mcc": 0.2571398739436987, "accuracy": 0.388671875}, {"mcc": 0.2772348055427674, "accuracy": 0.3603515625}, {"mcc": 0.3489469981896009, "accuracy": 0.47900390625}, {"mcc": 0.26259125564906466, "accuracy": 0.35791015625}, {"mcc": 0.27212088424530995, "accuracy": 0.36865234375}, {"mcc": 0.2691048106164198, "accuracy": 0.39111328125}, {"mcc": 0.23402580841315257, "accuracy": 0.349609375}], "total": {"test_mcc": 27.60991533346622, "test_mcc_se": 1.858883464802306, "test_accuracy": 38.57421875, "test_accuracy_se": 2.2633558627446004}}, "num_model_parameters": 68976648192, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-de", "task": "common-sense-reasoning", "dataset_languages": ["de"], "model": "seedboxai/KafkaLM-70B-German-V0.1", "results": {"raw": [{"mcc": 0.16745232250332304, "accuracy": 0.33935546875}, {"mcc": 0.103122014048178, "accuracy": 0.302734375}, {"mcc": 0.11880249373090154, "accuracy": 0.3232421875}, {"mcc": 0.08417575249779383, "accuracy": 0.27978515625}, {"mcc": 0.11048299770561365, "accuracy": 0.3017578125}, {"mcc": 0.12293982686332237, "accuracy": 0.3173828125}, {"mcc": 0.08299728355984493, "accuracy": 0.2841796875}, {"mcc": 0.14769687681188326, "accuracy": 0.30712890625}, {"mcc": 0.10168378102297018, "accuracy": 0.31640625}, {"mcc": 0.1348125761250453, "accuracy": 0.31787109375}], "total": {"test_mcc": 11.741659248688762, "test_mcc_se": 1.671447110003531, "test_accuracy": 30.898437499999996, "test_accuracy_se": 1.1112191092668957}}, "num_model_parameters": 68976648192, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "seedboxai/KafkaLM-70B-German-V0.1", "results": {"raw": [{"test_speed": 248.5, "test_speed_short": 28.0}, {"test_speed": 477.99, "test_speed_short": 51.9}, {"test_speed": 713.18, "test_speed_short": 99.76}, {"test_speed": 952.59, "test_speed_short": 124.2}, {"test_speed": 1175.8500000000001, "test_speed_short": 147.92}, {"test_speed": 1397.72, "test_speed_short": 194.94}, {"test_speed": 1630.12, "test_speed_short": 222.08}, {"test_speed": 1840.08, "test_speed_short": 242.82}, {"test_speed": 2038.1299999999999, "test_speed_short": 266.76}, {"test_speed": 2229.1800000000003, "test_speed_short": 296.65000000000003}], "total": {"test_speed": 1270.334, "test_speed_se": 416.8780088504878, "test_speed_short": 167.503, "test_speed_short_se": 56.809159710644465}}, "num_model_parameters": 68976648192, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "swerec", "task": "sentiment-classification", "dataset_languages": ["sv"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"mcc": 0.8222237688824086, "macro_f1": 0.8038009042260063}, {"mcc": 0.7951508575065586, "macro_f1": 0.7620679160343653}, {"mcc": 0.839945821764902, "macro_f1": 0.8286191853581956}, {"mcc": 0.808055737749993, "macro_f1": 0.788021283243347}, {"mcc": 0.8077832381652202, "macro_f1": 0.8071948393034152}, {"mcc": 0.8218791095494903, "macro_f1": 0.7948437565200374}, {"mcc": 0.8150277372095549, "macro_f1": 0.7672713702596387}, {"mcc": 0.788853938329471, "macro_f1": 0.739915782322487}, {"mcc": 0.7984037309440193, "macro_f1": 0.7346672794449313}, {"mcc": 0.797824787186823, "macro_f1": 0.7905857631771429}], "total": {"test_mcc": 80.95148727288442, "test_mcc_se": 0.9622575538473295, "test_macro_f1": 78.16988079889568, "test_macro_f1_se": 1.8689355937941627}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "angry-tweets", "task": "sentiment-classification", "dataset_languages": ["da"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"mcc": 0.5391688306538599, "macro_f1": 0.6884158112291824}, {"mcc": 0.5027919146325536, "macro_f1": 0.6543537318325442}, {"mcc": 0.5385765034253489, "macro_f1": 0.6899587711093162}, {"mcc": 0.5403198780580478, "macro_f1": 0.6840820799489536}, {"mcc": 0.515383059763758, "macro_f1": 0.6644241502533661}, {"mcc": 0.528550319127074, "macro_f1": 0.6720045949554384}, {"mcc": 0.46377748628457127, "macro_f1": 0.6222151690592947}, {"mcc": 0.4727670071146249, "macro_f1": 0.652548225936183}, {"mcc": 0.4881906101971624, "macro_f1": 0.6493080066874217}, {"mcc": 0.4588650167848354, "macro_f1": 0.6185050218685647}], "total": {"test_mcc": 50.483906260418365, "test_mcc_se": 1.9986747213774443, "test_macro_f1": 65.95815562880264, "test_macro_f1_se": 1.5731799971680993}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "norec", "task": "sentiment-classification", "dataset_languages": ["nb", "nn", "no"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"mcc": 0.36316039351098217, "macro_f1": 0.38745662879280457}, {"mcc": 0.36698847662403045, "macro_f1": 0.3856335732015927}, {"mcc": 0.3643544796696474, "macro_f1": 0.4016887564462661}, {"mcc": 0.3762354768783035, "macro_f1": 0.40074113301986997}, {"mcc": 0.3456288890603044, "macro_f1": 0.3754982562615295}, {"mcc": 0.38223764775350727, "macro_f1": 0.3888129882561784}, {"mcc": 0.3549055147330281, "macro_f1": 0.3582997216653822}, {"mcc": 0.375162140308825, "macro_f1": 0.3830600064628462}, {"mcc": 0.38748839237430766, "macro_f1": 0.39116003344815936}, {"mcc": 0.3613454277612432, "macro_f1": 0.39863190751326966}], "total": {"test_mcc": 36.77506838674179, "test_mcc_se": 0.7880608572673176, "test_macro_f1": 38.709830050678995, "test_macro_f1_se": 0.8091767460262085}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hotter-and-colder-sentiment", "task": "sentiment-classification", "dataset_languages": ["is"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"mcc": 0.3261111527484128, "macro_f1": 0.5234057704392755}, {"mcc": 0.3648919748039043, "macro_f1": 0.5629325992859515}, {"mcc": 0.27486035255912905, "macro_f1": 0.42488758982669417}, {"mcc": 0.4225814925452248, "macro_f1": 0.6093218416945382}, {"mcc": 0.25071893002624496, "macro_f1": 0.47863913403535424}, {"mcc": 0.3153805530269878, "macro_f1": 0.518736194929584}, {"mcc": 0.3547759001416274, "macro_f1": 0.5283653411754677}, {"mcc": 0.18273202643133504, "macro_f1": 0.3551465491198882}, {"mcc": 0.3385704578740109, "macro_f1": 0.5434763652276674}, {"mcc": 0.36636880883473955, "macro_f1": 0.5393236011180319}], "total": {"test_mcc": 31.969916489916166, "test_mcc_se": 4.233089554566382, "test_macro_f1": 50.842349868524536, "test_macro_f1_se": 4.499134090001774}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "sb10k", "task": "sentiment-classification", "dataset_languages": ["de"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"mcc": 0.4559685746312539, "macro_f1": 0.6264563981553825}, {"mcc": 0.6073434396095287, "macro_f1": 0.7309261621010449}, {"mcc": 0.5394141669013305, "macro_f1": 0.6846307517759257}, {"mcc": 0.4834681757326881, "macro_f1": 0.6598242784173562}, {"mcc": 0.5672449794794854, "macro_f1": 0.7108187492575081}, {"mcc": 0.5711141269064103, "macro_f1": 0.7122523553991482}, {"mcc": 0.5460986038678225, "macro_f1": 0.6699536129943695}, {"mcc": 0.503580946918677, "macro_f1": 0.660709597300535}, {"mcc": 0.6336537092737002, "macro_f1": 0.7537977804111223}, {"mcc": 0.6025833030012678, "macro_f1": 0.7341508200104766}], "total": {"test_mcc": 55.10470026322164, "test_mcc_se": 3.538501359734175, "test_macro_f1": 69.43520505822869, "test_macro_f1_se": 2.5024086128039507}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "dutch-social", "task": "sentiment-classification", "dataset_languages": ["nl"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"mcc": 0.13541548102138976, "macro_f1": 0.3748341919515254}, {"mcc": 0.16389261111934783, "macro_f1": 0.3524674240024362}, {"mcc": 0.125320797175342, "macro_f1": 0.3785045659822646}, {"mcc": 0.12656546734068708, "macro_f1": 0.38333814507303066}, {"mcc": 0.1725824364998179, "macro_f1": 0.41863592108959047}, {"mcc": 0.18773565476905565, "macro_f1": 0.4047446664158465}, {"mcc": 0.031205164906499182, "macro_f1": 0.33441727974909496}, {"mcc": 0.12751872409282858, "macro_f1": 0.38488305799726286}, {"mcc": 0.09979707000635352, "macro_f1": 0.3684430316595484}, {"mcc": 0.16117590968091305, "macro_f1": 0.4183392306994394}], "total": {"test_mcc": 13.312093166122347, "test_mcc_se": 2.7650534434199967, "test_macro_f1": 38.186075146200395, "test_macro_f1_se": 1.6750841825319145}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "sst5", "task": "sentiment-classification", "dataset_languages": ["en"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"mcc": 0.6446962117818195, "macro_f1": 0.7165214768340439}, {"mcc": 0.6070054988857093, "macro_f1": 0.686615735597328}, {"mcc": 0.6287442446924506, "macro_f1": 0.6978788428721693}, {"mcc": 0.6736370789033891, "macro_f1": 0.7093675469778843}, {"mcc": 0.6162829916659912, "macro_f1": 0.7001095194308985}, {"mcc": 0.5806881765164327, "macro_f1": 0.6725077561663495}, {"mcc": 0.6024078107502042, "macro_f1": 0.6895837003160303}, {"mcc": 0.6450962019556105, "macro_f1": 0.7084120841277972}, {"mcc": 0.6683022493046883, "macro_f1": 0.7036728201205563}, {"mcc": 0.6885230279619168, "macro_f1": 0.7404781421437909}], "total": {"test_mcc": 63.55383492418213, "test_mcc_se": 2.1515217731946077, "test_macro_f1": 70.25147624586849, "test_macro_f1_se": 1.1443922127592343}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fosent", "task": "sentiment-classification", "dataset_languages": ["fo"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"mcc": 0.381333370842073, "macro_f1": 0.5404478981166694}, {"mcc": 0.36205284463307774, "macro_f1": 0.571008255374037}, {"mcc": 0.30138198050288467, "macro_f1": 0.4299077931153403}, {"mcc": 0.2866408522803102, "macro_f1": 0.4824813938114431}, {"mcc": 0.3948183585106235, "macro_f1": 0.5647443775837576}, {"mcc": 0.4986236409511223, "macro_f1": 0.5557227580557379}, {"mcc": 0.07277329247028962, "macro_f1": 0.3085679689163296}, {"mcc": 0.44931472747667756, "macro_f1": 0.5237580776711211}, {"mcc": 0.4282852953347447, "macro_f1": 0.6075708731117629}, {"mcc": 0.4548955746162286, "macro_f1": 0.5658261312322841}], "total": {"test_mcc": 36.30119937618032, "test_mcc_se": 7.556088195725377, "test_macro_f1": 51.500355269884835, "test_macro_f1_se": 5.468484267436607}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "allocine", "task": "sentiment-classification", "dataset_languages": ["fr"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"mcc": 0.9363072373607167, "macro_f1": 0.9677457534581302}, {"mcc": 0.9255734530271266, "macro_f1": 0.9624008287763747}, {"mcc": 0.944336190021172, "macro_f1": 0.9715350247832009}, {"mcc": 0.9410592207419827, "macro_f1": 0.9702127913228732}, {"mcc": 0.944861675795007, "macro_f1": 0.972146392734049}, {"mcc": 0.9389557188748626, "macro_f1": 0.9692259475750622}, {"mcc": 0.9141236630310217, "macro_f1": 0.9555646158266246}, {"mcc": 0.9392417433773183, "macro_f1": 0.9692321119800031}, {"mcc": 0.933664008156267, "macro_f1": 0.9662296812787564}, {"mcc": 0.9341008502941971, "macro_f1": 0.9662448466850739}], "total": {"test_mcc": 93.52223760679671, "test_mcc_se": 0.5782130237501294, "test_macro_f1": 96.70537994420147, "test_macro_f1_se": 0.30708212494964154}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "sentipolc16", "task": "sentiment-classification", "dataset_languages": ["it"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"mcc": 0.5569620214651014, "macro_f1": 0.6868031743941333}, {"mcc": 0.565530689632025, "macro_f1": 0.7104659724232851}, {"mcc": 0.5657323200326408, "macro_f1": 0.7094924471529827}, {"mcc": 0.5864014051449499, "macro_f1": 0.7249213331196058}, {"mcc": 0.5606278402535547, "macro_f1": 0.7012856459684644}, {"mcc": 0.5853114894872883, "macro_f1": 0.7142064741258265}, {"mcc": 0.5076523362898088, "macro_f1": 0.6710245782402279}, {"mcc": 0.5686570224229547, "macro_f1": 0.6994808119827218}, {"mcc": 0.5914964197089623, "macro_f1": 0.7190034195917082}, {"mcc": 0.5983800854470979, "macro_f1": 0.7298860228768081}], "total": {"test_mcc": 56.86751629884383, "test_mcc_se": 1.592106447593897, "test_macro_f1": 70.66569879875763, "test_macro_f1_se": 1.1028264782042472}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "suc3", "task": "named-entity-recognition", "dataset_languages": ["sv"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"micro_f1_no_misc": 0.5699614890885751, "micro_f1": 0.4233016800584368}, {"micro_f1_no_misc": 0.5404825737265416, "micro_f1": 0.5085363077623491}, {"micro_f1_no_misc": 0.6112144565506245, "micro_f1": 0.5157364666386908}, {"micro_f1_no_misc": 0.5900558092338914, "micro_f1": 0.5093632958801498}, {"micro_f1_no_misc": 0.6212857914640735, "micro_f1": 0.5305759395152324}, {"micro_f1_no_misc": 0.5579078455790785, "micro_f1": 0.4613508900268228}, {"micro_f1_no_misc": 0.5469113466011889, "micro_f1": 0.4429294896766654}, {"micro_f1_no_misc": 0.5757837215990796, "micro_f1": 0.44199125182253696}, {"micro_f1_no_misc": 0.5716793446459919, "micro_f1": 0.4799440298507463}, {"micro_f1_no_misc": 0.6046511627906977, "micro_f1": 0.5135507579237483}], "total": {"test_micro_f1_no_misc": 57.899335412797434, "test_micro_f1_no_misc_se": 1.69235478397322, "test_micro_f1": 48.27280109155378, "test_micro_f1_se": 2.3505619753731306}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "dansk", "task": "named-entity-recognition", "dataset_languages": ["da"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"micro_f1_no_misc": 0.6004796163069545, "micro_f1": 0.5080812266887692}, {"micro_f1_no_misc": 0.5448851774530271, "micro_f1": 0.42516753536857776}, {"micro_f1_no_misc": 0.5258270458502612, "micro_f1": 0.44040036396724297}, {"micro_f1_no_misc": 0.5337477797513323, "micro_f1": 0.4334365325077399}, {"micro_f1_no_misc": 0.4539767649687221, "micro_f1": 0.3240131578947368}, {"micro_f1_no_misc": 0.47252231094410524, "micro_f1": 0.37093690248565964}, {"micro_f1_no_misc": 0.5616868550919696, "micro_f1": 0.46955887714181554}, {"micro_f1_no_misc": 0.5639211723092471, "micro_f1": 0.4459803564932702}, {"micro_f1_no_misc": 0.5502367175170962, "micro_f1": 0.40637450199203184}, {"micro_f1_no_misc": 0.5285857572718153, "micro_f1": 0.3887147335423198}], "total": {"test_micro_f1_no_misc": 53.35869197464531, "test_micro_f1_no_misc_se": 2.6731785283826275, "test_micro_f1": 42.126641880821644, "test_micro_f1_se": 3.219404178432951}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "norne-nb", "task": "named-entity-recognition", "dataset_languages": ["nb", "no"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"micro_f1_no_misc": 0.6339127350124159, "micro_f1": 0.5389118949021848}, {"micro_f1_no_misc": 0.610906411201179, "micro_f1": 0.5297378277153557}, {"micro_f1_no_misc": 0.6284694686756543, "micro_f1": 0.5703384207033841}, {"micro_f1_no_misc": 0.6431451612903226, "micro_f1": 0.6036745406824147}, {"micro_f1_no_misc": 0.6119264350733791, "micro_f1": 0.5375287797390638}, {"micro_f1_no_misc": 0.6105787658106554, "micro_f1": 0.5738453730333277}, {"micro_f1_no_misc": 0.6123778501628665, "micro_f1": 0.5538983050847457}, {"micro_f1_no_misc": 0.6237683490850593, "micro_f1": 0.594604381676625}, {"micro_f1_no_misc": 0.6393889798145118, "micro_f1": 0.6032310177705977}, {"micro_f1_no_misc": 0.6160962072155411, "micro_f1": 0.5171859002486471}], "total": {"test_micro_f1_no_misc": 62.305703633415845, "test_micro_f1_no_misc_se": 0.7751422378121792, "test_micro_f1": 56.22956441556346, "test_micro_f1_se": 1.9551449375816852}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "norne-nn", "task": "named-entity-recognition", "dataset_languages": ["nn"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"micro_f1_no_misc": 0.673736034841886, "micro_f1": 0.5989138867339023}, {"micro_f1_no_misc": 0.6670435274166195, "micro_f1": 0.6209453197405005}, {"micro_f1_no_misc": 0.6150598170302604, "micro_f1": 0.5081640780565512}, {"micro_f1_no_misc": 0.6368808567603748, "micro_f1": 0.57090395480226}, {"micro_f1_no_misc": 0.6538816153420615, "micro_f1": 0.5892351274787535}, {"micro_f1_no_misc": 0.6273247496423462, "micro_f1": 0.5143787303309821}, {"micro_f1_no_misc": 0.658676393955185, "micro_f1": 0.6371043560012535}, {"micro_f1_no_misc": 0.6448311156601843, "micro_f1": 0.5690419365837026}, {"micro_f1_no_misc": 0.6506151142355009, "micro_f1": 0.5807204316756599}, {"micro_f1_no_misc": 0.6409250464588064, "micro_f1": 0.5117914164347444}], "total": {"test_micro_f1_no_misc": 64.68974271343225, "test_micro_f1_no_misc_se": 1.1057585389339062, "test_micro_f1": 57.011992378383106, "test_micro_f1_se": 2.827249564467832}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mim-gold-ner", "task": "named-entity-recognition", "dataset_languages": ["is"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"micro_f1_no_misc": 0.527054935976869, "micro_f1": 0.4470697963625059}, {"micro_f1_no_misc": 0.5733456732993985, "micro_f1": 0.41363440241622323}, {"micro_f1_no_misc": 0.5862152357920194, "micro_f1": 0.50651066238913}, {"micro_f1_no_misc": 0.531444099378882, "micro_f1": 0.3792759051186017}, {"micro_f1_no_misc": 0.5310238982679237, "micro_f1": 0.48742616033755276}, {"micro_f1_no_misc": 0.4755102040816326, "micro_f1": 0.4394071490845684}, {"micro_f1_no_misc": 0.5583001328021248, "micro_f1": 0.524384980578334}, {"micro_f1_no_misc": 0.5756906077348066, "micro_f1": 0.4958217270194986}, {"micro_f1_no_misc": 0.6108567691301504, "micro_f1": 0.5768688293370945}, {"micro_f1_no_misc": 0.5965548504079783, "micro_f1": 0.5229718189581554}], "total": {"test_micro_f1_no_misc": 55.65996406871785, "test_micro_f1_no_misc_se": 2.513460090227543, "test_micro_f1": 47.93371431601664, "test_micro_f1_se": 3.666008358083882}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fone", "task": "named-entity-recognition", "dataset_languages": ["fo"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"micro_f1_no_misc": 0.6419022349219309, "micro_f1": 0.6249639110768935}, {"micro_f1_no_misc": 0.6796858855135358, "micro_f1": 0.6730882642028702}, {"micro_f1_no_misc": 0.6549613725293468, "micro_f1": 0.6338727473335785}, {"micro_f1_no_misc": 0.5737230966912946, "micro_f1": 0.5555245795000464}, {"micro_f1_no_misc": 0.644940698792606, "micro_f1": 0.636744966442953}, {"micro_f1_no_misc": 0.6714846169729507, "micro_f1": 0.6628299265727327}, {"micro_f1_no_misc": 0.5880752663795058, "micro_f1": 0.5899792469611621}, {"micro_f1_no_misc": 0.6960764383004676, "micro_f1": 0.6893055289841209}, {"micro_f1_no_misc": 0.5729840081660428, "micro_f1": 0.5767086516621929}, {"micro_f1_no_misc": 0.686078063034857, "micro_f1": 0.6593595120091498}], "total": {"test_micro_f1_no_misc": 64.09911681302538, "test_micro_f1_no_misc_se": 2.8965592096633515, "test_micro_f1": 63.023773347457, "test_micro_f1_se": 2.7242539602780997}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "germeval", "task": "named-entity-recognition", "dataset_languages": ["de"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"micro_f1_no_misc": 0.6188340807174888, "micro_f1": 0.541622760800843}, {"micro_f1_no_misc": 0.6565575951050812, "micro_f1": 0.6084520196124211}, {"micro_f1_no_misc": 0.6076073619631902, "micro_f1": 0.5215876089060987}, {"micro_f1_no_misc": 0.5857335127860027, "micro_f1": 0.4687255668491008}, {"micro_f1_no_misc": 0.5679707289614411, "micro_f1": 0.534372135655362}, {"micro_f1_no_misc": 0.6324435318275154, "micro_f1": 0.5424286007807684}, {"micro_f1_no_misc": 0.5767937219730942, "micro_f1": 0.5180269851802698}, {"micro_f1_no_misc": 0.6450116009280742, "micro_f1": 0.5692233478051133}, {"micro_f1_no_misc": 0.6000487448208629, "micro_f1": 0.5102001165727609}, {"micro_f1_no_misc": 0.5984470327232391, "micro_f1": 0.5324675324675325}], "total": {"test_micro_f1_no_misc": 60.8944791180599, "test_micro_f1_no_misc_se": 1.8082400137044392, "test_micro_f1": 53.47106674630271, "test_micro_f1_se": 2.2770107738472047}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "conll-nl", "task": "named-entity-recognition", "dataset_languages": ["nl"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"micro_f1_no_misc": 0.663470757430489, "micro_f1": 0.5001576789656259}, {"micro_f1_no_misc": 0.6238938053097345, "micro_f1": 0.5381037567084079}, {"micro_f1_no_misc": 0.5470158866466295, "micro_f1": 0.4527027027027027}, {"micro_f1_no_misc": 0.6145992853496681, "micro_f1": 0.5433604336043361}, {"micro_f1_no_misc": 0.6534260178748759, "micro_f1": 0.5666550643926209}, {"micro_f1_no_misc": 0.6531671858774662, "micro_f1": 0.5109641489731988}, {"micro_f1_no_misc": 0.6145833333333333, "micro_f1": 0.5111256544502618}, {"micro_f1_no_misc": 0.598941798941799, "micro_f1": 0.4945015963107485}, {"micro_f1_no_misc": 0.64633493846977, "micro_f1": 0.5598194130925508}, {"micro_f1_no_misc": 0.6514285714285714, "micro_f1": 0.5257367387033398}], "total": {"test_micro_f1_no_misc": 62.668615806623365, "test_micro_f1_no_misc_se": 2.185016949133444, "test_micro_f1": 52.031271879037924, "test_micro_f1_se": 2.107026024704406}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "conll-en", "task": "named-entity-recognition", "dataset_languages": ["en"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"micro_f1_no_misc": 0.6688936127138374, "micro_f1": 0.6369958275382476}, {"micro_f1_no_misc": 0.732637494981935, "micro_f1": 0.6956910423743966}, {"micro_f1_no_misc": 0.7445606694560669, "micro_f1": 0.6962989515189533}, {"micro_f1_no_misc": 0.7090109440523679, "micro_f1": 0.6488605287146764}, {"micro_f1_no_misc": 0.7173668608811203, "micro_f1": 0.6852976913730254}, {"micro_f1_no_misc": 0.7208121827411168, "micro_f1": 0.674500370096225}, {"micro_f1_no_misc": 0.6361704280943226, "micro_f1": 0.6370784553555827}, {"micro_f1_no_misc": 0.7056363065983479, "micro_f1": 0.6667881881152024}, {"micro_f1_no_misc": 0.7111390811831341, "micro_f1": 0.6686041061216837}, {"micro_f1_no_misc": 0.7135984290623465, "micro_f1": 0.676470588235294}], "total": {"test_micro_f1_no_misc": 70.59826009764596, "test_micro_f1_no_misc_se": 1.9481477655274464, "test_micro_f1": 66.86585749443287, "test_micro_f1_se": 1.3487902509730694}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "eltec", "task": "named-entity-recognition", "dataset_languages": ["fr"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"micro_f1_no_misc": 0.5450056116722785, "micro_f1": 0.5224920802534319}, {"micro_f1_no_misc": 0.6105167311632143, "micro_f1": 0.5928785928785929}, {"micro_f1_no_misc": 0.6096131301289566, "micro_f1": 0.5926086016857575}, {"micro_f1_no_misc": 0.5891404704330623, "micro_f1": 0.5724396336386344}, {"micro_f1_no_misc": 0.6337958374628344, "micro_f1": 0.6190812720848057}, {"micro_f1_no_misc": 0.5933503836317136, "micro_f1": 0.5814053127677805}, {"micro_f1_no_misc": 0.5943375837475685, "micro_f1": 0.5604506135586401}, {"micro_f1_no_misc": 0.5657894736842105, "micro_f1": 0.5550949159938904}, {"micro_f1_no_misc": 0.5755395683453238, "micro_f1": 0.5451096121416525}, {"micro_f1_no_misc": 0.5719991124916796, "micro_f1": 0.5450819672131149}], "total": {"test_micro_f1_no_misc": 58.89087902760842, "test_micro_f1_no_misc_se": 1.5851027621258298, "test_micro_f1": 56.86642602216301, "test_micro_f1_se": 1.773957687059962}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "multinerd-it", "task": "named-entity-recognition", "dataset_languages": ["it"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"micro_f1_no_misc": 0.732598833441348, "micro_f1": 0.5798387827189566}, {"micro_f1_no_misc": 0.7462759462759463, "micro_f1": 0.6759224748415952}, {"micro_f1_no_misc": 0.751101321585903, "micro_f1": 0.6624314178457984}, {"micro_f1_no_misc": 0.714414996767938, "micro_f1": 0.625297855440826}, {"micro_f1_no_misc": 0.7625618328560271, "micro_f1": 0.6770727751875126}, {"micro_f1_no_misc": 0.7442339904651463, "micro_f1": 0.6467339554370596}, {"micro_f1_no_misc": 0.7577325791310626, "micro_f1": 0.6103054086333389}, {"micro_f1_no_misc": 0.736629938188659, "micro_f1": 0.6443696383972202}, {"micro_f1_no_misc": 0.6894859526438474, "micro_f1": 0.5815602836879433}, {"micro_f1_no_misc": 0.7416697405631377, "micro_f1": 0.6406601650412603}], "total": {"test_micro_f1_no_misc": 73.76705131919016, "test_micro_f1_no_misc_se": 1.3429064565222277, "test_micro_f1": 63.4419275723151, "test_micro_f1_se": 2.1721646943564425}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-sv", "task": "linguistic-acceptability", "dataset_languages": ["sv"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"mcc": 0.530330647075916, "macro_f1": 0.7613458821633784}, {"mcc": 0.049811986581176346, "macro_f1": 0.3351532599155584}, {"mcc": 0.42899261402704947, "macro_f1": 0.6362828141034844}, {"mcc": 0.420973029704113, "macro_f1": 0.6457666695494249}, {"mcc": 0.300223288646495, "macro_f1": 0.5118989837685369}, {"mcc": 0.38533107655375365, "macro_f1": 0.5976680511144572}, {"mcc": 0.4935288445166094, "macro_f1": 0.7016569684212224}, {"mcc": 0.56532802381243, "macro_f1": 0.7817232413331856}, {"mcc": 0.4443607359538342, "macro_f1": 0.6593518319597534}, {"mcc": 0.4384837684167662, "macro_f1": 0.640868533704678}], "total": {"test_mcc": 40.57364015288144, "test_mcc_se": 9.011964145820583, "test_macro_f1": 62.7171623603368, "test_macro_f1_se": 7.962145442993261}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-da", "task": "linguistic-acceptability", "dataset_languages": ["da"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"mcc": 0.4327140926864803, "macro_f1": 0.6680950819548561}, {"mcc": 0.41148444693911856, "macro_f1": 0.6326997427416391}, {"mcc": 0.5052379528364198, "macro_f1": 0.7451033401598448}, {"mcc": 0.37535557775269063, "macro_f1": 0.619765897404319}, {"mcc": 0.34478358488909483, "macro_f1": 0.552614244786283}, {"mcc": 0.4389518714878117, "macro_f1": 0.6532452382748863}, {"mcc": 0.43603819819972356, "macro_f1": 0.6551717497407392}, {"mcc": 0.3456672689795103, "macro_f1": 0.572736097479875}, {"mcc": 0.5190497893269728, "macro_f1": 0.7480333840683382}, {"mcc": 0.5085931008798656, "macro_f1": 0.7418222506706278}], "total": {"test_mcc": 43.17875883977688, "test_mcc_se": 4.004148988090075, "test_macro_f1": 65.8928702728141, "test_macro_f1_se": 4.300159621260068}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-nb", "task": "linguistic-acceptability", "dataset_languages": ["nb", "no"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"mcc": 0.47868666574622076, "macro_f1": 0.6833033368501624}, {"mcc": 0.14516432522157965, "macro_f1": 0.3760555405105985}, {"mcc": 0.3244440423784421, "macro_f1": 0.5263730682725096}, {"mcc": 0.3064654025937491, "macro_f1": 0.5130749073823078}, {"mcc": 0.07574473205936329, "macro_f1": 0.3521592800853507}, {"mcc": 0.14756364648194262, "macro_f1": 0.3762471552081691}, {"mcc": 0.1690998043605464, "macro_f1": 0.39305201399554945}, {"mcc": 0.41083554082124124, "macro_f1": 0.6270098181929671}, {"mcc": 0.44558184096915426, "macro_f1": 0.6606311411607743}, {"mcc": 0.3477361006977311, "macro_f1": 0.54456234868606}], "total": {"test_mcc": 28.513221013299706, "test_mcc_se": 8.783659440639088, "test_macro_f1": 50.52468610344448, "test_macro_f1_se": 7.787632369722876}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-nn", "task": "linguistic-acceptability", "dataset_languages": ["nn"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"mcc": 0.05746532916557984, "macro_f1": 0.41074688467338677}, {"mcc": 0.050263288653450706, "macro_f1": 0.35328065103665107}, {"mcc": 0.0, "macro_f1": 0.3342002600780234}, {"mcc": 0.030509121111746976, "macro_f1": 0.35715492628092604}, {"mcc": 0.0, "macro_f1": 0.3324641460234681}, {"mcc": 0.017030201842166404, "macro_f1": 0.35515629979764163}, {"mcc": 0.022494479464640612, "macro_f1": 0.33831904412717323}, {"mcc": 0.08799295400189672, "macro_f1": 0.36872660133239776}, {"mcc": 0.05163192263341575, "macro_f1": 0.3449990183598376}, {"mcc": 0.04797613184305685, "macro_f1": 0.3460689268880053}], "total": {"test_mcc": 3.6536342871595386, "test_mcc_se": 1.7182516629958426, "test_macro_f1": 35.411167585975114, "test_macro_f1_se": 1.4155488124255367}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-is", "task": "linguistic-acceptability", "dataset_languages": ["is"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"mcc": 0.0, "macro_f1": 0.32342253055830855}, {"mcc": 0.0, "macro_f1": 0.3361426256077796}, {"mcc": 0.0, "macro_f1": 0.3335502766026684}, {"mcc": 0.0, "macro_f1": 0.3285245901639344}, {"mcc": 0.0, "macro_f1": 0.3263157894736842}, {"mcc": 0.0, "macro_f1": 0.34169077467052394}, {"mcc": 0.0, "macro_f1": 0.32940406024885394}, {"mcc": 0.0, "macro_f1": 0.3247609627431586}, {"mcc": 0.0, "macro_f1": 0.33376707872478856}, {"mcc": 0.0, "macro_f1": 0.3382875605815832}], "total": {"test_mcc": 0.0, "test_mcc_se": 0.0, "test_macro_f1": 33.15866249375283, "test_macro_f1_se": 0.3761789184945163}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-fo", "task": "linguistic-acceptability", "dataset_languages": ["fo"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"mcc": 0.0, "macro_f1": 0.3359273670557717}, {"mcc": 0.0, "macro_f1": 0.3324641460234681}, {"mcc": 0.0, "macro_f1": 0.3427471116816431}, {"mcc": 0.0, "macro_f1": 0.3342002600780234}, {"mcc": 0.0, "macro_f1": 0.326758711374096}, {"mcc": 0.0, "macro_f1": 0.33376707872478856}, {"mcc": 0.0, "macro_f1": 0.31505016722408025}, {"mcc": 0.0, "macro_f1": 0.3333333333333333}, {"mcc": 0.0, "macro_f1": 0.32587228439763}, {"mcc": 0.0, "macro_f1": 0.34568690095846644}], "total": {"test_mcc": 0.0, "test_mcc_se": 0.0, "test_macro_f1": 33.258073608513, "test_macro_f1_se": 0.5376250951098089}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-de", "task": "linguistic-acceptability", "dataset_languages": ["de"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"mcc": 0.22102913540319544, "macro_f1": 0.5286133397019809}, {"mcc": 0.4265590737453341, "macro_f1": 0.7026231191074371}, {"mcc": 0.2238047912674064, "macro_f1": 0.4454691001667889}, {"mcc": 0.3970223100761684, "macro_f1": 0.6984629584952737}, {"mcc": 0.32619937588734316, "macro_f1": 0.5423013659443611}, {"mcc": 0.4645084857852725, "macro_f1": 0.7164826383040458}, {"mcc": 0.4198958543145524, "macro_f1": 0.7008350954126159}, {"mcc": 0.3006996256150065, "macro_f1": 0.5794621990633161}, {"mcc": 0.22902421453824606, "macro_f1": 0.6065473939347072}, {"mcc": 0.38667966234633566, "macro_f1": 0.6798157324624905}], "total": {"test_mcc": 33.95422528978861, "test_mcc_se": 5.714804101267549, "test_macro_f1": 62.006129425930176, "test_macro_f1_se": 5.8098208935346385}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-nl", "task": "linguistic-acceptability", "dataset_languages": ["nl"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"mcc": 0.4449027219986931, "macro_f1": 0.6708110144642652}, {"mcc": 0.4920554434356634, "macro_f1": 0.731083837644813}, {"mcc": 0.27530317316613684, "macro_f1": 0.5149448703353452}, {"mcc": 0.5219222601527654, "macro_f1": 0.7510152198450031}, {"mcc": 0.35413252627120545, "macro_f1": 0.5712828044256177}, {"mcc": 0.4506576647401793, "macro_f1": 0.7229589919587164}, {"mcc": 0.5409891532966398, "macro_f1": 0.7704585582500518}, {"mcc": 0.37888794828158934, "macro_f1": 0.6314934409713462}, {"mcc": 0.4800248450507906, "macro_f1": 0.738206222333087}, {"mcc": 0.3401589934117437, "macro_f1": 0.6143792948347286}], "total": {"test_mcc": 42.790347298054066, "test_mcc_se": 5.391380269944525, "test_macro_f1": 67.16634255062974, "test_macro_f1_se": 5.314935949138335}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-en", "task": "linguistic-acceptability", "dataset_languages": ["en"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"mcc": 0.43887402750668725, "macro_f1": 0.7151575495889358}, {"mcc": 0.47701941735447184, "macro_f1": 0.7250693530842984}, {"mcc": 0.5366134142884327, "macro_f1": 0.7668809305172941}, {"mcc": 0.4572864485021104, "macro_f1": 0.7130271181134797}, {"mcc": 0.5215864314642958, "macro_f1": 0.7567216748230595}, {"mcc": 0.5200940814906947, "macro_f1": 0.7582151783554669}, {"mcc": 0.47967531295817045, "macro_f1": 0.7391145038167939}, {"mcc": 0.5204549858563964, "macro_f1": 0.7578905392181194}, {"mcc": 0.45844008007941756, "macro_f1": 0.7189876769154506}, {"mcc": 0.43390897655817395, "macro_f1": 0.6742500888407018}], "total": {"test_mcc": 48.4395317605885, "test_mcc_se": 2.3366068169788172, "test_macro_f1": 73.253146132736, "test_macro_f1_se": 1.7807657767770495}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-fr", "task": "linguistic-acceptability", "dataset_languages": ["fr"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"mcc": 0.28432325093613225, "macro_f1": 0.5309809251187989}, {"mcc": 0.14596395995062575, "macro_f1": 0.5710018214936248}, {"mcc": 0.3346214172533131, "macro_f1": 0.6545987180424557}, {"mcc": 0.21294177198665296, "macro_f1": 0.5980710450425377}, {"mcc": 0.3112031566239496, "macro_f1": 0.5743694497441476}, {"mcc": 0.3991250425560089, "macro_f1": 0.6736973353293039}, {"mcc": 0.33913856823883143, "macro_f1": 0.6694158598347814}, {"mcc": 0.35233889782640737, "macro_f1": 0.6103550046378278}, {"mcc": 0.3541975129254918, "macro_f1": 0.6063004276534404}, {"mcc": 0.40739073226678657, "macro_f1": 0.6904605425159931}], "total": {"test_mcc": 31.412443105641998, "test_mcc_se": 5.037376103715049, "test_macro_f1": 61.79251129412911, "test_macro_f1_se": 3.241283156893308}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-it", "task": "linguistic-acceptability", "dataset_languages": ["it"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"mcc": 0.24648738307012055, "macro_f1": 0.6212380719354049}, {"mcc": 0.18766723904634827, "macro_f1": 0.5404640048439521}, {"mcc": -0.012224918167857328, "macro_f1": 0.45417217644430846}, {"mcc": 0.10521080292025066, "macro_f1": 0.5461841756666199}, {"mcc": 0.18947852103719987, "macro_f1": 0.5368481627345765}, {"mcc": 0.22422894817176656, "macro_f1": 0.6100804793066164}, {"mcc": 0.014380224336480417, "macro_f1": 0.49711894296215564}, {"mcc": 0.2506765456559198, "macro_f1": 0.6253381236519477}, {"mcc": 0.025443886598495523, "macro_f1": 0.501011620971546}, {"mcc": 0.04588754043056815, "macro_f1": 0.5229436455341889}], "total": {"test_mcc": 12.772361730992927, "test_mcc_se": 6.402662345687044, "test_macro_f1": 54.55399404051317, "test_macro_f1_se": 3.5515857741915626}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"f1": 68.7817290586429, "em": 62.72727272727273}, {"f1": 68.62461942067704, "em": 63.83623957543594}, {"f1": 67.99791958882865, "em": 62.54856254856255}, {"f1": 68.32481424376397, "em": 63.39421613394216}, {"f1": 68.283534763874, "em": 63.99383191981496}, {"f1": 69.54659226511254, "em": 63.81679389312977}, {"f1": 66.84872351604018, "em": 61.07644305772231}, {"f1": 67.21392239827526, "em": 61.63522012578616}, {"f1": 67.64099081670778, "em": 62.84829721362229}, {"f1": 68.91989085521809, "em": 61.95987654320987}], "total": {"test_f1": 68.21827369271405, "test_f1_se": 0.5069591445918995, "test_em": 62.78367537384988, "test_em_se": 0.6193701855602406}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "norquad", "task": "reading-comprehension", "dataset_languages": ["nb", "nn", "no"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"f1": 84.79728911558236, "em": 64.86928104575163}, {"f1": 66.80330345128736, "em": 41.13821138211382}, {"f1": 81.83379466016274, "em": 61.69858451290591}, {"f1": 74.86364270486814, "em": 49.9591169255928}, {"f1": 86.21511019118726, "em": 67.65922249793218}, {"f1": 69.53106343512052, "em": 44.19753086419753}, {"f1": 69.43611343007353, "em": 44.705882352941174}, {"f1": 82.56947261567174, "em": 61.6554054054054}, {"f1": 65.48455656800931, "em": 39.20265780730897}, {"f1": 67.088746387001, "em": 40.728476821192054}], "total": {"test_f1": 74.8623092558964, "test_f1_se": 5.0891931412546665, "test_em": 51.58143696153415, "test_em_se": 6.924755859153584}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scandiqa-sv", "task": "reading-comprehension", "dataset_languages": ["sv"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"f1": 66.76839826839822, "em": 60.22727272727273}, {"f1": 66.73233799900612, "em": 60.65200909780136}, {"f1": 70.50452978774653, "em": 65.73426573426573}, {"f1": 68.18961796701518, "em": 63.16590563165906}, {"f1": 69.30428861808967, "em": 64.070932922128}, {"f1": 65.91736889065132, "em": 60.534351145038165}, {"f1": 68.41129770731949, "em": 62.01248049921997}, {"f1": 68.36836918066103, "em": 62.971698113207545}, {"f1": 68.35403137454219, "em": 62.693498452012385}, {"f1": 67.8535353535353, "em": 63.04012345679013}], "total": {"test_f1": 68.04037751469652, "test_f1_se": 0.8239852761353196, "test_em": 62.51025377793951, "test_em_se": 1.0656379070629358}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "nqii", "task": "reading-comprehension", "dataset_languages": ["is"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"f1": 59.88871373867452, "em": 28.76923076923077}, {"f1": 61.7753353219018, "em": 29.439252336448597}, {"f1": 54.94853355480377, "em": 32.398753894081}, {"f1": 56.94656208289267, "em": 32.13751868460388}, {"f1": 61.016369595756, "em": 28.213166144200628}, {"f1": 60.24507570738287, "em": 32.357473035439135}, {"f1": 60.01016640767376, "em": 33.07692307692308}, {"f1": 57.41533206996614, "em": 32.82208588957055}, {"f1": 60.3939818418717, "em": 29.819277108433734}, {"f1": 61.28702974612324, "em": 35.993975903614455}], "total": {"test_f1": 59.39271000670465, "test_f1_se": 1.3689217317089717, "test_em": 31.50276568425458, "test_em_se": 1.4856931703597192}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "foqa", "task": "reading-comprehension", "dataset_languages": ["fo"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"f1": 67.84601128856109, "em": 49.509803921568626}, {"f1": 67.30680129698824, "em": 49.631449631449634}, {"f1": 73.18632361013934, "em": 55.30864197530864}, {"f1": 60.781398169686604, "em": 44.90291262135922}, {"f1": 66.8097922407117, "em": 51.23762376237624}, {"f1": 68.26724531115153, "em": 51.60493827160494}, {"f1": 71.29363152553181, "em": 54.009433962264154}, {"f1": 66.79886462606831, "em": 50.12722646310433}, {"f1": 65.08949582868085, "em": 49.12718204488778}, {"f1": 63.65425856938707, "em": 45.945945945945944}], "total": {"test_f1": 67.10338224669064, "test_f1_se": 2.192382835836446, "test_em": 50.14051585998695, "test_em_se": 1.9788247536657038}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "germanquad", "task": "reading-comprehension", "dataset_languages": ["de"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"f1": 72.70177144407586, "em": 43.333333333333336}, {"f1": 66.37494574888008, "em": 35.78468536770281}, {"f1": 69.97409639394608, "em": 37.60683760683761}, {"f1": 73.26606138348966, "em": 43.531202435312025}, {"f1": 71.727021466396, "em": 42.40555127216654}, {"f1": 71.26256912248141, "em": 38.93129770992366}, {"f1": 63.97409413602583, "em": 35.17940717628705}, {"f1": 71.0566467843247, "em": 42.68867924528302}, {"f1": 70.41054127967428, "em": 40.63467492260062}, {"f1": 60.82869111485306, "em": 35.80246913580247}], "total": {"test_f1": 69.15764388741471, "test_f1_se": 2.532869405068918, "test_em": 39.58981382052491, "test_em_se": 2.074393200421472}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "squad", "task": "reading-comprehension", "dataset_languages": ["en"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"f1": 89.36371794254546, "em": 79.24242424242425}, {"f1": 87.72252943404159, "em": 76.87642153146324}, {"f1": 89.42712326296135, "em": 78.78787878787878}, {"f1": 88.04910089274901, "em": 78.84322678843226}, {"f1": 88.71676892287284, "em": 78.56592135697764}, {"f1": 80.82123096208582, "em": 65.95419847328245}, {"f1": 86.17068135498248, "em": 73.32293291731669}, {"f1": 88.9003956254588, "em": 77.59433962264151}, {"f1": 88.014121103542, "em": 76.16099071207431}, {"f1": 82.54460680268342, "em": 67.51543209876543}], "total": {"test_f1": 86.97302763039228, "test_f1_se": 1.840918998758117, "test_em": 75.28637665312564, "test_em_se": 3.00298575506329}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "squad-nl", "task": "reading-comprehension", "dataset_languages": ["nl"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"f1": 81.86823587194766, "em": 64.77272727272727}, {"f1": 81.51088221670082, "em": 65.27672479150871}, {"f1": 74.60970465992511, "em": 62.7039627039627}, {"f1": 77.42655828535307, "em": 63.850837138508375}, {"f1": 82.42369189989896, "em": 68.31148804934465}, {"f1": 82.89565479508603, "em": 66.6412213740458}, {"f1": 79.81728874081591, "em": 65.13260530421216}, {"f1": 81.47903075670915, "em": 68.63207547169812}, {"f1": 79.34751023755253, "em": 68.42105263157895}, {"f1": 82.77326809439994, "em": 66.35802469135803}], "total": {"test_f1": 80.41518255583892, "test_f1_se": 1.6569950746629456, "test_em": 66.01007194289448, "test_em_se": 1.255608901153463}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "squad-it", "task": "reading-comprehension", "dataset_languages": ["it"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"f1": 76.37767022066997, "em": 61.21212121212121}, {"f1": 75.49916643055994, "em": 55.799848369977255}, {"f1": 68.84284002017647, "em": 52.836052836052836}, {"f1": 75.99411450215644, "em": 59.208523592085236}, {"f1": 71.85833336636769, "em": 47.10871241326137}, {"f1": 75.0793758745325, "em": 59.23664122137404}, {"f1": 76.34786428461076, "em": 60.9204368174727}, {"f1": 73.79499613012955, "em": 53.85220125786164}, {"f1": 56.541606504069726, "em": 44.11764705882353}, {"f1": 72.73733026646592, "em": 48.91975308641975}], "total": {"test_f1": 72.3073297599739, "test_f1_se": 3.7389759328358383, "test_em": 54.32119378654496, "test_em_se": 3.747436723632353}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fquad", "task": "reading-comprehension", "dataset_languages": ["fr"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"f1": 75.39667335687099, "em": 45.90909090909091}, {"f1": 74.98055583725515, "em": 47.005307050796056}, {"f1": 77.46486446416975, "em": 49.184149184149184}, {"f1": 77.19574337117733, "em": 49.923896499238964}, {"f1": 68.3638893202978, "em": 40.40092521202776}, {"f1": 79.5728071851161, "em": 50.76335877862596}, {"f1": 75.93469390189236, "em": 46.56786271450858}, {"f1": 75.79828667746581, "em": 47.72012578616352}, {"f1": 69.83496017406578, "em": 43.4984520123839}, {"f1": 79.16089597970007, "em": 51.31172839506173}], "total": {"test_f1": 75.37033702680112, "test_f1_se": 2.264468931892345, "test_em": 47.22848965420466, "test_em_se": 2.0993820888795827}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "nordjylland-news", "task": "summarization", "dataset_languages": ["da"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"bertscore": 0.6874684540816816, "rouge_l": 0.25487857339922504}, {"bertscore": 0.680522779803141, "rouge_l": 0.23847160407600126}, {"bertscore": 0.6824266423500376, "rouge_l": 0.24632172572251207}, {"bertscore": 0.6852713623666205, "rouge_l": 0.2513680081033185}, {"bertscore": 0.6926801748777507, "rouge_l": 0.26029559177168515}, {"bertscore": 0.6836151375609916, "rouge_l": 0.24603513476170447}, {"bertscore": 0.6768975649320055, "rouge_l": 0.23379807710493533}, {"bertscore": 0.6836805467901286, "rouge_l": 0.24516123320835037}, {"bertscore": 0.6858180426206673, "rouge_l": 0.25012967421401266}, {"bertscore": 0.6848692157655023, "rouge_l": 0.25466283635623954}], "total": {"test_bertscore": 68.43249921148526, "test_bertscore_se": 0.25908423769425454, "test_rouge_l": 24.811224587179844, "test_rouge_l_se": 0.49146284084466}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mlsum", "task": "summarization", "dataset_languages": ["de"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"bertscore": 0.6911412594490685, "rouge_l": 0.2762948997585629}, {"bertscore": 0.7047675929206889, "rouge_l": 0.31986978986948167}, {"bertscore": 0.7005033997556893, "rouge_l": 0.29720478659236793}, {"bertscore": 0.680195468958118, "rouge_l": 0.2576744192041073}, {"bertscore": 0.718868833660963, "rouge_l": 0.34072150972687465}, {"bertscore": 0.6945324392727343, "rouge_l": 0.28379337825982287}, {"bertscore": 0.7202049970655935, "rouge_l": 0.3591233336113559}, {"bertscore": 0.6899703588860575, "rouge_l": 0.2716442839592823}, {"bertscore": 0.7137419636419509, "rouge_l": 0.3275325394832906}, {"bertscore": 0.6892183930322062, "rouge_l": 0.29606012010151056}], "total": {"test_bertscore": 70.0314470664307, "test_bertscore_se": 0.84997097205733, "test_rouge_l": 30.29919060566657, "test_rouge_l_se": 2.030654997266594}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "rrn", "task": "summarization", "dataset_languages": ["is"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"bertscore": 0.6774995021696668, "rouge_l": 0.22357039924291583}, {"bertscore": 0.6722674178017769, "rouge_l": 0.2147114246370922}, {"bertscore": 0.6817146459943615, "rouge_l": 0.23367301577584884}, {"bertscore": 0.6727128826605622, "rouge_l": 0.2098909149035296}, {"bertscore": 0.6698484402440954, "rouge_l": 0.21211602676943647}, {"bertscore": 0.6779496469825972, "rouge_l": 0.21797561917902009}, {"bertscore": 0.6772677585540805, "rouge_l": 0.21255930309499044}, {"bertscore": 0.6642953400150873, "rouge_l": 0.20146140402440554}, {"bertscore": 0.6642977301380597, "rouge_l": 0.19556360696262626}, {"bertscore": 0.6699792025319766, "rouge_l": 0.20642341295222483}], "total": {"test_bertscore": 67.27832567092264, "test_bertscore_se": 0.3635322042358913, "test_rouge_l": 21.279451275420904, "test_rouge_l_se": 0.6718322194003215}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "no-sammendrag", "task": "summarization", "dataset_languages": ["nb", "nn", "no"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"bertscore": 0.645824988707318, "rouge_l": 0.18916318889846695}, {"bertscore": 0.6692635903164046, "rouge_l": 0.2164177774887161}, {"bertscore": 0.6702083124837372, "rouge_l": 0.2178037324929524}, {"bertscore": 0.6666620721225627, "rouge_l": 0.21568625425720678}, {"bertscore": 0.6673392487864476, "rouge_l": 0.21195301262006422}, {"bertscore": 0.6654354273778154, "rouge_l": 0.21042040154810088}, {"bertscore": 0.6559668584814062, "rouge_l": 0.19451416639343222}, {"bertscore": 0.6641274752182653, "rouge_l": 0.20947414700949143}, {"bertscore": 0.6703594518330647, "rouge_l": 0.21736033693545853}, {"bertscore": 0.6554046225501224, "rouge_l": 0.19078604441158709}], "total": {"test_bertscore": 66.30592047877144, "test_bertscore_se": 0.5005134696513143, "test_rouge_l": 20.735790620554763, "test_rouge_l_se": 0.7055440124532976}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "wiki-lingua-nl", "task": "summarization", "dataset_languages": ["nl"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"bertscore": 0.6751050266466336, "rouge_l": 0.21376884207665636}, {"bertscore": 0.6816048355249222, "rouge_l": 0.22735725941733165}, {"bertscore": 0.6235454415291315, "rouge_l": 0.13279169739421331}, {"bertscore": 0.6967284290440148, "rouge_l": 0.2461255825381014}, {"bertscore": 0.6679829379936564, "rouge_l": 0.21204884692055453}, {"bertscore": 0.6846555700903991, "rouge_l": 0.23419984693023757}, {"bertscore": 0.6861562660633354, "rouge_l": 0.23602405167890939}, {"bertscore": 0.6725919146265369, "rouge_l": 0.21096890491239467}, {"bertscore": 0.6872061229369137, "rouge_l": 0.2368888455045624}, {"bertscore": 0.6785114301892463, "rouge_l": 0.22438947358274547}], "total": {"test_bertscore": 67.5408797464479, "test_bertscore_se": 1.2388739967262608, "test_rouge_l": 21.745633509557067, "test_rouge_l_se": 1.9842171504675585}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "swedn", "task": "summarization", "dataset_languages": ["sv"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"bertscore": 0.6510814205248607, "rouge_l": 0.19905690677666274}, {"bertscore": 0.6538722871628124, "rouge_l": 0.19518977608599153}, {"bertscore": 0.6584613629966043, "rouge_l": 0.2041467343429308}, {"bertscore": 0.6577969145728275, "rouge_l": 0.19782928507524827}, {"bertscore": 0.6534915443917271, "rouge_l": 0.19268640166556247}, {"bertscore": 0.6579065929399803, "rouge_l": 0.19954935478888702}, {"bertscore": 0.6572489543614211, "rouge_l": 0.19617893920206947}, {"bertscore": 0.6520037662412506, "rouge_l": 0.20171977013747855}, {"bertscore": 0.6599876786203822, "rouge_l": 0.20772147952500253}, {"bertscore": 0.6615773646626621, "rouge_l": 0.2105891936200921}], "total": {"test_bertscore": 65.63427886474528, "test_bertscore_se": 0.21796041128341773, "test_rouge_l": 20.046678412199256, "test_rouge_l_se": 0.3498588702320961}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "cnn-dailymail", "task": "summarization", "dataset_languages": ["en"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"bertscore": 0.7148376344703138, "rouge_l": 0.277440872808566}, {"bertscore": 0.7167618521634722, "rouge_l": 0.27713466373764206}, {"bertscore": 0.7120812675711932, "rouge_l": 0.2626985072463837}, {"bertscore": 0.7131105758890044, "rouge_l": 0.2432277597072735}, {"bertscore": 0.714284643807332, "rouge_l": 0.2660652719110706}, {"bertscore": 0.7040651041606907, "rouge_l": 0.2671459753008858}, {"bertscore": 0.7131464660196798, "rouge_l": 0.2778220577048657}, {"bertscore": 0.7163703028636519, "rouge_l": 0.2711186083940067}, {"bertscore": 0.6865953417291166, "rouge_l": 0.2467276025337901}, {"bertscore": 0.7168258693127427, "rouge_l": 0.2773759651112085}], "total": {"test_bertscore": 71.08079057987197, "test_bertscore_se": 0.575170221816892, "test_rouge_l": 26.66757284455693, "test_rouge_l_se": 0.7861475072697506}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "orange-sum", "task": "summarization", "dataset_languages": ["fr"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"bertscore": 0.6809871107689105, "rouge_l": 0.24325708097502113}, {"bertscore": 0.6784128711442463, "rouge_l": 0.23974710399394533}, {"bertscore": 0.6781581692048348, "rouge_l": 0.2450149015630614}, {"bertscore": 0.6563614651095122, "rouge_l": 0.2103033964674075}, {"bertscore": 0.6626665253716055, "rouge_l": 0.2215536163455175}, {"bertscore": 0.6791304040816613, "rouge_l": 0.24193017571427633}, {"bertscore": 0.6795181058696471, "rouge_l": 0.24215632491605055}, {"bertscore": 0.6600232673226856, "rouge_l": 0.21540804222951615}, {"bertscore": 0.6784884118242189, "rouge_l": 0.23940734239364878}, {"bertscore": 0.6685373700747732, "rouge_l": 0.22595279302999505}], "total": {"test_bertscore": 67.22283700772095, "test_bertscore_se": 0.5828480792988122, "test_rouge_l": 23.247307776284394, "test_rouge_l_se": 0.8007661462564829}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "ilpost-sum", "task": "summarization", "dataset_languages": ["it"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"bertscore": 0.6813709051348269, "rouge_l": 0.2603868652068473}, {"bertscore": 0.6737012731027789, "rouge_l": 0.24639530223066808}, {"bertscore": 0.6842440884502139, "rouge_l": 0.26517273631483224}, {"bertscore": 0.6771608752314933, "rouge_l": 0.249218730684596}, {"bertscore": 0.6680394121358404, "rouge_l": 0.22756829019848931}, {"bertscore": 0.6778241802530829, "rouge_l": 0.25320094012050465}, {"bertscore": 0.6834881468967069, "rouge_l": 0.259989343593101}, {"bertscore": 0.6818410457635764, "rouge_l": 0.2614877769151408}, {"bertscore": 0.6854388416977599, "rouge_l": 0.26388498067175004}, {"bertscore": 0.6860700545075815, "rouge_l": 0.2634531853391979}], "total": {"test_bertscore": 67.99178823173861, "test_bertscore_se": 0.356250545793116, "test_rouge_l": 25.50758151275127, "test_rouge_l_se": 0.719565519438649}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "danske-talemaader", "task": "knowledge", "dataset_languages": ["da"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"mcc": 0.6154652787658289, "accuracy": 0.7004950495049505}, {"mcc": 0.6299771182035171, "accuracy": 0.7165841584158416}, {"mcc": 0.5729099055072263, "accuracy": 0.6633663366336634}, {"mcc": 0.6071801422996362, "accuracy": 0.6967821782178217}, {"mcc": 0.6233303186933061, "accuracy": 0.7054455445544554}, {"mcc": 0.5533495312326057, "accuracy": 0.650990099009901}, {"mcc": 0.6189348888488597, "accuracy": 0.7054455445544554}, {"mcc": 0.5139268767906179, "accuracy": 0.6188118811881188}, {"mcc": 0.59807151066652, "accuracy": 0.6881188118811881}, {"mcc": 0.5926145185870013, "accuracy": 0.6794554455445545}], "total": {"test_mcc": 59.25760089595118, "test_mcc_se": 2.2554691107271445, "test_accuracy": 68.2549504950495, "test_accuracy_se": 1.8706898672003436}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "danish-citizen-tests", "task": "knowledge", "dataset_languages": ["da"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"mcc": 0.7268537880096484, "accuracy": 0.8114285714285714}, {"mcc": 0.8545379805438008, "accuracy": 0.9028571428571428}, {"mcc": 0.7377816636925609, "accuracy": 0.8247619047619048}, {"mcc": 0.7653411582885945, "accuracy": 0.8342857142857143}, {"mcc": 0.8130201240513081, "accuracy": 0.8723809523809524}, {"mcc": 0.7828476552094693, "accuracy": 0.8495238095238096}, {"mcc": 0.7780570214073046, "accuracy": 0.8438095238095238}, {"mcc": 0.765511905465197, "accuracy": 0.8438095238095238}, {"mcc": 0.8124305961853078, "accuracy": 0.8704761904761905}, {"mcc": 0.769564188483395, "accuracy": 0.8476190476190476}], "total": {"test_mcc": 78.05946081336586, "test_mcc_se": 2.341717638851359, "test_accuracy": 85.0095238095238, "test_accuracy_se": 1.6226047646368542}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "nrk-quiz-qa", "task": "knowledge", "dataset_languages": ["nb", "nn", "no"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"mcc": 0.43477328483184213, "accuracy": 0.57958984375}, {"mcc": 0.39168608119348985, "accuracy": 0.54541015625}, {"mcc": 0.40107838863266365, "accuracy": 0.5537109375}, {"mcc": 0.45187165487107306, "accuracy": 0.59033203125}, {"mcc": 0.44430952056851236, "accuracy": 0.58251953125}, {"mcc": 0.42110021585853324, "accuracy": 0.5546875}, {"mcc": 0.3991477469994505, "accuracy": 0.55224609375}, {"mcc": 0.41992692985471164, "accuracy": 0.56884765625}, {"mcc": 0.39058025016916464, "accuracy": 0.5498046875}, {"mcc": 0.4136708317619993, "accuracy": 0.56103515625}], "total": {"test_mcc": 41.68144904741441, "test_mcc_se": 1.342889772900729, "test_accuracy": 56.3818359375, "test_accuracy_se": 0.9666647308154835}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-sv", "task": "knowledge", "dataset_languages": ["sv"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"mcc": 0.4322132484495138, "accuracy": 0.57470703125}, {"mcc": 0.42592043015921244, "accuracy": 0.568359375}, {"mcc": 0.43278487665792686, "accuracy": 0.57421875}, {"mcc": 0.40764929611087164, "accuracy": 0.5576171875}, {"mcc": 0.44624374508304515, "accuracy": 0.5830078125}, {"mcc": 0.41999226793289296, "accuracy": 0.56494140625}, {"mcc": 0.4291485397897299, "accuracy": 0.57275390625}, {"mcc": 0.45212864847672496, "accuracy": 0.58740234375}, {"mcc": 0.4512811772733088, "accuracy": 0.58740234375}, {"mcc": 0.4420748610170088, "accuracy": 0.58154296875}], "total": {"test_mcc": 43.39437090950235, "test_mcc_se": 0.8829502683435414, "test_accuracy": 57.51953125, "test_accuracy_se": 0.6084658543718086}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "icelandic-knowledge", "task": "knowledge", "dataset_languages": ["is"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"mcc": 0.163744775406742, "accuracy": 0.37109375}, {"mcc": 0.13438054632115792, "accuracy": 0.33984375}, {"mcc": 0.15619774228930064, "accuracy": 0.359375}, {"mcc": 0.12722633435871747, "accuracy": 0.3310546875}, {"mcc": 0.1261329448809762, "accuracy": 0.337890625}, {"mcc": 0.17489165801408982, "accuracy": 0.376953125}, {"mcc": 0.15163626842962913, "accuracy": 0.3564453125}, {"mcc": 0.13638917915480162, "accuracy": 0.345703125}, {"mcc": 0.12199447066301457, "accuracy": 0.3359375}, {"mcc": 0.15693452418657286, "accuracy": 0.3583984375}], "total": {"test_mcc": 14.495284437050021, "test_mcc_se": 1.12113985820425, "test_accuracy": 35.126953125, "test_accuracy_se": 0.9646782165834179}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-de", "task": "knowledge", "dataset_languages": ["de"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"mcc": 0.4327168937548088, "accuracy": 0.5751953125}, {"mcc": 0.44659394184183326, "accuracy": 0.58447265625}, {"mcc": 0.43439291988465056, "accuracy": 0.5751953125}, {"mcc": 0.4491259094677987, "accuracy": 0.5859375}, {"mcc": 0.46421968466988967, "accuracy": 0.5966796875}, {"mcc": 0.4405142659818074, "accuracy": 0.5791015625}, {"mcc": 0.44828023021579183, "accuracy": 0.583984375}, {"mcc": 0.4648343524168879, "accuracy": 0.599609375}, {"mcc": 0.4757833518416788, "accuracy": 0.603515625}, {"mcc": 0.46983737197742936, "accuracy": 0.6015625}], "total": {"test_mcc": 45.26298922052576, "test_mcc_se": 0.9374787905715483, "test_accuracy": 58.8525390625, "test_accuracy_se": 0.6764299763605306}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-nl", "task": "knowledge", "dataset_languages": ["nl"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"mcc": 0.4419165839301969, "accuracy": 0.5771484375}, {"mcc": 0.45941224578962125, "accuracy": 0.59228515625}, {"mcc": 0.43994663715191107, "accuracy": 0.58056640625}, {"mcc": 0.4442358716300202, "accuracy": 0.58154296875}, {"mcc": 0.4280727658213473, "accuracy": 0.5703125}, {"mcc": 0.43947284605910564, "accuracy": 0.576171875}, {"mcc": 0.4710142842524444, "accuracy": 0.6015625}, {"mcc": 0.42943937331337134, "accuracy": 0.5703125}, {"mcc": 0.4301603196185621, "accuracy": 0.57080078125}, {"mcc": 0.40553569257255406, "accuracy": 0.55224609375}], "total": {"test_mcc": 43.892066201391344, "test_mcc_se": 1.1101595639145363, "test_accuracy": 57.7294921875, "test_accuracy_se": 0.829187791002537}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu", "task": "knowledge", "dataset_languages": ["en"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"mcc": 0.5398996002229585, "accuracy": 0.65185546875}, {"mcc": 0.5251400286372154, "accuracy": 0.64208984375}, {"mcc": 0.5659863030824446, "accuracy": 0.673828125}, {"mcc": 0.568557420391599, "accuracy": 0.67138671875}, {"mcc": 0.553352892400658, "accuracy": 0.66357421875}, {"mcc": 0.5413147393921255, "accuracy": 0.65380859375}, {"mcc": 0.5247753375603746, "accuracy": 0.63818359375}, {"mcc": 0.5425155589828071, "accuracy": 0.65576171875}, {"mcc": 0.5219147498633296, "accuracy": 0.64013671875}, {"mcc": 0.5362871507845142, "accuracy": 0.65283203125}], "total": {"test_mcc": 54.19743781318027, "test_mcc_se": 1.0187956753690197, "test_accuracy": 65.4345703125, "test_accuracy_se": 0.7672780819291999}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-fr", "task": "knowledge", "dataset_languages": ["fr"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"mcc": 0.47559508330535955, "accuracy": 0.60546875}, {"mcc": 0.47111514259979514, "accuracy": 0.60107421875}, {"mcc": 0.45389531399255134, "accuracy": 0.58984375}, {"mcc": 0.47883414213377085, "accuracy": 0.60791015625}, {"mcc": 0.4699973026047732, "accuracy": 0.6025390625}, {"mcc": 0.4731967429676379, "accuracy": 0.60107421875}, {"mcc": 0.5001907326372773, "accuracy": 0.623046875}, {"mcc": 0.4902930647609146, "accuracy": 0.61669921875}, {"mcc": 0.5024872112609311, "accuracy": 0.625}, {"mcc": 0.4592228531197404, "accuracy": 0.59228515625}], "total": {"test_mcc": 47.74827589382752, "test_mcc_se": 0.9940780539535784, "test_accuracy": 60.64941406249999, "test_accuracy_se": 0.7383489029895313}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-it", "task": "knowledge", "dataset_languages": ["it"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"mcc": 0.49094494421896495, "accuracy": 0.61572265625}, {"mcc": 0.4914740045289741, "accuracy": 0.615234375}, {"mcc": 0.45718660768974684, "accuracy": 0.58935546875}, {"mcc": 0.4590930637272173, "accuracy": 0.59326171875}, {"mcc": 0.4619562863414114, "accuracy": 0.595703125}, {"mcc": 0.45104612508391523, "accuracy": 0.5830078125}, {"mcc": 0.4381687912508751, "accuracy": 0.57666015625}, {"mcc": 0.46705248789300047, "accuracy": 0.5986328125}, {"mcc": 0.45069180385356494, "accuracy": 0.58544921875}, {"mcc": 0.4853405923837791, "accuracy": 0.6123046875}], "total": {"test_mcc": 46.529547069714496, "test_mcc_se": 1.134553632262152, "test_accuracy": 59.6533203125, "test_accuracy_se": 0.8605065767543116}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-da", "task": "common-sense-reasoning", "dataset_languages": ["da"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"mcc": 0.3382254299756799, "accuracy": 0.48779296875}, {"mcc": 0.4264200174779705, "accuracy": 0.5615234375}, {"mcc": 0.36337640843360325, "accuracy": 0.48095703125}, {"mcc": 0.4413830668693722, "accuracy": 0.5712890625}, {"mcc": 0.38017707950136176, "accuracy": 0.52294921875}, {"mcc": 0.35485559588024784, "accuracy": 0.49951171875}, {"mcc": 0.27765138830114544, "accuracy": 0.44091796875}, {"mcc": 0.3674264772280001, "accuracy": 0.50732421875}, {"mcc": 0.3803043269061694, "accuracy": 0.515625}, {"mcc": 0.3315651595292838, "accuracy": 0.4892578125}], "total": {"test_mcc": 36.61384950102834, "test_mcc_se": 2.894965251272501, "test_accuracy": 50.771484375, "test_accuracy_se": 2.3749805971658082}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "nor-common-sense-qa", "task": "common-sense-reasoning", "dataset_languages": ["nb", "nn", "no"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"mcc": 0.6072360604209148, "accuracy": 0.6674500587544065}, {"mcc": 0.6255872903610056, "accuracy": 0.699177438307873}, {"mcc": 0.6395821597060288, "accuracy": 0.7038777908343126}, {"mcc": 0.6290445692999351, "accuracy": 0.700352526439483}, {"mcc": 0.6278532782083501, "accuracy": 0.6944770857814336}, {"mcc": 0.6138376852221453, "accuracy": 0.6792009400705052}, {"mcc": 0.67643420946635, "accuracy": 0.7379553466509988}, {"mcc": 0.5678779590324621, "accuracy": 0.63689776733255}, {"mcc": 0.6530695374342518, "accuracy": 0.717978848413631}, {"mcc": 0.6040576295428788, "accuracy": 0.6780258519388954}], "total": {"test_mcc": 62.44580378694323, "test_mcc_se": 1.8257819243637536, "test_accuracy": 69.1539365452409, "test_accuracy_se": 1.7348699503192127}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-sv", "task": "common-sense-reasoning", "dataset_languages": ["sv"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"mcc": 0.3281727930775141, "accuracy": 0.44384765625}, {"mcc": 0.38481120558937953, "accuracy": 0.5224609375}, {"mcc": 0.42770530420690983, "accuracy": 0.541015625}, {"mcc": 0.34104186289019894, "accuracy": 0.478515625}, {"mcc": 0.34682600185283197, "accuracy": 0.48876953125}, {"mcc": 0.3313096516048091, "accuracy": 0.451171875}, {"mcc": 0.34923329794046054, "accuracy": 0.48876953125}, {"mcc": 0.24711086332126303, "accuracy": 0.375}, {"mcc": 0.3506759688085132, "accuracy": 0.48095703125}, {"mcc": 0.38257487460562517, "accuracy": 0.513671875}], "total": {"test_mcc": 34.89461823897505, "test_mcc_se": 2.9073883345790485, "test_accuracy": 47.841796875, "test_accuracy_se": 2.9184763469471338}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "winogrande-is", "task": "common-sense-reasoning", "dataset_languages": ["is"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"mcc": 0.06849979642835562, "accuracy": 0.5111607142857143}, {"mcc": 0.07233530351564621, "accuracy": 0.5066964285714286}, {"mcc": 0.06182631061512455, "accuracy": 0.5290178571428571}, {"mcc": 0.07162981343091884, "accuracy": 0.5524553571428571}, {"mcc": 0.08259480661537129, "accuracy": 0.47879464285714285}, {"mcc": 0.08827748254568742, "accuracy": 0.4921875}, {"mcc": -0.013934798639823725, "accuracy": 0.46986607142857145}, {"mcc": 0.04186452416704495, "accuracy": 0.5089285714285714}, {"mcc": 0.09200112855011738, "accuracy": 0.5212053571428571}, {"mcc": 0.07013739932570003, "accuracy": 0.47433035714285715}], "total": {"test_mcc": 6.352317665541427, "test_mcc_se": 1.9015363827861498, "test_accuracy": 50.446428571428555, "test_accuracy_se": 1.6229487587755527}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-de", "task": "common-sense-reasoning", "dataset_languages": ["de"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"mcc": 0.4032533056939798, "accuracy": 0.5205078125}, {"mcc": 0.3874522231298722, "accuracy": 0.53369140625}, {"mcc": 0.3499744485910968, "accuracy": 0.46875}, {"mcc": 0.35133991316717283, "accuracy": 0.51025390625}, {"mcc": 0.4199770275694457, "accuracy": 0.5361328125}, {"mcc": 0.3583209023357839, "accuracy": 0.49951171875}, {"mcc": 0.41211021982380713, "accuracy": 0.52490234375}, {"mcc": 0.32287651941663387, "accuracy": 0.4423828125}, {"mcc": 0.3617262841931227, "accuracy": 0.51513671875}, {"mcc": 0.30226904340434246, "accuracy": 0.4619140625}], "total": {"test_mcc": 36.69299887325258, "test_mcc_se": 2.3870612097758435, "test_accuracy": 50.1318359375, "test_accuracy_se": 2.0185092642076503}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-nl", "task": "common-sense-reasoning", "dataset_languages": ["nl"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"mcc": 0.43308619973094714, "accuracy": 0.55419921875}, {"mcc": 0.45780077962596794, "accuracy": 0.5849609375}, {"mcc": 0.27147287889739796, "accuracy": 0.41552734375}, {"mcc": 0.312539961623974, "accuracy": 0.4609375}, {"mcc": 0.2826182364221356, "accuracy": 0.412109375}, {"mcc": 0.3424962277444507, "accuracy": 0.46533203125}, {"mcc": 0.3686687541307767, "accuracy": 0.517578125}, {"mcc": 0.2731614847592177, "accuracy": 0.42529296875}, {"mcc": 0.40282549097199377, "accuracy": 0.5263671875}, {"mcc": 0.373446377226042, "accuracy": 0.52001953125}], "total": {"test_mcc": 35.18116391132903, "test_mcc_se": 4.144977608124025, "test_accuracy": 48.8232421875, "test_accuracy_se": 3.7727065287810455}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag", "task": "common-sense-reasoning", "dataset_languages": ["en"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"mcc": 0.3892639913627771, "accuracy": 0.49365234375}, {"mcc": 0.6051062098127018, "accuracy": 0.70361328125}, {"mcc": 0.5699266055966263, "accuracy": 0.6513671875}, {"mcc": 0.4364923486169294, "accuracy": 0.5322265625}, {"mcc": 0.41363090289210036, "accuracy": 0.51806640625}, {"mcc": 0.4741851621164374, "accuracy": 0.55908203125}, {"mcc": 0.47716461534198024, "accuracy": 0.5986328125}, {"mcc": 0.48038670606652023, "accuracy": 0.57177734375}, {"mcc": 0.5005230268929652, "accuracy": 0.61083984375}, {"mcc": 0.5573452013230823, "accuracy": 0.6591796875}], "total": {"test_mcc": 49.04024770022121, "test_mcc_se": 4.315587632853001, "test_accuracy": 58.984375, "test_accuracy_se": 4.185995319342574}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-fr", "task": "common-sense-reasoning", "dataset_languages": ["fr"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"mcc": 0.49905285814905614, "accuracy": 0.607421875}, {"mcc": 0.365535217804932, "accuracy": 0.5107421875}, {"mcc": 0.41559276959070107, "accuracy": 0.53515625}, {"mcc": 0.48829125361059345, "accuracy": 0.60009765625}, {"mcc": 0.4181532241446594, "accuracy": 0.54931640625}, {"mcc": 0.35757448156933674, "accuracy": 0.48876953125}, {"mcc": 0.3647080205008433, "accuracy": 0.50927734375}, {"mcc": 0.3798729577225988, "accuracy": 0.5263671875}, {"mcc": 0.5091357882637452, "accuracy": 0.62548828125}, {"mcc": 0.4105983110433534, "accuracy": 0.546875}], "total": {"test_mcc": 42.08514882399819, "test_mcc_se": 3.607734500922683, "test_accuracy": 54.9951171875, "test_accuracy_se": 2.865986813651173}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-it", "task": "common-sense-reasoning", "dataset_languages": ["it"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"mcc": 0.27256871087249884, "accuracy": 0.423828125}, {"mcc": 0.36794086278423765, "accuracy": 0.4833984375}, {"mcc": 0.287854230271376, "accuracy": 0.43505859375}, {"mcc": 0.4045277392318658, "accuracy": 0.537109375}, {"mcc": 0.35063856474575794, "accuracy": 0.48095703125}, {"mcc": 0.3789918988593517, "accuracy": 0.5244140625}, {"mcc": 0.40781728763712577, "accuracy": 0.53515625}, {"mcc": 0.3624463128987195, "accuracy": 0.48388671875}, {"mcc": 0.4156569212128226, "accuracy": 0.54736328125}, {"mcc": 0.38517123063288067, "accuracy": 0.525390625}], "total": {"test_mcc": 36.33613759146637, "test_mcc_se": 3.012009847940634, "test_accuracy": 49.765625, "test_accuracy_se": 2.690557380798352}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "meta-llama/Llama-2-70b-hf", "results": {"raw": [{"test_speed": 239.98, "test_speed_short": 27.04}, {"test_speed": 470.94, "test_speed_short": 50.550000000000004}, {"test_speed": 696.3, "test_speed_short": 96.86}, {"test_speed": 930.11, "test_speed_short": 122.39999999999999}, {"test_speed": 1165.32, "test_speed_short": 145.34}, {"test_speed": 1389.3, "test_speed_short": 194.37}, {"test_speed": 1615.39, "test_speed_short": 215.04}, {"test_speed": 1840.08, "test_speed_short": 239.98}, {"test_speed": 1993.96, "test_speed_short": 263.64}, {"test_speed": 2187.12, "test_speed_short": 287.3}], "total": {"test_speed": 1252.85, "test_speed_se": 411.95538373281795, "test_speed_short": 164.25199999999998, "test_speed_short_se": 55.685406644711186}}, "num_model_parameters": 68976653312, "max_sequence_length": 4096, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "swerec", "task": "sentiment-classification", "dataset_languages": ["sv"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"mcc": 0.799654184396702, "macro_f1": 0.791004422449198}, {"mcc": 0.7796718988088924, "macro_f1": 0.7706286654884485}, {"mcc": 0.7989728506740114, "macro_f1": 0.782945424506997}, {"mcc": 0.7725801438304987, "macro_f1": 0.7718097064240688}, {"mcc": 0.792994095670026, "macro_f1": 0.8048477640661681}, {"mcc": 0.8045766067558089, "macro_f1": 0.8010307310766027}, {"mcc": 0.8071735404087165, "macro_f1": 0.7831150657987257}, {"mcc": 0.7919446678240265, "macro_f1": 0.7960475451025406}, {"mcc": 0.8114506166869331, "macro_f1": 0.806438267157318}, {"mcc": 0.7728730592450856, "macro_f1": 0.7792411972041996}], "total": {"test_mcc": 79.31891664300701, "test_mcc_se": 0.8656959838154942, "test_macro_f1": 78.87108789274268, "test_macro_f1_se": 0.8140149663700691}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "angry-tweets", "task": "sentiment-classification", "dataset_languages": ["da"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"mcc": 0.5315877911582404, "macro_f1": 0.6945340435550454}, {"mcc": 0.49058987751783373, "macro_f1": 0.6659354894406946}, {"mcc": 0.5248216837302762, "macro_f1": 0.690475226337456}, {"mcc": 0.4868889098395273, "macro_f1": 0.6665084266064126}, {"mcc": 0.49953257416618796, "macro_f1": 0.6751633072459268}, {"mcc": 0.5369634796604392, "macro_f1": 0.6980907626573841}, {"mcc": 0.5010243828812203, "macro_f1": 0.6731779084540253}, {"mcc": 0.48142592034313675, "macro_f1": 0.6600223236227555}, {"mcc": 0.5093315191790092, "macro_f1": 0.6730318729693479}, {"mcc": 0.45925473893829594, "macro_f1": 0.6457973710446646}], "total": {"test_mcc": 50.21420877414167, "test_mcc_se": 1.502423334972013, "test_macro_f1": 67.42736731933712, "test_macro_f1_se": 1.0095186101202427}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "norec", "task": "sentiment-classification", "dataset_languages": ["nb", "nn", "no"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"mcc": 0.398100151702591, "macro_f1": 0.3994935902976556}, {"mcc": 0.3808748688840894, "macro_f1": 0.39205682272909165}, {"mcc": 0.38742849266698876, "macro_f1": 0.402907148528351}, {"mcc": 0.39561450542094934, "macro_f1": 0.40482218431878164}, {"mcc": 0.4088535484043018, "macro_f1": 0.4051181249384677}, {"mcc": 0.39879039170383573, "macro_f1": 0.39842485379626996}, {"mcc": 0.3802470353795615, "macro_f1": 0.3853825753480415}, {"mcc": 0.40644050254423936, "macro_f1": 0.400807571316462}, {"mcc": 0.39825493182236427, "macro_f1": 0.4006283951159859}, {"mcc": 0.4020222413804191, "macro_f1": 0.4060634631783457}], "total": {"test_mcc": 39.566266699093404, "test_mcc_se": 0.6118960137746295, "test_macro_f1": 39.95704729567453, "test_macro_f1_se": 0.39921210670827156}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hotter-and-colder-sentiment", "task": "sentiment-classification", "dataset_languages": ["is"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"mcc": 0.28248510724160947, "macro_f1": 0.5097244212934707}, {"mcc": 0.12084127778070908, "macro_f1": 0.3679442201029792}, {"mcc": 0.281011617104946, "macro_f1": 0.49890676770059206}, {"mcc": 0.2771436450035364, "macro_f1": 0.49350545499623993}, {"mcc": 0.16072502696547541, "macro_f1": 0.3562318260896465}, {"mcc": 0.15284783833262655, "macro_f1": 0.37392663980554647}, {"mcc": 0.2886069544412549, "macro_f1": 0.4886261048738483}, {"mcc": 0.18226978153128273, "macro_f1": 0.3742196392239014}, {"mcc": 0.1658289881088757, "macro_f1": 0.42193376173707436}, {"mcc": 0.2085663714602029, "macro_f1": 0.4532736274408847}], "total": {"test_mcc": 21.20326607970519, "test_mcc_se": 3.9906054958609842, "test_macro_f1": 43.38292463264183, "test_macro_f1_se": 3.8420509349048824}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "sb10k", "task": "sentiment-classification", "dataset_languages": ["de"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"mcc": 0.5243073820692488, "macro_f1": 0.6164469038491891}, {"mcc": 0.5099933820071592, "macro_f1": 0.660615840239896}, {"mcc": 0.531952918043128, "macro_f1": 0.6731789690258504}, {"mcc": 0.4856993195692288, "macro_f1": 0.637480801247647}, {"mcc": 0.502424570217259, "macro_f1": 0.6657841564904774}, {"mcc": 0.5344901526342178, "macro_f1": 0.6756429364862707}, {"mcc": 0.5289233551004977, "macro_f1": 0.6853016777831789}, {"mcc": 0.4889052848853101, "macro_f1": 0.6293581206079927}, {"mcc": 0.5432323427568733, "macro_f1": 0.6768300117097601}, {"mcc": 0.5496444349600355, "macro_f1": 0.6972626379096315}], "total": {"test_mcc": 51.99573142242959, "test_mcc_se": 1.3738620982162661, "test_macro_f1": 66.17902055349893, "test_macro_f1_se": 1.6103061347522651}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "dutch-social", "task": "sentiment-classification", "dataset_languages": ["nl"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"mcc": 0.15266912136654812, "macro_f1": 0.36515797017615864}, {"mcc": 0.09618661221137179, "macro_f1": 0.3329454312104806}, {"mcc": 0.10360717912554433, "macro_f1": 0.3484819972031832}, {"mcc": 0.14808214384988222, "macro_f1": 0.3349247678462217}, {"mcc": 0.14712927235848486, "macro_f1": 0.3664524053384819}, {"mcc": 0.09544628027615489, "macro_f1": 0.3164878824523039}, {"mcc": 0.1060027229345387, "macro_f1": 0.32963047676400364}, {"mcc": 0.09907829470631625, "macro_f1": 0.32579555885536343}, {"mcc": 0.10276756013241295, "macro_f1": 0.3442155488956619}, {"mcc": 0.16709561023974753, "macro_f1": 0.34858255276802846}], "total": {"test_mcc": 12.180647972010018, "test_mcc_se": 1.7465549374727773, "test_macro_f1": 34.12674591509887, "test_macro_f1_se": 1.0158444214509372}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "sst5", "task": "sentiment-classification", "dataset_languages": ["en"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"mcc": 0.6834564769892264, "macro_f1": 0.7221756763237197}, {"mcc": 0.6683523164137681, "macro_f1": 0.7238621972758832}, {"mcc": 0.6843127740236441, "macro_f1": 0.7326224021164599}, {"mcc": 0.6735133056916409, "macro_f1": 0.7262417738380496}, {"mcc": 0.6837253606464102, "macro_f1": 0.725713468711629}, {"mcc": 0.6600915168989063, "macro_f1": 0.7188906197830383}, {"mcc": 0.6878362507458335, "macro_f1": 0.7299516920084511}, {"mcc": 0.6829846333270461, "macro_f1": 0.7213266622479911}, {"mcc": 0.7125852770917687, "macro_f1": 0.7469738448265755}, {"mcc": 0.7169401422965772, "macro_f1": 0.743540808312889}], "total": {"test_mcc": 68.53798054124822, "test_mcc_se": 1.098258035183511, "test_macro_f1": 72.91299145444687, "test_macro_f1_se": 0.584654903142229}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fosent", "task": "sentiment-classification", "dataset_languages": ["fo"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"mcc": 0.4569117687726003, "macro_f1": 0.6348642432979782}, {"mcc": 0.48803860354636114, "macro_f1": 0.6554531490015362}, {"mcc": 0.5322692423226456, "macro_f1": 0.6784764380382567}, {"mcc": 0.4970605711636977, "macro_f1": 0.6534797441970438}, {"mcc": 0.3763197957996063, "macro_f1": 0.5828244827233705}, {"mcc": 0.4963070152859718, "macro_f1": 0.6575665829267728}, {"mcc": 0.3932068445345316, "macro_f1": 0.5898117387974128}, {"mcc": 0.3774675976581332, "macro_f1": 0.5848452920966932}, {"mcc": 0.517639960066285, "macro_f1": 0.6697790845739305}, {"mcc": 0.5467714228183442, "macro_f1": 0.7009370074551863}], "total": {"test_mcc": 46.81992821968177, "test_mcc_se": 3.9829169435193164, "test_macro_f1": 64.0803776310818, "test_macro_f1_se": 2.5853759439792396}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "allocine", "task": "sentiment-classification", "dataset_languages": ["fr"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"mcc": 0.937839485383387, "macro_f1": 0.9687113283935529}, {"mcc": 0.9257290334490101, "macro_f1": 0.9628608379027139}, {"mcc": 0.9509447847995115, "macro_f1": 0.9753958765026776}, {"mcc": 0.9387070897092343, "macro_f1": 0.9692312315019306}, {"mcc": 0.9231648191980302, "macro_f1": 0.9613892445353415}, {"mcc": 0.9248456534624191, "macro_f1": 0.9623711142178994}, {"mcc": 0.9283128436971925, "macro_f1": 0.9638583638583639}, {"mcc": 0.940490937475512, "macro_f1": 0.9701933467087254}, {"mcc": 0.9411778659189415, "macro_f1": 0.9705655778230026}, {"mcc": 0.9445398736806655, "macro_f1": 0.972073682646742}], "total": {"test_mcc": 93.55752386773905, "test_mcc_se": 0.5865132782641225, "test_macro_f1": 96.7665060409095, "test_macro_f1_se": 0.2940272156287189}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "sentipolc16", "task": "sentiment-classification", "dataset_languages": ["it"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"mcc": 0.6691649940797025, "macro_f1": 0.7735400613624236}, {"mcc": 0.6184721662016089, "macro_f1": 0.7305838010807361}, {"mcc": 0.5783082906336329, "macro_f1": 0.6990116689874558}, {"mcc": 0.6232553194598563, "macro_f1": 0.723423865165444}, {"mcc": 0.5948422077145643, "macro_f1": 0.713448170671172}, {"mcc": 0.5807179188322626, "macro_f1": 0.6944354619403509}, {"mcc": 0.5970457923706269, "macro_f1": 0.7009312912508131}, {"mcc": 0.6122392135157114, "macro_f1": 0.7276928247826269}, {"mcc": 0.5765980395027039, "macro_f1": 0.6935896615321241}, {"mcc": 0.6187829981050389, "macro_f1": 0.7375123881360425}], "total": {"test_mcc": 60.69426940415708, "test_mcc_se": 1.7422977161765865, "test_macro_f1": 71.94169194909189, "test_macro_f1_se": 1.5392684568499304}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "suc3", "task": "named-entity-recognition", "dataset_languages": ["sv"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"micro_f1_no_misc": 0.4052776683733753, "micro_f1": 0.19017054832202457}, {"micro_f1_no_misc": 0.4345594525235244, "micro_f1": 0.2893797116783494}, {"micro_f1_no_misc": 0.4400715563506261, "micro_f1": 0.24824153230169896}, {"micro_f1_no_misc": 0.4440093970242756, "micro_f1": 0.23603750509580104}, {"micro_f1_no_misc": 0.4459016393442623, "micro_f1": 0.2699093943729137}, {"micro_f1_no_misc": 0.43619987132747157, "micro_f1": 0.26848004029719175}, {"micro_f1_no_misc": 0.4237253707089173, "micro_f1": 0.2429865735181749}, {"micro_f1_no_misc": 0.4307141258115066, "micro_f1": 0.2582673205079844}, {"micro_f1_no_misc": 0.36573045059608, "micro_f1": 0.2235809875403784}, {"micro_f1_no_misc": 0.4338130954794243, "micro_f1": 0.25464619646773634}], "total": {"test_micro_f1_no_misc": 42.60002627539464, "test_micro_f1_no_misc_se": 1.4964041889111552, "test_micro_f1": 24.816998101022534, "test_micro_f1_se": 1.7124143018157294}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "dansk", "task": "named-entity-recognition", "dataset_languages": ["da"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"micro_f1_no_misc": 0.5540098199672667, "micro_f1": 0.3195924038906901}, {"micro_f1_no_misc": 0.5136986301369862, "micro_f1": 0.29881728216268405}, {"micro_f1_no_misc": 0.4943820224719101, "micro_f1": 0.34438202247191013}, {"micro_f1_no_misc": 0.525056095736724, "micro_f1": 0.32202643171806167}, {"micro_f1_no_misc": 0.45356715823037064, "micro_f1": 0.24372169617126388}, {"micro_f1_no_misc": 0.46798227950060417, "micro_f1": 0.26962305986696233}, {"micro_f1_no_misc": 0.4920152091254753, "micro_f1": 0.27373612823674476}, {"micro_f1_no_misc": 0.5656146179401993, "micro_f1": 0.31244323342415986}, {"micro_f1_no_misc": 0.46945337620578775, "micro_f1": 0.2798335887891395}, {"micro_f1_no_misc": 0.5159620362381363, "micro_f1": 0.2750322303394929}], "total": {"test_micro_f1_no_misc": 50.517412455534604, "test_micro_f1_no_misc_se": 2.2852128267990164, "test_micro_f1": 29.392080770711086, "test_micro_f1_se": 1.8975175147662604}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "norne-nb", "task": "named-entity-recognition", "dataset_languages": ["nb", "no"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"micro_f1_no_misc": 0.5780657395701644, "micro_f1": 0.3953438701409654}, {"micro_f1_no_misc": 0.5404308460817983, "micro_f1": 0.34897540983606556}, {"micro_f1_no_misc": 0.5924633635729241, "micro_f1": 0.3686567164179104}, {"micro_f1_no_misc": 0.6070546737213404, "micro_f1": 0.4293471702666828}, {"micro_f1_no_misc": 0.5166798418972333, "micro_f1": 0.3185142417244034}, {"micro_f1_no_misc": 0.5068810886036802, "micro_f1": 0.3157180257926888}, {"micro_f1_no_misc": 0.5423490051065328, "micro_f1": 0.36308340156743474}, {"micro_f1_no_misc": 0.5067755102040816, "micro_f1": 0.3521281263712155}, {"micro_f1_no_misc": 0.497802031226315, "micro_f1": 0.3063561748425272}, {"micro_f1_no_misc": 0.5358295674628794, "micro_f1": 0.333032219211081}], "total": {"test_micro_f1_no_misc": 54.24331667446949, "test_micro_f1_no_misc_se": 2.3724956559768695, "test_micro_f1": 35.31155356170975, "test_micro_f1_se": 2.36819566315139}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "norne-nn", "task": "named-entity-recognition", "dataset_languages": ["nn"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"micro_f1_no_misc": 0.622638514049849, "micro_f1": 0.44295302013422816}, {"micro_f1_no_misc": 0.5573515697855145, "micro_f1": 0.34840174011726877}, {"micro_f1_no_misc": 0.5902744342802119, "micro_f1": 0.3707138027624843}, {"micro_f1_no_misc": 0.6069770923790987, "micro_f1": 0.4107838242636046}, {"micro_f1_no_misc": 0.5945058226336221, "micro_f1": 0.4387553335414715}, {"micro_f1_no_misc": 0.5914786967418547, "micro_f1": 0.3536530298925314}, {"micro_f1_no_misc": 0.6025363439529848, "micro_f1": 0.39003419638495357}, {"micro_f1_no_misc": 0.593160967472894, "micro_f1": 0.42817216321967577}, {"micro_f1_no_misc": 0.5690448791714614, "micro_f1": 0.36501692434360994}, {"micro_f1_no_misc": 0.57084058888713, "micro_f1": 0.38799877538524336}], "total": {"test_micro_f1_no_misc": 58.988089093546215, "test_micro_f1_no_misc_se": 1.2073749270268384, "test_micro_f1": 39.36482810045071, "test_micro_f1_se": 2.165750407869023}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mim-gold-ner", "task": "named-entity-recognition", "dataset_languages": ["is"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"micro_f1_no_misc": 0.4765465669612508, "micro_f1": 0.3088612793921347}, {"micro_f1_no_misc": 0.4270462633451958, "micro_f1": 0.2052619104053093}, {"micro_f1_no_misc": 0.5235727073801472, "micro_f1": 0.3561051746908392}, {"micro_f1_no_misc": 0.48752598752598747, "micro_f1": 0.2479802704311591}, {"micro_f1_no_misc": 0.4715797023489331, "micro_f1": 0.2808988764044944}, {"micro_f1_no_misc": 0.43723150357995233, "micro_f1": 0.28207767898923725}, {"micro_f1_no_misc": 0.48236226714229097, "micro_f1": 0.3105336626221411}, {"micro_f1_no_misc": 0.5164926931106472, "micro_f1": 0.3022914550174963}, {"micro_f1_no_misc": 0.501975540921919, "micro_f1": 0.2686319668355596}, {"micro_f1_no_misc": 0.46455672774384604, "micro_f1": 0.2415855727167217}], "total": {"test_micro_f1_no_misc": 47.88889960060169, "test_micro_f1_no_misc_se": 1.9290117970624945, "test_micro_f1": 28.04227847505093, "test_micro_f1_se": 2.635571880092457}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fone", "task": "named-entity-recognition", "dataset_languages": ["fo"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"micro_f1_no_misc": 0.6434535441463849, "micro_f1": 0.4257569081639076}, {"micro_f1_no_misc": 0.6554476058292852, "micro_f1": 0.52508524111057}, {"micro_f1_no_misc": 0.650985818056036, "micro_f1": 0.5099023331524688}, {"micro_f1_no_misc": 0.6637238456784673, "micro_f1": 0.4829616063836746}, {"micro_f1_no_misc": 0.614481409001957, "micro_f1": 0.3932673026675579}, {"micro_f1_no_misc": 0.6185063568241138, "micro_f1": 0.4406191950464396}, {"micro_f1_no_misc": 0.6591976870256596, "micro_f1": 0.4554984048440654}, {"micro_f1_no_misc": 0.6442857142857144, "micro_f1": 0.49826171603393127}, {"micro_f1_no_misc": 0.6331789988643313, "micro_f1": 0.5175655616031668}, {"micro_f1_no_misc": 0.6680122860903905, "micro_f1": 0.5185595567867035}], "total": {"test_micro_f1_no_misc": 64.5127326580234, "test_micro_f1_no_misc_se": 1.1320737304638646, "test_micro_f1": 47.67477825792486, "test_micro_f1_se": 2.823532617802833}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "germeval", "task": "named-entity-recognition", "dataset_languages": ["de"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"micro_f1_no_misc": 0.5911949685534591, "micro_f1": 0.41767313866381284}, {"micro_f1_no_misc": 0.5918412348401323, "micro_f1": 0.4550974752317034}, {"micro_f1_no_misc": 0.5884436572946645, "micro_f1": 0.3976507663658502}, {"micro_f1_no_misc": 0.6090373280943024, "micro_f1": 0.3814648729446936}, {"micro_f1_no_misc": 0.612674333251774, "micro_f1": 0.45332449013430615}, {"micro_f1_no_misc": 0.5993968916724658, "micro_f1": 0.4213933415536375}, {"micro_f1_no_misc": 0.5933908045977011, "micro_f1": 0.35901802185327086}, {"micro_f1_no_misc": 0.5922930542340628, "micro_f1": 0.43567006859628576}, {"micro_f1_no_misc": 0.634935744883389, "micro_f1": 0.42003279177224623}, {"micro_f1_no_misc": 0.5958549222797928, "micro_f1": 0.4089157030333501}], "total": {"test_micro_f1_no_misc": 60.09062939701744, "test_micro_f1_no_misc_se": 0.8893188058069194, "test_micro_f1": 41.502406701491566, "test_micro_f1_se": 1.867867395839155}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "conll-nl", "task": "named-entity-recognition", "dataset_languages": ["nl"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"micro_f1_no_misc": 0.6010452961672474, "micro_f1": 0.3612450679526524}, {"micro_f1_no_misc": 0.5386119257087, "micro_f1": 0.36548715462918085}, {"micro_f1_no_misc": 0.5548172757475083, "micro_f1": 0.33394346146023995}, {"micro_f1_no_misc": 0.4909492273730684, "micro_f1": 0.33415564027808925}, {"micro_f1_no_misc": 0.5100563153660498, "micro_f1": 0.305965741287655}, {"micro_f1_no_misc": 0.5309951060358891, "micro_f1": 0.3110642781875658}, {"micro_f1_no_misc": 0.48629531388152075, "micro_f1": 0.33371851167090366}, {"micro_f1_no_misc": 0.5114200595829196, "micro_f1": 0.33666260657734476}, {"micro_f1_no_misc": 0.5105363984674329, "micro_f1": 0.3191586648376772}, {"micro_f1_no_misc": 0.4715374271627073, "micro_f1": 0.3595992418088275}], "total": {"test_micro_f1_no_misc": 52.06264345493044, "test_micro_f1_no_misc_se": 2.34059170770061, "test_micro_f1": 33.61000368690136, "test_micro_f1_se": 1.2876440279596255}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "conll-en", "task": "named-entity-recognition", "dataset_languages": ["en"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"micro_f1_no_misc": 0.6875905359729599, "micro_f1": 0.5105712033254497}, {"micro_f1_no_misc": 0.7377527033380349, "micro_f1": 0.6074553341666054}, {"micro_f1_no_misc": 0.7435118989325238, "micro_f1": 0.5675003656574521}, {"micro_f1_no_misc": 0.6946638412637257, "micro_f1": 0.5772696297418035}, {"micro_f1_no_misc": 0.7153608645310691, "micro_f1": 0.6345636763627475}, {"micro_f1_no_misc": 0.7385284810126582, "micro_f1": 0.6340759737137363}, {"micro_f1_no_misc": 0.6929507885236557, "micro_f1": 0.5991794106676613}, {"micro_f1_no_misc": 0.7474551623848764, "micro_f1": 0.5738549756841111}, {"micro_f1_no_misc": 0.712773190869354, "micro_f1": 0.6551521757357256}, {"micro_f1_no_misc": 0.7074868322046651, "micro_f1": 0.5692738633915104}], "total": {"test_micro_f1_no_misc": 71.78074299033523, "test_micro_f1_no_misc_se": 1.3968606831398054, "test_micro_f1": 59.28896608446802, "test_micro_f1_se": 2.6252565598422373}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "eltec", "task": "named-entity-recognition", "dataset_languages": ["fr"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"micro_f1_no_misc": 0.4841979725700656, "micro_f1": 0.3987934592792507}, {"micro_f1_no_misc": 0.47617260787992494, "micro_f1": 0.416318663668487}, {"micro_f1_no_misc": 0.5562459824298265, "micro_f1": 0.44010589013898077}, {"micro_f1_no_misc": 0.5888026840008388, "micro_f1": 0.4524160524160524}, {"micro_f1_no_misc": 0.580215053763441, "micro_f1": 0.46681303833811855}, {"micro_f1_no_misc": 0.4763572679509632, "micro_f1": 0.3859318574344481}, {"micro_f1_no_misc": 0.5117370892018781, "micro_f1": 0.3805166051660517}, {"micro_f1_no_misc": 0.5033277870216306, "micro_f1": 0.3769947859061463}, {"micro_f1_no_misc": 0.48577680525164113, "micro_f1": 0.35153129161118507}, {"micro_f1_no_misc": 0.5259975816203144, "micro_f1": 0.3768545994065282}], "total": {"test_micro_f1_no_misc": 51.88830831690524, "test_micro_f1_no_misc_se": 2.6394754146123853, "test_micro_f1": 40.462762433652486, "test_micro_f1_se": 2.34338134859552}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "multinerd-it", "task": "named-entity-recognition", "dataset_languages": ["it"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"micro_f1_no_misc": 0.6635620172648796, "micro_f1": 0.39540864329172676}, {"micro_f1_no_misc": 0.6815533980582524, "micro_f1": 0.4932883596062504}, {"micro_f1_no_misc": 0.6426039316790204, "micro_f1": 0.47820416008786987}, {"micro_f1_no_misc": 0.7000926354793886, "micro_f1": 0.5284205796450235}, {"micro_f1_no_misc": 0.7143022851177357, "micro_f1": 0.49286023366508}, {"micro_f1_no_misc": 0.7057353776263485, "micro_f1": 0.526782469656225}, {"micro_f1_no_misc": 0.6782645592447226, "micro_f1": 0.41388453657948654}, {"micro_f1_no_misc": 0.7190063810391978, "micro_f1": 0.531905195989061}, {"micro_f1_no_misc": 0.6655847668505895, "micro_f1": 0.4163861543347094}, {"micro_f1_no_misc": 0.6406299882941364, "micro_f1": 0.4235097421387692}], "total": {"test_micro_f1_no_misc": 68.11335340654271, "test_micro_f1_no_misc_se": 1.7510325450714985, "test_micro_f1": 47.00650074994201, "test_micro_f1_se": 3.289471664423388}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-sv", "task": "linguistic-acceptability", "dataset_languages": ["sv"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"mcc": 0.49834786735083675, "macro_f1": 0.7457424983223084}, {"mcc": 0.4608335231403274, "macro_f1": 0.7106409259210706}, {"mcc": 0.49475678469125467, "macro_f1": 0.744332122406374}, {"mcc": 0.5219440631211418, "macro_f1": 0.7562333928466651}, {"mcc": 0.5032353576581543, "macro_f1": 0.7436909902173325}, {"mcc": 0.5015381869957241, "macro_f1": 0.7507626403582788}, {"mcc": 0.5034379158277664, "macro_f1": 0.7345381823568624}, {"mcc": 0.5206156785267128, "macro_f1": 0.7525431530494822}, {"mcc": 0.5061038623158983, "macro_f1": 0.7477963706921475}, {"mcc": 0.5061130685777867, "macro_f1": 0.7526066957153648}], "total": {"test_mcc": 50.16926308205603, "test_mcc_se": 1.0415312548310125, "test_macro_f1": 74.38886971885886, "test_macro_f1_se": 0.816623372216199}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-da", "task": "linguistic-acceptability", "dataset_languages": ["da"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"mcc": 0.4483797868871508, "macro_f1": 0.7164210229755663}, {"mcc": 0.47840155459170813, "macro_f1": 0.7361256312497306}, {"mcc": 0.4739566296117146, "macro_f1": 0.7367108848088655}, {"mcc": 0.4555313637793081, "macro_f1": 0.7098180128201494}, {"mcc": 0.49617843064104683, "macro_f1": 0.7455110282696489}, {"mcc": 0.4511732589466657, "macro_f1": 0.7255859375}, {"mcc": 0.49043546811471217, "macro_f1": 0.7439626204970764}, {"mcc": 0.4810162321639035, "macro_f1": 0.7390764109645778}, {"mcc": 0.4976055911446657, "macro_f1": 0.7449691173540546}, {"mcc": 0.46218881646269255, "macro_f1": 0.7307239566748616}], "total": {"test_mcc": 47.348671323435674, "test_mcc_se": 1.1397342949121885, "test_macro_f1": 73.2890462311453, "test_macro_f1_se": 0.7598700262723153}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-nb", "task": "linguistic-acceptability", "dataset_languages": ["nb", "no"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"mcc": 0.4874410651339665, "macro_f1": 0.7408264145279904}, {"mcc": 0.38763195102829356, "macro_f1": 0.6484989828318164}, {"mcc": 0.4571202745158437, "macro_f1": 0.7233528696909564}, {"mcc": 0.43723926631038407, "macro_f1": 0.6949568360841061}, {"mcc": 0.43224451925282453, "macro_f1": 0.7147579661335495}, {"mcc": 0.42678255672147797, "macro_f1": 0.7035140691637028}, {"mcc": 0.4124800664066525, "macro_f1": 0.6611231370771236}, {"mcc": 0.4362974163988209, "macro_f1": 0.6778038200272358}, {"mcc": 0.36494888501698286, "macro_f1": 0.6612926566353126}, {"mcc": 0.43049388032838565, "macro_f1": 0.7145472927110312}], "total": {"test_mcc": 42.72679881113633, "test_mcc_se": 2.1071080481277558, "test_macro_f1": 69.40674044882824, "test_macro_f1_se": 1.9027701917705184}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-nn", "task": "linguistic-acceptability", "dataset_languages": ["nn"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"mcc": 0.22039401736216227, "macro_f1": 0.604928281553335}, {"mcc": 0.2639407987691376, "macro_f1": 0.6319697947284154}, {"mcc": 0.2729423443484502, "macro_f1": 0.6230716696664469}, {"mcc": 0.23808887968167053, "macro_f1": 0.6129860918819923}, {"mcc": 0.24825164168910835, "macro_f1": 0.6222164322817414}, {"mcc": 0.27161802488882336, "macro_f1": 0.6277228026689894}, {"mcc": 0.23595855792138046, "macro_f1": 0.5854455306680212}, {"mcc": 0.2504914568519947, "macro_f1": 0.6051335125467887}, {"mcc": 0.23303159049140093, "macro_f1": 0.5798547160127319}, {"mcc": 0.2776179161087284, "macro_f1": 0.6358504685332375}], "total": {"test_mcc": 25.123352281128568, "test_mcc_se": 1.212165911464355, "test_macro_f1": 61.29179300541699, "test_macro_f1_se": 1.1814255461104486}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-is", "task": "linguistic-acceptability", "dataset_languages": ["is"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"mcc": 0.11328845794490185, "macro_f1": 0.5477379214993471}, {"mcc": 0.06310458067316736, "macro_f1": 0.5008935402630739}, {"mcc": 0.08036024199651767, "macro_f1": 0.5076584930283673}, {"mcc": 0.16174862949302368, "macro_f1": 0.5782487409117243}, {"mcc": 0.10546069480020093, "macro_f1": 0.5279432165745396}, {"mcc": 0.0896702051119557, "macro_f1": 0.5448318502736349}, {"mcc": 0.09217269303131871, "macro_f1": 0.544080103951693}, {"mcc": 0.1139564636305008, "macro_f1": 0.5568501799076755}, {"mcc": 0.11330811205866487, "macro_f1": 0.5563969465648855}, {"mcc": 0.08571768936076338, "macro_f1": 0.5369971978146987}], "total": {"test_mcc": 10.18787768101015, "test_mcc_se": 1.6624520942010952, "test_macro_f1": 54.016381907896395, "test_macro_f1_se": 1.4380795436142682}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-fo", "task": "linguistic-acceptability", "dataset_languages": ["fo"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"mcc": 0.08751774289020364, "macro_f1": 0.48051013986463054}, {"mcc": 0.1161551267335732, "macro_f1": 0.4763979342696501}, {"mcc": 0.13320383113174278, "macro_f1": 0.5638348733907632}, {"mcc": 0.08321081335492875, "macro_f1": 0.5203994554323795}, {"mcc": 0.06630157535023662, "macro_f1": 0.5328319708938158}, {"mcc": 0.13489279903128998, "macro_f1": 0.5672899629990338}, {"mcc": 0.11979845758736586, "macro_f1": 0.5230188679245282}, {"mcc": 0.04715792121147197, "macro_f1": 0.5220079583715947}, {"mcc": 0.11251646903820817, "macro_f1": 0.5322368168998312}, {"mcc": 0.12671791007148775, "macro_f1": 0.5630227391843526}], "total": {"test_mcc": 10.274726464005086, "test_mcc_se": 1.86326676064787, "test_macro_f1": 52.81550719230581, "test_macro_f1_se": 1.974324712281046}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-de", "task": "linguistic-acceptability", "dataset_languages": ["de"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"mcc": 0.4264492767812762, "macro_f1": 0.6950734495162052}, {"mcc": 0.37388371054309555, "macro_f1": 0.6852941176470588}, {"mcc": 0.43995770740715423, "macro_f1": 0.7191348733283829}, {"mcc": 0.43515480799168044, "macro_f1": 0.7062400683016906}, {"mcc": 0.41673474015012507, "macro_f1": 0.7064949973239469}, {"mcc": 0.4698754175693933, "macro_f1": 0.7331310493513562}, {"mcc": 0.37946911422923213, "macro_f1": 0.6893864745681552}, {"mcc": 0.3696575631360127, "macro_f1": 0.6844355609130671}, {"mcc": 0.237292018295936, "macro_f1": 0.6143392977925353}, {"mcc": 0.42250652374810466, "macro_f1": 0.709267777867}], "total": {"test_mcc": 39.7098087985201, "test_mcc_se": 4.006970083829171, "test_macro_f1": 69.42797666609397, "test_macro_f1_se": 1.9856177700233513}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-nl", "task": "linguistic-acceptability", "dataset_languages": ["nl"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"mcc": 0.5517187054664783, "macro_f1": 0.7668415768828722}, {"mcc": 0.5171962860048386, "macro_f1": 0.7505944027557092}, {"mcc": 0.5563443269709051, "macro_f1": 0.7753539667673796}, {"mcc": 0.5469702586602436, "macro_f1": 0.7526284786682192}, {"mcc": 0.5375092464063189, "macro_f1": 0.765293996190602}, {"mcc": 0.5368125015299626, "macro_f1": 0.7605676857987513}, {"mcc": 0.5561309161072469, "macro_f1": 0.7724969703964453}, {"mcc": 0.5451320630384587, "macro_f1": 0.7708308132195447}, {"mcc": 0.5017817633143895, "macro_f1": 0.7252110531991196}, {"mcc": 0.5283531548160918, "macro_f1": 0.7618777847529512}], "total": {"test_mcc": 53.77949222314935, "test_mcc_se": 1.097838445355338, "test_macro_f1": 76.01696728631595, "test_macro_f1_se": 0.9101509048617458}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-en", "task": "linguistic-acceptability", "dataset_languages": ["en"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"mcc": 0.3886904096449465, "macro_f1": 0.6943286497270995}, {"mcc": 0.4654595787720505, "macro_f1": 0.7304927850656746}, {"mcc": 0.48984719708158364, "macro_f1": 0.7441237183837739}, {"mcc": 0.4558089259885354, "macro_f1": 0.7199746460735058}, {"mcc": 0.41650984448669764, "macro_f1": 0.7075615868940645}, {"mcc": 0.5590988896780107, "macro_f1": 0.7675726623963683}, {"mcc": 0.4407958386765048, "macro_f1": 0.7180778452043637}, {"mcc": 0.5066345644726307, "macro_f1": 0.7501496147480275}, {"mcc": 0.43059103082196276, "macro_f1": 0.7130692576937054}, {"mcc": 0.42705050213827683, "macro_f1": 0.7125921371816548}], "total": {"test_mcc": 45.80486781761199, "test_mcc_se": 3.077701488770345, "test_macro_f1": 72.57942903368239, "test_macro_f1_se": 1.378024055213194}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-fr", "task": "linguistic-acceptability", "dataset_languages": ["fr"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"mcc": 0.3127133292264385, "macro_f1": 0.6520644797532109}, {"mcc": 0.29792810930011776, "macro_f1": 0.6436423930437465}, {"mcc": 0.3746806452582186, "macro_f1": 0.6828745647950449}, {"mcc": 0.32546173040666515, "macro_f1": 0.6536210804045597}, {"mcc": 0.46673699522923595, "macro_f1": 0.7323817857665856}, {"mcc": 0.44034464220601605, "macro_f1": 0.7171357842174766}, {"mcc": 0.3996068909997674, "macro_f1": 0.6615582114999189}, {"mcc": 0.33492998955915393, "macro_f1": 0.6534865634401543}, {"mcc": 0.463805687657164, "macro_f1": 0.7310942344732225}, {"mcc": 0.4512598012391764, "macro_f1": 0.7218632986050482}], "total": {"test_mcc": 38.67467821081953, "test_mcc_se": 4.105756351801406, "test_macro_f1": 68.49722395998968, "test_macro_f1_se": 2.2715230403427733}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-it", "task": "linguistic-acceptability", "dataset_languages": ["it"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"mcc": 0.25598586115208755, "macro_f1": 0.5833660553484007}, {"mcc": 0.3615543496637389, "macro_f1": 0.6322563698450223}, {"mcc": 0.30455759603909854, "macro_f1": 0.6518892635362445}, {"mcc": 0.3419819222931043, "macro_f1": 0.6649407010252466}, {"mcc": 0.32408633968766665, "macro_f1": 0.6485052736691006}, {"mcc": 0.352383560836116, "macro_f1": 0.6731209178452169}, {"mcc": 0.3693917495455008, "macro_f1": 0.6810350847680113}, {"mcc": 0.32454099673515135, "macro_f1": 0.6264543415689932}, {"mcc": 0.31658470170492153, "macro_f1": 0.6580152671755726}, {"mcc": 0.3380912694778191, "macro_f1": 0.65493879402754}], "total": {"test_mcc": 32.89158347135205, "test_mcc_se": 2.025948918203135, "test_macro_f1": 64.74522068809347, "test_macro_f1_se": 1.7359279506003218}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"f1": 56.864866301197736, "em": 43.25757575757576}, {"f1": 56.62276381330219, "em": 44.95830174374526}, {"f1": 59.37659377742729, "em": 47.00854700854701}, {"f1": 55.55407631197935, "em": 43.074581430745816}, {"f1": 57.521900021044694, "em": 45.720894371626834}, {"f1": 57.25569227326209, "em": 44.50381679389313}, {"f1": 51.66212825238244, "em": 36.42745709828393}, {"f1": 56.178002396739686, "em": 43.632075471698116}, {"f1": 58.13238203887303, "em": 46.749226006191954}, {"f1": 54.16530233832391, "em": 40.4320987654321}], "total": {"test_f1": 56.333370752453256, "test_f1_se": 1.3433847078279533, "test_em": 43.57645744477399, "test_em_se": 1.9661496933901235}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "norquad", "task": "reading-comprehension", "dataset_languages": ["nb", "nn", "no"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"f1": 63.23514051766355, "em": 27.941176470588236}, {"f1": 62.11042934378089, "em": 27.723577235772357}, {"f1": 62.29151555083946, "em": 27.643630308076602}, {"f1": 65.41543769114757, "em": 33.0335241210139}, {"f1": 63.37939406251177, "em": 30.190239867659223}, {"f1": 66.29192443751843, "em": 32.34567901234568}, {"f1": 64.76618390089043, "em": 31.680672268907564}, {"f1": 69.34478481933597, "em": 40.70945945945946}, {"f1": 65.80262818629019, "em": 33.056478405315616}, {"f1": 62.55166228627549, "em": 28.14569536423841}], "total": {"test_f1": 64.51891007962539, "test_f1_se": 1.4092049545866414, "test_em": 31.247013251337705, "test_em_se": 2.483193313564059}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scandiqa-sv", "task": "reading-comprehension", "dataset_languages": ["sv"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"f1": 58.049120254289654, "em": 45.75757575757576}, {"f1": 51.60504295259067, "em": 36.61865049279757}, {"f1": 59.17997067011091, "em": 46.464646464646464}, {"f1": 57.29105369462304, "em": 44.596651445966515}, {"f1": 57.365126679495305, "em": 43.48496530454896}, {"f1": 60.088238432394625, "em": 50.229007633587784}, {"f1": 53.377183921101214, "em": 39.62558502340094}, {"f1": 55.537684831574275, "em": 42.05974842767296}, {"f1": 56.34019848741567, "em": 42.72445820433437}, {"f1": 58.32363379473386, "em": 45.601851851851855}], "total": {"test_f1": 56.715725371832924, "test_f1_se": 1.6183176013797655, "test_em": 43.71631406063832, "test_em_se": 2.3498022008707444}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "nqii", "task": "reading-comprehension", "dataset_languages": ["is"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"f1": 51.54117579826838, "em": 12.153846153846153}, {"f1": 51.268579931081774, "em": 11.838006230529595}, {"f1": 50.50614640115959, "em": 21.962616822429908}, {"f1": 48.43396688762269, "em": 14.200298953662182}, {"f1": 50.98861748088588, "em": 12.852664576802507}, {"f1": 56.044571392296405, "em": 22.033898305084747}, {"f1": 49.37461627134205, "em": 13.384615384615385}, {"f1": 50.3719932279117, "em": 19.478527607361965}, {"f1": 48.29720360592552, "em": 12.5}, {"f1": 53.02016360152181, "em": 19.728915662650603}], "total": {"test_f1": 50.98470345980157, "test_f1_se": 1.4173170067894822, "test_em": 16.013338969698303, "test_em_se": 2.632162250893644}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "foqa", "task": "reading-comprehension", "dataset_languages": ["fo"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"f1": 43.142271899182134, "em": 15.196078431372548}, {"f1": 40.42477334370911, "em": 13.267813267813267}, {"f1": 52.33716242956919, "em": 25.432098765432098}, {"f1": 53.8619253566648, "em": 28.640776699029125}, {"f1": 48.40672758213182, "em": 22.02970297029703}, {"f1": 44.863678183578315, "em": 18.271604938271604}, {"f1": 47.722616687803395, "em": 20.754716981132077}, {"f1": 50.917544740485184, "em": 23.155216284987276}, {"f1": 52.749183885252855, "em": 25.6857855361596}, {"f1": 57.03211962666084, "em": 31.44963144963145}], "total": {"test_f1": 49.14580037350377, "test_f1_se": 3.225581674516291, "test_em": 22.388342532412604, "test_em_se": 3.558017389815869}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "germanquad", "task": "reading-comprehension", "dataset_languages": ["de"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"f1": 58.76087859544747, "em": 22.045454545454547}, {"f1": 53.47693598818355, "em": 18.27141774071266}, {"f1": 51.9277236543538, "em": 12.354312354312354}, {"f1": 58.392346834093104, "em": 22.070015220700153}, {"f1": 54.4783696258334, "em": 17.733230531996917}, {"f1": 56.72759494001354, "em": 19.389312977099237}, {"f1": 56.73096233218355, "em": 22.69890795631825}, {"f1": 53.2081576509577, "em": 14.544025157232705}, {"f1": 54.598493264504846, "em": 18.962848297213622}, {"f1": 56.117009494972585, "em": 24.382716049382715}], "total": {"test_f1": 55.44184723805436, "test_f1_se": 1.4062773639288502, "test_em": 19.245224083042316, "test_em_se": 2.3288684882116333}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "squad", "task": "reading-comprehension", "dataset_languages": ["en"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"f1": 75.72584017320926, "em": 54.39393939393939}, {"f1": 70.56759509610832, "em": 46.019711902956786}, {"f1": 73.90280522331896, "em": 50.582750582750585}, {"f1": 71.54706104788002, "em": 49.923896499238964}, {"f1": 73.54001393075518, "em": 50.34695451040864}, {"f1": 77.33564505441795, "em": 58.47328244274809}, {"f1": 72.23556505461299, "em": 49.06396255850234}, {"f1": 70.08639235507042, "em": 45.911949685534594}, {"f1": 70.73001037953544, "em": 45.743034055727556}, {"f1": 74.1956459291939, "em": 54.47530864197531}], "total": {"test_f1": 72.98665742441025, "test_f1_se": 1.4756720746097396, "test_em": 50.49347902737822, "test_em_se": 2.6191654366439296}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "squad-nl", "task": "reading-comprehension", "dataset_languages": ["nl"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"f1": 66.9779270874174, "em": 38.18181818181818}, {"f1": 71.12079932006368, "em": 44.95830174374526}, {"f1": 72.83012569111128, "em": 50.116550116550115}, {"f1": 71.69037355955571, "em": 48.93455098934551}, {"f1": 73.16595265653362, "em": 49.113338473400155}, {"f1": 66.41658702500634, "em": 34.80916030534351}, {"f1": 72.04414265530932, "em": 49.68798751950078}, {"f1": 72.21331808878304, "em": 48.742138364779876}, {"f1": 70.98025559179929, "em": 47.8328173374613}, {"f1": 69.87752938824629, "em": 43.05555555555556}], "total": {"test_f1": 70.73170110638259, "test_f1_se": 1.443549216989816, "test_em": 45.543221858750016, "test_em_se": 3.2911243091443563}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "squad-it", "task": "reading-comprehension", "dataset_languages": ["it"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"f1": 61.685350192090446, "em": 38.10606060606061}, {"f1": 55.427870914623746, "em": 26.383623957543595}, {"f1": 62.5932057217931, "em": 40.01554001554002}, {"f1": 58.67604080305439, "em": 34.93150684931507}, {"f1": 60.36551093249257, "em": 34.07864302235929}, {"f1": 59.52792309707211, "em": 34.80916030534351}, {"f1": 64.38009854364647, "em": 40.01560062402496}, {"f1": 51.22270380271858, "em": 23.270440251572328}, {"f1": 61.04374411116141, "em": 38.69969040247678}, {"f1": 61.74205813703812, "em": 34.876543209876544}], "total": {"test_f1": 59.6664506255691, "test_f1_se": 2.372028467109973, "test_em": 34.51868092441127, "test_em_se": 3.4820907136809542}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fquad", "task": "reading-comprehension", "dataset_languages": ["fr"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"f1": 68.6807786408227, "em": 34.84848484848485}, {"f1": 66.04782294846362, "em": 32.600454890068235}, {"f1": 64.01638640142733, "em": 29.75912975912976}, {"f1": 63.613807737338426, "em": 27.85388127853881}, {"f1": 65.12550245716452, "em": 34.30994602929838}, {"f1": 63.21205358720205, "em": 27.251908396946565}, {"f1": 68.98099974254556, "em": 36.349453978159126}, {"f1": 65.5407762134319, "em": 35.22012578616352}, {"f1": 63.64355547652411, "em": 33.90092879256966}, {"f1": 64.59335571032446, "em": 28.858024691358025}], "total": {"test_f1": 65.34550389152446, "test_f1_se": 1.2685432340430318, "test_em": 32.09523384507169, "test_em_se": 2.078875875118749}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "nordjylland-news", "task": "summarization", "dataset_languages": ["da"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"bertscore": 0.649016293202294, "rouge_l": 0.14500070229307352}, {"bertscore": 0.6487414421280846, "rouge_l": 0.15203966209355776}, {"bertscore": 0.6446743915439583, "rouge_l": 0.13870083405646483}, {"bertscore": 0.644496528111631, "rouge_l": 0.13546591402539918}, {"bertscore": 0.6495183387451107, "rouge_l": 0.14643437177886076}, {"bertscore": 0.6450079292844748, "rouge_l": 0.13882655271360717}, {"bertscore": 0.6475466597476043, "rouge_l": 0.1400335599194812}, {"bertscore": 0.6464171568950405, "rouge_l": 0.14147553569846794}, {"bertscore": 0.6435233369120397, "rouge_l": 0.13419987149098095}, {"bertscore": 0.6465839891607175, "rouge_l": 0.1380602327941184}], "total": {"test_bertscore": 64.65526065730955, "test_bertscore_se": 0.1306363588508623, "test_rouge_l": 14.10237236864012, "test_rouge_l_se": 0.33574391952540417}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mlsum", "task": "summarization", "dataset_languages": ["de"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"bertscore": 0.6406252412125468, "rouge_l": 0.1224574418152001}, {"bertscore": 0.6394210048601963, "rouge_l": 0.12238677271304266}, {"bertscore": 0.6387420957616996, "rouge_l": 0.12279530158479866}, {"bertscore": 0.6395116773783229, "rouge_l": 0.12859954704784476}, {"bertscore": 0.6410481206257828, "rouge_l": 0.12460403833592418}, {"bertscore": 0.6394152880820911, "rouge_l": 0.12212716211466794}, {"bertscore": 0.6410426081856713, "rouge_l": 0.12426485764485863}, {"bertscore": 0.6419580090150703, "rouge_l": 0.12443880081025625}, {"bertscore": 0.6412748897855636, "rouge_l": 0.12473603915664305}, {"bertscore": 0.6389110099407844, "rouge_l": 0.1211769751810657}], "total": {"test_bertscore": 64.01949944847729, "test_bertscore_se": 0.0695707509762289, "test_rouge_l": 12.375869364043018, "test_rouge_l_se": 0.1300988857876343}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "rrn", "task": "summarization", "dataset_languages": ["is"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"bertscore": 0.6514228480518796, "rouge_l": 0.1350451032200075}, {"bertscore": 0.6642171712010168, "rouge_l": 0.15821392172589493}, {"bertscore": 0.6673215594491921, "rouge_l": 0.16335538701245256}, {"bertscore": 0.6557051884592511, "rouge_l": 0.14481217718057562}, {"bertscore": 0.6582085592090152, "rouge_l": 0.14685690868550633}, {"bertscore": 0.6597010862315074, "rouge_l": 0.1500086643220198}, {"bertscore": 0.6618934045545757, "rouge_l": 0.15279330603473062}, {"bertscore": 0.6470302338711917, "rouge_l": 0.12482809835571819}, {"bertscore": 0.6626562683377415, "rouge_l": 0.15581552794866305}, {"bertscore": 0.6634560320526361, "rouge_l": 0.15920318172524542}], "total": {"test_bertscore": 65.91612351418007, "test_bertscore_se": 0.3867743185012147, "test_rouge_l": 14.90932276210814, "test_rouge_l_se": 0.7327826283834712}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "no-sammendrag", "task": "summarization", "dataset_languages": ["nb", "nn", "no"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"bertscore": 0.6218699980236124, "rouge_l": 0.09121719171814735}, {"bertscore": 0.6183737499377457, "rouge_l": 0.07918672480045487}, {"bertscore": 0.6241020141751505, "rouge_l": 0.0952790489907906}, {"bertscore": 0.6249445027206093, "rouge_l": 0.10007333980824788}, {"bertscore": 0.6175063944538124, "rouge_l": 0.07906839469513521}, {"bertscore": 0.6230401541397441, "rouge_l": 0.09519392263678167}, {"bertscore": 0.6108348794368794, "rouge_l": 0.07044379071544013}, {"bertscore": 0.6182549945660867, "rouge_l": 0.08244547166455235}, {"bertscore": 0.6223524505039677, "rouge_l": 0.09598061058558671}, {"bertscore": 0.6201413236412918, "rouge_l": 0.08992435549734351}], "total": {"test_bertscore": 62.014204615989, "test_bertscore_se": 0.257444438299511, "test_rouge_l": 8.788128511124802, "test_rouge_l_se": 0.593284929007405}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "wiki-lingua-nl", "task": "summarization", "dataset_languages": ["nl"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"bertscore": 0.6614631821867079, "rouge_l": 0.15667988492238344}, {"bertscore": 0.6526650818996131, "rouge_l": 0.1513992467114786}, {"bertscore": 0.6553393133945065, "rouge_l": 0.14805193971744157}, {"bertscore": 0.6634657506947406, "rouge_l": 0.16285396690115214}, {"bertscore": 0.6580579737492371, "rouge_l": 0.15537807918717397}, {"bertscore": 0.6639136999001494, "rouge_l": 0.1642468102394163}, {"bertscore": 0.6639135308068944, "rouge_l": 0.1625984298640263}, {"bertscore": 0.6554538584023248, "rouge_l": 0.15037064041489484}, {"bertscore": 0.6652679970138706, "rouge_l": 0.16310297506397547}, {"bertscore": 0.6630778390390333, "rouge_l": 0.16309309304623748}], "total": {"test_bertscore": 66.02618227087078, "test_bertscore_se": 0.2781745471797979, "test_rouge_l": 15.777750660681802, "test_rouge_l_se": 0.38350847854786724}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "swedn", "task": "summarization", "dataset_languages": ["sv"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"bertscore": 0.6615956037130672, "rouge_l": 0.15973702105784499}, {"bertscore": 0.6653842639061622, "rouge_l": 0.1755526282886158}, {"bertscore": 0.6628235463285819, "rouge_l": 0.17279585109471227}, {"bertscore": 0.660711038770387, "rouge_l": 0.16504682410126315}, {"bertscore": 0.6630742273409851, "rouge_l": 0.17089144822623986}, {"bertscore": 0.6554210875474382, "rouge_l": 0.14467761395362588}, {"bertscore": 0.6627477834990714, "rouge_l": 0.1689963736855754}, {"bertscore": 0.6633845592732541, "rouge_l": 0.1683398607449656}, {"bertscore": 0.649321936070919, "rouge_l": 0.11113661144919772}, {"bertscore": 0.6648822818533517, "rouge_l": 0.17474314864451945}], "total": {"test_bertscore": 66.09346328303218, "test_bertscore_se": 0.30543556775393244, "test_rouge_l": 16.119173812465597, "test_rouge_l_se": 1.2267772746221022}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "cnn-dailymail", "task": "summarization", "dataset_languages": ["en"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"bertscore": 0.691922725061886, "rouge_l": 0.2157147903757866}, {"bertscore": 0.6926396501075942, "rouge_l": 0.2112001726521034}, {"bertscore": 0.6915434830298182, "rouge_l": 0.21358397360630418}, {"bertscore": 0.6926875335047953, "rouge_l": 0.2129496695065317}, {"bertscore": 0.690497624600539, "rouge_l": 0.21009093029168344}, {"bertscore": 0.6916806857916526, "rouge_l": 0.21229333801118738}, {"bertscore": 0.6918630271102302, "rouge_l": 0.21525141888049298}, {"bertscore": 0.6888128168648109, "rouge_l": 0.20958816619198195}, {"bertscore": 0.691515695041744, "rouge_l": 0.20992434039991503}, {"bertscore": 0.6930152353015728, "rouge_l": 0.21478525002866594}], "total": {"test_bertscore": 69.16178476414643, "test_bertscore_se": 0.07573335284916942, "test_rouge_l": 21.253820499446523, "test_rouge_l_se": 0.1418483288252939}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "orange-sum", "task": "summarization", "dataset_languages": ["fr"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"bertscore": 0.6619569466565736, "rouge_l": 0.15883202603826574}, {"bertscore": 0.6615418356959708, "rouge_l": 0.15625863226953662}, {"bertscore": 0.6621607764391229, "rouge_l": 0.1567915104595865}, {"bertscore": 0.6612833311082795, "rouge_l": 0.1584233992199267}, {"bertscore": 0.6625131022010464, "rouge_l": 0.15961528844124695}, {"bertscore": 0.6648293618345633, "rouge_l": 0.16188438066380595}, {"bertscore": 0.662372961291112, "rouge_l": 0.15920724191369262}, {"bertscore": 0.6605265569523908, "rouge_l": 0.1577647693401414}, {"bertscore": 0.6649149825097993, "rouge_l": 0.1586561263127565}, {"bertscore": 0.6643615752691403, "rouge_l": 0.16095683038609965}], "total": {"test_bertscore": 66.26461429957999, "test_bertscore_se": 0.09519199250810984, "test_rouge_l": 15.883902050450583, "test_rouge_l_se": 0.10677248343056141}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "ilpost-sum", "task": "summarization", "dataset_languages": ["it"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"bertscore": 0.6494320885976776, "rouge_l": 0.1521994200023016}, {"bertscore": 0.6461315991473384, "rouge_l": 0.15040191489025861}, {"bertscore": 0.6475241825392004, "rouge_l": 0.15303610585796784}, {"bertscore": 0.6476796063943766, "rouge_l": 0.15340294671309437}, {"bertscore": 0.6495258520590141, "rouge_l": 0.15375607825142185}, {"bertscore": 0.6480914064159151, "rouge_l": 0.15281157090463107}, {"bertscore": 0.6500022077816539, "rouge_l": 0.15378582448789999}, {"bertscore": 0.6504266715201084, "rouge_l": 0.15444150860914616}, {"bertscore": 0.6510678015183657, "rouge_l": 0.1566243583523988}, {"bertscore": 0.6500531701312866, "rouge_l": 0.15639688543559033}], "total": {"test_bertscore": 64.89934586104937, "test_bertscore_se": 0.09660128818058354, "test_rouge_l": 15.368566135047107, "test_rouge_l_se": 0.1147167767154601}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "danske-talemaader", "task": "knowledge", "dataset_languages": ["da"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"mcc": 0.694857328831345, "accuracy": 0.7710396039603961}, {"mcc": 0.7326484562157091, "accuracy": 0.7995049504950495}, {"mcc": 0.7046719252424913, "accuracy": 0.7784653465346535}, {"mcc": 0.7367570387037328, "accuracy": 0.8032178217821783}, {"mcc": 0.7066517328220727, "accuracy": 0.780940594059406}, {"mcc": 0.6993871698771084, "accuracy": 0.7747524752475248}, {"mcc": 0.7240940175957704, "accuracy": 0.7933168316831684}, {"mcc": 0.664483105564484, "accuracy": 0.7487623762376238}, {"mcc": 0.7265666622145629, "accuracy": 0.7945544554455446}, {"mcc": 0.7099767660038272, "accuracy": 0.7834158415841584}], "total": {"test_mcc": 71.00094203071106, "test_mcc_se": 1.3317918482911566, "test_accuracy": 78.27970297029702, "test_accuracy_se": 0.9970493807617868}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "danish-citizen-tests", "task": "knowledge", "dataset_languages": ["da"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"mcc": 0.7804099799598343, "accuracy": 0.8495238095238096}, {"mcc": 0.8521060222750915, "accuracy": 0.8990476190476191}, {"mcc": 0.7974101282731013, "accuracy": 0.8628571428571429}, {"mcc": 0.8071410002307214, "accuracy": 0.8685714285714285}, {"mcc": 0.8158070075951559, "accuracy": 0.8742857142857143}, {"mcc": 0.759647952057761, "accuracy": 0.8361904761904762}, {"mcc": 0.7273627962342438, "accuracy": 0.8114285714285714}, {"mcc": 0.8382609498706478, "accuracy": 0.8895238095238095}, {"mcc": 0.8267630577097055, "accuracy": 0.88}, {"mcc": 0.7919430168385825, "accuracy": 0.8571428571428571}], "total": {"test_mcc": 79.96851911044845, "test_mcc_se": 2.3149496368228397, "test_accuracy": 86.28571428571428, "test_accuracy_se": 1.6062495232261398}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "nrk-quiz-qa", "task": "knowledge", "dataset_languages": ["nb", "nn", "no"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"mcc": 0.4422506602583229, "accuracy": 0.59033203125}, {"mcc": 0.391956519934764, "accuracy": 0.55322265625}, {"mcc": 0.42076030731428155, "accuracy": 0.57080078125}, {"mcc": 0.44106355239089723, "accuracy": 0.58642578125}, {"mcc": 0.4527608386237963, "accuracy": 0.59521484375}, {"mcc": 0.4168196535855691, "accuracy": 0.56787109375}, {"mcc": 0.4196349035422822, "accuracy": 0.57421875}, {"mcc": 0.43591524946008264, "accuracy": 0.58349609375}, {"mcc": 0.40417665156639426, "accuracy": 0.5625}, {"mcc": 0.41630626695010703, "accuracy": 0.5693359375}], "total": {"test_mcc": 42.41644603626497, "test_mcc_se": 1.1601386807676646, "test_accuracy": 57.5341796875, "test_accuracy_se": 0.82042756354472}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-sv", "task": "knowledge", "dataset_languages": ["sv"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"mcc": 0.5257793219718736, "accuracy": 0.64111328125}, {"mcc": 0.5325746252584136, "accuracy": 0.6474609375}, {"mcc": 0.5290675033285082, "accuracy": 0.6435546875}, {"mcc": 0.5285226777144093, "accuracy": 0.64306640625}, {"mcc": 0.5443662776819554, "accuracy": 0.65771484375}, {"mcc": 0.49535927647316885, "accuracy": 0.62109375}, {"mcc": 0.5288760381608588, "accuracy": 0.6455078125}, {"mcc": 0.5432944588552377, "accuracy": 0.65576171875}, {"mcc": 0.5315341596664968, "accuracy": 0.646484375}, {"mcc": 0.5440265478640542, "accuracy": 0.6572265625}], "total": {"test_mcc": 53.03400886974976, "test_mcc_se": 0.8789243395724738, "test_accuracy": 64.58984375, "test_accuracy_se": 0.6597888164259953}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "icelandic-knowledge", "task": "knowledge", "dataset_languages": ["is"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"mcc": 0.1496946062837647, "accuracy": 0.353515625}, {"mcc": 0.15423322732806674, "accuracy": 0.3642578125}, {"mcc": 0.14539596309504343, "accuracy": 0.3525390625}, {"mcc": 0.1633824434016034, "accuracy": 0.3642578125}, {"mcc": 0.18820456841899313, "accuracy": 0.3896484375}, {"mcc": 0.20065130230066688, "accuracy": 0.3994140625}, {"mcc": 0.16015476990563854, "accuracy": 0.365234375}, {"mcc": 0.19339828375271892, "accuracy": 0.3916015625}, {"mcc": 0.15608141519512256, "accuracy": 0.365234375}, {"mcc": 0.1857376450771933, "accuracy": 0.384765625}], "total": {"test_mcc": 16.969342247588116, "test_mcc_se": 1.2508950541022352, "test_accuracy": 37.3046875, "test_accuracy_se": 1.0394062388299274}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-de", "task": "knowledge", "dataset_languages": ["de"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"mcc": 0.5587524601755135, "accuracy": 0.669921875}, {"mcc": 0.5579763544086381, "accuracy": 0.66845703125}, {"mcc": 0.5675822895392605, "accuracy": 0.6748046875}, {"mcc": 0.5840766745108075, "accuracy": 0.68896484375}, {"mcc": 0.5509427092801976, "accuracy": 0.6640625}, {"mcc": 0.5612437716298881, "accuracy": 0.671875}, {"mcc": 0.5647818635427444, "accuracy": 0.673828125}, {"mcc": 0.5730835897375879, "accuracy": 0.68017578125}, {"mcc": 0.5906163140353263, "accuracy": 0.69091796875}, {"mcc": 0.5536517621235185, "accuracy": 0.6650390625}], "total": {"test_mcc": 56.62707788983483, "test_mcc_se": 0.8014695984633832, "test_accuracy": 67.48046875, "test_accuracy_se": 0.5745730966859915}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-nl", "task": "knowledge", "dataset_languages": ["nl"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"mcc": 0.5575708840496427, "accuracy": 0.66455078125}, {"mcc": 0.5646767906675616, "accuracy": 0.67138671875}, {"mcc": 0.5451422346476623, "accuracy": 0.65771484375}, {"mcc": 0.5557869430311856, "accuracy": 0.66552734375}, {"mcc": 0.5159953232450604, "accuracy": 0.6357421875}, {"mcc": 0.53418988472522, "accuracy": 0.650390625}, {"mcc": 0.540524654656531, "accuracy": 0.65380859375}, {"mcc": 0.5291445513643309, "accuracy": 0.64404296875}, {"mcc": 0.5366821957375328, "accuracy": 0.650390625}, {"mcc": 0.5140641280286415, "accuracy": 0.63232421875}], "total": {"test_mcc": 53.93777590153369, "test_mcc_se": 1.053722267130591, "test_accuracy": 65.2587890625, "test_accuracy_se": 0.7917951646888904}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu", "task": "knowledge", "dataset_languages": ["en"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"mcc": 0.6522879670666439, "accuracy": 0.7392578125}, {"mcc": 0.6245022884459336, "accuracy": 0.7177734375}, {"mcc": 0.6702449079676359, "accuracy": 0.75244140625}, {"mcc": 0.6435716925960092, "accuracy": 0.7314453125}, {"mcc": 0.6550492461861465, "accuracy": 0.7412109375}, {"mcc": 0.6347515149811317, "accuracy": 0.7255859375}, {"mcc": 0.658823582740994, "accuracy": 0.74365234375}, {"mcc": 0.663343612036904, "accuracy": 0.74755859375}, {"mcc": 0.6399820501644782, "accuracy": 0.72998046875}, {"mcc": 0.6625133734097872, "accuracy": 0.7470703125}], "total": {"test_mcc": 65.05070235595663, "test_mcc_se": 0.8945751973128189, "test_accuracy": 73.759765625, "test_accuracy_se": 0.6836356757976483}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-fr", "task": "knowledge", "dataset_languages": ["fr"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"mcc": 0.5497779096309291, "accuracy": 0.66162109375}, {"mcc": 0.5631431283421772, "accuracy": 0.67041015625}, {"mcc": 0.5479896648963349, "accuracy": 0.66015625}, {"mcc": 0.5533757844245577, "accuracy": 0.6640625}, {"mcc": 0.5258192186092814, "accuracy": 0.64453125}, {"mcc": 0.540509091853227, "accuracy": 0.65478515625}, {"mcc": 0.5594996844449172, "accuracy": 0.6689453125}, {"mcc": 0.5576468580734806, "accuracy": 0.66845703125}, {"mcc": 0.5827046783748477, "accuracy": 0.68701171875}, {"mcc": 0.5660095413650338, "accuracy": 0.6728515625}], "total": {"test_mcc": 55.46475560014786, "test_mcc_se": 0.9516351421530473, "test_accuracy": 66.5283203125, "test_accuracy_se": 0.7040307660604989}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-it", "task": "knowledge", "dataset_languages": ["it"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"mcc": 0.6131069077579349, "accuracy": 0.70947265625}, {"mcc": 0.6144193878168746, "accuracy": 0.7099609375}, {"mcc": 0.5899928168542073, "accuracy": 0.6923828125}, {"mcc": 0.590839614619242, "accuracy": 0.693359375}, {"mcc": 0.5999709297343188, "accuracy": 0.701171875}, {"mcc": 0.5958014192602226, "accuracy": 0.6962890625}, {"mcc": 0.6039233684935643, "accuracy": 0.703125}, {"mcc": 0.5722717329277371, "accuracy": 0.6796875}, {"mcc": 0.605026936657429, "accuracy": 0.70361328125}, {"mcc": 0.5810790983069896, "accuracy": 0.6865234375}], "total": {"test_mcc": 59.664322124285206, "test_mcc_se": 0.8357141285910927, "test_accuracy": 69.755859375, "test_accuracy_se": 0.6071598806407765}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-da", "task": "common-sense-reasoning", "dataset_languages": ["da"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"mcc": 0.49856814030768837, "accuracy": 0.6162109375}, {"mcc": 0.46839717867401376, "accuracy": 0.58349609375}, {"mcc": 0.5414892179726102, "accuracy": 0.6435546875}, {"mcc": 0.5166318030902483, "accuracy": 0.6328125}, {"mcc": 0.42638018315816006, "accuracy": 0.55224609375}, {"mcc": 0.5126453804152435, "accuracy": 0.6240234375}, {"mcc": 0.24618908108510895, "accuracy": 0.4013671875}, {"mcc": 0.48421576854816767, "accuracy": 0.60205078125}, {"mcc": 0.5207227981440884, "accuracy": 0.6318359375}, {"mcc": 0.5179463233530152, "accuracy": 0.6220703125}], "total": {"test_mcc": 47.33185874748344, "test_mcc_se": 5.342980426094561, "test_accuracy": 59.0966796875, "test_accuracy_se": 4.455630035338089}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "nor-common-sense-qa", "task": "common-sense-reasoning", "dataset_languages": ["nb", "nn", "no"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"mcc": 0.7119571304355281, "accuracy": 0.7685076380728555}, {"mcc": 0.6667936996650161, "accuracy": 0.7320799059929495}, {"mcc": 0.6707800396717557, "accuracy": 0.7356051703877791}, {"mcc": 0.6925141739695975, "accuracy": 0.7520564042303173}, {"mcc": 0.745864289058343, "accuracy": 0.7967097532314924}, {"mcc": 0.708356772439999, "accuracy": 0.7661574618096357}, {"mcc": 0.6820517202644487, "accuracy": 0.745005875440658}, {"mcc": 0.6826465113474396, "accuracy": 0.7438307873090482}, {"mcc": 0.7003976743376397, "accuracy": 0.7591069330199764}, {"mcc": 0.6772732019233945, "accuracy": 0.7414806110458284}], "total": {"test_mcc": 69.38635213113162, "test_mcc_se": 1.4743100055136802, "test_accuracy": 75.4054054054054, "test_accuracy_se": 1.1997395570283858}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-sv", "task": "common-sense-reasoning", "dataset_languages": ["sv"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"mcc": 0.5148499431781076, "accuracy": 0.62255859375}, {"mcc": 0.464344854704383, "accuracy": 0.583984375}, {"mcc": 0.4738875642394574, "accuracy": 0.59130859375}, {"mcc": 0.4725262559661545, "accuracy": 0.5810546875}, {"mcc": 0.44735171392230266, "accuracy": 0.5615234375}, {"mcc": 0.48699514144082656, "accuracy": 0.5986328125}, {"mcc": 0.6243250562928788, "accuracy": 0.71435546875}, {"mcc": 0.5821799962128293, "accuracy": 0.6826171875}, {"mcc": 0.5112969155624977, "accuracy": 0.6181640625}, {"mcc": 0.5257936327701509, "accuracy": 0.6376953125}], "total": {"test_mcc": 51.03551074289589, "test_mcc_se": 3.4447687048219087, "test_accuracy": 61.91894531249999, "test_accuracy_se": 2.969143931883263}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "winogrande-is", "task": "common-sense-reasoning", "dataset_languages": ["is"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"mcc": 0.15304609279867068, "accuracy": 0.5703125}, {"mcc": 0.11071024895568739, "accuracy": 0.5345982142857143}, {"mcc": 0.0622869482758801, "accuracy": 0.5279017857142857}, {"mcc": 0.16243317299036503, "accuracy": 0.5535714285714286}, {"mcc": 0.18131747852886304, "accuracy": 0.5837053571428571}, {"mcc": 0.11607073761957899, "accuracy": 0.5357142857142857}, {"mcc": 0.18776401891995748, "accuracy": 0.5859375}, {"mcc": 0.10300550801207568, "accuracy": 0.5446428571428571}, {"mcc": 0.10838963743393364, "accuracy": 0.5145089285714286}, {"mcc": 0.15897445749029365, "accuracy": 0.5703125}], "total": {"test_mcc": 13.439983010253057, "test_mcc_se": 2.4917639399641156, "test_accuracy": 55.21205357142858, "test_accuracy_se": 1.5236094098162276}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-de", "task": "common-sense-reasoning", "dataset_languages": ["de"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"mcc": 0.6466926830025878, "accuracy": 0.73388671875}, {"mcc": 0.5959315082414037, "accuracy": 0.6904296875}, {"mcc": 0.578855220733998, "accuracy": 0.6787109375}, {"mcc": 0.5889114061693467, "accuracy": 0.68798828125}, {"mcc": 0.5699622325260281, "accuracy": 0.66845703125}, {"mcc": 0.5681105617477611, "accuracy": 0.669921875}, {"mcc": 0.6257248209536225, "accuracy": 0.71484375}, {"mcc": 0.5683046976068696, "accuracy": 0.66455078125}, {"mcc": 0.6182570016164238, "accuracy": 0.7099609375}, {"mcc": 0.50120675278808, "accuracy": 0.607421875}], "total": {"test_mcc": 58.61956885386122, "test_mcc_se": 2.491266649006885, "test_accuracy": 68.26171875, "test_accuracy_se": 2.153261235686521}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-nl", "task": "common-sense-reasoning", "dataset_languages": ["nl"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"mcc": 0.5568555531603894, "accuracy": 0.66064453125}, {"mcc": 0.5483735887514661, "accuracy": 0.65234375}, {"mcc": 0.5832732771362313, "accuracy": 0.68408203125}, {"mcc": 0.5917587419592348, "accuracy": 0.689453125}, {"mcc": 0.5470396628339346, "accuracy": 0.6484375}, {"mcc": 0.5804787631805417, "accuracy": 0.68212890625}, {"mcc": 0.5628260301888806, "accuracy": 0.66552734375}, {"mcc": 0.538403265088797, "accuracy": 0.640625}, {"mcc": 0.5439640171276726, "accuracy": 0.650390625}, {"mcc": 0.5058019924285339, "accuracy": 0.619140625}], "total": {"test_mcc": 55.58774891855683, "test_mcc_se": 1.5709515691475913, "test_accuracy": 65.927734375, "test_accuracy_se": 1.3535819112556562}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag", "task": "common-sense-reasoning", "dataset_languages": ["en"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"mcc": 0.6363631457007806, "accuracy": 0.720703125}, {"mcc": 0.7145055097275035, "accuracy": 0.78564453125}, {"mcc": 0.6469367010379637, "accuracy": 0.7275390625}, {"mcc": 0.7078897027397705, "accuracy": 0.77978515625}, {"mcc": 0.738176445656073, "accuracy": 0.8037109375}, {"mcc": 0.7202558081795742, "accuracy": 0.7900390625}, {"mcc": 0.7234178816107611, "accuracy": 0.7919921875}, {"mcc": 0.757242645721691, "accuracy": 0.81787109375}, {"mcc": 0.6805285250990392, "accuracy": 0.75927734375}, {"mcc": 0.645225454556253, "accuracy": 0.7294921875}], "total": {"test_mcc": 69.7054182002941, "test_mcc_se": 2.623398746260629, "test_accuracy": 77.060546875, "test_accuracy_se": 2.131018236770935}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-fr", "task": "common-sense-reasoning", "dataset_languages": ["fr"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"mcc": 0.6520579717736703, "accuracy": 0.73583984375}, {"mcc": 0.5692115632476329, "accuracy": 0.6708984375}, {"mcc": 0.49022353567052657, "accuracy": 0.60546875}, {"mcc": 0.5687160486244304, "accuracy": 0.67236328125}, {"mcc": 0.5500781673821673, "accuracy": 0.65087890625}, {"mcc": 0.5871080110653082, "accuracy": 0.68408203125}, {"mcc": 0.5480083694480519, "accuracy": 0.65478515625}, {"mcc": 0.5814513764140811, "accuracy": 0.68359375}, {"mcc": 0.6206404532689427, "accuracy": 0.7109375}, {"mcc": 0.6312297723410137, "accuracy": 0.72119140625}], "total": {"test_mcc": 57.98725269235824, "test_mcc_se": 2.902981893942345, "test_accuracy": 67.900390625, "test_accuracy_se": 2.353042977119232}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-it", "task": "common-sense-reasoning", "dataset_languages": ["it"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"mcc": 0.5651617211798186, "accuracy": 0.66650390625}, {"mcc": 0.5386055595441176, "accuracy": 0.64892578125}, {"mcc": 0.5883196281203056, "accuracy": 0.68701171875}, {"mcc": 0.5258275800248364, "accuracy": 0.64208984375}, {"mcc": 0.5733732055063325, "accuracy": 0.67578125}, {"mcc": 0.5823448156774262, "accuracy": 0.68701171875}, {"mcc": 0.5705177823373463, "accuracy": 0.673828125}, {"mcc": 0.5906486262316291, "accuracy": 0.68994140625}, {"mcc": 0.5825393149639124, "accuracy": 0.68115234375}, {"mcc": 0.5033996995087777, "accuracy": 0.6181640625}], "total": {"test_mcc": 56.20737933094502, "test_mcc_se": 1.8305958222864511, "test_accuracy": 66.7041015625, "test_accuracy_se": 1.458505716943781}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "152334H/miqu-1-70b-sf", "results": {"raw": [{"test_speed": 257.02, "test_speed_short": 29.28}, {"test_speed": 503.37, "test_speed_short": 54.900000000000006}, {"test_speed": 757.49, "test_speed_short": 105.56}, {"test_speed": 1000.36, "test_speed_short": 131.04}, {"test_speed": 1249.56, "test_speed_short": 155.66}, {"test_speed": 1435.6100000000001, "test_speed_short": 204.63}, {"test_speed": 1703.7700000000002, "test_speed_short": 232.32}, {"test_speed": 1929.84, "test_speed_short": 257.73}, {"test_speed": 2101.23, "test_speed_short": 280.8}, {"test_speed": 2299.2799999999997, "test_speed_short": 305.15}], "total": {"test_speed": 1323.753, "test_speed_se": 429.51778821343163, "test_speed_short": 175.70699999999997, "test_speed_short_se": 59.13377544808944}}, "num_model_parameters": 68976648192, "max_sequence_length": 32764, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}