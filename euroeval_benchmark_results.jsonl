
{"dataset": "allocine", "task": "sentiment-classification", "dataset_languages": ["fr"], "model": "meta-llama/Llama-3.1-70B-Instruct", "results": {"raw": [{"mcc": 0.94936677108302, "macro_f1": 0.9745705727733969}, {"mcc": 0.9443675291125764, "macro_f1": 0.9721556939788263}, {"mcc": 0.9556923540744713, "macro_f1": 0.9778345542847784}, {"mcc": 0.9442990974735204, "macro_f1": 0.9721493166519344}, {"mcc": 0.9423754456722386, "macro_f1": 0.9711484760377291}, {"mcc": 0.9452456725014704, "macro_f1": 0.9726144629867352}, {"mcc": 0.9478549059696882, "macro_f1": 0.9736263736263736}, {"mcc": 0.9385070951398887, "macro_f1": 0.9692144340744753}, {"mcc": 0.9490034649118624, "macro_f1": 0.9744867206178998}, {"mcc": 0.9549771199849664, "macro_f1": 0.977442492819085}], "total": {"test_mcc": 94.71689455923702, "test_mcc_se": 0.33318126789712005, "test_macro_f1": 97.35243097851234, "test_macro_f1_se": 0.16627665129449537}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "sentipolc16", "task": "sentiment-classification", "dataset_languages": ["it"], "model": "meta-llama/Llama-3.1-70B-Instruct", "results": {"raw": [{"mcc": 0.4812559406716472, "macro_f1": 0.49456678130181664}, {"mcc": 0.5144384867690885, "macro_f1": 0.5056392633712221}, {"mcc": 0.5067504203338918, "macro_f1": 0.5028084164137739}, {"mcc": 0.4798547952032152, "macro_f1": 0.4877808402238369}, {"mcc": 0.5121947006017273, "macro_f1": 0.5029872117710956}, {"mcc": 0.49928012019694584, "macro_f1": 0.49890041183094436}, {"mcc": 0.5002922068861326, "macro_f1": 0.5004062266419681}, {"mcc": 0.5076948302775661, "macro_f1": 0.5010434766362125}, {"mcc": 0.4998425854355079, "macro_f1": 0.49801906219816666}, {"mcc": 0.4876131521422453, "macro_f1": 0.4891078054159121}], "total": {"test_mcc": 49.89217238517968, "test_mcc_se": 0.7623489767557768, "test_macro_f1": 49.812594958049495, "test_macro_f1_se": 0.36804533763633424}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "eltec", "task": "named-entity-recognition", "dataset_languages": ["fr"], "model": "meta-llama/Llama-3.1-70B-Instruct", "results": {"raw": [{"micro_f1_no_misc": 0.6100320170757737, "micro_f1": 0.5622188905547226}, {"micro_f1_no_misc": 0.6686541737649063, "micro_f1": 0.6350379156134551}, {"micro_f1_no_misc": 0.6969342382401124, "micro_f1": 0.6325994748535648}, {"micro_f1_no_misc": 0.7097353073126964, "micro_f1": 0.6413086322428064}, {"micro_f1_no_misc": 0.7436578883788355, "micro_f1": 0.6931239388794568}, {"micro_f1_no_misc": 0.6867770469494217, "micro_f1": 0.6236472945891784}, {"micro_f1_no_misc": 0.6518046709129511, "micro_f1": 0.5985401459854015}, {"micro_f1_no_misc": 0.6781818181818181, "micro_f1": 0.6234753049390122}, {"micro_f1_no_misc": 0.6511116002641426, "micro_f1": 0.5777347531461762}, {"micro_f1_no_misc": 0.6626452189454871, "micro_f1": 0.5603416264389157}], "total": {"test_micro_f1_no_misc": 67.59533980026144, "test_micro_f1_no_misc_se": 2.2692138368164154, "test_micro_f1": 61.480279772426904, "test_micro_f1_se": 2.5375907272425153}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "multinerd-it", "task": "named-entity-recognition", "dataset_languages": ["it"], "model": "meta-llama/Llama-3.1-70B-Instruct", "results": {"raw": [{"micro_f1_no_misc": 0.8505246992577424, "micro_f1": 0.5521435543208949}, {"micro_f1_no_misc": 0.8441952814762907, "micro_f1": 0.6279683377308708}, {"micro_f1_no_misc": 0.8152941176470588, "micro_f1": 0.5586786438713416}, {"micro_f1_no_misc": 0.8573518653986832, "micro_f1": 0.6373732496378561}, {"micro_f1_no_misc": 0.862142767216417, "micro_f1": 0.6510478786846141}, {"micro_f1_no_misc": 0.83056640625, "micro_f1": 0.6138582677165354}, {"micro_f1_no_misc": 0.8049921996879876, "micro_f1": 0.5504666188083274}, {"micro_f1_no_misc": 0.846957590657652, "micro_f1": 0.6626881162428646}, {"micro_f1_no_misc": 0.8444916755224937, "micro_f1": 0.5589130596183829}, {"micro_f1_no_misc": 0.8372588472008522, "micro_f1": 0.5814432989690722}], "total": {"test_micro_f1_no_misc": 83.93775450315178, "test_micro_f1_no_misc_se": 1.1158239210216594, "test_micro_f1": 59.945810256007604, "test_micro_f1_se": 2.7242923730873496}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-fr", "task": "linguistic-acceptability", "dataset_languages": ["fr"], "model": "meta-llama/Llama-3.1-70B-Instruct", "results": {"raw": [{"mcc": 0.396198137014601, "macro_f1": 0.6086954654942187}, {"mcc": 0.39976841669547497, "macro_f1": 0.6288184097306977}, {"mcc": 0.4262643507588093, "macro_f1": 0.651973126920687}, {"mcc": 0.39762836275933755, "macro_f1": 0.6292814628459226}, {"mcc": 0.3719547850663212, "macro_f1": 0.5979984876031728}, {"mcc": 0.4109698654816255, "macro_f1": 0.6398536144864625}, {"mcc": 0.4374666072993036, "macro_f1": 0.6690656811255917}, {"mcc": 0.3796108889901981, "macro_f1": 0.6261107234850671}, {"mcc": 0.43027837595157326, "macro_f1": 0.6464454649034975}, {"mcc": 0.3805788240783069, "macro_f1": 0.5954004551080098}], "total": {"test_mcc": 40.307186140955515, "test_mcc_se": 1.4066984185995675, "test_macro_f1": 62.93642891703327, "test_macro_f1_se": 1.4703046999539742}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-it", "task": "linguistic-acceptability", "dataset_languages": ["it"], "model": "meta-llama/Llama-3.1-70B-Instruct", "results": {"raw": [{"mcc": 0.17671386484649523, "macro_f1": 0.5466035856165367}, {"mcc": 0.26971023315311093, "macro_f1": 0.595633104404576}, {"mcc": 0.20169796521193578, "macro_f1": 0.5552440180811953}, {"mcc": 0.24093691786143048, "macro_f1": 0.5437498941477034}, {"mcc": 0.16522648294634282, "macro_f1": 0.5261146323228634}, {"mcc": 0.20276122334598115, "macro_f1": 0.5392612859097128}, {"mcc": 0.1588680409035216, "macro_f1": 0.5271014133399454}, {"mcc": 0.3019736041917631, "macro_f1": 0.6469560611532443}, {"mcc": 0.21033679967049274, "macro_f1": 0.5446394738902371}, {"mcc": 0.21391040032333963, "macro_f1": 0.5469516492764914}], "total": {"test_mcc": 21.421355324544137, "test_mcc_se": 2.8228119772144487, "test_macro_f1": 55.72255118142506, "test_macro_f1_se": 2.2901249173560863}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "squad-it", "task": "reading-comprehension", "dataset_languages": ["it"], "model": "meta-llama/Llama-3.1-70B-Instruct", "results": {"raw": [{"f1": 72.75495764568358, "em": 48.78787878787879}, {"f1": 73.2212782683279, "em": 47.99090219863533}, {"f1": 72.93485859551353, "em": 53.69075369075369}, {"f1": 72.8517278618178, "em": 49.54337899543379}, {"f1": 68.9669491384747, "em": 41.094834232845024}, {"f1": 71.20904323935275, "em": 46.56488549618321}, {"f1": 75.29108121490476, "em": 54.13416536661467}, {"f1": 69.81872128080968, "em": 41.90251572327044}, {"f1": 71.96990883133977, "em": 49.845201238390096}, {"f1": 70.23393998817876, "em": 44.21296296296296}], "total": {"test_f1": 71.92524660644034, "test_f1_se": 1.1712551747864024, "test_em": 47.7767478692968, "test_em_se": 2.750088698622679}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fquad", "task": "reading-comprehension", "dataset_languages": ["fr"], "model": "meta-llama/Llama-3.1-70B-Instruct", "results": {"raw": [{"f1": 71.8956063511102, "em": 41.89393939393939}, {"f1": 69.90132682423464, "em": 40.10614101592115}, {"f1": 71.34055734161349, "em": 40.94794094794095}, {"f1": 71.43118559133897, "em": 40.41095890410959}, {"f1": 67.789945481697, "em": 38.16499614494988}, {"f1": 72.33787091648126, "em": 42.900763358778626}, {"f1": 70.53326543798424, "em": 41.65366614664587}, {"f1": 69.9501754072553, "em": 40.56603773584906}, {"f1": 68.524207898654, "em": 38.85448916408669}, {"f1": 74.13166693197653, "em": 43.82716049382716}], "total": {"test_f1": 70.78358081823457, "test_f1_se": 1.153904432804268, "test_em": 40.93260933060484, "test_em_se": 1.0688377615404479}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "orange-sum", "task": "summarization", "dataset_languages": ["fr"], "model": "meta-llama/Llama-3.1-70B-Instruct", "results": {"raw": [{"bertscore": 0.6786719934316352, "rouge_l": 0.22523338644458835}, {"bertscore": 0.6802215885254554, "rouge_l": 0.2270797192734706}, {"bertscore": 0.6798073055688292, "rouge_l": 0.2266841071848561}, {"bertscore": 0.6738334292895161, "rouge_l": 0.21715972335146633}, {"bertscore": 0.6764012625208125, "rouge_l": 0.2307764514442902}, {"bertscore": 0.6784973667818122, "rouge_l": 0.22001265431986672}, {"bertscore": 0.6797272520780098, "rouge_l": 0.22068743123193657}, {"bertscore": 0.6759132213483099, "rouge_l": 0.2217698195556339}, {"bertscore": 0.6803429461142514, "rouge_l": 0.22817528814376148}, {"bertscore": 0.6775528457947075, "rouge_l": 0.21562423877776948}], "total": {"test_bertscore": 67.80969211453339, "test_bertscore_se": 0.13349517341545847, "test_rouge_l": 22.3320281972764, "test_rouge_l_se": 0.31040691505225626}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "ilpost-sum", "task": "summarization", "dataset_languages": ["it"], "model": "meta-llama/Llama-3.1-70B-Instruct", "results": {"raw": [{"bertscore": 0.6833176146756159, "rouge_l": 0.252916881703454}, {"bertscore": 0.6722295910294633, "rouge_l": 0.22542460872584696}, {"bertscore": 0.6790720994176809, "rouge_l": 0.24430764649316802}, {"bertscore": 0.680148717132397, "rouge_l": 0.25016068055746915}, {"bertscore": 0.6715286978287622, "rouge_l": 0.21913953948595793}, {"bertscore": 0.6721191934484523, "rouge_l": 0.22261466498392213}, {"bertscore": 0.6782850751042133, "rouge_l": 0.23868844754004026}, {"bertscore": 0.6812655951216584, "rouge_l": 0.2519841120889551}, {"bertscore": 0.6782630771631375, "rouge_l": 0.24021686736564066}, {"bertscore": 0.6800111327320337, "rouge_l": 0.24005596055894257}], "total": {"test_bertscore": 67.76240793653415, "test_bertscore_se": 0.25907673310787144, "test_rouge_l": 23.85509409503397, "test_rouge_l_se": 0.7627182882686983}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-fr", "task": "knowledge", "dataset_languages": ["fr"], "model": "meta-llama/Llama-3.1-70B-Instruct", "results": {"raw": [{"mcc": 0.6978137974746945, "accuracy": 0.7724609375}, {"mcc": 0.7014775085058984, "accuracy": 0.775390625}, {"mcc": 0.6930886836404639, "accuracy": 0.76904296875}, {"mcc": 0.6928346962244241, "accuracy": 0.7685546875}, {"mcc": 0.6792269409992017, "accuracy": 0.7587890625}, {"mcc": 0.6853559758627419, "accuracy": 0.7626953125}, {"mcc": 0.7031073822511391, "accuracy": 0.77685546875}, {"mcc": 0.6923835772272289, "accuracy": 0.76904296875}, {"mcc": 0.7077412138032424, "accuracy": 0.78076171875}, {"mcc": 0.6893128425398907, "accuracy": 0.7646484375}], "total": {"test_mcc": 69.42342618528924, "test_mcc_se": 0.5313363186177646, "test_accuracy": 76.982421875, "test_accuracy_se": 0.4179401721473716}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-it", "task": "knowledge", "dataset_languages": ["it"], "model": "meta-llama/Llama-3.1-70B-Instruct", "results": {"raw": [{"mcc": 0.7174186374832023, "accuracy": 0.787109375}, {"mcc": 0.7170438887490672, "accuracy": 0.78662109375}, {"mcc": 0.6997523443151092, "accuracy": 0.7744140625}, {"mcc": 0.6877001556413215, "accuracy": 0.765625}, {"mcc": 0.6904979593687663, "accuracy": 0.767578125}, {"mcc": 0.6946015167418456, "accuracy": 0.77001953125}, {"mcc": 0.6892006496292836, "accuracy": 0.76611328125}, {"mcc": 0.6911500621067949, "accuracy": 0.76806640625}, {"mcc": 0.6827944445163988, "accuracy": 0.76171875}, {"mcc": 0.7007663092469499, "accuracy": 0.775390625}], "total": {"test_mcc": 69.70925967798739, "test_mcc_se": 0.7367673287522584, "test_accuracy": 77.2265625, "test_accuracy_se": 0.5382113222599968}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-fr", "task": "common-sense-reasoning", "dataset_languages": ["fr"], "model": "meta-llama/Llama-3.1-70B-Instruct", "results": {"raw": [{"mcc": 0.7110308282858266, "accuracy": 0.77978515625}, {"mcc": 0.7427197693462066, "accuracy": 0.80322265625}, {"mcc": 0.6825570418669009, "accuracy": 0.759765625}, {"mcc": 0.6496925569547255, "accuracy": 0.72900390625}, {"mcc": 0.7119767924040227, "accuracy": 0.7802734375}, {"mcc": 0.7382170942701173, "accuracy": 0.80029296875}, {"mcc": 0.6920508325978055, "accuracy": 0.763671875}, {"mcc": 0.7083258813860898, "accuracy": 0.77734375}, {"mcc": 0.695384518841369, "accuracy": 0.76806640625}, {"mcc": 0.7274124526194234, "accuracy": 0.7900390625}], "total": {"test_mcc": 70.59367768572487, "test_mcc_se": 1.7209530363847936, "test_accuracy": 77.5146484375, "test_accuracy_se": 1.3456984713196622}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-it", "task": "common-sense-reasoning", "dataset_languages": ["it"], "model": "meta-llama/Llama-3.1-70B-Instruct", "results": {"raw": [{"mcc": 0.687619041702407, "accuracy": 0.75732421875}, {"mcc": 0.6705478973052977, "accuracy": 0.744140625}, {"mcc": 0.6959412175477057, "accuracy": 0.76708984375}, {"mcc": 0.6833995540209219, "accuracy": 0.75439453125}, {"mcc": 0.6668924563520735, "accuracy": 0.740234375}, {"mcc": 0.7152798023996924, "accuracy": 0.783203125}, {"mcc": 0.7275526593317296, "accuracy": 0.79150390625}, {"mcc": 0.7022494639863731, "accuracy": 0.7705078125}, {"mcc": 0.7009799406332958, "accuracy": 0.77099609375}, {"mcc": 0.6797225069605626, "accuracy": 0.7548828125}], "total": {"test_mcc": 69.3018454024006, "test_mcc_se": 1.1924489768387507, "test_accuracy": 76.3427734375, "test_accuracy_se": 1.0125503192637242}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "meta-llama/Llama-3.1-70B-Instruct", "results": {"raw": [{"test_speed": 254.89, "test_speed_short": 28.88}, {"test_speed": 501.96, "test_speed_short": 54.0}, {"test_speed": 746.94, "test_speed_short": 104.4}, {"test_speed": 994.74, "test_speed_short": 128.88}, {"test_speed": 1242.54, "test_speed_short": 154.8}, {"test_speed": 1498.76, "test_speed_short": 207.48000000000002}, {"test_speed": 1733.2299999999998, "test_speed_short": 232.96}, {"test_speed": 1935.45, "test_speed_short": 257.73}, {"test_speed": 2158.02, "test_speed_short": 281.58}, {"test_speed": 2327.3199999999997, "test_speed_short": 305.15}], "total": {"test_speed": 1339.3849999999998, "test_speed_se": 440.14242016167736, "test_speed_short": 175.586, "test_speed_short_se": 59.544786609317484}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "allocine", "task": "sentiment-classification", "dataset_languages": ["fr"], "model": "Qwen/Qwen2.5-72B-Instruct", "results": {"raw": [{"mcc": 0.9384326031517211, "macro_f1": 0.9687266174966024}, {"mcc": 0.9332436309032404, "macro_f1": 0.9663062721521025}, {"mcc": 0.9529132703100698, "macro_f1": 0.9763800414425705}, {"mcc": 0.9458436826219949, "macro_f1": 0.9726536420480774}, {"mcc": 0.9485293410014741, "macro_f1": 0.9740949991635002}, {"mcc": 0.9599409849602625, "macro_f1": 0.9799590196703196}, {"mcc": 0.9418278760913863, "macro_f1": 0.970213643576089}, {"mcc": 0.9368866586929161, "macro_f1": 0.9682489935298261}, {"mcc": 0.9455444083523088, "macro_f1": 0.972562049271945}, {"mcc": 0.9639357800998487, "macro_f1": 0.9818638520406637}], "total": {"test_mcc": 94.67098236185223, "test_mcc_se": 0.6149243650924974, "test_macro_f1": 97.31009130391698, "test_macro_f1_se": 0.3155221885848264}}, "num_model_parameters": 72706203648, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "sentipolc16", "task": "sentiment-classification", "dataset_languages": ["it"], "model": "Qwen/Qwen2.5-72B-Instruct", "results": {"raw": [{"mcc": 0.48574745023638505, "macro_f1": 0.49913972384346134}, {"mcc": 0.47177568062223346, "macro_f1": 0.4858687951967791}, {"mcc": 0.4858251338893662, "macro_f1": 0.49401514120472517}, {"mcc": 0.48274683515084704, "macro_f1": 0.488292419949822}, {"mcc": 0.48542882158816764, "macro_f1": 0.4911603030753231}, {"mcc": 0.5114542543597171, "macro_f1": 0.5051480051480052}, {"mcc": 0.5161370174956572, "macro_f1": 0.5073366424151765}, {"mcc": 0.4653854738025716, "macro_f1": 0.48011630678297346}, {"mcc": 0.48206989667616246, "macro_f1": 0.49033377154542795}, {"mcc": 0.4726373716069476, "macro_f1": 0.4790851802025065}], "total": {"test_mcc": 48.59207935428056, "test_mcc_se": 1.009689695815816, "test_macro_f1": 49.204962893642005, "test_macro_f1_se": 0.5937532820398816}}, "num_model_parameters": 72706203648, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "eltec", "task": "named-entity-recognition", "dataset_languages": ["fr"], "model": "Qwen/Qwen2.5-72B-Instruct", "results": {"raw": [{"micro_f1_no_misc": 0.5980206359233523, "micro_f1": 0.5161059413027917}, {"micro_f1_no_misc": 0.5760341813944455, "micro_f1": 0.5197391005835907}, {"micro_f1_no_misc": 0.6515641855447681, "micro_f1": 0.5833965125094769}, {"micro_f1_no_misc": 0.6769883351007424, "micro_f1": 0.5930447650758417}, {"micro_f1_no_misc": 0.6881525192918747, "micro_f1": 0.6404380450212939}, {"micro_f1_no_misc": 0.6079039933788537, "micro_f1": 0.5326633165829145}, {"micro_f1_no_misc": 0.6545529630394384, "micro_f1": 0.5626319493314567}, {"micro_f1_no_misc": 0.6611606172571179, "micro_f1": 0.572352158204982}, {"micro_f1_no_misc": 0.5742171614477429, "micro_f1": 0.48106519107729556}, {"micro_f1_no_misc": 0.5881868697262811, "micro_f1": 0.47887323943661964}], "total": {"test_micro_f1_no_misc": 62.76781462104617, "test_micro_f1_no_misc_se": 2.6824756803454806, "test_micro_f1": 54.80310219126263, "test_micro_f1_se": 3.1969135489108647}}, "num_model_parameters": 72706203648, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "multinerd-it", "task": "named-entity-recognition", "dataset_languages": ["it"], "model": "Qwen/Qwen2.5-72B-Instruct", "results": {"raw": [{"micro_f1_no_misc": 0.7310834813499112, "micro_f1": 0.43396603396603395}, {"micro_f1_no_misc": 0.7246876346402413, "micro_f1": 0.5051104972375692}, {"micro_f1_no_misc": 0.7347167694896242, "micro_f1": 0.5145790261638269}, {"micro_f1_no_misc": 0.6021560574948666, "micro_f1": 0.49833835690547956}, {"micro_f1_no_misc": 0.7783430232558141, "micro_f1": 0.5527669045974365}, {"micro_f1_no_misc": 0.7365380171019181, "micro_f1": 0.5232896652110626}, {"micro_f1_no_misc": 0.690083578043822, "micro_f1": 0.4723341391351061}, {"micro_f1_no_misc": 0.7246446063791662, "micro_f1": 0.5483364720652857}, {"micro_f1_no_misc": 0.7351977277692812, "micro_f1": 0.4918214582755752}, {"micro_f1_no_misc": 0.7375585806739566, "micro_f1": 0.477963317820969}], "total": {"test_micro_f1_no_misc": 71.95009476198602, "test_micro_f1_no_misc_se": 2.876103548975509, "test_micro_f1": 50.18505871378345, "test_micro_f1_se": 2.2202509582490584}}, "num_model_parameters": 72706203648, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-fr", "task": "linguistic-acceptability", "dataset_languages": ["fr"], "model": "Qwen/Qwen2.5-72B-Instruct", "results": {"raw": [{"mcc": 0.4318119667487807, "macro_f1": 0.6553552492046659}, {"mcc": 0.4650704329977707, "macro_f1": 0.6818654035446122}, {"mcc": 0.4822308238156501, "macro_f1": 0.699305684541973}, {"mcc": 0.4756412648905439, "macro_f1": 0.6822098755016692}, {"mcc": 0.46475406614707065, "macro_f1": 0.6758397904930464}, {"mcc": 0.4755483610299895, "macro_f1": 0.6923174739907807}, {"mcc": 0.4756108346387524, "macro_f1": 0.6867016793101213}, {"mcc": 0.422651273730738, "macro_f1": 0.6670878119897037}, {"mcc": 0.47436833886044155, "macro_f1": 0.685904565154022}, {"mcc": 0.4432561586105979, "macro_f1": 0.6621068186290999}], "total": {"test_mcc": 46.10943521470335, "test_mcc_se": 1.2963260666010823, "test_macro_f1": 67.88694352359694, "test_macro_f1_se": 0.8546811561072948}}, "num_model_parameters": 72706203648, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-it", "task": "linguistic-acceptability", "dataset_languages": ["it"], "model": "Qwen/Qwen2.5-72B-Instruct", "results": {"raw": [{"mcc": 0.35230565768949634, "macro_f1": 0.6490232973461695}, {"mcc": 0.4649073514856071, "macro_f1": 0.7275101118091447}, {"mcc": 0.39687559118219984, "macro_f1": 0.6784296353174545}, {"mcc": 0.3607271978267065, "macro_f1": 0.6334725088384444}, {"mcc": 0.3186832239252427, "macro_f1": 0.6210628488231011}, {"mcc": 0.3434769824809389, "macro_f1": 0.6344554314068468}, {"mcc": 0.31110041709945674, "macro_f1": 0.5943316990421812}, {"mcc": 0.37840353637757457, "macro_f1": 0.6529870487208045}, {"mcc": 0.29860013433136995, "macro_f1": 0.5610425946566554}, {"mcc": 0.3172839149240636, "macro_f1": 0.5919280265727496}], "total": {"test_mcc": 35.423640073226565, "test_mcc_se": 3.0919620682094853, "test_macro_f1": 63.44243202533551, "test_macro_f1_se": 2.931771300525812}}, "num_model_parameters": 72706203648, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "squad-it", "task": "reading-comprehension", "dataset_languages": ["it"], "model": "Qwen/Qwen2.5-72B-Instruct", "results": {"raw": [{"f1": 72.74960532491124, "em": 49.92424242424242}, {"f1": 70.26752954884758, "em": 43.669446550416986}, {"f1": 72.98404347149706, "em": 51.981351981351985}, {"f1": 72.13283438528453, "em": 47.71689497716895}, {"f1": 69.39248458619633, "em": 43.25366229760987}, {"f1": 70.30681952068976, "em": 46.79389312977099}, {"f1": 73.71294807316394, "em": 51.014040561622465}, {"f1": 69.16425589975896, "em": 41.588050314465406}, {"f1": 71.91961626754426, "em": 49.76780185758514}, {"f1": 69.76737610140795, "em": 43.28703703703704}], "total": {"test_f1": 71.23975131793017, "test_f1_se": 1.0205867917641005, "test_em": 46.89964211312713, "test_em_se": 2.3172337778191996}}, "num_model_parameters": 72706203648, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fquad", "task": "reading-comprehension", "dataset_languages": ["fr"], "model": "Qwen/Qwen2.5-72B-Instruct", "results": {"raw": [{"f1": 71.6413727919635, "em": 38.40909090909091}, {"f1": 70.68360882940324, "em": 37.6800606520091}, {"f1": 72.0585667903638, "em": 38.77233877233877}, {"f1": 71.34660904093484, "em": 37.51902587519026}, {"f1": 68.29675908019617, "em": 35.92906707787201}, {"f1": 71.1915041302204, "em": 36.56488549618321}, {"f1": 71.6332426329535, "em": 40.5616224648986}, {"f1": 71.18157648476829, "em": 40.25157232704402}, {"f1": 70.20253271434957, "em": 40.01547987616099}, {"f1": 71.63325967000938, "em": 37.114197530864196}], "total": {"test_f1": 70.98690321651625, "test_f1_se": 0.6715545955061207, "test_em": 38.28173409816521, "test_em_se": 0.9936344825627262}}, "num_model_parameters": 72706203648, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "orange-sum", "task": "summarization", "dataset_languages": ["fr"], "model": "Qwen/Qwen2.5-72B-Instruct", "results": {"raw": [{"bertscore": 0.6632946155732498, "rouge_l": 0.1660163778517221}, {"bertscore": 0.6615190334850922, "rouge_l": 0.1576043356587636}, {"bertscore": 0.6641684235655703, "rouge_l": 0.1641749837431774}, {"bertscore": 0.6613074163615238, "rouge_l": 0.15909774591885695}, {"bertscore": 0.6632818244979717, "rouge_l": 0.16327318933864737}, {"bertscore": 0.6633794842637144, "rouge_l": 0.16120285734201636}, {"bertscore": 0.6616456420742907, "rouge_l": 0.1598133618988858}, {"bertscore": 0.6607642391463742, "rouge_l": 0.15860870196131932}, {"bertscore": 0.6657400036638137, "rouge_l": 0.16590559181413744}, {"bertscore": 0.6627792607469019, "rouge_l": 0.15779361985236495}], "total": {"test_bertscore": 66.27879943378503, "test_bertscore_se": 0.09393774795457596, "test_rouge_l": 16.134907653798916, "test_rouge_l_se": 0.20227583322814788}}, "num_model_parameters": 72706203648, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "ilpost-sum", "task": "summarization", "dataset_languages": ["it"], "model": "Qwen/Qwen2.5-72B-Instruct", "results": {"raw": [{"bertscore": 0.6482558840507409, "rouge_l": 0.15249573353193036}, {"bertscore": 0.6454841164959362, "rouge_l": 0.1474273790993777}, {"bertscore": 0.6460280791798141, "rouge_l": 0.14810136762936826}, {"bertscore": 0.6474101626808988, "rouge_l": 0.15140323305107067}, {"bertscore": 0.64641465623572, "rouge_l": 0.14674840067217793}, {"bertscore": 0.647441564273322, "rouge_l": 0.15172008193907577}, {"bertscore": 0.6493601826659869, "rouge_l": 0.15420507870374456}, {"bertscore": 0.649682809074875, "rouge_l": 0.15505545536809584}, {"bertscore": 0.6528959429560928, "rouge_l": 0.16447141960927375}, {"bertscore": 0.6511710307677276, "rouge_l": 0.15860410030799105}], "total": {"test_bertscore": 64.84144428381114, "test_bertscore_se": 0.14674090606914375, "test_rouge_l": 15.302322499121058, "test_rouge_l_se": 0.33774645469120174}}, "num_model_parameters": 72706203648, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-fr", "task": "knowledge", "dataset_languages": ["fr"], "model": "Qwen/Qwen2.5-72B-Instruct", "results": {"raw": [{"mcc": 0.7383701752956613, "accuracy": 0.802734375}, {"mcc": 0.7359766432308731, "accuracy": 0.80126953125}, {"mcc": 0.7324718231584136, "accuracy": 0.798828125}, {"mcc": 0.7187754131912278, "accuracy": 0.78759765625}, {"mcc": 0.7257423877741621, "accuracy": 0.7939453125}, {"mcc": 0.74552498851486, "accuracy": 0.80859375}, {"mcc": 0.7509586844989641, "accuracy": 0.8125}, {"mcc": 0.7429175819099259, "accuracy": 0.806640625}, {"mcc": 0.7473480684656059, "accuracy": 0.810546875}, {"mcc": 0.7369079005880059, "accuracy": 0.8017578125}], "total": {"test_mcc": 73.74993666627701, "test_mcc_se": 0.6161573583071688, "test_accuracy": 80.244140625, "test_accuracy_se": 0.47557167319043064}}, "num_model_parameters": 72706203648, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-it", "task": "knowledge", "dataset_languages": ["it"], "model": "Qwen/Qwen2.5-72B-Instruct", "results": {"raw": [{"mcc": 0.7579018708911776, "accuracy": 0.81787109375}, {"mcc": 0.766632669170291, "accuracy": 0.82421875}, {"mcc": 0.7639683646816806, "accuracy": 0.82275390625}, {"mcc": 0.749412004559809, "accuracy": 0.8115234375}, {"mcc": 0.7449167354561523, "accuracy": 0.80810546875}, {"mcc": 0.7312842346319485, "accuracy": 0.79736328125}, {"mcc": 0.7361212359979511, "accuracy": 0.80126953125}, {"mcc": 0.7481828232570205, "accuracy": 0.81103515625}, {"mcc": 0.7289245855192162, "accuracy": 0.79638671875}, {"mcc": 0.7329786117157993, "accuracy": 0.7998046875}], "total": {"test_mcc": 74.60323135881046, "test_mcc_se": 0.8475884655478746, "test_accuracy": 80.9033203125, "test_accuracy_se": 0.63665560314958}}, "num_model_parameters": 72706203648, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-fr", "task": "common-sense-reasoning", "dataset_languages": ["fr"], "model": "Qwen/Qwen2.5-72B-Instruct", "results": {"raw": [{"mcc": 0.7579635533739547, "accuracy": 0.81689453125}, {"mcc": 0.7847105828181367, "accuracy": 0.837890625}, {"mcc": 0.7577110602735674, "accuracy": 0.81787109375}, {"mcc": 0.7410806732467947, "accuracy": 0.80322265625}, {"mcc": 0.7865862759094822, "accuracy": 0.83837890625}, {"mcc": 0.8149935895888939, "accuracy": 0.8603515625}, {"mcc": 0.7705081345866968, "accuracy": 0.82666015625}, {"mcc": 0.7999167800715671, "accuracy": 0.84912109375}, {"mcc": 0.7705441340876575, "accuracy": 0.82666015625}, {"mcc": 0.7962253878499229, "accuracy": 0.84619140625}], "total": {"test_mcc": 77.80240171806673, "test_mcc_se": 1.399360223585755, "test_accuracy": 83.232421875, "test_accuracy_se": 1.0706780426193794}}, "num_model_parameters": 72706203648, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-it", "task": "common-sense-reasoning", "dataset_languages": ["it"], "model": "Qwen/Qwen2.5-72B-Instruct", "results": {"raw": [{"mcc": 0.7867890179715157, "accuracy": 0.83935546875}, {"mcc": 0.7656515486040225, "accuracy": 0.82373046875}, {"mcc": 0.7971680048056207, "accuracy": 0.84765625}, {"mcc": 0.7642588087262807, "accuracy": 0.822265625}, {"mcc": 0.7767931320153318, "accuracy": 0.83203125}, {"mcc": 0.7655892835760174, "accuracy": 0.82373046875}, {"mcc": 0.7840492991984319, "accuracy": 0.83740234375}, {"mcc": 0.7828888719928933, "accuracy": 0.8369140625}, {"mcc": 0.7507113380763106, "accuracy": 0.81201171875}, {"mcc": 0.7886917019216051, "accuracy": 0.84130859375}], "total": {"test_mcc": 77.6259100688803, "test_mcc_se": 0.8839620746144102, "test_accuracy": 83.1640625, "test_accuracy_se": 0.6754890208914095}}, "num_model_parameters": 72706203648, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "Qwen/Qwen2.5-72B-Instruct", "results": {"raw": [{"test_speed": 237.85, "test_speed_short": 27.2}, {"test_speed": 463.89, "test_speed_short": 50.699999999999996}, {"test_speed": 700.52, "test_speed_short": 98.02}, {"test_speed": 924.49, "test_speed_short": 120.24}, {"test_speed": 1126.71, "test_speed_short": 143.19}, {"test_speed": 1385.09, "test_speed_short": 193.79999999999998}, {"test_speed": 1635.03, "test_speed_short": 218.88}, {"test_speed": 1834.47, "test_speed_short": 240.69}, {"test_speed": 1987.6499999999999, "test_speed_short": 264.42}, {"test_speed": 2187.12, "test_speed_short": 288.15000000000003}], "total": {"test_speed": 1248.282, "test_speed_se": 412.99227861116265, "test_speed_short": 164.529, "test_speed_short_se": 56.02244574154025}}, "num_model_parameters": 72706203648, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "allocine", "task": "sentiment-classification", "dataset_languages": ["fr"], "model": "Qwen/QwQ-32B-Preview", "results": {"raw": [{"mcc": 0.9530084371041373, "macro_f1": 0.9765042185520687}, {"mcc": 0.938510869779241, "macro_f1": 0.9692033236083267}, {"mcc": 0.9615596204606185, "macro_f1": 0.9807604803617045}, {"mcc": 0.9415885342489505, "macro_f1": 0.9706605670189246}, {"mcc": 0.9424854744078595, "macro_f1": 0.9710970754302897}, {"mcc": 0.9310937372696825, "macro_f1": 0.9652298921349034}, {"mcc": 0.9501492856313287, "macro_f1": 0.9750725463648776}, {"mcc": 0.9424129643215295, "macro_f1": 0.9711393245572026}, {"mcc": 0.9529149011180333, "macro_f1": 0.9764112369127285}, {"mcc": 0.9569002341205874, "macro_f1": 0.9783735421484572}], "total": {"test_mcc": 94.70624058461968, "test_mcc_se": 0.5794974357830457, "test_macro_f1": 97.34452207089483, "test_macro_f1_se": 0.29412689262775443}}, "num_model_parameters": 32763876352, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "sentipolc16", "task": "sentiment-classification", "dataset_languages": ["it"], "model": "Qwen/QwQ-32B-Preview", "results": {"raw": [{"mcc": 0.4806768377491414, "macro_f1": 0.49566120442009104}, {"mcc": 0.48636156344696196, "macro_f1": 0.4934655715687794}, {"mcc": 0.4831254225370413, "macro_f1": 0.491297530434208}, {"mcc": 0.4775948593829106, "macro_f1": 0.4862234613471557}, {"mcc": 0.4868363417724969, "macro_f1": 0.4909442394115137}, {"mcc": 0.47522846754775233, "macro_f1": 0.48793338391032953}, {"mcc": 0.49063327130133205, "macro_f1": 0.49476806772362814}, {"mcc": 0.4710528335430336, "macro_f1": 0.483354242397995}, {"mcc": 0.48289528809897425, "macro_f1": 0.4902398093332678}, {"mcc": 0.4800886757960338, "macro_f1": 0.482557974511084}], "total": {"test_mcc": 48.14493561175678, "test_mcc_se": 0.3616931035137291, "test_macro_f1": 48.96445485058052, "test_macro_f1_se": 0.28195409710321434}}, "num_model_parameters": 32763876352, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "eltec", "task": "named-entity-recognition", "dataset_languages": ["fr"], "model": "Qwen/QwQ-32B-Preview", "results": {"raw": [{"micro_f1_no_misc": 0.6961556276053729, "micro_f1": 0.6573780873970868}, {"micro_f1_no_misc": 0.6743008147985027, "micro_f1": 0.652648938363224}, {"micro_f1_no_misc": 0.6913925822253324, "micro_f1": 0.6335046248715314}, {"micro_f1_no_misc": 0.703905540417802, "micro_f1": 0.6635847867198991}, {"micro_f1_no_misc": 0.7070321811680572, "micro_f1": 0.685425368212794}, {"micro_f1_no_misc": 0.6733200266134398, "micro_f1": 0.5985970381917382}, {"micro_f1_no_misc": 0.6530078465562337, "micro_f1": 0.6196899536943828}, {"micro_f1_no_misc": 0.7038948850305021, "micro_f1": 0.6637781629116117}, {"micro_f1_no_misc": 0.6583463338533542, "micro_f1": 0.6219715956558062}, {"micro_f1_no_misc": 0.6796296296296296, "micro_f1": 0.6247697031729785}], "total": {"test_micro_f1_no_misc": 68.40985467898226, "test_micro_f1_no_misc_se": 1.1994835470990652, "test_micro_f1": 64.21348259191052, "test_micro_f1_se": 1.643416391884909}}, "num_model_parameters": 32763876352, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "multinerd-it", "task": "named-entity-recognition", "dataset_languages": ["it"], "model": "Qwen/QwQ-32B-Preview", "results": {"raw": [{"micro_f1_no_misc": 0.8334972946384653, "micro_f1": 0.6106437911218456}, {"micro_f1_no_misc": 0.8130280124499777, "micro_f1": 0.6303570008746123}, {"micro_f1_no_misc": 0.8058973763640584, "micro_f1": 0.5879293199724622}, {"micro_f1_no_misc": 0.8502835082639644, "micro_f1": 0.69847533632287}, {"micro_f1_no_misc": 0.8376152427781192, "micro_f1": 0.6391644908616189}, {"micro_f1_no_misc": 0.8147888147888147, "micro_f1": 0.678481452523673}, {"micro_f1_no_misc": 0.8303249097472923, "micro_f1": 0.6013261184360947}, {"micro_f1_no_misc": 0.8231038225009043, "micro_f1": 0.6745325830459249}, {"micro_f1_no_misc": 0.8314167433302668, "micro_f1": 0.6613328644276419}, {"micro_f1_no_misc": 0.8270799347471451, "micro_f1": 0.6633749133749134}], "total": {"test_micro_f1_no_misc": 82.67035659609009, "test_micro_f1_no_misc_se": 0.808535485743633, "test_micro_f1": 64.45617870961658, "test_micro_f1_se": 2.267787896353144}}, "num_model_parameters": 32763876352, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-fr", "task": "linguistic-acceptability", "dataset_languages": ["fr"], "model": "Qwen/QwQ-32B-Preview", "results": {"raw": [{"mcc": 0.6114522353914031, "macro_f1": 0.8056638771666309}, {"mcc": 0.5209631063017867, "macro_f1": 0.7531416277684935}, {"mcc": 0.5684034962577635, "macro_f1": 0.7817092907923218}, {"mcc": 0.4387021652527085, "macro_f1": 0.6860377358490566}, {"mcc": 0.540933584227519, "macro_f1": 0.7527527527527527}, {"mcc": 0.5918857438745824, "macro_f1": 0.7941379187085256}, {"mcc": 0.5866201382814398, "macro_f1": 0.7917053437388983}, {"mcc": 0.5861578112522261, "macro_f1": 0.7919278952007723}, {"mcc": 0.6240330278554352, "macro_f1": 0.8120116739300904}, {"mcc": 0.5253708271471978, "macro_f1": 0.7334507573577915}], "total": {"test_mcc": 55.94522135842062, "test_mcc_se": 3.396359121007067, "test_macro_f1": 77.02538873265333, "test_macro_f1_se": 2.4160201853892733}}, "num_model_parameters": 32763876352, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-it", "task": "linguistic-acceptability", "dataset_languages": ["it"], "model": "Qwen/QwQ-32B-Preview", "results": {"raw": [{"mcc": 0.5189663302563168, "macro_f1": 0.7573645188412443}, {"mcc": 0.4833578225678222, "macro_f1": 0.7280115057059442}, {"mcc": 0.4388432097779861, "macro_f1": 0.7114680888854521}, {"mcc": 0.44462640542709875, "macro_f1": 0.7065123931850751}, {"mcc": 0.4613260753207008, "macro_f1": 0.7152391546162402}, {"mcc": 0.4463500627570273, "macro_f1": 0.7044534412955465}, {"mcc": 0.3944309460292508, "macro_f1": 0.6628638670111757}, {"mcc": 0.47062966961236746, "macro_f1": 0.7343017704121104}, {"mcc": 0.4627803442891086, "macro_f1": 0.709126232168285}, {"mcc": 0.46808507961893003, "macro_f1": 0.7286862560442819}], "total": {"test_mcc": 45.893959456566094, "test_mcc_se": 1.9997547619572655, "test_macro_f1": 71.58027228165355, "test_macro_f1_se": 1.5274534448714623}}, "num_model_parameters": 32763876352, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "squad-it", "task": "reading-comprehension", "dataset_languages": ["it"], "model": "Qwen/QwQ-32B-Preview", "results": {"raw": [{"f1": 73.18657240149622, "em": 53.40909090909091}, {"f1": 73.39656438204914, "em": 51.70583775587566}, {"f1": 68.53921435970015, "em": 48.64024864024864}, {"f1": 72.7219815053455, "em": 52.28310502283105}, {"f1": 73.6920903776561, "em": 55.05011565150347}, {"f1": 72.07945377740694, "em": 52.13740458015267}, {"f1": 74.40426516314112, "em": 55.22620904836194}, {"f1": 72.2705463433228, "em": 51.41509433962264}, {"f1": 69.71305175117985, "em": 53.48297213622291}, {"f1": 72.95991797379708, "em": 51.92901234567901}], "total": {"test_f1": 72.29636580350947, "test_f1_se": 1.1288317768296718, "test_em": 52.52790904295889, "test_em_se": 1.1855819728498578}}, "num_model_parameters": 32763876352, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fquad", "task": "reading-comprehension", "dataset_languages": ["fr"], "model": "Qwen/QwQ-32B-Preview", "results": {"raw": [{"f1": 73.27909568154266, "em": 40.833333333333336}, {"f1": 72.51839860317727, "em": 40.71266110689917}, {"f1": 74.89212724998362, "em": 40.63714063714064}, {"f1": 74.25683539157318, "em": 40.41095890410959}, {"f1": 68.22868290982066, "em": 38.319198149575946}, {"f1": 72.76970166413955, "em": 39.083969465648856}, {"f1": 72.1971536112248, "em": 40.09360374414977}, {"f1": 70.98466138913741, "em": 40.801886792452834}, {"f1": 68.72502920357132, "em": 40.092879256965944}, {"f1": 74.15647768289067, "em": 41.589506172839506}], "total": {"test_f1": 72.20081633870612, "test_f1_se": 1.404592851309649, "test_em": 40.25751375631155, "test_em_se": 0.5830859881978652}}, "num_model_parameters": 32763876352, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "orange-sum", "task": "summarization", "dataset_languages": ["fr"], "model": "Qwen/QwQ-32B-Preview", "results": {"raw": [{"bertscore": 0.6606685960432515, "rouge_l": 0.16244070710143169}, {"bertscore": 0.664805612701457, "rouge_l": 0.16875796357438505}, {"bertscore": 0.6644678291049786, "rouge_l": 0.1704943171769101}, {"bertscore": 0.644360936479643, "rouge_l": 0.15442452929627581}, {"bertscore": 0.6591782003815752, "rouge_l": 0.17029350041044752}, {"bertscore": 0.664642866293434, "rouge_l": 0.1687799095073539}, {"bertscore": 0.6622150595649146, "rouge_l": 0.16318982429816442}, {"bertscore": 0.656541735981591, "rouge_l": 0.15909274021623823}, {"bertscore": 0.6667182258097455, "rouge_l": 0.17547725452538154}, {"bertscore": 0.6557516889297403, "rouge_l": 0.16272250872234462}], "total": {"test_bertscore": 65.9935075129033, "test_bertscore_se": 0.40817437635872045, "test_rouge_l": 16.556732548289325, "test_rouge_l_se": 0.38922600548790753}}, "num_model_parameters": 32763876352, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "ilpost-sum", "task": "summarization", "dataset_languages": ["it"], "model": "Qwen/QwQ-32B-Preview", "results": {"raw": [{"bertscore": 0.6514114381716354, "rouge_l": 0.15628472875628863}, {"bertscore": 0.6479049881163519, "rouge_l": 0.15620523012956414}, {"bertscore": 0.6531618768058252, "rouge_l": 0.1687135267289689}, {"bertscore": 0.6528248513641302, "rouge_l": 0.1695855180739317}, {"bertscore": 0.641592187792412, "rouge_l": 0.13905658713237165}, {"bertscore": 0.6467353194311727, "rouge_l": 0.1567235234023283}, {"bertscore": 0.652589791076025, "rouge_l": 0.16369856439406705}, {"bertscore": 0.6518426612310577, "rouge_l": 0.15971380271339866}, {"bertscore": 0.6547289823647588, "rouge_l": 0.16748105300874339}, {"bertscore": 0.6509494734345935, "rouge_l": 0.1567824986376801}], "total": {"test_bertscore": 65.03741569787962, "test_bertscore_se": 0.2424167654982685, "test_rouge_l": 15.942450329773425, "test_rouge_l_se": 0.5551952926239432}}, "num_model_parameters": 32763876352, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-fr", "task": "knowledge", "dataset_languages": ["fr"], "model": "Qwen/QwQ-32B-Preview", "results": {"raw": [{"mcc": 0.7051699795834354, "accuracy": 0.7783203125}, {"mcc": 0.7140103555852597, "accuracy": 0.78466796875}, {"mcc": 0.7063022821417541, "accuracy": 0.7783203125}, {"mcc": 0.6927774761174693, "accuracy": 0.76806640625}, {"mcc": 0.6988174666164186, "accuracy": 0.77392578125}, {"mcc": 0.7037588293777007, "accuracy": 0.77734375}, {"mcc": 0.7336194371432857, "accuracy": 0.798828125}, {"mcc": 0.7304705444044002, "accuracy": 0.79736328125}, {"mcc": 0.7203104587756454, "accuracy": 0.79052734375}, {"mcc": 0.6860709325002283, "accuracy": 0.76318359375}], "total": {"test_mcc": 70.91307762245597, "test_mcc_se": 0.9606384348540976, "test_accuracy": 78.10546875, "test_accuracy_se": 0.7311117449097104}}, "num_model_parameters": 32763876352, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-it", "task": "knowledge", "dataset_languages": ["it"], "model": "Qwen/QwQ-32B-Preview", "results": {"raw": [{"mcc": 0.690321630905072, "accuracy": 0.76708984375}, {"mcc": 0.7205367246291757, "accuracy": 0.7900390625}, {"mcc": 0.6932617880754732, "accuracy": 0.76953125}, {"mcc": 0.6727065285677194, "accuracy": 0.75439453125}, {"mcc": 0.6845325273452917, "accuracy": 0.763671875}, {"mcc": 0.6913605623147737, "accuracy": 0.76806640625}, {"mcc": 0.6883093882748903, "accuracy": 0.76513671875}, {"mcc": 0.7029534419459744, "accuracy": 0.77734375}, {"mcc": 0.6702117064428048, "accuracy": 0.751953125}, {"mcc": 0.6980297936338573, "accuracy": 0.77392578125}], "total": {"test_mcc": 69.12224092135033, "test_mcc_se": 0.8971872331822981, "test_accuracy": 76.8115234375, "test_accuracy_se": 0.6791925811504185}}, "num_model_parameters": 32763876352, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-fr", "task": "common-sense-reasoning", "dataset_languages": ["fr"], "model": "Qwen/QwQ-32B-Preview", "results": {"raw": [{"mcc": 0.7886078523366064, "accuracy": 0.837890625}, {"mcc": 0.8078200036371108, "accuracy": 0.8544921875}, {"mcc": 0.7719516785350322, "accuracy": 0.826171875}, {"mcc": 0.7813032304849482, "accuracy": 0.8330078125}, {"mcc": 0.8098581180595544, "accuracy": 0.85546875}, {"mcc": 0.8202458113834502, "accuracy": 0.86328125}, {"mcc": 0.8065505392682868, "accuracy": 0.8525390625}, {"mcc": 0.8338994552191397, "accuracy": 0.8740234375}, {"mcc": 0.7845739232064244, "accuracy": 0.83642578125}, {"mcc": 0.8048879574935851, "accuracy": 0.8515625}], "total": {"test_mcc": 80.09698569624139, "test_mcc_se": 1.1830239101941926, "test_accuracy": 84.8486328125, "test_accuracy_se": 0.9186226085364617}}, "num_model_parameters": 32763876352, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-it", "task": "common-sense-reasoning", "dataset_languages": ["it"], "model": "Qwen/QwQ-32B-Preview", "results": {"raw": [{"mcc": 0.7963971241606956, "accuracy": 0.845703125}, {"mcc": 0.8167434584932604, "accuracy": 0.86181640625}, {"mcc": 0.8045542551680385, "accuracy": 0.85205078125}, {"mcc": 0.8032844351016809, "accuracy": 0.85107421875}, {"mcc": 0.8112544047654485, "accuracy": 0.85693359375}, {"mcc": 0.8217959896568708, "accuracy": 0.86572265625}, {"mcc": 0.7949928598138478, "accuracy": 0.84423828125}, {"mcc": 0.8101770652190832, "accuracy": 0.8564453125}, {"mcc": 0.7918524704329143, "accuracy": 0.8427734375}, {"mcc": 0.8002873119476402, "accuracy": 0.849609375}], "total": {"test_mcc": 80.5133937475948, "test_mcc_se": 0.6052121932198807, "test_accuracy": 85.263671875, "test_accuracy_se": 0.4694550181344846}}, "num_model_parameters": 32763876352, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "Qwen/QwQ-32B-Preview", "results": {"raw": [{"test_speed": 300.33000000000004, "test_speed_short": 34.08}, {"test_speed": 586.5600000000001, "test_speed_short": 63.6}, {"test_speed": 877.76, "test_speed_short": 123.25}, {"test_speed": 1166.15, "test_speed_short": 154.08}, {"test_speed": 1446.1200000000001, "test_speed_short": 181.89000000000001}, {"test_speed": 1730.3100000000002, "test_speed_short": 242.25}, {"test_speed": 1988.55, "test_speed_short": 270.72}, {"test_speed": 5475.36, "test_speed_short": 302.46}, {"test_speed": 6152.25, "test_speed_short": 331.5}, {"test_speed": 6785.679999999999, "test_speed_short": 360.40000000000003}], "total": {"test_speed": 2650.907, "test_speed_se": 1535.073466267853, "test_speed_short": 206.423, "test_speed_short_se": 69.86509760892184}}, "num_model_parameters": 32763876352, "max_sequence_length": 32768, "vocabulary_size": 152064, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "allocine", "task": "sentiment-classification", "dataset_languages": ["fr"], "model": "ThatsGroes/gemma-2-27b-it-FP8-Dynamic", "results": {"raw": [{"mcc": 0.9533259263588385, "macro_f1": 0.9765284536151396}, {"mcc": 0.9423925286316577, "macro_f1": 0.971177490811084}, {"mcc": 0.9595797769112395, "macro_f1": 0.9797896504274795}, {"mcc": 0.9511422391157586, "macro_f1": 0.9755701879950377}, {"mcc": 0.9433397365659492, "macro_f1": 0.9716364080934043}, {"mcc": 0.9481731473621545, "macro_f1": 0.9740863408248758}, {"mcc": 0.9459709984216846, "macro_f1": 0.9726503814085168}, {"mcc": 0.9394272308169881, "macro_f1": 0.9696987920180601}, {"mcc": 0.9538533263338099, "macro_f1": 0.9769151164784617}, {"mcc": 0.9597483499239028, "macro_f1": 0.9798739388744717}], "total": {"test_mcc": 94.96953260441984, "test_mcc_se": 0.43664727326453523, "test_macro_f1": 97.47926760546531, "test_macro_f1_se": 0.21960045493361646}}, "num_model_parameters": 28410968576, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "sentipolc16", "task": "sentiment-classification", "dataset_languages": ["it"], "model": "ThatsGroes/gemma-2-27b-it-FP8-Dynamic", "results": {"raw": [{"mcc": 0.4688566003778971, "macro_f1": 0.4891743267596525}, {"mcc": 0.47026356709740413, "macro_f1": 0.48420101637492946}, {"mcc": 0.4600510937240484, "macro_f1": 0.4814511261124063}, {"mcc": 0.45389921282702694, "macro_f1": 0.47351670253419725}, {"mcc": 0.4690785062696305, "macro_f1": 0.4806317799961204}, {"mcc": 0.46640311251844446, "macro_f1": 0.48205714201187816}, {"mcc": 0.48282701100404224, "macro_f1": 0.4912803586836736}, {"mcc": 0.4618402995943875, "macro_f1": 0.47764145097478433}, {"mcc": 0.4491462152753162, "macro_f1": 0.4742604194159292}, {"mcc": 0.4623871323815712, "macro_f1": 0.47452054535305077}], "total": {"test_mcc": 46.44752751069768, "test_mcc_se": 0.5814677037510134, "test_macro_f1": 48.08734868216622, "test_macro_f1_se": 0.3802452558673126}}, "num_model_parameters": 28410968576, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "eltec", "task": "named-entity-recognition", "dataset_languages": ["fr"], "model": "ThatsGroes/gemma-2-27b-it-FP8-Dynamic", "results": {"raw": [{"micro_f1_no_misc": 0.40326975476839244, "micro_f1": 0.3341082696778479}, {"micro_f1_no_misc": 0.4085192697768762, "micro_f1": 0.3443072702331962}, {"micro_f1_no_misc": 0.47850055126791624, "micro_f1": 0.39504424778761066}, {"micro_f1_no_misc": 0.4112837285363859, "micro_f1": 0.3458898376025448}, {"micro_f1_no_misc": 0.44223985890652556, "micro_f1": 0.40263901979264843}, {"micro_f1_no_misc": 0.4067368421052632, "micro_f1": 0.3355172413793103}, {"micro_f1_no_misc": 0.3827009383924928, "micro_f1": 0.3289363833695108}, {"micro_f1_no_misc": 0.4389388292041219, "micro_f1": 0.38294744579198825}, {"micro_f1_no_misc": 0.38909013424248884, "micro_f1": 0.30916225900017064}, {"micro_f1_no_misc": 0.40932420872540626, "micro_f1": 0.3386988598256204}], "total": {"test_micro_f1_no_misc": 41.7060411592587, "test_micro_f1_no_misc_se": 1.7668286748736526, "test_micro_f1": 35.17250834460449, "test_micro_f1_se": 1.916655141336009}}, "num_model_parameters": 28410968576, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "multinerd-it", "task": "named-entity-recognition", "dataset_languages": ["it"], "model": "ThatsGroes/gemma-2-27b-it-FP8-Dynamic", "results": {"raw": [{"micro_f1_no_misc": 0.5674060300172666, "micro_f1": 0.32884469304606034}, {"micro_f1_no_misc": 0.6011854360711262, "micro_f1": 0.41426692004092963}, {"micro_f1_no_misc": 0.5842696629213483, "micro_f1": 0.3845936794582393}, {"micro_f1_no_misc": 0.5916216910819841, "micro_f1": 0.4443740597038562}, {"micro_f1_no_misc": 0.6083969465648854, "micro_f1": 0.4038663269893601}, {"micro_f1_no_misc": 0.6170132325141776, "micro_f1": 0.4168313208927918}, {"micro_f1_no_misc": 0.5378192534381139, "micro_f1": 0.32028422040487997}, {"micro_f1_no_misc": 0.6346777331085462, "micro_f1": 0.4507420271550363}, {"micro_f1_no_misc": 0.578481945467944, "micro_f1": 0.33548903260217344}, {"micro_f1_no_misc": 0.556726907630522, "micro_f1": 0.3401029350396439}], "total": {"test_micro_f1_no_misc": 58.77598838815914, "test_micro_f1_no_misc_se": 1.8067642076999197, "test_micro_f1": 38.39395215332971, "test_micro_f1_se": 3.0553773636403574}}, "num_model_parameters": 28410968576, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-fr", "task": "linguistic-acceptability", "dataset_languages": ["fr"], "model": "ThatsGroes/gemma-2-27b-it-FP8-Dynamic", "results": {"raw": [{"mcc": 0.3737264645214941, "macro_f1": 0.6454558749987009}, {"mcc": 0.40983247100606707, "macro_f1": 0.6660879847813239}, {"mcc": 0.4291745752466439, "macro_f1": 0.6771895898799452}, {"mcc": 0.42044266634188604, "macro_f1": 0.671370874525749}, {"mcc": 0.4189673653431798, "macro_f1": 0.6619687313740773}, {"mcc": 0.4243032395947041, "macro_f1": 0.6775266893284202}, {"mcc": 0.438263349670444, "macro_f1": 0.6969202784921282}, {"mcc": 0.35852062015830005, "macro_f1": 0.6596059956390772}, {"mcc": 0.4052355814404919, "macro_f1": 0.6781990115132989}, {"mcc": 0.4055283076314652, "macro_f1": 0.6687038179450271}], "total": {"test_mcc": 40.83994640954677, "test_mcc_se": 1.5382537929935691, "test_macro_f1": 67.03028848477747, "test_macro_f1_se": 0.850127759112561}}, "num_model_parameters": 28410968576, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-it", "task": "linguistic-acceptability", "dataset_languages": ["it"], "model": "ThatsGroes/gemma-2-27b-it-FP8-Dynamic", "results": {"raw": [{"mcc": 0.30459687212408226, "macro_f1": 0.6179780654974005}, {"mcc": 0.2634476149014612, "macro_f1": 0.5703728037558442}, {"mcc": 0.25697014185955913, "macro_f1": 0.5620812158725048}, {"mcc": 0.26188416796156183, "macro_f1": 0.5407396176897783}, {"mcc": 0.21768952512927392, "macro_f1": 0.5400059682662399}, {"mcc": 0.2584717582364517, "macro_f1": 0.560008464680904}, {"mcc": 0.3018446458011024, "macro_f1": 0.5986931167609925}, {"mcc": 0.32470571044659763, "macro_f1": 0.6298212023621204}, {"mcc": 0.2648956391110225, "macro_f1": 0.5502051983584131}, {"mcc": 0.24881720268924845, "macro_f1": 0.5535403566255406}], "total": {"test_mcc": 27.033232782603605, "test_mcc_se": 1.940879996465408, "test_macro_f1": 57.23446009869739, "test_macro_f1_se": 1.9848904591751366}}, "num_model_parameters": 28410968576, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "squad-it", "task": "reading-comprehension", "dataset_languages": ["it"], "model": "ThatsGroes/gemma-2-27b-it-FP8-Dynamic", "results": {"raw": [{"f1": 70.83992941608557, "em": 50.75757575757576}, {"f1": 72.25519578069924, "em": 48.29416224412434}, {"f1": 72.59567264329445, "em": 54.85625485625486}, {"f1": 71.39528387533659, "em": 50.83713850837138}, {"f1": 71.35655797263533, "em": 49.113338473400155}, {"f1": 70.95255285984472, "em": 50.0763358778626}, {"f1": 73.47317495798467, "em": 52.730109204368176}, {"f1": 70.57778195381472, "em": 46.776729559748425}, {"f1": 72.44554038124721, "em": 53.1733746130031}, {"f1": 70.06365530000981, "em": 47.608024691358025}], "total": {"test_f1": 71.59553451409523, "test_f1_se": 0.6581347310709088, "test_em": 50.42230437860668, "test_em_se": 1.6058080421815346}}, "num_model_parameters": 28410968576, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fquad", "task": "reading-comprehension", "dataset_languages": ["fr"], "model": "ThatsGroes/gemma-2-27b-it-FP8-Dynamic", "results": {"raw": [{"f1": 69.67828607125571, "em": 38.18181818181818}, {"f1": 68.89315152329625, "em": 38.51402577710387}, {"f1": 69.93270312355982, "em": 37.995337995338}, {"f1": 70.22983575057806, "em": 39.117199391171994}, {"f1": 65.6742124937578, "em": 34.46414803392444}, {"f1": 70.57428540345153, "em": 37.93893129770992}, {"f1": 69.4465176781517, "em": 39.46957878315133}, {"f1": 68.43917314031462, "em": 37.57861635220126}, {"f1": 66.18234365852867, "em": 37.461300309597526}, {"f1": 70.41302628486561, "em": 36.882716049382715}], "total": {"test_f1": 68.94635351277599, "test_f1_se": 1.0698343335336384, "test_em": 37.760367217139915, "test_em_se": 0.8606392295164772}}, "num_model_parameters": 28410968576, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "orange-sum", "task": "summarization", "dataset_languages": ["fr"], "model": "ThatsGroes/gemma-2-27b-it-FP8-Dynamic", "results": {"raw": [{"bertscore": 0.665451243519783, "rouge_l": 0.18187079205658513}, {"bertscore": 0.6659451677114703, "rouge_l": 0.18119122997415982}, {"bertscore": 0.6676151237334125, "rouge_l": 0.1874313737342349}, {"bertscore": 0.6650439903314691, "rouge_l": 0.18223124088918358}, {"bertscore": 0.6666269602137618, "rouge_l": 0.18516015822172907}, {"bertscore": 0.6675751530565321, "rouge_l": 0.18569791765251914}, {"bertscore": 0.665341705083847, "rouge_l": 0.17941050283588766}, {"bertscore": 0.6646085072425194, "rouge_l": 0.1809750458465187}, {"bertscore": 0.6685305880382657, "rouge_l": 0.18521113687993523}, {"bertscore": 0.6660251223365776, "rouge_l": 0.18033966679251773}], "total": {"test_bertscore": 66.62763561267639, "test_bertscore_se": 0.07924286929808738, "test_rouge_l": 18.29519064883271, "test_rouge_l_se": 0.1673963993131067}}, "num_model_parameters": 28410968576, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "ilpost-sum", "task": "summarization", "dataset_languages": ["it"], "model": "ThatsGroes/gemma-2-27b-it-FP8-Dynamic", "results": {"raw": [{"bertscore": 0.6611570219683927, "rouge_l": 0.18824347852190654}, {"bertscore": 0.6573479755898006, "rouge_l": 0.18237373306002164}, {"bertscore": 0.6598520419793203, "rouge_l": 0.1861332995176633}, {"bertscore": 0.6603934661543462, "rouge_l": 0.18788125468820832}, {"bertscore": 0.6583917787647806, "rouge_l": 0.1815059762292425}, {"bertscore": 0.6605956534622237, "rouge_l": 0.1903911761404117}, {"bertscore": 0.660556194401579, "rouge_l": 0.1872153353573809}, {"bertscore": 0.6630107221426442, "rouge_l": 0.19389917609437116}, {"bertscore": 0.6613633965607733, "rouge_l": 0.18930159045967426}, {"bertscore": 0.661284712550696, "rouge_l": 0.18828351745228983}], "total": {"test_bertscore": 66.03952963574557, "test_bertscore_se": 0.09867358791213306, "test_rouge_l": 18.752285375211702, "test_rouge_l_se": 0.2244432623354008}}, "num_model_parameters": 28410968576, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-fr", "task": "knowledge", "dataset_languages": ["fr"], "model": "ThatsGroes/gemma-2-27b-it-FP8-Dynamic", "results": {"raw": [{"mcc": 0.6023746777593956, "accuracy": 0.7001953125}, {"mcc": 0.6099132641274628, "accuracy": 0.70654296875}, {"mcc": 0.611207372870713, "accuracy": 0.70703125}, {"mcc": 0.60376181913372, "accuracy": 0.70166015625}, {"mcc": 0.5938813099439194, "accuracy": 0.69384765625}, {"mcc": 0.6045935239041059, "accuracy": 0.7021484375}, {"mcc": 0.6132163343659301, "accuracy": 0.70947265625}, {"mcc": 0.6116503110756366, "accuracy": 0.7080078125}, {"mcc": 0.6195876771903358, "accuracy": 0.712890625}, {"mcc": 0.6172341023140073, "accuracy": 0.71044921875}], "total": {"test_mcc": 60.87420392685227, "test_mcc_se": 0.4752151653486322, "test_accuracy": 70.5224609375, "test_accuracy_se": 0.35410156249999997}}, "num_model_parameters": 28410968576, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-it", "task": "knowledge", "dataset_languages": ["it"], "model": "ThatsGroes/gemma-2-27b-it-FP8-Dynamic", "results": {"raw": [{"mcc": 0.6050986980559713, "accuracy": 0.703125}, {"mcc": 0.613539473988179, "accuracy": 0.70947265625}, {"mcc": 0.5902755439238021, "accuracy": 0.69140625}, {"mcc": 0.5817451748182597, "accuracy": 0.68603515625}, {"mcc": 0.6001041067513411, "accuracy": 0.69970703125}, {"mcc": 0.580873482002496, "accuracy": 0.6845703125}, {"mcc": 0.5998100683796117, "accuracy": 0.697265625}, {"mcc": 0.574400312263471, "accuracy": 0.6796875}, {"mcc": 0.5871743621676606, "accuracy": 0.689453125}, {"mcc": 0.5819112104274425, "accuracy": 0.68603515625}], "total": {"test_mcc": 59.14932432778235, "test_mcc_se": 0.7802847761578715, "test_accuracy": 69.267578125, "test_accuracy_se": 0.5830477206062323}}, "num_model_parameters": 28410968576, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-fr", "task": "common-sense-reasoning", "dataset_languages": ["fr"], "model": "ThatsGroes/gemma-2-27b-it-FP8-Dynamic", "results": {"raw": [{"mcc": 0.6709412376099132, "accuracy": 0.7509765625}, {"mcc": 0.6781061279296514, "accuracy": 0.75439453125}, {"mcc": 0.6654936112758494, "accuracy": 0.748046875}, {"mcc": 0.6569184348508965, "accuracy": 0.7373046875}, {"mcc": 0.698778547109312, "accuracy": 0.77001953125}, {"mcc": 0.7027447627388601, "accuracy": 0.77490234375}, {"mcc": 0.6657469882091788, "accuracy": 0.74658203125}, {"mcc": 0.6978360448012578, "accuracy": 0.77099609375}, {"mcc": 0.6648166174586841, "accuracy": 0.74609375}, {"mcc": 0.6960922284012637, "accuracy": 0.77001953125}], "total": {"test_mcc": 67.97474600384867, "test_mcc_se": 1.0753100480702096, "test_accuracy": 75.693359375, "test_accuracy_se": 0.8248501522754103}}, "num_model_parameters": 28410968576, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-it", "task": "common-sense-reasoning", "dataset_languages": ["it"], "model": "ThatsGroes/gemma-2-27b-it-FP8-Dynamic", "results": {"raw": [{"mcc": 0.6833893317511477, "accuracy": 0.7607421875}, {"mcc": 0.7099833271831728, "accuracy": 0.7822265625}, {"mcc": 0.6995468588116524, "accuracy": 0.77392578125}, {"mcc": 0.7126943659962383, "accuracy": 0.7841796875}, {"mcc": 0.6969788688350911, "accuracy": 0.77294921875}, {"mcc": 0.7139062537362992, "accuracy": 0.78515625}, {"mcc": 0.6904614954399575, "accuracy": 0.7666015625}, {"mcc": 0.7174228291627829, "accuracy": 0.787109375}, {"mcc": 0.7114766290328273, "accuracy": 0.783203125}, {"mcc": 0.6909426350490231, "accuracy": 0.7666015625}], "total": {"test_mcc": 70.26802594998192, "test_mcc_se": 0.7379756510739758, "test_accuracy": 77.626953125, "test_accuracy_se": 0.5796516711280835}}, "num_model_parameters": 28410968576, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "ThatsGroes/gemma-2-27b-it-FP8-Dynamic", "results": {"raw": [{"test_speed": 673.79, "test_speed_short": 46.96}, {"test_speed": 1321.1699999999998, "test_speed_short": 87.75}, {"test_speed": 1968.63, "test_speed_short": 170.23}, {"test_speed": 2613.3, "test_speed_short": 210.24}, {"test_speed": 3260.7899999999995, "test_speed_short": 249.82999999999998}, {"test_speed": 3847.94, "test_speed_short": 327.75}, {"test_speed": 4531.93, "test_speed_short": 609.92}, {"test_speed": 5222.91, "test_speed_short": 672.37}, {"test_speed": 5817.820000000001, "test_speed_short": 733.98}, {"test_speed": 4871.95, "test_speed_short": 804.1}], "total": {"test_speed": 3413.0229999999997, "test_speed_se": 1078.624693332799, "test_speed_short": 391.313, "test_speed_short_se": 176.64762585008404}}, "num_model_parameters": 28410968576, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "allocine", "task": "sentiment-classification", "dataset_languages": ["fr"], "model": "mistralai/Mixtral-8x7B-v0.1", "results": {"raw": [{"mcc": 0.9343516099395951, "macro_f1": 0.9667683520477706}, {"mcc": 0.9438275134619517, "macro_f1": 0.9716764191119314}, {"mcc": 0.9455768162131247, "macro_f1": 0.9724840813232055}, {"mcc": 0.9487428173667201, "macro_f1": 0.9741183724885625}, {"mcc": 0.9561308145133125, "macro_f1": 0.9779978365494228}, {"mcc": 0.9409101350163378, "macro_f1": 0.9702029016202983}, {"mcc": 0.9206124637819958, "macro_f1": 0.95898374914168}, {"mcc": 0.9257234874406645, "macro_f1": 0.962396740427677}, {"mcc": 0.9346721087580994, "macro_f1": 0.9671667254900788}, {"mcc": 0.9579713044698153, "macro_f1": 0.9789176477798343}], "total": {"test_mcc": 94.08519070961617, "test_mcc_se": 0.7559819206692395, "test_macro_f1": 97.00712825980462, "test_macro_f1_se": 0.3958254135390497}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "sentipolc16", "task": "sentiment-classification", "dataset_languages": ["it"], "model": "mistralai/Mixtral-8x7B-v0.1", "results": {"raw": [{"mcc": 0.4973193975122564, "macro_f1": 0.6331892979688653}, {"mcc": 0.4714559138613655, "macro_f1": 0.5864590774176557}, {"mcc": 0.5392636150052179, "macro_f1": 0.629651286679452}, {"mcc": 0.4656654753348443, "macro_f1": 0.5635525340507125}, {"mcc": 0.5319108575334511, "macro_f1": 0.6584727855358273}, {"mcc": 0.454890475010658, "macro_f1": 0.573493733945959}, {"mcc": 0.47508542024013944, "macro_f1": 0.6174549628780407}, {"mcc": 0.5196649209216159, "macro_f1": 0.6526687268645155}, {"mcc": 0.489818551102142, "macro_f1": 0.5999005175306747}, {"mcc": 0.48763822257271683, "macro_f1": 0.6185560391182946}], "total": {"test_mcc": 49.32712849094408, "test_mcc_se": 1.77807260875817, "test_macro_f1": 61.333989619899974, "test_macro_f1_se": 1.987855124581046}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "eltec", "task": "named-entity-recognition", "dataset_languages": ["fr"], "model": "mistralai/Mixtral-8x7B-v0.1", "results": {"raw": [{"micro_f1_no_misc": 0.5899538698103537, "micro_f1": 0.5333035315154224}, {"micro_f1_no_misc": 0.598302362927277, "micro_f1": 0.5822454308093995}, {"micro_f1_no_misc": 0.5814006888633755, "micro_f1": 0.5630815840727115}, {"micro_f1_no_misc": 0.5760943524608755, "micro_f1": 0.5623250807319697}, {"micro_f1_no_misc": 0.5863273453093812, "micro_f1": 0.564437194127243}, {"micro_f1_no_misc": 0.5856712265244609, "micro_f1": 0.5667610542365497}, {"micro_f1_no_misc": 0.5863752615670774, "micro_f1": 0.5731330092797172}, {"micro_f1_no_misc": 0.5737586951307269, "micro_f1": 0.5454136690647482}, {"micro_f1_no_misc": 0.5098585015077708, "micro_f1": 0.4858071505958829}, {"micro_f1_no_misc": 0.6070577014783023, "micro_f1": 0.5636441038739889}], "total": {"test_micro_f1_no_misc": 57.948000055796015, "test_micro_f1_no_misc_se": 1.6336355381883298, "test_micro_f1": 55.401518083076326, "test_micro_f1_se": 1.7077861028276453}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "multinerd-it", "task": "named-entity-recognition", "dataset_languages": ["it"], "model": "mistralai/Mixtral-8x7B-v0.1", "results": {"raw": [{"micro_f1_no_misc": 0.7456061577934574, "micro_f1": 0.5801500682128239}, {"micro_f1_no_misc": 0.6898231222763394, "micro_f1": 0.6107896210565615}, {"micro_f1_no_misc": 0.6688508064516129, "micro_f1": 0.57735104091888}, {"micro_f1_no_misc": 0.6170241108298921, "micro_f1": 0.5182405306336185}, {"micro_f1_no_misc": 0.7233930453108535, "micro_f1": 0.6251843657817109}, {"micro_f1_no_misc": 0.6925738621240352, "micro_f1": 0.5682761055697766}, {"micro_f1_no_misc": 0.7170552147239263, "micro_f1": 0.5581092094539528}, {"micro_f1_no_misc": 0.7277360515021459, "micro_f1": 0.6103932849193624}, {"micro_f1_no_misc": 0.7118434093161546, "micro_f1": 0.5628602455935178}, {"micro_f1_no_misc": 0.7361686112156568, "micro_f1": 0.6075072886297376}], "total": {"test_micro_f1_no_misc": 70.30074391544073, "test_micro_f1_no_misc_se": 2.360993342114025, "test_micro_f1": 58.18861760769942, "test_micro_f1_se": 1.9994725157404216}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-fr", "task": "linguistic-acceptability", "dataset_languages": ["fr"], "model": "mistralai/Mixtral-8x7B-v0.1", "results": {"raw": [{"mcc": 0.45114048114476246, "macro_f1": 0.7250079001940168}, {"mcc": 0.4124936343669984, "macro_f1": 0.7060521645254905}, {"mcc": 0.3899975570559855, "macro_f1": 0.6948183251105151}, {"mcc": 0.39509953669174463, "macro_f1": 0.693998587004258}, {"mcc": 0.45770116282742535, "macro_f1": 0.7238206293678557}, {"mcc": 0.4696063477789877, "macro_f1": 0.7301118837775763}, {"mcc": 0.48867796677530045, "macro_f1": 0.7438582421569189}, {"mcc": 0.348454781539491, "macro_f1": 0.6730591705708833}, {"mcc": 0.39847393258633335, "macro_f1": 0.6991702649829694}, {"mcc": 0.4353842246673321, "macro_f1": 0.7036803983659075}], "total": {"test_mcc": 42.47029625434361, "test_mcc_se": 2.6732968559789603, "test_macro_f1": 70.9357756605639, "test_macro_f1_se": 1.3045333940935777}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-it", "task": "linguistic-acceptability", "dataset_languages": ["it"], "model": "mistralai/Mixtral-8x7B-v0.1", "results": {"raw": [{"mcc": 0.14002955413219567, "macro_f1": 0.553270858691499}, {"mcc": 0.2917612617561677, "macro_f1": 0.6454136979496897}, {"mcc": 0.15823718063460784, "macro_f1": 0.5790915272599997}, {"mcc": 0.10491688500471895, "macro_f1": 0.3522021806003133}, {"mcc": 0.17319395781319935, "macro_f1": 0.5430889362133657}, {"mcc": 0.2704143639351132, "macro_f1": 0.6239055062584474}, {"mcc": 0.08681583933638609, "macro_f1": 0.54240317775571}, {"mcc": 0.2661593908197124, "macro_f1": 0.5826461502996174}, {"mcc": 0.11995806866655116, "macro_f1": 0.558640515230316}, {"mcc": 0.23902906534424076, "macro_f1": 0.5687313816127375}], "total": {"test_mcc": 18.505155674428934, "test_mcc_se": 4.682235403637561, "test_macro_f1": 55.49393931871697, "test_macro_f1_se": 4.8847135570110005}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "squad-it", "task": "reading-comprehension", "dataset_languages": ["it"], "model": "mistralai/Mixtral-8x7B-v0.1", "results": {"raw": [{"f1": 75.41983619240457, "em": 60.984848484848484}, {"f1": 75.6403061604644, "em": 56.785443517816525}, {"f1": 69.46134427680151, "em": 54.77855477855478}, {"f1": 75.78721632116942, "em": 58.98021308980213}, {"f1": 72.26868363085607, "em": 54.43330763299923}, {"f1": 73.73651835027245, "em": 57.786259541984734}, {"f1": 75.7086967435218, "em": 60.9204368174727}, {"f1": 73.3052674826382, "em": 54.009433962264154}, {"f1": 64.85110859559457, "em": 51.1609907120743}, {"f1": 72.32776365158, "em": 50.23148148148148}], "total": {"test_f1": 72.8506741405303, "test_f1_se": 2.1496388584751474, "test_em": 56.00709700192986, "test_em_se": 2.3223114245730856}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fquad", "task": "reading-comprehension", "dataset_languages": ["fr"], "model": "mistralai/Mixtral-8x7B-v0.1", "results": {"raw": [{"f1": 76.14486389525983, "em": 50.15151515151515}, {"f1": 74.65226319126725, "em": 50.03790750568613}, {"f1": 78.52705483256712, "em": 51.67055167055167}, {"f1": 75.86101161225888, "em": 48.55403348554034}, {"f1": 68.15658208289257, "em": 44.71858134155744}, {"f1": 78.2804175121847, "em": 51.98473282442748}, {"f1": 71.71700036951225, "em": 45.94383775351014}, {"f1": 74.91037131316156, "em": 50.0}, {"f1": 68.3588527423156, "em": 44.969040247678016}, {"f1": 78.21166220804747, "em": 50.23148148148148}], "total": {"test_f1": 74.48200797594673, "test_f1_se": 2.398090881170108, "test_em": 48.82616814619478, "test_em_se": 1.6628107683535824}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "orange-sum", "task": "summarization", "dataset_languages": ["fr"], "model": "mistralai/Mixtral-8x7B-v0.1", "results": {"raw": [{"bertscore": 0.6833288996131159, "rouge_l": 0.2485630570360659}, {"bertscore": 0.6800649159995373, "rouge_l": 0.2386611027032613}, {"bertscore": 0.685911480395589, "rouge_l": 0.254448283058216}, {"bertscore": 0.657989228493534, "rouge_l": 0.21715252780824845}, {"bertscore": 0.6565442374849226, "rouge_l": 0.2085637825024657}, {"bertscore": 0.6814837857964449, "rouge_l": 0.24266282529733507}, {"bertscore": 0.6792436297400855, "rouge_l": 0.24069086793969408}, {"bertscore": 0.6647059981478378, "rouge_l": 0.22368353849760547}, {"bertscore": 0.6847547920770012, "rouge_l": 0.2518067925234738}, {"bertscore": 0.6770602953620255, "rouge_l": 0.22750445239308298}], "total": {"test_bertscore": 67.51087263110094, "test_bertscore_se": 0.6881317753232921, "test_rouge_l": 23.537372297594487, "test_rouge_l_se": 0.9578463278010109}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "ilpost-sum", "task": "summarization", "dataset_languages": ["it"], "model": "mistralai/Mixtral-8x7B-v0.1", "results": {"raw": [{"bertscore": 0.6862033743673237, "rouge_l": 0.2668016960308076}, {"bertscore": 0.6748660411394667, "rouge_l": 0.2467076613010068}, {"bertscore": 0.6895044963166583, "rouge_l": 0.2734709553228889}, {"bertscore": 0.6877960723213619, "rouge_l": 0.26825612091623413}, {"bertscore": 0.6682277271320345, "rouge_l": 0.2309090339798998}, {"bertscore": 0.6884320274402853, "rouge_l": 0.27512553438829856}, {"bertscore": 0.6833193673810456, "rouge_l": 0.2604620393884814}, {"bertscore": 0.6848636295762844, "rouge_l": 0.2652949001747843}, {"bertscore": 0.6881995736330282, "rouge_l": 0.2690476507478209}, {"bertscore": 0.6896517913555726, "rouge_l": 0.2704297203870604}], "total": {"test_bertscore": 68.41064100663061, "test_bertscore_se": 0.43927920674577914, "test_rouge_l": 26.265053126372827, "test_rouge_l_se": 0.8509101574154243}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-fr", "task": "knowledge", "dataset_languages": ["fr"], "model": "mistralai/Mixtral-8x7B-v0.1", "results": {"raw": [{"mcc": 0.5006859420321625, "accuracy": 0.62451171875}, {"mcc": 0.48717164216668196, "accuracy": 0.61376953125}, {"mcc": 0.4676514692116121, "accuracy": 0.59814453125}, {"mcc": 0.4611930659871347, "accuracy": 0.59375}, {"mcc": 0.4884915174641235, "accuracy": 0.6162109375}, {"mcc": 0.4952237949995888, "accuracy": 0.62158203125}, {"mcc": 0.5027110960460932, "accuracy": 0.62451171875}, {"mcc": 0.46858027486964765, "accuracy": 0.60107421875}, {"mcc": 0.49853595006116735, "accuracy": 0.62255859375}, {"mcc": 0.4918200155627594, "accuracy": 0.61572265625}], "total": {"test_mcc": 48.62064768400972, "test_mcc_se": 0.9317028365255045, "test_accuracy": 61.318359375, "test_accuracy_se": 0.7101547340047686}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-it", "task": "knowledge", "dataset_languages": ["it"], "model": "mistralai/Mixtral-8x7B-v0.1", "results": {"raw": [{"mcc": 0.5197492297201288, "accuracy": 0.63818359375}, {"mcc": 0.5293050767124098, "accuracy": 0.6455078125}, {"mcc": 0.5088287654395456, "accuracy": 0.630859375}, {"mcc": 0.4871003241734459, "accuracy": 0.615234375}, {"mcc": 0.5069446654095616, "accuracy": 0.62939453125}, {"mcc": 0.5179588667559633, "accuracy": 0.63671875}, {"mcc": 0.5078471115322906, "accuracy": 0.63037109375}, {"mcc": 0.47404575134590715, "accuracy": 0.60595703125}, {"mcc": 0.492981272281797, "accuracy": 0.6171875}, {"mcc": 0.47116942959008407, "accuracy": 0.6044921875}], "total": {"test_mcc": 50.15930492961134, "test_mcc_se": 1.2142519552655608, "test_accuracy": 62.5390625, "test_accuracy_se": 0.8648418876385147}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-fr", "task": "common-sense-reasoning", "dataset_languages": ["fr"], "model": "mistralai/Mixtral-8x7B-v0.1", "results": {"raw": [{"mcc": 0.3654267286505184, "accuracy": 0.513671875}, {"mcc": 0.3137197292209792, "accuracy": 0.48046875}, {"mcc": 0.26721321202380754, "accuracy": 0.439453125}, {"mcc": 0.2686255642992917, "accuracy": 0.4384765625}, {"mcc": 0.31708277105408583, "accuracy": 0.4658203125}, {"mcc": 0.23703570289644293, "accuracy": 0.38232421875}, {"mcc": 0.26629673750735533, "accuracy": 0.447265625}, {"mcc": 0.2561128035594591, "accuracy": 0.42529296875}, {"mcc": 0.46862701084455965, "accuracy": 0.59912109375}, {"mcc": 0.31119355848624175, "accuracy": 0.47265625}], "total": {"test_mcc": 30.713338185427414, "test_mcc_se": 4.22894189977854, "test_accuracy": 46.6455078125, "test_accuracy_se": 3.6209874543581906}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-it", "task": "common-sense-reasoning", "dataset_languages": ["it"], "model": "mistralai/Mixtral-8x7B-v0.1", "results": {"raw": [{"mcc": 0.193471879351847, "accuracy": 0.38037109375}, {"mcc": 0.3117517347006753, "accuracy": 0.4736328125}, {"mcc": 0.22827430397200119, "accuracy": 0.40185546875}, {"mcc": 0.28118716044128333, "accuracy": 0.45361328125}, {"mcc": 0.250404474584061, "accuracy": 0.41650390625}, {"mcc": 0.40131360491498047, "accuracy": 0.54931640625}, {"mcc": 0.2692846388983576, "accuracy": 0.43408203125}, {"mcc": 0.2774718822613239, "accuracy": 0.44482421875}, {"mcc": 0.3255541109775055, "accuracy": 0.4736328125}, {"mcc": 0.2854320722272773, "accuracy": 0.44873046875}], "total": {"test_mcc": 28.24145862329312, "test_mcc_se": 3.5142242902733454, "test_accuracy": 44.765625, "test_accuracy_se": 2.8856885988925836}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "mistralai/Mixtral-8x7B-v0.1", "results": {"raw": [{"test_speed": 425.29, "test_speed_short": 111.68}, {"test_speed": 822.03, "test_speed_short": 91.2}, {"test_speed": 1247.01, "test_speed_short": 174.87}, {"test_speed": 1677.57, "test_speed_short": 219.6}, {"test_speed": 2035.8, "test_speed_short": 255.85}, {"test_speed": 2454.43, "test_speed_short": 340.86}, {"test_speed": 2901.81, "test_speed_short": 382.72}, {"test_speed": 3236.97, "test_speed_short": 428.13}, {"test_speed": 3640.87, "test_speed_short": 460.98}, {"test_speed": 4065.7999999999997, "test_speed_short": 504.04999999999995}], "total": {"test_speed": 2250.758, "test_speed_se": 756.8694667214673, "test_speed_short": 296.99399999999997, "test_speed_short_se": 91.35525196605855}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "allocine", "task": "sentiment-classification", "dataset_languages": ["fr"], "model": "mistralai/Mixtral-8x7B-Instruct-v0.1", "results": {"raw": [{"mcc": 0.9198165849405936, "macro_f1": 0.9589702494240209}, {"mcc": 0.9276055054310063, "macro_f1": 0.9633778497504759}, {"mcc": 0.9394646711018596, "macro_f1": 0.9695214686227821}, {"mcc": 0.9449670156613538, "macro_f1": 0.9721660509082586}, {"mcc": 0.9177910224635979, "macro_f1": 0.9584723215207985}, {"mcc": 0.922442511600236, "macro_f1": 0.9609240470704961}, {"mcc": 0.9316935996636108, "macro_f1": 0.9648437164723553}, {"mcc": 0.9320891031000796, "macro_f1": 0.9658085411866841}, {"mcc": 0.9340643374116406, "macro_f1": 0.9666972757881849}, {"mcc": 0.9366993192806362, "macro_f1": 0.9681541995094427}], "total": {"test_mcc": 93.06633670654617, "test_mcc_se": 0.5440480715394123, "test_macro_f1": 96.489357202535, "test_macro_f1_se": 0.2797943918138441}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "sentipolc16", "task": "sentiment-classification", "dataset_languages": ["it"], "model": "mistralai/Mixtral-8x7B-Instruct-v0.1", "results": {"raw": [{"mcc": 0.5918776913599149, "macro_f1": 0.7062239501036499}, {"mcc": 0.5780200844610323, "macro_f1": 0.6825901679241869}, {"mcc": 0.5763602114563356, "macro_f1": 0.6712785944664095}, {"mcc": 0.5772674319483121, "macro_f1": 0.6763341687552215}, {"mcc": 0.5743736653274051, "macro_f1": 0.6715974979976836}, {"mcc": 0.5856542159723728, "macro_f1": 0.6568949948617114}, {"mcc": 0.5652363057731246, "macro_f1": 0.6613011773422349}, {"mcc": 0.5435555660617, "macro_f1": 0.6408358888797537}, {"mcc": 0.5376438894137935, "macro_f1": 0.6372863166373545}, {"mcc": 0.5908771162413272, "macro_f1": 0.686499684131789}], "total": {"test_mcc": 57.20866178015318, "test_mcc_se": 1.1431726808811735, "test_macro_f1": 66.90842441099994, "test_macro_f1_se": 1.2979794900833215}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "eltec", "task": "named-entity-recognition", "dataset_languages": ["fr"], "model": "mistralai/Mixtral-8x7B-Instruct-v0.1", "results": {"raw": [{"micro_f1_no_misc": 0.5518920115069705, "micro_f1": 0.48762048762048765}, {"micro_f1_no_misc": 0.521431543903454, "micro_f1": 0.4776556776556777}, {"micro_f1_no_misc": 0.5786330457863305, "micro_f1": 0.48757309941520466}, {"micro_f1_no_misc": 0.5866896403187595, "micro_f1": 0.5046388939421502}, {"micro_f1_no_misc": 0.5773052923503582, "micro_f1": 0.5115925503610795}, {"micro_f1_no_misc": 0.5027411167512691, "micro_f1": 0.43420150788211104}, {"micro_f1_no_misc": 0.5349915682967961, "micro_f1": 0.44196194798394134}, {"micro_f1_no_misc": 0.5582835655828355, "micro_f1": 0.46949749675505287}, {"micro_f1_no_misc": 0.5276507276507276, "micro_f1": 0.4321989980998446}, {"micro_f1_no_misc": 0.5588697017268446, "micro_f1": 0.48254086181277855}], "total": {"test_micro_f1_no_misc": 54.98488213874345, "test_micro_f1_no_misc_se": 1.7082514103472928, "test_micro_f1": 47.29481521528328, "test_micro_f1_se": 1.7500174377653726}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "multinerd-it", "task": "named-entity-recognition", "dataset_languages": ["it"], "model": "mistralai/Mixtral-8x7B-Instruct-v0.1", "results": {"raw": [{"micro_f1_no_misc": 0.6323657655412981, "micro_f1": 0.4205515889682206}, {"micro_f1_no_misc": 0.5564371640644996, "micro_f1": 0.4818863210493441}, {"micro_f1_no_misc": 0.49412415318678277, "micro_f1": 0.4272469542563788}, {"micro_f1_no_misc": 0.5346148488647444, "micro_f1": 0.47203297165447056}, {"micro_f1_no_misc": 0.6105430885672553, "micro_f1": 0.477076923076923}, {"micro_f1_no_misc": 0.5422487454224875, "micro_f1": 0.45014028967922953}, {"micro_f1_no_misc": 0.5383067221253186, "micro_f1": 0.3763635146891521}, {"micro_f1_no_misc": 0.6102758530526867, "micro_f1": 0.46088474087563547}, {"micro_f1_no_misc": 0.5347005126922596, "micro_f1": 0.3672522373218429}, {"micro_f1_no_misc": 0.6368662510292907, "micro_f1": 0.45326430831176306}], "total": {"test_micro_f1_no_misc": 56.90483104546623, "test_micro_f1_no_misc_se": 3.052630827289838, "test_micro_f1": 43.866998498829595, "test_micro_f1_se": 2.507918852960048}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-fr", "task": "linguistic-acceptability", "dataset_languages": ["fr"], "model": "mistralai/Mixtral-8x7B-Instruct-v0.1", "results": {"raw": [{"mcc": 0.3717296721516209, "macro_f1": 0.6531773604114832}, {"mcc": 0.42993500052541556, "macro_f1": 0.6784347742565275}, {"mcc": 0.44134417491433175, "macro_f1": 0.6845616825400416}, {"mcc": 0.40484549971594874, "macro_f1": 0.6620411804058575}, {"mcc": 0.4468503475665241, "macro_f1": 0.6733593131767108}, {"mcc": 0.42279099731708714, "macro_f1": 0.6579338577850165}, {"mcc": 0.5143942355009601, "macro_f1": 0.743583322899712}, {"mcc": 0.3940681838466933, "macro_f1": 0.6708854893478029}, {"mcc": 0.4527488999989021, "macro_f1": 0.7040025971227982}, {"mcc": 0.44547221569650486, "macro_f1": 0.6908556447127989}], "total": {"test_mcc": 43.24179227233989, "test_mcc_se": 2.409790508947492, "test_macro_f1": 68.18835222658748, "test_macro_f1_se": 1.650316872167443}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-it", "task": "linguistic-acceptability", "dataset_languages": ["it"], "model": "mistralai/Mixtral-8x7B-Instruct-v0.1", "results": {"raw": [{"mcc": 0.2968827036741089, "macro_f1": 0.6167418562460867}, {"mcc": 0.2534886680390129, "macro_f1": 0.555488449343198}, {"mcc": 0.30528187308541455, "macro_f1": 0.5959767977349799}, {"mcc": 0.26115737287507, "macro_f1": 0.5377024162780839}, {"mcc": 0.27583415612982204, "macro_f1": 0.5698441715253121}, {"mcc": 0.24756151184486896, "macro_f1": 0.5413512755817214}, {"mcc": 0.25808674302914103, "macro_f1": 0.5715348375898834}, {"mcc": 0.3009587416275648, "macro_f1": 0.6207536052724479}, {"mcc": 0.23372722162822096, "macro_f1": 0.5451463985997346}, {"mcc": 0.27705026144766665, "macro_f1": 0.545921672806444}], "total": {"test_mcc": 27.10029253380891, "test_mcc_se": 1.5057258412113783, "test_macro_f1": 57.00461480977892, "test_macro_f1_se": 1.9281435263776818}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "squad-it", "task": "reading-comprehension", "dataset_languages": ["it"], "model": "mistralai/Mixtral-8x7B-Instruct-v0.1", "results": {"raw": [{"f1": 67.57387786119232, "em": 42.803030303030305}, {"f1": 66.4680842696858, "em": 38.81728582259287}, {"f1": 67.58186370312008, "em": 44.21134421134421}, {"f1": 67.60610521734455, "em": 40.791476407914764}, {"f1": 65.22569155443044, "em": 37.39398612181958}, {"f1": 65.84429286159818, "em": 38.62595419847328}, {"f1": 68.62424436910985, "em": 42.979719188767554}, {"f1": 64.32277950184064, "em": 35.92767295597484}, {"f1": 67.42533662402509, "em": 44.6594427244582}, {"f1": 67.08898270740707, "em": 39.120370370370374}], "total": {"test_f1": 66.77612586697542, "test_f1_se": 0.8080056452231204, "test_em": 40.5330282304746, "test_em_se": 1.8640333321237836}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fquad", "task": "reading-comprehension", "dataset_languages": ["fr"], "model": "mistralai/Mixtral-8x7B-Instruct-v0.1", "results": {"raw": [{"f1": 71.53478051768761, "em": 37.42424242424242}, {"f1": 69.86562602219848, "em": 37.90750568612585}, {"f1": 71.20308733763437, "em": 37.60683760683761}, {"f1": 71.3993368256039, "em": 36.68188736681888}, {"f1": 65.14142814948207, "em": 35.31225905936777}, {"f1": 74.3835815447374, "em": 40.0}, {"f1": 68.86357886212467, "em": 36.349453978159126}, {"f1": 68.1646865940102, "em": 36.79245283018868}, {"f1": 65.14626787584396, "em": 34.98452012383901}, {"f1": 71.87451571700038, "em": 37.5}], "total": {"test_f1": 69.75768894463229, "test_f1_se": 1.8449685018891822, "test_em": 37.05591590755793, "test_em_se": 0.8782084312325723}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "orange-sum", "task": "summarization", "dataset_languages": ["fr"], "model": "mistralai/Mixtral-8x7B-Instruct-v0.1", "results": {"raw": [{"bertscore": 0.6664245546562597, "rouge_l": 0.17264837747188844}, {"bertscore": 0.6630417229025625, "rouge_l": 0.17073932396882788}, {"bertscore": 0.6642167239042465, "rouge_l": 0.1749362498547416}, {"bertscore": 0.6648351939511485, "rouge_l": 0.1759828979810677}, {"bertscore": 0.6641257353767287, "rouge_l": 0.17401495875746176}, {"bertscore": 0.6651173714490142, "rouge_l": 0.17230676854797122}, {"bertscore": 0.6647482554835733, "rouge_l": 0.17494760512865956}, {"bertscore": 0.6624582103104331, "rouge_l": 0.1702781372001379}, {"bertscore": 0.668338658491848, "rouge_l": 0.18047248429016988}, {"bertscore": 0.6656269554514438, "rouge_l": 0.17055523800046923}], "total": {"test_bertscore": 66.48933381977258, "test_bertscore_se": 0.10378137527750425, "test_rouge_l": 17.368820412013953, "test_rouge_l_se": 0.19344855341083428}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "ilpost-sum", "task": "summarization", "dataset_languages": ["it"], "model": "mistralai/Mixtral-8x7B-Instruct-v0.1", "results": {"raw": [{"bertscore": 0.6556652686558664, "rouge_l": 0.17286451525220056}, {"bertscore": 0.6527383733919123, "rouge_l": 0.1680039661225873}, {"bertscore": 0.6537106012256118, "rouge_l": 0.17264553293774382}, {"bertscore": 0.6546510687912814, "rouge_l": 0.17205999002096073}, {"bertscore": 0.6499761949380627, "rouge_l": 0.16392090425192024}, {"bertscore": 0.6551072656438919, "rouge_l": 0.17607972428410607}, {"bertscore": 0.6528925608145073, "rouge_l": 0.16378710595056512}, {"bertscore": 0.6545998970250366, "rouge_l": 0.16801748992186216}, {"bertscore": 0.6557892111450201, "rouge_l": 0.17502173207783842}, {"bertscore": 0.6607258957519662, "rouge_l": 0.18412511689579403}], "total": {"test_bertscore": 65.45856337383157, "test_bertscore_se": 0.17147366232515257, "test_rouge_l": 17.165260777155783, "test_rouge_l_se": 0.37944351883375577}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-fr", "task": "knowledge", "dataset_languages": ["fr"], "model": "mistralai/Mixtral-8x7B-Instruct-v0.1", "results": {"raw": [{"mcc": 0.48790926685537633, "accuracy": 0.61376953125}, {"mcc": 0.5070321558260423, "accuracy": 0.6298828125}, {"mcc": 0.47730114003034757, "accuracy": 0.607421875}, {"mcc": 0.4709491318263019, "accuracy": 0.60107421875}, {"mcc": 0.5052481516728093, "accuracy": 0.62841796875}, {"mcc": 0.5001433445994252, "accuracy": 0.625}, {"mcc": 0.49591345303828493, "accuracy": 0.62158203125}, {"mcc": 0.4861050093639055, "accuracy": 0.6142578125}, {"mcc": 0.50546477462812, "accuracy": 0.6279296875}, {"mcc": 0.4938714619731698, "accuracy": 0.61962890625}], "total": {"test_mcc": 49.299378898137824, "test_mcc_se": 0.7643484338754354, "test_accuracy": 61.8896484375, "test_accuracy_se": 0.5970269507411666}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-it", "task": "knowledge", "dataset_languages": ["it"], "model": "mistralai/Mixtral-8x7B-Instruct-v0.1", "results": {"raw": [{"mcc": 0.498042043995566, "accuracy": 0.6220703125}, {"mcc": 0.5393095907479433, "accuracy": 0.6533203125}, {"mcc": 0.5148048169663574, "accuracy": 0.63525390625}, {"mcc": 0.45156232525136225, "accuracy": 0.587890625}, {"mcc": 0.4929365599350935, "accuracy": 0.6171875}, {"mcc": 0.4879447877840731, "accuracy": 0.61474609375}, {"mcc": 0.4964757032507167, "accuracy": 0.61962890625}, {"mcc": 0.4690957617438959, "accuracy": 0.6015625}, {"mcc": 0.4829127710247168, "accuracy": 0.611328125}, {"mcc": 0.48081674227040305, "accuracy": 0.6103515625}], "total": {"test_mcc": 49.13901102970129, "test_mcc_se": 1.4892346269553722, "test_accuracy": 61.7333984375, "test_accuracy_se": 1.1031535192062436}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-fr", "task": "common-sense-reasoning", "dataset_languages": ["fr"], "model": "mistralai/Mixtral-8x7B-Instruct-v0.1", "results": {"raw": [{"mcc": 0.4975560860258938, "accuracy": 0.619140625}, {"mcc": 0.4950092209590244, "accuracy": 0.61669921875}, {"mcc": 0.4646598750319235, "accuracy": 0.59521484375}, {"mcc": 0.4722588349583453, "accuracy": 0.59716796875}, {"mcc": 0.45435034990100454, "accuracy": 0.58349609375}, {"mcc": 0.48704137516480417, "accuracy": 0.609375}, {"mcc": 0.4421395802076741, "accuracy": 0.57373046875}, {"mcc": 0.5224165773606652, "accuracy": 0.6376953125}, {"mcc": 0.5182507534512586, "accuracy": 0.6337890625}, {"mcc": 0.5086620038899519, "accuracy": 0.62890625}], "total": {"test_mcc": 48.62344656950545, "test_mcc_se": 1.6866563797310619, "test_accuracy": 60.9521484375, "test_accuracy_se": 1.3418209153115896}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-it", "task": "common-sense-reasoning", "dataset_languages": ["it"], "model": "mistralai/Mixtral-8x7B-Instruct-v0.1", "results": {"raw": [{"mcc": 0.39477351675646505, "accuracy": 0.54150390625}, {"mcc": 0.47311298372505606, "accuracy": 0.60009765625}, {"mcc": 0.429886606323744, "accuracy": 0.56298828125}, {"mcc": 0.4136774127034324, "accuracy": 0.55615234375}, {"mcc": 0.4807776829418131, "accuracy": 0.6025390625}, {"mcc": 0.47119751408000016, "accuracy": 0.60107421875}, {"mcc": 0.48861261231975045, "accuracy": 0.611328125}, {"mcc": 0.5002016155963279, "accuracy": 0.6201171875}, {"mcc": 0.4831979506632357, "accuracy": 0.609375}, {"mcc": 0.5052320896818246, "accuracy": 0.62939453125}], "total": {"test_mcc": 46.4066998479165, "test_mcc_se": 2.345609797534557, "test_accuracy": 59.34570312500001, "test_accuracy_se": 1.8221723089644557}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "mistralai/Mixtral-8x7B-Instruct-v0.1", "results": {"raw": [{"test_speed": 445.88, "test_speed_short": 50.96}, {"test_speed": 878.4300000000001, "test_speed_short": 96.3}, {"test_speed": 1322.9699999999998, "test_speed_short": 185.31}, {"test_speed": 1756.25, "test_speed_short": 232.92}, {"test_speed": 2204.28, "test_speed_short": 276.06}, {"test_speed": 2618.62, "test_speed_short": 361.38}, {"test_speed": 3044.2000000000003, "test_speed_short": 402.56}, {"test_speed": 3472.59, "test_speed_short": 457.95}, {"test_speed": 3931.13, "test_speed_short": 500.76}, {"test_speed": 4353.21, "test_speed_short": 542.3}], "total": {"test_speed": 2402.7560000000003, "test_speed_se": 813.8924568925443, "test_speed_short": 310.65, "test_speed_short_se": 105.11464565095262}}, "num_model_parameters": 46702792704, "max_sequence_length": 32768, "vocabulary_size": 32000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hotter-and-colder-sentiment", "task": "sentiment-classification", "dataset_languages": ["is"], "model": "google/gemma-2-27b-it", "results": {"raw": [{"mcc": 0.5296418885679082, "macro_f1": 0.6800405843001157}, {"mcc": 0.5115532671897405, "macro_f1": 0.6738836093473508}, {"mcc": 0.4965337694820813, "macro_f1": 0.6593330762880919}, {"mcc": 0.4905099606283613, "macro_f1": 0.6542378945577932}, {"mcc": 0.4712814771633221, "macro_f1": 0.6263121612543571}, {"mcc": 0.5182182080537155, "macro_f1": 0.6724578247372511}, {"mcc": 0.5185839973358988, "macro_f1": 0.677001756729349}, {"mcc": 0.48828545460071926, "macro_f1": 0.650238153700885}, {"mcc": 0.5033677056077046, "macro_f1": 0.6553082493897957}, {"mcc": 0.5018559074472281, "macro_f1": 0.664554487734074}], "total": {"test_mcc": 50.298316360766805, "test_mcc_se": 1.0694012009790095, "test_macro_f1": 66.13367798039064, "test_macro_f1_se": 0.9966992624869969}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fosent", "task": "sentiment-classification", "dataset_languages": ["fo"], "model": "google/gemma-2-27b-it", "results": {"raw": [{"mcc": 0.642344623933186, "macro_f1": 0.7527966706048899}, {"mcc": 0.6182760743413565, "macro_f1": 0.7185492801771872}, {"mcc": 0.5711069335175734, "macro_f1": 0.690628332374998}, {"mcc": 0.6537762759105618, "macro_f1": 0.7557544951065928}, {"mcc": 0.5807789268103354, "macro_f1": 0.7024978466838933}, {"mcc": 0.6422700976221607, "macro_f1": 0.7606209870064872}, {"mcc": 0.5781039132818693, "macro_f1": 0.724158799322198}, {"mcc": 0.6168832005581921, "macro_f1": 0.7231884342393616}, {"mcc": 0.577753777596556, "macro_f1": 0.721037896083323}, {"mcc": 0.6214996401227534, "macro_f1": 0.7120377140698677}], "total": {"test_mcc": 61.027934636945446, "test_mcc_se": 1.924280226258606, "test_macro_f1": 72.61270455668797, "test_macro_f1_se": 1.4449933014184226}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "allocine", "task": "sentiment-classification", "dataset_languages": ["fr"], "model": "google/gemma-2-27b-it", "results": {"raw": [{"mcc": 0.9495177096302705, "macro_f1": 0.9745761821683774}, {"mcc": 0.9423446630874471, "macro_f1": 0.9711720996923532}, {"mcc": 0.9595797769112395, "macro_f1": 0.9797896504274795}, {"mcc": 0.9472307358307027, "macro_f1": 0.9736116479835073}, {"mcc": 0.9423487500297776, "macro_f1": 0.9711462713924899}, {"mcc": 0.9471947660917065, "macro_f1": 0.9735964520232405}, {"mcc": 0.9459709984216846, "macro_f1": 0.9726503814085168}, {"mcc": 0.9413527549652895, "macro_f1": 0.9706726670593526}, {"mcc": 0.9509191009619803, "macro_f1": 0.9754360108081552}, {"mcc": 0.9617101438043867, "macro_f1": 0.9808529443704104}], "total": {"test_mcc": 94.88169399734487, "test_mcc_se": 0.43321089639806276, "test_macro_f1": 97.43504307333882, "test_macro_f1_se": 0.2176650031672505}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "sentipolc16", "task": "sentiment-classification", "dataset_languages": ["it"], "model": "google/gemma-2-27b-it", "results": {"raw": [{"mcc": 0.4622656584934561, "macro_f1": 0.4857823291259824}, {"mcc": 0.463722753884752, "macro_f1": 0.48099482130780347}, {"mcc": 0.4598340118747765, "macro_f1": 0.48105047317884386}, {"mcc": 0.46066816445333264, "macro_f1": 0.4768659236744343}, {"mcc": 0.46490685853132047, "macro_f1": 0.4787344079348306}, {"mcc": 0.47257220575042874, "macro_f1": 0.48546796044983137}, {"mcc": 0.48282701100404224, "macro_f1": 0.4912803586836736}, {"mcc": 0.468004163545104, "macro_f1": 0.480946069722227}, {"mcc": 0.461067062573838, "macro_f1": 0.47980025218439454}, {"mcc": 0.4607039349702313, "macro_f1": 0.4743653925020859}], "total": {"test_mcc": 46.56571825081283, "test_mcc_se": 0.44711099100307566, "test_macro_f1": 48.15287988764106, "test_macro_f1_se": 0.3018154094674805}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mim-gold-ner", "task": "named-entity-recognition", "dataset_languages": ["is"], "model": "google/gemma-2-27b-it", "results": {"raw": [{"micro_f1_no_misc": 0.5217179111761835, "micro_f1": 0.29666344294003866}, {"micro_f1_no_misc": 0.4665432425284316, "micro_f1": 0.1929841800137565}, {"micro_f1_no_misc": 0.5574168487496965, "micro_f1": 0.31503347534996956}, {"micro_f1_no_misc": 0.566420664206642, "micro_f1": 0.27492447129909364}, {"micro_f1_no_misc": 0.5400904116107542, "micro_f1": 0.307031793919703}, {"micro_f1_no_misc": 0.47119130613081817, "micro_f1": 0.27258353536418256}, {"micro_f1_no_misc": 0.5147347740667977, "micro_f1": 0.29311345952851003}, {"micro_f1_no_misc": 0.5385987261146498, "micro_f1": 0.29482758620689653}, {"micro_f1_no_misc": 0.5160997732426305, "micro_f1": 0.229264475743349}, {"micro_f1_no_misc": 0.5424309122034727, "micro_f1": 0.2690987124463519}], "total": {"test_micro_f1_no_misc": 52.35244570030076, "test_micro_f1_no_misc_se": 2.0628334340801064, "test_micro_f1": 27.455251328118514, "test_micro_f1_se": 2.3261099091558233}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fone", "task": "named-entity-recognition", "dataset_languages": ["fo"], "model": "google/gemma-2-27b-it", "results": {"raw": [{"micro_f1_no_misc": 0.6015549076773566, "micro_f1": 0.3664464823959469}, {"micro_f1_no_misc": 0.6073522568636575, "micro_f1": 0.48375611927013795}, {"micro_f1_no_misc": 0.5683855507233514, "micro_f1": 0.4018571060098014}, {"micro_f1_no_misc": 0.5593123209169054, "micro_f1": 0.4491126403477001}, {"micro_f1_no_misc": 0.6352267303102624, "micro_f1": 0.4919583727530748}, {"micro_f1_no_misc": 0.5435917262710226, "micro_f1": 0.4369538077403246}, {"micro_f1_no_misc": 0.5950107191580588, "micro_f1": 0.4706482899438489}, {"micro_f1_no_misc": 0.6468678380443087, "micro_f1": 0.4784445091756482}, {"micro_f1_no_misc": 0.574006488240065, "micro_f1": 0.4886649874055416}, {"micro_f1_no_misc": 0.6202097235462346, "micro_f1": 0.5208139173656414}], "total": {"test_micro_f1_no_misc": 59.51518261751223, "test_micro_f1_no_misc_se": 2.087068683185799, "test_micro_f1": 45.88656232407666, "test_micro_f1_se": 2.8723598520872446}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "eltec", "task": "named-entity-recognition", "dataset_languages": ["fr"], "model": "google/gemma-2-27b-it", "results": {"raw": [{"micro_f1_no_misc": 0.40312698077329395, "micro_f1": 0.334390121808777}, {"micro_f1_no_misc": 0.420319541171651, "micro_f1": 0.3510928961748634}, {"micro_f1_no_misc": 0.48963210702341137, "micro_f1": 0.40078375489846807}, {"micro_f1_no_misc": 0.403157255616272, "micro_f1": 0.33449593090848695}, {"micro_f1_no_misc": 0.43816725978647686, "micro_f1": 0.3991715307851629}, {"micro_f1_no_misc": 0.4196371398078976, "micro_f1": 0.33836650652024713}, {"micro_f1_no_misc": 0.4018961253091508, "micro_f1": 0.33912324234904884}, {"micro_f1_no_misc": 0.4641031340297844, "micro_f1": 0.3955482576172232}, {"micro_f1_no_misc": 0.38412563667232597, "micro_f1": 0.30387020449552143}, {"micro_f1_no_misc": 0.4144568006843456, "micro_f1": 0.3287581699346405}], "total": {"test_micro_f1_no_misc": 42.3862198087461, "test_micro_f1_no_misc_se": 1.9804362430414189, "test_micro_f1": 35.2560061549244, "test_micro_f1_se": 2.0992229436347674}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "multinerd-it", "task": "named-entity-recognition", "dataset_languages": ["it"], "model": "google/gemma-2-27b-it", "results": {"raw": [{"micro_f1_no_misc": 0.5695715612150153, "micro_f1": 0.328084702049681}, {"micro_f1_no_misc": 0.6135621902574641, "micro_f1": 0.42487121816730755}, {"micro_f1_no_misc": 0.586124702044913, "micro_f1": 0.3828655421012109}, {"micro_f1_no_misc": 0.5862642281621692, "micro_f1": 0.43746573733260236}, {"micro_f1_no_misc": 0.6218722215165757, "micro_f1": 0.40730837789661317}, {"micro_f1_no_misc": 0.6161041587662749, "micro_f1": 0.409443116807879}, {"micro_f1_no_misc": 0.5745667561630461, "micro_f1": 0.3320605661619486}, {"micro_f1_no_misc": 0.6442795782897306, "micro_f1": 0.4505520319473808}, {"micro_f1_no_misc": 0.57895375652385, "micro_f1": 0.3336172443163205}, {"micro_f1_no_misc": 0.5520485208565417, "micro_f1": 0.33174775754281055}], "total": {"test_micro_f1_no_misc": 59.433476737955814, "test_micro_f1_no_misc_se": 1.760078941678038, "test_micro_f1": 38.38016294323754, "test_micro_f1_se": 3.0111994347419566}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-is", "task": "linguistic-acceptability", "dataset_languages": ["is"], "model": "google/gemma-2-27b-it", "results": {"raw": [{"mcc": 0.1825232810271985, "macro_f1": 0.5912339609523919}, {"mcc": 0.2192233440621069, "macro_f1": 0.6066262978054664}, {"mcc": 0.18884841262349827, "macro_f1": 0.5924424901359724}, {"mcc": 0.19846203320749034, "macro_f1": 0.5931063138400063}, {"mcc": 0.18864652240577712, "macro_f1": 0.5872211547602173}, {"mcc": 0.24402804048072532, "macro_f1": 0.615686274509804}, {"mcc": 0.19153646079070244, "macro_f1": 0.5889417099747112}, {"mcc": 0.20808479479076789, "macro_f1": 0.603845134656753}, {"mcc": 0.23398608078739466, "macro_f1": 0.615140676701796}, {"mcc": 0.21750744643059633, "macro_f1": 0.6053881983271607}], "total": {"test_mcc": 20.728464166062576, "test_mcc_se": 1.296330137293017, "test_macro_f1": 59.99632211664279, "test_macro_f1_se": 0.6630861195506094}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-fo", "task": "linguistic-acceptability", "dataset_languages": ["fo"], "model": "google/gemma-2-27b-it", "results": {"raw": [{"mcc": 0.10053694324079931, "macro_f1": 0.485617278330861}, {"mcc": 0.11631592955680266, "macro_f1": 0.4990067991214477}, {"mcc": 0.12399247595053746, "macro_f1": 0.5124804461674488}, {"mcc": 0.12843065169658358, "macro_f1": 0.5221994903269804}, {"mcc": 0.19120976972394607, "macro_f1": 0.5837989123456397}, {"mcc": 0.2134889090840803, "macro_f1": 0.5738274464389115}, {"mcc": 0.24672950911861122, "macro_f1": 0.5569178631405358}, {"mcc": 0.08267972847076845, "macro_f1": 0.45}, {"mcc": 0.15482766911609325, "macro_f1": 0.5092167723167585}, {"mcc": 0.06732781934155299, "macro_f1": 0.47756871407798757}], "total": {"test_mcc": 14.255394052997753, "test_mcc_se": 3.6166647455140346, "test_macro_f1": 51.70633722266571, "test_macro_f1_se": 2.673510369955275}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-fr", "task": "linguistic-acceptability", "dataset_languages": ["fr"], "model": "google/gemma-2-27b-it", "results": {"raw": [{"mcc": 0.36975695102002365, "macro_f1": 0.6441678013356675}, {"mcc": 0.41029840224348924, "macro_f1": 0.6659425204235969}, {"mcc": 0.42567435283436655, "macro_f1": 0.6757195724667542}, {"mcc": 0.42739377279799734, "macro_f1": 0.6742840620403301}, {"mcc": 0.4155079873714341, "macro_f1": 0.6595213446818059}, {"mcc": 0.4251122125515364, "macro_f1": 0.6772024427945165}, {"mcc": 0.43496958041449735, "macro_f1": 0.693847779581948}, {"mcc": 0.3539452983876945, "macro_f1": 0.6556473685943938}, {"mcc": 0.40369197958531006, "macro_f1": 0.6778681291361182}, {"mcc": 0.41801233337469745, "macro_f1": 0.6741101669973891}], "total": {"test_mcc": 40.84362870581047, "test_mcc_se": 1.6361642223829367, "test_macro_f1": 66.98311188052519, "test_macro_f1_se": 0.8642145715001915}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "scala-it", "task": "linguistic-acceptability", "dataset_languages": ["it"], "model": "google/gemma-2-27b-it", "results": {"raw": [{"mcc": 0.3142943322893227, "macro_f1": 0.6271651987776761}, {"mcc": 0.2725243310477935, "macro_f1": 0.5737009544008483}, {"mcc": 0.2555011827897916, "macro_f1": 0.5616883116883117}, {"mcc": 0.26710040565605303, "macro_f1": 0.5429033886311693}, {"mcc": 0.2291000967720246, "macro_f1": 0.5477001895073998}, {"mcc": 0.2619509519787617, "macro_f1": 0.5661480011099016}, {"mcc": 0.29816867718323203, "macro_f1": 0.5972313542861454}, {"mcc": 0.34303411059552774, "macro_f1": 0.640905733936888}, {"mcc": 0.2585148103334326, "macro_f1": 0.5486627820110563}, {"mcc": 0.2572886333388937, "macro_f1": 0.5574669027742643}], "total": {"test_mcc": 27.57477531984833, "test_mcc_se": 2.0675858407602985, "test_macro_f1": 57.6357281712366, "test_macro_f1_se": 2.124525263748282}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "nqii", "task": "reading-comprehension", "dataset_languages": ["is"], "model": "google/gemma-2-27b-it", "results": {"raw": [{"f1": 50.39785890338492, "em": 20.46153846153846}, {"f1": 51.96593137138411, "em": 19.626168224299064}, {"f1": 48.09058742074159, "em": 21.65109034267913}, {"f1": 48.39403772618975, "em": 21.524663677130047}, {"f1": 50.415537198547, "em": 21.786833855799372}, {"f1": 51.38550985569753, "em": 25.731895223420647}, {"f1": 49.04201210469516, "em": 19.53846153846154}, {"f1": 49.13542358162343, "em": 24.539877300613497}, {"f1": 47.83807266701112, "em": 15.963855421686747}, {"f1": 50.50910755691582, "em": 23.343373493975903}], "total": {"test_f1": 49.71740783861904, "test_f1_se": 0.8799463360686949, "test_em": 21.416775753960444, "test_em_se": 1.7244837272279947}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "foqa", "task": "reading-comprehension", "dataset_languages": ["fo"], "model": "google/gemma-2-27b-it", "results": {"raw": [{"f1": 69.23686511921807, "em": 45.34313725490196}, {"f1": 67.22901706906721, "em": 45.7002457002457}, {"f1": 71.73668498761516, "em": 50.123456790123456}, {"f1": 69.69671653596951, "em": 47.57281553398058}, {"f1": 71.89290452756306, "em": 50.24752475247525}, {"f1": 68.92219247482402, "em": 47.160493827160494}, {"f1": 72.14420622231391, "em": 51.65094339622642}, {"f1": 73.32038911352599, "em": 46.81933842239186}, {"f1": 73.41616213721943, "em": 52.61845386533666}, {"f1": 72.6926886960661, "em": 50.122850122850124}], "total": {"test_f1": 71.02878268833824, "test_f1_se": 1.307857405908687, "test_em": 48.735925966569255, "test_em_se": 1.5726151443627037}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "squad-it", "task": "reading-comprehension", "dataset_languages": ["it"], "model": "google/gemma-2-27b-it", "results": {"raw": [{"f1": 71.441077167896, "em": 51.515151515151516}, {"f1": 72.08472134179856, "em": 47.83927217589083}, {"f1": 72.97159280027462, "em": 54.77855477855478}, {"f1": 71.32495606167784, "em": 50.91324200913242}, {"f1": 71.51441723340413, "em": 49.113338473400155}, {"f1": 71.25820080247183, "em": 50.458015267175576}, {"f1": 73.1140873590205, "em": 52.18408736349454}, {"f1": 70.78826087384041, "em": 47.09119496855346}, {"f1": 72.5521748028971, "em": 53.40557275541796}, {"f1": 70.32444315005044, "em": 48.07098765432099}], "total": {"test_f1": 71.73739315933314, "test_f1_se": 0.5709689511010848, "test_em": 50.53694169610922, "test_em_se": 1.5638609395777643}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "fquad", "task": "reading-comprehension", "dataset_languages": ["fr"], "model": "google/gemma-2-27b-it", "results": {"raw": [{"f1": 69.93123667178699, "em": 38.56060606060606}, {"f1": 69.46626438214713, "em": 38.89310083396513}, {"f1": 70.28106711048684, "em": 38.61693861693862}, {"f1": 70.97303838925164, "em": 39.80213089802131}, {"f1": 66.55914784629938, "em": 35.31225905936777}, {"f1": 70.73563947211645, "em": 38.396946564885496}, {"f1": 69.49818231859375, "em": 39.39157566302652}, {"f1": 68.51772733929118, "em": 37.893081761006286}, {"f1": 66.79071053222624, "em": 37.693498452012385}, {"f1": 70.76419764883146, "em": 36.882716049382715}], "total": {"test_f1": 69.35172117110312, "test_f1_se": 0.9870711974016373, "test_em": 38.14428539592123, "test_em_se": 0.8055818153957817}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "rrn", "task": "summarization", "dataset_languages": ["is"], "model": "google/gemma-2-27b-it", "results": {"raw": [{"bertscore": 0.6810641674674116, "rouge_l": 0.19456942810595523}, {"bertscore": 0.6795429697376676, "rouge_l": 0.19273154486022087}, {"bertscore": 0.6824548945878632, "rouge_l": 0.20069627007128385}, {"bertscore": 0.678387980820844, "rouge_l": 0.18988627713355294}, {"bertscore": 0.6802410261007026, "rouge_l": 0.19246771000029747}, {"bertscore": 0.6785540980054066, "rouge_l": 0.19250689268170035}, {"bertscore": 0.6778045689570718, "rouge_l": 0.18925130869522816}, {"bertscore": 0.6764641523477621, "rouge_l": 0.1835097208272929}, {"bertscore": 0.6774426776973996, "rouge_l": 0.18766236488276747}, {"bertscore": 0.6790697348187678, "rouge_l": 0.18887896760649953}], "total": {"test_bertscore": 67.91026270540897, "test_bertscore_se": 0.11085473296704042, "test_rouge_l": 19.121604848647987, "test_rouge_l_se": 0.2845984906588884}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "orange-sum", "task": "summarization", "dataset_languages": ["fr"], "model": "google/gemma-2-27b-it", "results": {"raw": [{"bertscore": 0.6663798555964604, "rouge_l": 0.18295179863007205}, {"bertscore": 0.6658922226633877, "rouge_l": 0.1814628836806638}, {"bertscore": 0.6674255357938819, "rouge_l": 0.18570984313801953}, {"bertscore": 0.6645783431595191, "rouge_l": 0.18188384698764984}, {"bertscore": 0.6674308131623548, "rouge_l": 0.18889804878020908}, {"bertscore": 0.6675225311773829, "rouge_l": 0.18740526118849377}, {"bertscore": 0.6658571760635823, "rouge_l": 0.1817613347694145}, {"bertscore": 0.6656030387384817, "rouge_l": 0.18388918170101437}, {"bertscore": 0.668602712568827, "rouge_l": 0.186589378396107}, {"bertscore": 0.6649729025666602, "rouge_l": 0.17895876487572876}], "total": {"test_bertscore": 66.64265131490538, "test_bertscore_se": 0.0793803449542764, "test_rouge_l": 18.395103421473728, "test_rouge_l_se": 0.19341634730706805}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "ilpost-sum", "task": "summarization", "dataset_languages": ["it"], "model": "google/gemma-2-27b-it", "results": {"raw": [{"bertscore": 0.6614408652239945, "rouge_l": 0.18873125208419408}, {"bertscore": 0.6580503574514296, "rouge_l": 0.18379961580602017}, {"bertscore": 0.6597225074656308, "rouge_l": 0.18636288356193498}, {"bertscore": 0.6602725537959486, "rouge_l": 0.18792174385445576}, {"bertscore": 0.6585559037921485, "rouge_l": 0.1803164704809693}, {"bertscore": 0.6604433770698961, "rouge_l": 0.18937329725131752}, {"bertscore": 0.6600934312737081, "rouge_l": 0.18681414853999512}, {"bertscore": 0.6623647137603257, "rouge_l": 0.1935083388287003}, {"bertscore": 0.6611730706645176, "rouge_l": 0.18937346540784994}, {"bertscore": 0.6611928019265179, "rouge_l": 0.18821078137079322}], "total": {"test_bertscore": 66.03309582424117, "test_bertscore_se": 0.08163954002562186, "test_rouge_l": 18.744119971862304, "test_rouge_l_se": 0.21898179980113786}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "icelandic-knowledge", "task": "knowledge", "dataset_languages": ["is"], "model": "google/gemma-2-27b-it", "results": {"raw": [{"mcc": 0.2699789903854144, "accuracy": 0.4541015625}, {"mcc": 0.2448903922870643, "accuracy": 0.4296875}, {"mcc": 0.21872704417785663, "accuracy": 0.4140625}, {"mcc": 0.2976819848275505, "accuracy": 0.462890625}, {"mcc": 0.2265857241664451, "accuracy": 0.4150390625}, {"mcc": 0.2148962552619551, "accuracy": 0.40625}, {"mcc": 0.2176939721077193, "accuracy": 0.40625}, {"mcc": 0.25156937784681327, "accuracy": 0.4248046875}, {"mcc": 0.23611140331168465, "accuracy": 0.421875}, {"mcc": 0.2620136883695016, "accuracy": 0.4443359375}], "total": {"test_mcc": 24.401488327420047, "test_mcc_se": 1.6634342401481084, "test_accuracy": 42.79296875, "test_accuracy_se": 1.226527639056968}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-fr", "task": "knowledge", "dataset_languages": ["fr"], "model": "google/gemma-2-27b-it", "results": {"raw": [{"mcc": 0.614969568717549, "accuracy": 0.70947265625}, {"mcc": 0.6015204347761882, "accuracy": 0.7001953125}, {"mcc": 0.6055978295203517, "accuracy": 0.70263671875}, {"mcc": 0.5982598204015253, "accuracy": 0.697265625}, {"mcc": 0.5969652442157903, "accuracy": 0.6962890625}, {"mcc": 0.6033650809680383, "accuracy": 0.701171875}, {"mcc": 0.6157393042290962, "accuracy": 0.71142578125}, {"mcc": 0.6096029012375477, "accuracy": 0.70654296875}, {"mcc": 0.6120480382364, "accuracy": 0.70751953125}, {"mcc": 0.613230473738382, "accuracy": 0.70751953125}], "total": {"test_mcc": 60.71298696040868, "test_mcc_se": 0.42995528990601717, "test_accuracy": 70.400390625, "test_accuracy_se": 0.323697340523086}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "mmlu-it", "task": "knowledge", "dataset_languages": ["it"], "model": "google/gemma-2-27b-it", "results": {"raw": [{"mcc": 0.6011264512535901, "accuracy": 0.7001953125}, {"mcc": 0.6128926899176095, "accuracy": 0.708984375}, {"mcc": 0.5961102818011058, "accuracy": 0.69580078125}, {"mcc": 0.5821636563539893, "accuracy": 0.6865234375}, {"mcc": 0.6052489812579012, "accuracy": 0.70361328125}, {"mcc": 0.5871384386928261, "accuracy": 0.68896484375}, {"mcc": 0.582914426916136, "accuracy": 0.6845703125}, {"mcc": 0.5783036722931618, "accuracy": 0.6826171875}, {"mcc": 0.5879194704379607, "accuracy": 0.68994140625}, {"mcc": 0.5849885973681382, "accuracy": 0.6884765625}], "total": {"test_mcc": 59.188066662924186, "test_mcc_se": 0.7062107459436463, "test_accuracy": 69.296875, "test_accuracy_se": 0.544527513736584}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "winogrande-is", "task": "common-sense-reasoning", "dataset_languages": ["is"], "model": "google/gemma-2-27b-it", "results": {"raw": [{"mcc": 0.38475728593693836, "accuracy": 0.6941964285714286}, {"mcc": 0.36772162758397037, "accuracy": 0.6852678571428571}, {"mcc": 0.3830599930714779, "accuracy": 0.6941964285714286}, {"mcc": 0.3094785172800032, "accuracy": 0.6607142857142857}, {"mcc": 0.34017671772617913, "accuracy": 0.6763392857142857}, {"mcc": 0.30542946933514176, "accuracy": 0.6506696428571429}, {"mcc": 0.2760699661504071, "accuracy": 0.6428571428571429}, {"mcc": 0.2911426435834336, "accuracy": 0.6462053571428571}, {"mcc": 0.35464628420741195, "accuracy": 0.6863839285714286}, {"mcc": 0.2813506929007269, "accuracy": 0.6428571428571429}], "total": {"test_mcc": 32.9383319777569, "test_mcc_se": 2.593147932408415, "test_accuracy": 66.79687500000001, "test_accuracy_se": 1.334295307274176}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-fr", "task": "common-sense-reasoning", "dataset_languages": ["fr"], "model": "google/gemma-2-27b-it", "results": {"raw": [{"mcc": 0.6759307371040449, "accuracy": 0.75439453125}, {"mcc": 0.679204295823618, "accuracy": 0.75537109375}, {"mcc": 0.6472182880461436, "accuracy": 0.734375}, {"mcc": 0.6495937871867964, "accuracy": 0.73193359375}, {"mcc": 0.704991030892868, "accuracy": 0.77392578125}, {"mcc": 0.6891991669943004, "accuracy": 0.7646484375}, {"mcc": 0.6693748123078546, "accuracy": 0.7490234375}, {"mcc": 0.6922641022386983, "accuracy": 0.76708984375}, {"mcc": 0.6739865698574696, "accuracy": 0.7529296875}, {"mcc": 0.6852859215226008, "accuracy": 0.76123046875}], "total": {"test_mcc": 67.67048711974395, "test_mcc_se": 1.121066153490109, "test_accuracy": 75.44921875, "test_accuracy_se": 0.8345400300570236}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "hellaswag-it", "task": "common-sense-reasoning", "dataset_languages": ["it"], "model": "google/gemma-2-27b-it", "results": {"raw": [{"mcc": 0.6866186183656724, "accuracy": 0.763671875}, {"mcc": 0.7068051724412849, "accuracy": 0.77978515625}, {"mcc": 0.6958890999109296, "accuracy": 0.77099609375}, {"mcc": 0.7134376993662509, "accuracy": 0.78466796875}, {"mcc": 0.6956973805433561, "accuracy": 0.77197265625}, {"mcc": 0.7132294740609367, "accuracy": 0.78466796875}, {"mcc": 0.6855835739491356, "accuracy": 0.7626953125}, {"mcc": 0.7110240207837872, "accuracy": 0.7822265625}, {"mcc": 0.713739967248178, "accuracy": 0.78515625}, {"mcc": 0.682031901724461, "accuracy": 0.759765625}], "total": {"test_mcc": 70.04056908393991, "test_mcc_se": 0.7882705084205565, "test_accuracy": 77.4560546875, "test_accuracy_se": 0.61990709613698}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "google/gemma-2-27b-it", "results": {"raw": [{"test_speed": 493.45, "test_speed_short": 34.64}, {"test_speed": 978.5400000000001, "test_speed_short": 63.45}, {"test_speed": 1443.24, "test_speed_short": 124.69999999999999}, {"test_speed": 1927.66, "test_speed_short": 153.0}, {"test_speed": 2383.29, "test_speed_short": 184.04000000000002}, {"test_speed": 2854.38, "test_speed_short": 241.68}, {"test_speed": 3348.6200000000003, "test_speed_short": 444.8}, {"test_speed": 3826.02, "test_speed_short": 494.16}, {"test_speed": 4265.5599999999995, "test_speed_short": 543.66}, {"test_speed": 4703.71, "test_speed_short": 586.5}], "total": {"test_speed": 2622.447, "test_speed_se": 882.0410564519767, "test_speed_short": 287.063, "test_speed_short_se": 129.6834242512887}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "euroeval_version": "15.3.1"}