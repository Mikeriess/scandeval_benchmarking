
{"dataset": "swerec", "task": "sentiment-classification", "dataset_languages": ["sv"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.8037987606015371, "macro_f1": 0.7810110154206006}, {"mcc": 0.7963158186364466, "macro_f1": 0.7772650990271884}, {"mcc": 0.802812625118714, "macro_f1": 0.7793189349722752}, {"mcc": 0.7752094308473652, "macro_f1": 0.7700186732692704}, {"mcc": 0.7812870322814675, "macro_f1": 0.780248209802919}, {"mcc": 0.8003479350192356, "macro_f1": 0.7860364241522965}, {"mcc": 0.7711643133669939, "macro_f1": 0.7347995167186593}, {"mcc": 0.7772808730244798, "macro_f1": 0.7697838427117017}, {"mcc": 0.7990034862802823, "macro_f1": 0.7758641616424903}, {"mcc": 0.7658464261919189, "macro_f1": 0.7631970645996452}], "total": {"test_mcc": 78.73066701368441, "test_mcc_se": 0.9016691513665487, "test_macro_f1": 77.17542942317046, "test_macro_f1_se": 0.9034145619787847}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "angry-tweets", "task": "sentiment-classification", "dataset_languages": ["da"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.5522289912545023, "macro_f1": 0.7041680271382061}, {"mcc": 0.5487242578845286, "macro_f1": 0.6999930193288426}, {"mcc": 0.5721452846287203, "macro_f1": 0.7195779322546345}, {"mcc": 0.5381217475713098, "macro_f1": 0.6925918822125059}, {"mcc": 0.5350081479765657, "macro_f1": 0.6940597140699566}, {"mcc": 0.575874937521164, "macro_f1": 0.7210553747428232}, {"mcc": 0.525213731488075, "macro_f1": 0.68487150510358}, {"mcc": 0.5328668863849474, "macro_f1": 0.6923612083329903}, {"mcc": 0.5268832857426982, "macro_f1": 0.6624809009380822}, {"mcc": 0.5003047972340449, "macro_f1": 0.6754849518491827}], "total": {"test_mcc": 54.07372067686557, "test_mcc_se": 1.39915195028418, "test_macro_f1": 69.46644515970803, "test_macro_f1_se": 1.1212570641882553}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "norec", "task": "sentiment-classification", "dataset_languages": ["nb", "nn", "no"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.376547836575547, "macro_f1": 0.39042800265428007}, {"mcc": 0.3640535826244135, "macro_f1": 0.3807009363565264}, {"mcc": 0.3640166233607444, "macro_f1": 0.39414652385173016}, {"mcc": 0.3652005455170701, "macro_f1": 0.3933836545406915}, {"mcc": 0.37249829994716255, "macro_f1": 0.3864785837461578}, {"mcc": 0.3720414028717185, "macro_f1": 0.3890869376729917}, {"mcc": 0.3509777790620508, "macro_f1": 0.3741706451321521}, {"mcc": 0.37482417001194984, "macro_f1": 0.38323468500447383}, {"mcc": 0.3495295516856532, "macro_f1": 0.38458917803409204}, {"mcc": 0.37634477294986934, "macro_f1": 0.39548033309663183}], "total": {"test_mcc": 36.66034564606179, "test_mcc_se": 0.6116701416563566, "test_macro_f1": 38.71699480089727, "test_macro_f1_se": 0.41531124519255624}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "hotter-and-colder-sentiment", "task": "sentiment-classification", "dataset_languages": ["is"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.40465882500911365, "macro_f1": 0.5645536443374773}, {"mcc": 0.4214542222689414, "macro_f1": 0.5853539824957825}, {"mcc": 0.3471642630939401, "macro_f1": 0.5549879279338434}, {"mcc": 0.38801862203634824, "macro_f1": 0.5835548172757475}, {"mcc": 0.3757791684405052, "macro_f1": 0.5714989941896375}, {"mcc": 0.41867777461052025, "macro_f1": 0.5725073758012905}, {"mcc": 0.3840100626405812, "macro_f1": 0.565089105054985}, {"mcc": 0.3486834111077327, "macro_f1": 0.5464215818599002}, {"mcc": 0.35399744088685525, "macro_f1": 0.5137219048995881}, {"mcc": 0.38213514620389955, "macro_f1": 0.5727014982752175}], "total": {"test_mcc": 38.245789362984375, "test_mcc_se": 1.678427020619952, "test_macro_f1": 56.303908321234694, "test_macro_f1_se": 1.298979476474688}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "sb10k", "task": "sentiment-classification", "dataset_languages": ["de"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.5528180957894937, "macro_f1": 0.6829464412076193}, {"mcc": 0.576769551645263, "macro_f1": 0.7129607272643493}, {"mcc": 0.6220019347079564, "macro_f1": 0.7438539462240561}, {"mcc": 0.5397196666339042, "macro_f1": 0.6861412250432467}, {"mcc": 0.5731073157343236, "macro_f1": 0.7159376689857848}, {"mcc": 0.6190304464382017, "macro_f1": 0.7439008836296042}, {"mcc": 0.5885678957881159, "macro_f1": 0.7252020446244319}, {"mcc": 0.5311380651848374, "macro_f1": 0.683132387585608}, {"mcc": 0.6195016076880335, "macro_f1": 0.7314864357185945}, {"mcc": 0.6166562255935193, "macro_f1": 0.7457052065362633}], "total": {"test_mcc": 58.39310805203649, "test_mcc_se": 2.158697152768194, "test_macro_f1": 71.71266966819559, "test_macro_f1_se": 1.5764324102214615}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "dutch-social", "task": "sentiment-classification", "dataset_languages": ["nl"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.13384006485775102, "macro_f1": 0.37461079162365873}, {"mcc": 0.1316954248045042, "macro_f1": 0.33025894440988784}, {"mcc": 0.11875765094576227, "macro_f1": 0.34857956459384054}, {"mcc": 0.12346707093930606, "macro_f1": 0.3294805662723425}, {"mcc": 0.1346101148911577, "macro_f1": 0.3503861003861004}, {"mcc": 0.11909130884706558, "macro_f1": 0.31618538910493604}, {"mcc": 0.1056678351463652, "macro_f1": 0.3289647045460999}, {"mcc": 0.1417394575880408, "macro_f1": 0.33370547150985624}, {"mcc": 0.08728446445476405, "macro_f1": 0.29289085486354516}, {"mcc": 0.14038006115535448, "macro_f1": 0.34822750778376266}], "total": {"test_mcc": 12.365334536300717, "test_mcc_se": 1.0507042975984644, "test_macro_f1": 33.5328989509403, "test_macro_f1_se": 1.3676762298944944}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "sst5", "task": "sentiment-classification", "dataset_languages": ["en"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.6859156146486706, "macro_f1": 0.6783654712208734}, {"mcc": 0.6722247546869361, "macro_f1": 0.6862892305348586}, {"mcc": 0.6831583113885842, "macro_f1": 0.6927964196488449}, {"mcc": 0.7079055013563201, "macro_f1": 0.7104521737363024}, {"mcc": 0.7065684879446914, "macro_f1": 0.6860481162572708}, {"mcc": 0.6938118488675107, "macro_f1": 0.7035134940342402}, {"mcc": 0.6901554488819365, "macro_f1": 0.6963646360428601}, {"mcc": 0.6725018140293328, "macro_f1": 0.6733533563343407}, {"mcc": 0.7028325100240628, "macro_f1": 0.6858014859965275}, {"mcc": 0.6931485611665225, "macro_f1": 0.690934991080951}], "total": {"test_mcc": 69.08222852994568, "test_mcc_se": 0.7902736075396057, "test_macro_f1": 69.0391937488707, "test_macro_f1_se": 0.6877276846294905}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "fosent", "task": "sentiment-classification", "dataset_languages": ["fo"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.4471391394500706, "macro_f1": 0.6146578446432608}, {"mcc": 0.508897773937807, "macro_f1": 0.6541247642991829}, {"mcc": 0.43177422479031824, "macro_f1": 0.5952712215320911}, {"mcc": 0.4206926951029492, "macro_f1": 0.6080253239507184}, {"mcc": 0.48502634657442567, "macro_f1": 0.6550129265365275}, {"mcc": 0.4688458368544216, "macro_f1": 0.6549834750831159}, {"mcc": 0.5191933345811242, "macro_f1": 0.6859750144783652}, {"mcc": 0.46161769373431394, "macro_f1": 0.6260112601126012}, {"mcc": 0.5274112819841121, "macro_f1": 0.6903689908091963}, {"mcc": 0.5327824847189776, "macro_f1": 0.6766775529824264}], "total": {"test_mcc": 48.033808117285204, "test_mcc_se": 2.5146445104298665, "test_macro_f1": 64.61108374427486, "test_macro_f1_se": 2.0797059747899045}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "allocine", "task": "sentiment-classification", "dataset_languages": ["fr"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.9093024823785567, "macro_f1": 0.9531205292252756}, {"mcc": 0.9287063216647645, "macro_f1": 0.9638666361486229}, {"mcc": 0.9524208221869737, "macro_f1": 0.9759216936777411}, {"mcc": 0.9386552988639156, "macro_f1": 0.9687498807902557}, {"mcc": 0.9307165670009329, "macro_f1": 0.9648275151418005}, {"mcc": 0.9354089002849433, "macro_f1": 0.9672776588988785}, {"mcc": 0.9162289455085961, "macro_f1": 0.9565406374081176}, {"mcc": 0.9361276755829488, "macro_f1": 0.9677645530279398}, {"mcc": 0.9251126545034509, "macro_f1": 0.9618336587449718}, {"mcc": 0.9544670147962082, "macro_f1": 0.9769781915146784}], "total": {"test_mcc": 93.27146682771289, "test_mcc_se": 0.8798086222117095, "test_macro_f1": 96.56880954578281, "test_macro_f1_se": 0.4660883954178928}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "suc3", "task": "named-entity-recognition", "dataset_languages": ["sv"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"micro_f1_no_misc": 0.5542168674698795, "micro_f1": 0.32704941445301344}, {"micro_f1_no_misc": 0.567617054669108, "micro_f1": 0.48019700389903547}, {"micro_f1_no_misc": 0.5746968657057882, "micro_f1": 0.42340565417488485}, {"micro_f1_no_misc": 0.5813132501765121, "micro_f1": 0.400192246074976}, {"micro_f1_no_misc": 0.5934897804693414, "micro_f1": 0.4116938950988822}, {"micro_f1_no_misc": 0.5832426550598477, "micro_f1": 0.36667781866845095}, {"micro_f1_no_misc": 0.5284077624503156, "micro_f1": 0.3315736909687545}, {"micro_f1_no_misc": 0.4972839506172839, "micro_f1": 0.36301950805767597}, {"micro_f1_no_misc": 0.4461118690313779, "micro_f1": 0.29174443646805454}, {"micro_f1_no_misc": 0.5409134157944814, "micro_f1": 0.35789473684210527}], "total": {"test_micro_f1_no_misc": 54.67293471443935, "test_micro_f1_no_misc_se": 2.8424563333729527, "test_micro_f1": 37.53448404705833, "test_micro_f1_se": 3.3955356748125958}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "dansk", "task": "named-entity-recognition", "dataset_languages": ["da"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"micro_f1_no_misc": 0.6033092914722105, "micro_f1": 0.43291592128801426}, {"micro_f1_no_misc": 0.5578800557880056, "micro_f1": 0.3172595520421607}, {"micro_f1_no_misc": 0.5355404089581305, "micro_f1": 0.4226336011177087}, {"micro_f1_no_misc": 0.5792307692307692, "micro_f1": 0.4148148148148148}, {"micro_f1_no_misc": 0.5174765558397272, "micro_f1": 0.29261559696342304}, {"micro_f1_no_misc": 0.5183917878528658, "micro_f1": 0.30222643297015633}, {"micro_f1_no_misc": 0.6147540983606558, "micro_f1": 0.474969474969475}, {"micro_f1_no_misc": 0.592270950933565, "micro_f1": 0.386240520043337}, {"micro_f1_no_misc": 0.5925925925925927, "micro_f1": 0.3330894903682028}, {"micro_f1_no_misc": 0.5273118279569892, "micro_f1": 0.3337423312883436}], "total": {"test_micro_f1_no_misc": 56.38758338985511, "test_micro_f1_no_misc_se": 2.301344567529102, "test_micro_f1": 37.10507735865636, "test_micro_f1_se": 3.922641951607649}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "norne-nb", "task": "named-entity-recognition", "dataset_languages": ["nb", "no"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"micro_f1_no_misc": 0.6435866366626276, "micro_f1": 0.4278629395852119}, {"micro_f1_no_misc": 0.6301086394205897, "micro_f1": 0.5130705675199783}, {"micro_f1_no_misc": 0.6383853485329845, "micro_f1": 0.4052760592973036}, {"micro_f1_no_misc": 0.6708715596330275, "micro_f1": 0.5723905723905723}, {"micro_f1_no_misc": 0.6709973251815056, "micro_f1": 0.4430223972434162}, {"micro_f1_no_misc": 0.6039355992844365, "micro_f1": 0.40531246294319934}, {"micro_f1_no_misc": 0.6207792207792207, "micro_f1": 0.5164675021859516}, {"micro_f1_no_misc": 0.5840230298668586, "micro_f1": 0.44368868541530415}, {"micro_f1_no_misc": 0.6392742796157951, "micro_f1": 0.4895291449913298}, {"micro_f1_no_misc": 0.634954954954955, "micro_f1": 0.3885135135135135}], "total": {"test_micro_f1_no_misc": 63.369165939320006, "test_micro_f1_no_misc_se": 1.6603828184856344, "test_micro_f1": 46.051338450857806, "test_micro_f1_se": 3.706702788146917}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "norne-nn", "task": "named-entity-recognition", "dataset_languages": ["nn"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"micro_f1_no_misc": 0.7032470915089426, "micro_f1": 0.5712464950293142}, {"micro_f1_no_misc": 0.6702279202279203, "micro_f1": 0.45145852324521424}, {"micro_f1_no_misc": 0.6499829758256725, "micro_f1": 0.4524825881824309}, {"micro_f1_no_misc": 0.7042491958693076, "micro_f1": 0.5230247212796898}, {"micro_f1_no_misc": 0.6570654015587937, "micro_f1": 0.5883154024240773}, {"micro_f1_no_misc": 0.6671161301196697, "micro_f1": 0.46866914163572954}, {"micro_f1_no_misc": 0.6613172851889001, "micro_f1": 0.5342606903657908}, {"micro_f1_no_misc": 0.6759828582075648, "micro_f1": 0.5583284968078933}, {"micro_f1_no_misc": 0.6608177454063897, "micro_f1": 0.5138234939031752}, {"micro_f1_no_misc": 0.6552026671345851, "micro_f1": 0.48051792351037037}], "total": {"test_micro_f1_no_misc": 67.05209271047747, "test_micro_f1_no_misc_se": 1.1807950546299217, "test_micro_f1": 51.421274763836855, "test_micro_f1_se": 3.0733233333363366}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "mim-gold-ner", "task": "named-entity-recognition", "dataset_languages": ["is"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"micro_f1_no_misc": 0.5895582329317268, "micro_f1": 0.4275716360755315}, {"micro_f1_no_misc": 0.5337962017561773, "micro_f1": 0.2573707321444574}, {"micro_f1_no_misc": 0.5290785498489425, "micro_f1": 0.4369552585705985}, {"micro_f1_no_misc": 0.567002612915267, "micro_f1": 0.33097363291269116}, {"micro_f1_no_misc": 0.5237303785780241, "micro_f1": 0.4063205417607223}, {"micro_f1_no_misc": 0.46479658183169237, "micro_f1": 0.3556085918854416}, {"micro_f1_no_misc": 0.5692497938994229, "micro_f1": 0.4393095011553622}, {"micro_f1_no_misc": 0.6034682080924856, "micro_f1": 0.41363636363636364}, {"micro_f1_no_misc": 0.546751783702591, "micro_f1": 0.37916966642668587}, {"micro_f1_no_misc": 0.5846679478966474, "micro_f1": 0.31002162941600575}], "total": {"test_micro_f1_no_misc": 55.12100291452977, "test_micro_f1_no_misc_se": 2.524531732406949, "test_micro_f1": 37.5693755398386, "test_micro_f1_se": 3.786935630744228}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "fone", "task": "named-entity-recognition", "dataset_languages": ["fo"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"micro_f1_no_misc": 0.6921595598349382, "micro_f1": 0.5778048594713991}, {"micro_f1_no_misc": 0.7339932794478249, "micro_f1": 0.7095115681233933}, {"micro_f1_no_misc": 0.6807005130019458, "micro_f1": 0.6602020860495437}, {"micro_f1_no_misc": 0.7060801344914541, "micro_f1": 0.615152717175724}, {"micro_f1_no_misc": 0.7496003761165961, "micro_f1": 0.704330469495791}, {"micro_f1_no_misc": 0.7489353823366043, "micro_f1": 0.6360761934140511}, {"micro_f1_no_misc": 0.7510024823372159, "micro_f1": 0.7012831479897348}, {"micro_f1_no_misc": 0.718084620305218, "micro_f1": 0.687764427145033}, {"micro_f1_no_misc": 0.7260209374705272, "micro_f1": 0.6805863303221409}, {"micro_f1_no_misc": 0.7302219692657941, "micro_f1": 0.6897393426902625}], "total": {"test_micro_f1_no_misc": 72.36799254608118, "test_micro_f1_no_misc_se": 1.5171825547086855, "test_micro_f1": 66.62451141877074, "test_micro_f1_se": 2.7055942891731055}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "germeval", "task": "named-entity-recognition", "dataset_languages": ["de"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"micro_f1_no_misc": 0.6559139784946236, "micro_f1": 0.4920217429423111}, {"micro_f1_no_misc": 0.6343424787133396, "micro_f1": 0.5012224938875306}, {"micro_f1_no_misc": 0.6375420067210753, "micro_f1": 0.44764114462490334}, {"micro_f1_no_misc": 0.6167695473251029, "micro_f1": 0.3896222121074192}, {"micro_f1_no_misc": 0.6337554923752907, "micro_f1": 0.5067415730337079}, {"micro_f1_no_misc": 0.6436443791329904, "micro_f1": 0.4646109602172871}, {"micro_f1_no_misc": 0.6140617096611026, "micro_f1": 0.37945833949977803}, {"micro_f1_no_misc": 0.6774954152475765, "micro_f1": 0.5034184958618207}, {"micro_f1_no_misc": 0.6588579795021963, "micro_f1": 0.46086816201387765}, {"micro_f1_no_misc": 0.6507282220390218, "micro_f1": 0.4701949860724234}], "total": {"test_micro_f1_no_misc": 64.2311120921232, "test_micro_f1_no_misc_se": 1.2010036198760754, "test_micro_f1": 46.15800110261059, "test_micro_f1_se": 2.809207823593912}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "conll-nl", "task": "named-entity-recognition", "dataset_languages": ["nl"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"micro_f1_no_misc": 0.6136263736263736, "micro_f1": 0.4072484501669051}, {"micro_f1_no_misc": 0.568359375, "micro_f1": 0.44431452791581405}, {"micro_f1_no_misc": 0.5835671342685371, "micro_f1": 0.4228736184526669}, {"micro_f1_no_misc": 0.5623540401681457, "micro_f1": 0.4363636363636364}, {"micro_f1_no_misc": 0.5684560965459842, "micro_f1": 0.4095679765682207}, {"micro_f1_no_misc": 0.5488879563575326, "micro_f1": 0.36688617121354655}, {"micro_f1_no_misc": 0.5841357175607519, "micro_f1": 0.42037533512064346}, {"micro_f1_no_misc": 0.5718733783082512, "micro_f1": 0.44411237298266587}, {"micro_f1_no_misc": 0.6009852216748768, "micro_f1": 0.4014598540145986}, {"micro_f1_no_misc": 0.5811283075386919, "micro_f1": 0.4028021015761822}], "total": {"test_micro_f1_no_misc": 57.83373601049144, "test_micro_f1_no_misc_se": 1.1666560542152364, "test_micro_f1": 41.560040443748804, "test_micro_f1_se": 1.4590292615491198}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "conll-en", "task": "named-entity-recognition", "dataset_languages": ["en"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"micro_f1_no_misc": 0.7603289408500941, "micro_f1": 0.6395255649943901}, {"micro_f1_no_misc": 0.7794420520501778, "micro_f1": 0.6746148486490671}, {"micro_f1_no_misc": 0.780269058295964, "micro_f1": 0.6644444444444445}, {"micro_f1_no_misc": 0.7340560601621252, "micro_f1": 0.6211326075037205}, {"micro_f1_no_misc": 0.7499006754072308, "micro_f1": 0.6770441041091305}, {"micro_f1_no_misc": 0.7871871871871872, "micro_f1": 0.6286833855799372}, {"micro_f1_no_misc": 0.7477901072033102, "micro_f1": 0.633742377474968}, {"micro_f1_no_misc": 0.7634920634920634, "micro_f1": 0.6650020136931131}, {"micro_f1_no_misc": 0.7732155406083726, "micro_f1": 0.7126436781609194}, {"micro_f1_no_misc": 0.7551020408163265, "micro_f1": 0.6575025098463201}], "total": {"test_micro_f1_no_misc": 76.30783726072852, "test_micro_f1_no_misc_se": 1.047029921097732, "test_micro_f1": 65.7433553445601, "test_micro_f1_se": 1.7112677212812135}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "eltec", "task": "named-entity-recognition", "dataset_languages": ["fr"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"micro_f1_no_misc": 0.5996563573883162, "micro_f1": 0.5067000178667144}, {"micro_f1_no_misc": 0.5815459328207203, "micro_f1": 0.548975791433892}, {"micro_f1_no_misc": 0.5955249569707401, "micro_f1": 0.5266654364273851}, {"micro_f1_no_misc": 0.6740185266872518, "micro_f1": 0.5701475807958154}, {"micro_f1_no_misc": 0.6385082004044035, "micro_f1": 0.5829046299960428}, {"micro_f1_no_misc": 0.5948850204169353, "micro_f1": 0.49149812063719356}, {"micro_f1_no_misc": 0.6002897950734838, "micro_f1": 0.502154058245735}, {"micro_f1_no_misc": 0.5860934415441983, "micro_f1": 0.5013948298307607}, {"micro_f1_no_misc": 0.5738451528952505, "micro_f1": 0.49561812418422524}, {"micro_f1_no_misc": 0.5702861952861953, "micro_f1": 0.4182438721687869}], "total": {"test_micro_f1_no_misc": 60.14653579487496, "test_micro_f1_no_misc_se": 1.969501841934439, "test_micro_f1": 51.4430246158655, "test_micro_f1_se": 2.895987043743449}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "scala-sv", "task": "linguistic-acceptability", "dataset_languages": ["sv"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.2534677170192706, "macro_f1": 0.6163792726944508}, {"mcc": 0.3054318800897274, "macro_f1": 0.6461260648094338}, {"mcc": 0.26112377824513966, "macro_f1": 0.6290866972907231}, {"mcc": 0.29354493157068573, "macro_f1": 0.628416448132014}, {"mcc": 0.3220263951967541, "macro_f1": 0.658838719945313}, {"mcc": 0.3157961791427399, "macro_f1": 0.6572147939039045}, {"mcc": 0.3122424712383724, "macro_f1": 0.6453811912442826}, {"mcc": 0.30012418276424063, "macro_f1": 0.6498722085636357}, {"mcc": 0.3086257172691474, "macro_f1": 0.6311992151682707}, {"mcc": 0.2605363103678409, "macro_f1": 0.6178645233581086}], "total": {"test_mcc": 29.329195629039184, "test_mcc_se": 1.575355801414961, "test_macro_f1": 63.80379135110137, "test_macro_f1_se": 0.9591649400377112}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "scala-da", "task": "linguistic-acceptability", "dataset_languages": ["da"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.24923305770091966, "macro_f1": 0.6183048162603084}, {"mcc": 0.33031422608033034, "macro_f1": 0.663860126006381}, {"mcc": 0.30699485956503203, "macro_f1": 0.6176661176627256}, {"mcc": 0.2950745478765937, "macro_f1": 0.6385989380436198}, {"mcc": 0.2807058754363913, "macro_f1": 0.5783470913858406}, {"mcc": 0.3057286116622228, "macro_f1": 0.6528081086977515}, {"mcc": 0.28352660845655436, "macro_f1": 0.6399668523636852}, {"mcc": 0.282329839898715, "macro_f1": 0.6251668908892705}, {"mcc": 0.3442169524142952, "macro_f1": 0.6655480524303223}, {"mcc": 0.25138994296710887, "macro_f1": 0.6107590377927001}], "total": {"test_mcc": 29.295145220581638, "test_mcc_se": 1.8912128551865697, "test_macro_f1": 63.11026031532606, "test_macro_f1_se": 1.658231953447558}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "scala-nb", "task": "linguistic-acceptability", "dataset_languages": ["nb", "no"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.313429402581259, "macro_f1": 0.6277021842446739}, {"mcc": 0.296336545107273, "macro_f1": 0.602889816067359}, {"mcc": 0.3729640282729722, "macro_f1": 0.6640152620312765}, {"mcc": 0.25334229188778856, "macro_f1": 0.5613454686712045}, {"mcc": 0.2550219187952949, "macro_f1": 0.5992256066716963}, {"mcc": 0.3252478200493812, "macro_f1": 0.6615304326945269}, {"mcc": 0.2698966955623756, "macro_f1": 0.6045226636445988}, {"mcc": 0.3450985071623489, "macro_f1": 0.6498113207547169}, {"mcc": 0.29811348086959993, "macro_f1": 0.6302525384398282}, {"mcc": 0.30444794965563887, "macro_f1": 0.6514824640228745}], "total": {"test_mcc": 30.338986399439317, "test_mcc_se": 2.373988667685809, "test_macro_f1": 62.52777757242755, "test_macro_f1_se": 2.0486042907638597}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "scala-nn", "task": "linguistic-acceptability", "dataset_languages": ["nn"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.17955907781843322, "macro_f1": 0.5790193741678897}, {"mcc": 0.21786946720126874, "macro_f1": 0.6088643410519601}, {"mcc": 0.18161623483461464, "macro_f1": 0.5419169066227889}, {"mcc": 0.21911915333655388, "macro_f1": 0.5946765675359285}, {"mcc": 0.20404196202277997, "macro_f1": 0.5891514797702374}, {"mcc": 0.2063295994814384, "macro_f1": 0.5969071542888666}, {"mcc": 0.21725095589132448, "macro_f1": 0.6044740177928127}, {"mcc": 0.16721715208376936, "macro_f1": 0.5500959301746365}, {"mcc": 0.20908190611419628, "macro_f1": 0.5543458372400504}, {"mcc": 0.19898339227005318, "macro_f1": 0.5734073945495574}], "total": {"test_mcc": 20.01068901054432, "test_mcc_se": 1.1226628271278078, "test_macro_f1": 57.928590031947266, "test_macro_f1_se": 1.4695160129313596}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "scala-is", "task": "linguistic-acceptability", "dataset_languages": ["is"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.04805185258339183, "macro_f1": 0.48275542854842934}, {"mcc": 0.03974831304736054, "macro_f1": 0.44715447154471544}, {"mcc": 0.03906554730227905, "macro_f1": 0.5195307917888563}, {"mcc": 0.08681973967124691, "macro_f1": 0.5268261156922365}, {"mcc": 0.05280888172490973, "macro_f1": 0.5262589910865774}, {"mcc": 0.04949202894750107, "macro_f1": 0.521853813127088}, {"mcc": 0.06706499506185015, "macro_f1": 0.5074008571098791}, {"mcc": 0.08178742803228277, "macro_f1": 0.5340883130133192}, {"mcc": 0.09842311948104528, "macro_f1": 0.49461348154249013}, {"mcc": 0.054826503785126865, "macro_f1": 0.523410775507077}], "total": {"test_mcc": 6.180884096369943, "test_mcc_se": 1.2848521715378107, "test_macro_f1": 50.838930389606695, "test_macro_f1_se": 1.6590424289257193}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "scala-fo", "task": "linguistic-acceptability", "dataset_languages": ["fo"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.10959986464774955, "macro_f1": 0.49783549783549785}, {"mcc": 0.027448990242931657, "macro_f1": 0.38832683174534355}, {"mcc": 0.08455719811181168, "macro_f1": 0.5422703099211443}, {"mcc": 0.07244940190275322, "macro_f1": 0.533631014304603}, {"mcc": 0.06530665236103944, "macro_f1": 0.5322261163960613}, {"mcc": 0.06545122114674011, "macro_f1": 0.49279638203940995}, {"mcc": 0.02487939662853241, "macro_f1": 0.3643074608402777}, {"mcc": 0.06664889599684098, "macro_f1": 0.5009625495429912}, {"mcc": 0.051450116244258264, "macro_f1": 0.47406266050333845}, {"mcc": 0.05853897337903162, "macro_f1": 0.5150126121954867}], "total": {"test_mcc": 6.263307106616889, "test_mcc_se": 1.548717515724732, "test_macro_f1": 48.41431435324154, "test_macro_f1_se": 3.7690560087699962}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "scala-de", "task": "linguistic-acceptability", "dataset_languages": ["de"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.42213085491463703, "macro_f1": 0.702282308475069}, {"mcc": 0.395333843359371, "macro_f1": 0.6974855288631677}, {"mcc": 0.4442884620724769, "macro_f1": 0.6991702346011011}, {"mcc": 0.4157427772958062, "macro_f1": 0.7065345026671463}, {"mcc": 0.4194345574527053, "macro_f1": 0.7026859394518157}, {"mcc": 0.4513284287369613, "macro_f1": 0.7043140710398781}, {"mcc": 0.4187173887487383, "macro_f1": 0.701194655110148}, {"mcc": 0.43530528978471983, "macro_f1": 0.7171519451920725}, {"mcc": 0.34180282456592265, "macro_f1": 0.6708604565765818}, {"mcc": 0.3986743516106627, "macro_f1": 0.6806806806806807}], "total": {"test_mcc": 41.42758778542001, "test_mcc_se": 1.924909615026689, "test_macro_f1": 69.82360322657661, "test_macro_f1_se": 0.8179970516482973}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "scala-nl", "task": "linguistic-acceptability", "dataset_languages": ["nl"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.5426790847130217, "macro_f1": 0.765782643773099}, {"mcc": 0.48314270632223005, "macro_f1": 0.724016809885216}, {"mcc": 0.5254549498073766, "macro_f1": 0.7624914572825949}, {"mcc": 0.4866201107909832, "macro_f1": 0.7248335549137024}, {"mcc": 0.5401234866714489, "macro_f1": 0.768145848917626}, {"mcc": 0.5403764522218709, "macro_f1": 0.7700437666035969}, {"mcc": 0.4986638482763968, "macro_f1": 0.7384934131254841}, {"mcc": 0.5036309897198736, "macro_f1": 0.7445734816541707}, {"mcc": 0.47439700926735806, "macro_f1": 0.713711650251276}, {"mcc": 0.5290862959845161, "macro_f1": 0.7639074756283989}], "total": {"test_mcc": 51.24174933775076, "test_mcc_se": 1.619865439004672, "test_macro_f1": 74.76000102035164, "test_macro_f1_se": 1.316290794348716}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "scala-en", "task": "linguistic-acceptability", "dataset_languages": ["en"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.4603638977878672, "macro_f1": 0.7269312378416333}, {"mcc": 0.4770104248700011, "macro_f1": 0.7287004307458715}, {"mcc": 0.43379702594620373, "macro_f1": 0.7153475371536333}, {"mcc": 0.45025692560378616, "macro_f1": 0.7136572639347334}, {"mcc": 0.41420008479736153, "macro_f1": 0.6985311019599568}, {"mcc": 0.4747841037646109, "macro_f1": 0.7334721499219157}, {"mcc": 0.4658488225747295, "macro_f1": 0.7213021517298123}, {"mcc": 0.46792700967416007, "macro_f1": 0.7256603577056164}, {"mcc": 0.43127767412009194, "macro_f1": 0.7104772328248626}, {"mcc": 0.44247250069149396, "macro_f1": 0.7079641124584944}], "total": {"test_mcc": 45.17938469830306, "test_mcc_se": 1.2973288959554654, "test_macro_f1": 71.8204357627653, "test_macro_f1_se": 0.6742530690269587}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "scala-fr", "task": "linguistic-acceptability", "dataset_languages": ["fr"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.2906647337830743, "macro_f1": 0.6438169698979197}, {"mcc": 0.4051331123102699, "macro_f1": 0.7023368771805973}, {"mcc": 0.34549164711048663, "macro_f1": 0.6666678722950221}, {"mcc": 0.35298889940000655, "macro_f1": 0.6629942059300495}, {"mcc": 0.40864154012819576, "macro_f1": 0.6917851565700537}, {"mcc": 0.43651562186274667, "macro_f1": 0.6869128526724719}, {"mcc": 0.42164158875091173, "macro_f1": 0.686918441754218}, {"mcc": 0.3639096257710906, "macro_f1": 0.6819468389416454}, {"mcc": 0.3908233868852559, "macro_f1": 0.6944774167492731}, {"mcc": 0.3975408919576666, "macro_f1": 0.6845329725863383}], "total": {"test_mcc": 38.13351047959704, "test_mcc_se": 2.689573298093589, "test_macro_f1": 68.0238960457759, "test_macro_f1_se": 1.082050045652808}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"f1": 63.7526205252773, "em": 51.13636363636363}, {"f1": 64.5790381802511, "em": 53.29795299469295}, {"f1": 65.3310889456627, "em": 55.08935508935509}, {"f1": 64.88058630180599, "em": 52.663622526636225}, {"f1": 66.3608636913893, "em": 57.05474171164225}, {"f1": 65.12352204216856, "em": 53.969465648854964}, {"f1": 63.3406503241754, "em": 50.46801872074883}, {"f1": 63.75030460237787, "em": 52.672955974842765}, {"f1": 65.26843798060328, "em": 55.10835913312693}, {"f1": 65.83390680460597, "em": 54.32098765432099}], "total": {"test_f1": 64.82210193983174, "test_f1_se": 0.6020038193053209, "test_em": 53.57818230905847, "test_em_se": 1.2185391924647349}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "norquad", "task": "reading-comprehension", "dataset_languages": ["nb", "nn", "no"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"f1": 73.64354037258124, "em": 41.911764705882355}, {"f1": 66.02429961784112, "em": 36.016260162601625}, {"f1": 71.32467430437785, "em": 42.547876769358865}, {"f1": 67.2386904903486, "em": 38.021259198691745}, {"f1": 72.16863193140986, "em": 40.69478908188586}, {"f1": 65.48896420383545, "em": 35.63786008230453}, {"f1": 65.74634861205743, "em": 37.47899159663866}, {"f1": 71.18649804423808, "em": 42.060810810810814}, {"f1": 65.36656539022496, "em": 35.79734219269103}, {"f1": 66.77209465110076, "em": 35.9271523178808}], "total": {"test_f1": 68.49603076180153, "test_f1_se": 1.9847118555339633, "test_em": 38.60941069187463, "test_em_se": 1.788732819950405}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "scandiqa-sv", "task": "reading-comprehension", "dataset_languages": ["sv"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"f1": 66.62691264963986, "em": 58.10606060606061}, {"f1": 64.76632256694772, "em": 53.525398028809704}, {"f1": 66.39991677211927, "em": 55.71095571095571}, {"f1": 66.10923453299077, "em": 55.09893455098935}, {"f1": 65.02590598309213, "em": 53.430994602929836}, {"f1": 64.60327119869099, "em": 55.343511450381676}, {"f1": 65.99064831208625, "em": 53.51014040561623}, {"f1": 67.02816184322468, "em": 57.075471698113205}, {"f1": 66.1520854624761, "em": 55.804953560371516}, {"f1": 66.81442753126754, "em": 56.867283950617285}], "total": {"test_f1": 65.95168868525352, "test_f1_se": 0.5350493736365329, "test_em": 55.44737045648451, "test_em_se": 1.0031943076258392}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "nqii", "task": "reading-comprehension", "dataset_languages": ["is"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"f1": 59.71560727318223, "em": 27.53846153846154}, {"f1": 58.15892858024838, "em": 24.766355140186917}, {"f1": 51.053506648629615, "em": 24.143302180685357}, {"f1": 53.03586442986816, "em": 24.364723467862483}, {"f1": 55.02013144600614, "em": 24.137931034482758}, {"f1": 57.44963706306248, "em": 28.043143297380585}, {"f1": 55.623823230306165, "em": 24.307692307692307}, {"f1": 51.65738707486516, "em": 25.30674846625767}, {"f1": 55.72266971834554, "em": 24.397590361445783}, {"f1": 57.89996448900181, "em": 30.572289156626507}], "total": {"test_f1": 55.533751995351565, "test_f1_se": 1.7927411580859423, "test_em": 25.757823695108193, "test_em_se": 1.3686279104278718}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "foqa", "task": "reading-comprehension", "dataset_languages": ["fo"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"f1": 67.63256689273439, "em": 48.77450980392157}, {"f1": 59.793184659193436, "em": 39.31203931203931}, {"f1": 76.06661485445301, "em": 55.80246913580247}, {"f1": 73.63198218640856, "em": 56.310679611650485}, {"f1": 71.08587205706507, "em": 54.20792079207921}, {"f1": 70.31885999033462, "em": 51.358024691358025}, {"f1": 69.85794522357354, "em": 51.65094339622642}, {"f1": 72.91228933864672, "em": 51.908396946564885}, {"f1": 76.4495022072186, "em": 58.60349127182045}, {"f1": 71.50522936164086, "em": 51.842751842751845}], "total": {"test_f1": 70.92540467712689, "test_f1_se": 2.957077778775924, "test_em": 51.97712268042147, "test_em_se": 3.2892704457781945}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "germanquad", "task": "reading-comprehension", "dataset_languages": ["de"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"f1": 55.463895706117356, "em": 26.742424242424242}, {"f1": 53.89237253821542, "em": 25.549658832448824}, {"f1": 58.946193466053934, "em": 27.117327117327118}, {"f1": 56.75762582423414, "em": 27.77777777777778}, {"f1": 56.96479212277398, "em": 28.218966846569007}, {"f1": 57.2164833443238, "em": 27.709923664122137}, {"f1": 53.77635391646707, "em": 26.365054602184088}, {"f1": 57.810729097748464, "em": 28.930817610062892}, {"f1": 56.12438835755127, "em": 25.851393188854487}, {"f1": 51.40873415930987, "em": 25.617283950617285}], "total": {"test_f1": 55.83615685327953, "test_f1_se": 1.3927543601816614, "test_em": 26.988062783238785, "test_em_se": 0.7198357493256847}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "squad", "task": "reading-comprehension", "dataset_languages": ["en"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"f1": 85.45745299421112, "em": 70.22727272727273}, {"f1": 84.02617647024812, "em": 66.94465504169825}, {"f1": 86.48947145211763, "em": 71.87257187257187}, {"f1": 84.7339940458752, "em": 69.7869101978691}, {"f1": 85.75262339834897, "em": 71.85813415574403}, {"f1": 82.95467269202634, "em": 67.17557251908397}, {"f1": 85.10934566961441, "em": 69.65678627145085}, {"f1": 83.42821873446518, "em": 65.25157232704403}, {"f1": 83.93929008878327, "em": 66.56346749226006}, {"f1": 84.06897721065734, "em": 67.66975308641975}], "total": {"test_f1": 84.59602227563475, "test_f1_se": 0.6864375133670515, "test_em": 68.70066956914147, "test_em_se": 1.4222774542699266}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "squad-nl", "task": "reading-comprehension", "dataset_languages": ["nl"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"f1": 76.40455619309617, "em": 55.378787878787875}, {"f1": 76.13349467490048, "em": 56.86125852918878}, {"f1": 74.77307538671323, "em": 56.87645687645688}, {"f1": 75.41527723604155, "em": 56.77321156773212}, {"f1": 75.81124279218217, "em": 56.437933693138014}, {"f1": 78.36219529439784, "em": 57.48091603053435}, {"f1": 75.5000151676685, "em": 56.63026521060842}, {"f1": 76.37168741885338, "em": 57.31132075471698}, {"f1": 76.2823586693553, "em": 58.978328173374614}, {"f1": 76.38826254043818, "em": 56.40432098765432}], "total": {"test_f1": 76.14421653736467, "test_f1_se": 0.586362704549725, "test_em": 56.91327997021923, "test_em_se": 0.5728697454667023}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "fquad", "task": "reading-comprehension", "dataset_languages": ["fr"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"f1": 68.55070013184384, "em": 37.27272727272727}, {"f1": 69.27993459616282, "em": 39.575435936315394}, {"f1": 70.0736437719349, "em": 38.92773892773893}, {"f1": 69.26744996869459, "em": 38.58447488584475}, {"f1": 65.50724668223005, "em": 35.774865073245955}, {"f1": 69.74451506991437, "em": 38.01526717557252}, {"f1": 67.72602660924635, "em": 38.76755070202808}, {"f1": 68.34667830576998, "em": 40.172955974842765}, {"f1": 66.12555879818709, "em": 37.84829721362229}, {"f1": 70.62447981352604, "em": 39.19753086419753}], "total": {"test_f1": 68.524623374751, "test_f1_se": 1.0319659567567812, "test_em": 38.41368440261355, "test_em_se": 0.7791204969648675}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "nordjylland-news", "task": "summarization", "dataset_languages": ["da"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"bertscore": 0.6550542182958452, "rouge_l": 0.1717811036681433}, {"bertscore": 0.6548563724936685, "rouge_l": 0.16884908002657062}, {"bertscore": 0.6497382136585657, "rouge_l": 0.158680307847539}, {"bertscore": 0.6514058057509828, "rouge_l": 0.1585942001685965}, {"bertscore": 0.6553702252567746, "rouge_l": 0.16967957218279195}, {"bertscore": 0.6500445229758043, "rouge_l": 0.15710370865103618}, {"bertscore": 0.6533339025918394, "rouge_l": 0.1625119123164937}, {"bertscore": 0.6505100691720145, "rouge_l": 0.15757579249617432}, {"bertscore": 0.6504996601870516, "rouge_l": 0.15750529901257826}, {"bertscore": 0.6565495272661792, "rouge_l": 0.16721913639733754}], "total": {"test_bertscore": 65.27362517648726, "test_bertscore_se": 0.15958715835841772, "test_rouge_l": 16.295001127672613, "test_rouge_l_se": 0.36164309683172724}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "mlsum", "task": "summarization", "dataset_languages": ["de"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"bertscore": 0.6608472929510754, "rouge_l": 0.17608061466859753}, {"bertscore": 0.6631467337429058, "rouge_l": 0.18433916040086}, {"bertscore": 0.6552633823885117, "rouge_l": 0.16645466136342496}, {"bertscore": 0.662476489902474, "rouge_l": 0.18334459676321585}, {"bertscore": 0.6563647607108578, "rouge_l": 0.1637550035923322}, {"bertscore": 0.6605382321577054, "rouge_l": 0.1773866610857404}, {"bertscore": 0.6560774752870202, "rouge_l": 0.16732353343803877}, {"bertscore": 0.6641511400521267, "rouge_l": 0.18487797408055645}, {"bertscore": 0.6548271189385559, "rouge_l": 0.16021053536882338}, {"bertscore": 0.6569611292507034, "rouge_l": 0.16866376375215297}], "total": {"test_bertscore": 65.90653755381936, "test_bertscore_se": 0.21920225654596748, "test_rouge_l": 17.324365045137426, "test_rouge_l_se": 0.5649425411114872}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "rrn", "task": "summarization", "dataset_languages": ["is"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"bertscore": 0.6615710979676805, "rouge_l": 0.1656080998821285}, {"bertscore": 0.6597461358760484, "rouge_l": 0.16199343451101228}, {"bertscore": 0.6622365965740755, "rouge_l": 0.16753308884782925}, {"bertscore": 0.6606344757019542, "rouge_l": 0.16207129569232354}, {"bertscore": 0.6612844728515483, "rouge_l": 0.16655814773082642}, {"bertscore": 0.6596792709315196, "rouge_l": 0.16675143180387453}, {"bertscore": 0.6585823352797888, "rouge_l": 0.1631982162825394}, {"bertscore": 0.657288731134031, "rouge_l": 0.15717740853744583}, {"bertscore": 0.6607625119504519, "rouge_l": 0.16619048015548635}, {"bertscore": 0.6591855097212829, "rouge_l": 0.16572551448609357}], "total": {"test_bertscore": 66.00971137988381, "test_bertscore_se": 0.09286006011602942, "test_rouge_l": 16.428071179295596, "test_rouge_l_se": 0.19720239570378942}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "no-sammendrag", "task": "summarization", "dataset_languages": ["nb", "nn", "no"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"bertscore": 0.6403641367942328, "rouge_l": 0.13983834761139133}, {"bertscore": 0.641764542349847, "rouge_l": 0.14232576487363569}, {"bertscore": 0.6480035139829852, "rouge_l": 0.15545576466743022}, {"bertscore": 0.6415330415184144, "rouge_l": 0.14445930021395442}, {"bertscore": 0.6435112423350802, "rouge_l": 0.14741737903507035}, {"bertscore": 0.6403757146326825, "rouge_l": 0.14042124063864583}, {"bertscore": 0.6368938499072101, "rouge_l": 0.13671665770369756}, {"bertscore": 0.6453917911858298, "rouge_l": 0.15216995769928732}, {"bertscore": 0.6445820547087351, "rouge_l": 0.14890199404792387}, {"bertscore": 0.6397122445050627, "rouge_l": 0.1421046914035366}], "total": {"test_bertscore": 64.2213213192008, "test_bertscore_se": 0.19917252666411464, "test_rouge_l": 14.498110978945732, "test_rouge_l_se": 0.366009656101152}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "wiki-lingua-nl", "task": "summarization", "dataset_languages": ["nl"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"bertscore": 0.7558101326867472, "rouge_l": 0.3373028933856176}, {"bertscore": 0.7536739726492669, "rouge_l": 0.33396793735378083}, {"bertscore": 0.7520425892289495, "rouge_l": 0.3315913580377332}, {"bertscore": 0.7557786268298514, "rouge_l": 0.33563917623318185}, {"bertscore": 0.7580443412152817, "rouge_l": 0.34685174358107523}, {"bertscore": 0.7559836541331606, "rouge_l": 0.33863557395842736}, {"bertscore": 0.75437833648175, "rouge_l": 0.33823102065896316}, {"bertscore": 0.7500261124223471, "rouge_l": 0.32706104310855966}, {"bertscore": 0.7562781162851024, "rouge_l": 0.3452616416319785}, {"bertscore": 0.7559195268549956, "rouge_l": 0.34190560205405307}], "total": {"test_bertscore": 75.47935408787453, "test_bertscore_se": 0.14488934748925403, "test_rouge_l": 33.7644799000337, "test_rouge_l_se": 0.3745045626601296}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "swedn", "task": "summarization", "dataset_languages": ["sv"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"bertscore": 0.6548368237563409, "rouge_l": 0.1715608221175688}, {"bertscore": 0.6537464958673809, "rouge_l": 0.17064823142371388}, {"bertscore": 0.6553868831833825, "rouge_l": 0.17495223189231623}, {"bertscore": 0.6542450126435142, "rouge_l": 0.17622977122383102}, {"bertscore": 0.6549564265296794, "rouge_l": 0.17168642672373247}, {"bertscore": 0.6524755837017437, "rouge_l": 0.16988301420509044}, {"bertscore": 0.6544211354921572, "rouge_l": 0.17203023984402085}, {"bertscore": 0.6566813107056078, "rouge_l": 0.17825290256793158}, {"bertscore": 0.6547838786209468, "rouge_l": 0.17007116592057353}, {"bertscore": 0.6541080019960646, "rouge_l": 0.16954077920289312}], "total": {"test_bertscore": 65.45641552496818, "test_bertscore_se": 0.06788637032187894, "test_rouge_l": 17.24855585121672, "test_rouge_l_se": 0.18446337921103165}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "cnn-dailymail", "task": "summarization", "dataset_languages": ["en"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"bertscore": 0.727875191019848, "rouge_l": 0.3225222517102898}, {"bertscore": 0.7305901076470036, "rouge_l": 0.3202556783036487}, {"bertscore": 0.7292760409181938, "rouge_l": 0.3188580293607183}, {"bertscore": 0.7320075463212561, "rouge_l": 0.31482552827381693}, {"bertscore": 0.7289284110884182, "rouge_l": 0.3159071324605103}, {"bertscore": 0.7372340864967555, "rouge_l": 0.32505357060355955}, {"bertscore": 0.7355116295802873, "rouge_l": 0.3190119887890327}, {"bertscore": 0.7300645193317905, "rouge_l": 0.3174123301738647}, {"bertscore": 0.7297805294510908, "rouge_l": 0.31983171274563793}, {"bertscore": 0.7337962258025073, "rouge_l": 0.3183575257633674}], "total": {"test_bertscore": 73.15064287657151, "test_bertscore_se": 0.19052946873811077, "test_rouge_l": 31.92035748184446, "test_rouge_l_se": 0.18577178403932904}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "orange-sum", "task": "summarization", "dataset_languages": ["fr"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"bertscore": 0.6718376319040544, "rouge_l": 0.19670839821166825}, {"bertscore": 0.669601836882066, "rouge_l": 0.18992792922624985}, {"bertscore": 0.6741781342425384, "rouge_l": 0.20270772174280016}, {"bertscore": 0.6692539266659878, "rouge_l": 0.1948807017059903}, {"bertscore": 0.6695184295531362, "rouge_l": 0.19026623730433131}, {"bertscore": 0.6743935121339746, "rouge_l": 0.20004129782412988}, {"bertscore": 0.671543304604711, "rouge_l": 0.19161925790107232}, {"bertscore": 0.6726957735372707, "rouge_l": 0.20072158545802327}, {"bertscore": 0.6749642740469426, "rouge_l": 0.19863276094603266}, {"bertscore": 0.6743318856752012, "rouge_l": 0.19715465898040918}], "total": {"test_bertscore": 67.22318709245883, "test_bertscore_se": 0.13755649199666523, "test_rouge_l": 19.62660549300707, "test_rouge_l_se": 0.2786200034073319}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "danske-talemaader", "task": "knowledge", "dataset_languages": ["da"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.5239115634797089, "accuracy": 0.6361386138613861}, {"mcc": 0.5928514771596956, "accuracy": 0.6918316831683168}, {"mcc": 0.6027995400381193, "accuracy": 0.698019801980198}, {"mcc": 0.5894635492307158, "accuracy": 0.6881188118811881}, {"mcc": 0.5220776073264124, "accuracy": 0.6311881188118812}, {"mcc": 0.5506324139386175, "accuracy": 0.6584158415841584}, {"mcc": 0.5896105342315119, "accuracy": 0.6868811881188119}, {"mcc": 0.5272744778623185, "accuracy": 0.6435643564356436}, {"mcc": 0.5842672568837578, "accuracy": 0.6831683168316832}, {"mcc": 0.541938486985766, "accuracy": 0.6522277227722773}], "total": {"test_mcc": 56.24826907136622, "test_mcc_se": 2.003996002970338, "test_accuracy": 66.69554455445544, "test_accuracy_se": 1.5676670922015266}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "danish-citizen-tests", "task": "knowledge", "dataset_languages": ["da"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.6913916049903418, "accuracy": 0.794921875}, {"mcc": 0.6948557175662455, "accuracy": 0.796875}, {"mcc": 0.7138806043638017, "accuracy": 0.810546875}, {"mcc": 0.6780401749849037, "accuracy": 0.78515625}, {"mcc": 0.73384397802787, "accuracy": 0.8203125}, {"mcc": 0.6489534316160204, "accuracy": 0.763671875}, {"mcc": 0.6808570039707887, "accuracy": 0.7890625}, {"mcc": 0.7284943857202019, "accuracy": 0.818359375}, {"mcc": 0.7113637341445865, "accuracy": 0.80859375}, {"mcc": 0.7118906228161642, "accuracy": 0.806640625}], "total": {"test_mcc": 69.93571258200924, "test_mcc_se": 1.5965163524929624, "test_accuracy": 79.94140625, "test_accuracy_se": 1.0676893100439464}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "mmlu-no", "task": "knowledge", "dataset_languages": ["nb", "nn", "no"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.47746810620415153, "accuracy": 0.60400390625}, {"mcc": 0.47872857815030606, "accuracy": 0.6083984375}, {"mcc": 0.49134984976059537, "accuracy": 0.6162109375}, {"mcc": 0.5024279679432058, "accuracy": 0.6259765625}, {"mcc": 0.4792193195712731, "accuracy": 0.60888671875}, {"mcc": 0.48300348976402757, "accuracy": 0.6103515625}, {"mcc": 0.4632132115982573, "accuracy": 0.59375}, {"mcc": 0.46005887978458315, "accuracy": 0.59521484375}, {"mcc": 0.4927577454090865, "accuracy": 0.6181640625}, {"mcc": 0.4775859948133754, "accuracy": 0.60693359375}], "total": {"test_mcc": 48.05813142998861, "test_mcc_se": 0.7986029331272552, "test_accuracy": 60.87890625, "test_accuracy_se": 0.6121011070889181}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "mmlu-sv", "task": "knowledge", "dataset_languages": ["sv"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.5052259034939799, "accuracy": 0.62548828125}, {"mcc": 0.5020794854385962, "accuracy": 0.6259765625}, {"mcc": 0.48359054431047527, "accuracy": 0.60791015625}, {"mcc": 0.47172767736666743, "accuracy": 0.6005859375}, {"mcc": 0.5128028912145197, "accuracy": 0.63232421875}, {"mcc": 0.5019319465990222, "accuracy": 0.62548828125}, {"mcc": 0.5098330089866738, "accuracy": 0.62939453125}, {"mcc": 0.5402053219839853, "accuracy": 0.65234375}, {"mcc": 0.5184750434040246, "accuracy": 0.63623046875}, {"mcc": 0.4874801919052904, "accuracy": 0.6142578125}], "total": {"test_mcc": 50.33352014703235, "test_mcc_se": 1.1989811899674911, "test_accuracy": 62.5, "test_accuracy_se": 0.912726656606404}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "mmlu-de", "task": "knowledge", "dataset_languages": ["de"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.5784608242800482, "accuracy": 0.68212890625}, {"mcc": 0.5574153629931663, "accuracy": 0.66845703125}, {"mcc": 0.5757793370953354, "accuracy": 0.68017578125}, {"mcc": 0.5942683521478586, "accuracy": 0.6953125}, {"mcc": 0.5442946319317842, "accuracy": 0.65673828125}, {"mcc": 0.5567226517606843, "accuracy": 0.6669921875}, {"mcc": 0.5645037376485231, "accuracy": 0.671875}, {"mcc": 0.5647543197535356, "accuracy": 0.6708984375}, {"mcc": 0.5721978964425426, "accuracy": 0.677734375}, {"mcc": 0.5737298367588086, "accuracy": 0.68017578125}], "total": {"test_mcc": 56.82126950812286, "test_mcc_se": 0.8602914410258505, "test_accuracy": 67.5048828125, "test_accuracy_se": 0.650538819479009}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "mmlu-nl", "task": "knowledge", "dataset_languages": ["nl"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.5955549869199539, "accuracy": 0.69677734375}, {"mcc": 0.5811181977080477, "accuracy": 0.685546875}, {"mcc": 0.5902605377743112, "accuracy": 0.69140625}, {"mcc": 0.57586311741391, "accuracy": 0.68115234375}, {"mcc": 0.5613706311906215, "accuracy": 0.669921875}, {"mcc": 0.5748346101381722, "accuracy": 0.6806640625}, {"mcc": 0.5886393349802304, "accuracy": 0.6904296875}, {"mcc": 0.59160953037634, "accuracy": 0.69287109375}, {"mcc": 0.5993368389303171, "accuracy": 0.69921875}, {"mcc": 0.5426599796246505, "accuracy": 0.65673828125}], "total": {"test_mcc": 58.01247765056554, "test_mcc_se": 1.0768687687635097, "test_accuracy": 68.447265625, "test_accuracy_se": 0.8082744458958464}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "mmlu", "task": "knowledge", "dataset_languages": ["en"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.6444373280241542, "accuracy": 0.732421875}, {"mcc": 0.6584178963896838, "accuracy": 0.7421875}, {"mcc": 0.6430862584125006, "accuracy": 0.73193359375}, {"mcc": 0.6439462522572359, "accuracy": 0.73095703125}, {"mcc": 0.6362904856184398, "accuracy": 0.72607421875}, {"mcc": 0.6471268549591408, "accuracy": 0.73486328125}, {"mcc": 0.6460327030075789, "accuracy": 0.73291015625}, {"mcc": 0.6718203667824827, "accuracy": 0.75244140625}, {"mcc": 0.6278352125160617, "accuracy": 0.71923828125}, {"mcc": 0.654440748553109, "accuracy": 0.740234375}], "total": {"test_mcc": 64.73434106520388, "test_mcc_se": 0.7491283544782554, "test_accuracy": 73.4326171875, "test_accuracy_se": 0.5633854017704093}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "mmlu-fr", "task": "knowledge", "dataset_languages": ["fr"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.6005822025962044, "accuracy": 0.69970703125}, {"mcc": 0.5779623047132645, "accuracy": 0.68310546875}, {"mcc": 0.5763607418658778, "accuracy": 0.681640625}, {"mcc": 0.5737867086860354, "accuracy": 0.67822265625}, {"mcc": 0.5701048615898745, "accuracy": 0.67626953125}, {"mcc": 0.5969749948055282, "accuracy": 0.697265625}, {"mcc": 0.5929406309366133, "accuracy": 0.69384765625}, {"mcc": 0.5903110603510787, "accuracy": 0.6923828125}, {"mcc": 0.6033593614525311, "accuracy": 0.70166015625}, {"mcc": 0.6063352103686577, "accuracy": 0.70361328125}], "total": {"test_mcc": 58.88718077365667, "test_mcc_se": 0.8242322182463285, "test_accuracy": 69.0771484375, "test_accuracy_se": 0.629194977314944}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "arc-is", "task": "knowledge", "dataset_languages": ["is"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.26240366972427154, "accuracy": 0.4453125}, {"mcc": 0.29653115445091394, "accuracy": 0.470703125}, {"mcc": 0.24755985175832498, "accuracy": 0.43359375}, {"mcc": 0.25844462725257406, "accuracy": 0.4453125}, {"mcc": 0.2871664973039111, "accuracy": 0.46484375}, {"mcc": 0.2812753217561731, "accuracy": 0.4609375}, {"mcc": 0.25740962807340284, "accuracy": 0.4423828125}, {"mcc": 0.270019920817437, "accuracy": 0.4521484375}, {"mcc": 0.22405834254739493, "accuracy": 0.4140625}, {"mcc": 0.24723848478391824, "accuracy": 0.4375}], "total": {"test_mcc": 26.321074984683218, "test_mcc_se": 1.330430512066025, "test_accuracy": 44.66796875, "test_accuracy_se": 1.0292523923684513}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "hellaswag-da", "task": "common-sense-reasoning", "dataset_languages": ["da"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.5845451544058906, "accuracy": 0.68408203125}, {"mcc": 0.5732955808634774, "accuracy": 0.67333984375}, {"mcc": 0.534487010799677, "accuracy": 0.6376953125}, {"mcc": 0.5779739940253632, "accuracy": 0.6787109375}, {"mcc": 0.5124044176935446, "accuracy": 0.6181640625}, {"mcc": 0.5442340173163445, "accuracy": 0.6484375}, {"mcc": 0.5012665292199572, "accuracy": 0.615234375}, {"mcc": 0.5877711951804447, "accuracy": 0.6865234375}, {"mcc": 0.5777714095947992, "accuracy": 0.67822265625}, {"mcc": 0.5587264504239914, "accuracy": 0.66357421875}], "total": {"test_mcc": 55.524757595234895, "test_mcc_se": 1.9136722501312, "test_accuracy": 65.83984375, "test_accuracy_se": 1.6677381301178165}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "hellaswag-no", "task": "common-sense-reasoning", "dataset_languages": ["nb", "nn", "no"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.5459855754197228, "accuracy": 0.65478515625}, {"mcc": 0.5323094362661911, "accuracy": 0.64306640625}, {"mcc": 0.5904260881473447, "accuracy": 0.69091796875}, {"mcc": 0.5200084936400693, "accuracy": 0.6318359375}, {"mcc": 0.5734244425194336, "accuracy": 0.67529296875}, {"mcc": 0.5697642751068273, "accuracy": 0.67578125}, {"mcc": 0.5771354235910331, "accuracy": 0.6796875}, {"mcc": 0.5815141325620911, "accuracy": 0.68212890625}, {"mcc": 0.6084453505036117, "accuracy": 0.70458984375}, {"mcc": 0.579028574522225, "accuracy": 0.68115234375}], "total": {"test_mcc": 56.78041792278549, "test_mcc_se": 1.681018249681208, "test_accuracy": 67.1923828125, "test_accuracy_se": 1.3762221775058687}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "hellaswag-sv", "task": "common-sense-reasoning", "dataset_languages": ["sv"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.5744029477340434, "accuracy": 0.67333984375}, {"mcc": 0.5881348331769843, "accuracy": 0.6865234375}, {"mcc": 0.5590362750640306, "accuracy": 0.66259765625}, {"mcc": 0.5712713489164383, "accuracy": 0.67041015625}, {"mcc": 0.5551787331184573, "accuracy": 0.6591796875}, {"mcc": 0.578227856492008, "accuracy": 0.67724609375}, {"mcc": 0.6014898203030338, "accuracy": 0.697265625}, {"mcc": 0.5565176227554266, "accuracy": 0.65966796875}, {"mcc": 0.5439678397126708, "accuracy": 0.6474609375}, {"mcc": 0.564153349384705, "accuracy": 0.669921875}], "total": {"test_mcc": 56.92380626657799, "test_mcc_se": 1.0609135135828431, "test_accuracy": 67.0361328125, "test_accuracy_se": 0.8923685632799409}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "winogrande-is", "task": "common-sense-reasoning", "dataset_languages": ["is"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.1208779840946516, "accuracy": 0.5703125}, {"mcc": 0.165035167477031, "accuracy": 0.5814732142857143}, {"mcc": 0.10209876987255585, "accuracy": 0.5736607142857143}, {"mcc": 0.10271119303514152, "accuracy": 0.5691964285714286}, {"mcc": 0.1370377378413517, "accuracy": 0.5881696428571429}, {"mcc": 0.11876130724752046, "accuracy": 0.5647321428571429}, {"mcc": 0.09958713943687668, "accuracy": 0.5758928571428571}, {"mcc": 0.15505347570117115, "accuracy": 0.5870535714285714}, {"mcc": 0.14070726415837229, "accuracy": 0.6060267857142857}, {"mcc": 0.1177776111996506, "accuracy": 0.5658482142857143}], "total": {"test_mcc": 12.596476500643227, "test_mcc_se": 1.4108807898106337, "test_accuracy": 57.82366071428571, "test_accuracy_se": 0.7910366188057701}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "hellaswag-de", "task": "common-sense-reasoning", "dataset_languages": ["de"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.7027046368832116, "accuracy": 0.77490234375}, {"mcc": 0.7006252471317014, "accuracy": 0.7705078125}, {"mcc": 0.7360783236730201, "accuracy": 0.7998046875}, {"mcc": 0.7052519438294426, "accuracy": 0.77587890625}, {"mcc": 0.7088445002567818, "accuracy": 0.77880859375}, {"mcc": 0.6924377469638927, "accuracy": 0.76708984375}, {"mcc": 0.7316184356485441, "accuracy": 0.796875}, {"mcc": 0.6927649751705071, "accuracy": 0.765625}, {"mcc": 0.7314739411628179, "accuracy": 0.79638671875}, {"mcc": 0.6968628357554101, "accuracy": 0.76953125}], "total": {"test_mcc": 70.9866258647533, "test_mcc_se": 1.043583883309481, "test_accuracy": 77.9541015625, "test_accuracy_se": 0.816224169060349}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "hellaswag-nl", "task": "common-sense-reasoning", "dataset_languages": ["nl"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.6995428777090597, "accuracy": 0.7724609375}, {"mcc": 0.6934263259685323, "accuracy": 0.7666015625}, {"mcc": 0.6749360027184496, "accuracy": 0.7529296875}, {"mcc": 0.6425990558403097, "accuracy": 0.72607421875}, {"mcc": 0.6982450479614367, "accuracy": 0.76708984375}, {"mcc": 0.6470090137946632, "accuracy": 0.72900390625}, {"mcc": 0.6874245995594229, "accuracy": 0.759765625}, {"mcc": 0.6401230998624905, "accuracy": 0.7216796875}, {"mcc": 0.6824755525713289, "accuracy": 0.75634765625}, {"mcc": 0.6280865612660456, "accuracy": 0.7119140625}], "total": {"test_mcc": 66.9386813725174, "test_mcc_se": 1.681997757056183, "test_accuracy": 74.638671875, "test_accuracy_se": 1.3629027040800985}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "hellaswag", "task": "common-sense-reasoning", "dataset_languages": ["en"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.8384087448320903, "accuracy": 0.8779296875}, {"mcc": 0.8522897890117185, "accuracy": 0.888671875}, {"mcc": 0.8337518639238762, "accuracy": 0.87451171875}, {"mcc": 0.844992683158355, "accuracy": 0.8828125}, {"mcc": 0.8477403690122417, "accuracy": 0.88525390625}, {"mcc": 0.8679306202117459, "accuracy": 0.900390625}, {"mcc": 0.8249845340111218, "accuracy": 0.8681640625}, {"mcc": 0.845673324947246, "accuracy": 0.88330078125}, {"mcc": 0.8466444310413505, "accuracy": 0.8837890625}, {"mcc": 0.8245841748070444, "accuracy": 0.86767578125}], "total": {"test_mcc": 84.2700053495679, "test_mcc_se": 0.8065727534921354, "test_accuracy": 88.125, "test_accuracy_se": 0.606757477052571}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "hellaswag-fr", "task": "common-sense-reasoning", "dataset_languages": ["fr"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"mcc": 0.7278189246441602, "accuracy": 0.79296875}, {"mcc": 0.7488273461519814, "accuracy": 0.80908203125}, {"mcc": 0.7441625406722395, "accuracy": 0.80615234375}, {"mcc": 0.732959273117899, "accuracy": 0.7978515625}, {"mcc": 0.7661140070823728, "accuracy": 0.82373046875}, {"mcc": 0.7532263526839921, "accuracy": 0.814453125}, {"mcc": 0.7316776437099777, "accuracy": 0.796875}, {"mcc": 0.7507362625088544, "accuracy": 0.81201171875}, {"mcc": 0.7206336305406291, "accuracy": 0.787109375}, {"mcc": 0.7214020140052487, "accuracy": 0.7900390625}], "total": {"test_mcc": 73.97557995117356, "test_mcc_se": 0.9365598723859102, "test_accuracy": 80.302734375, "test_accuracy_se": 0.7374869537636385}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "CohereForAI/aya-expanse-32b", "results": {"raw": [{"test_speed": 379.14, "test_speed_short": 44.16}, {"test_speed": 727.5600000000001, "test_speed_short": 80.55}, {"test_speed": 1111.9699999999998, "test_speed_short": 157.18}, {"test_speed": 1469.63, "test_speed_short": 195.12}, {"test_speed": 1832.2199999999998, "test_speed_short": 234.35}, {"test_speed": 2201.8300000000004, "test_speed_short": 306.09000000000003}, {"test_speed": 2572.84, "test_speed_short": 344.96}, {"test_speed": 2962.08, "test_speed_short": 381.98}, {"test_speed": 3331.6800000000003, "test_speed_short": 414.96000000000004}, {"test_speed": 3652.21, "test_speed_short": 456.45}], "total": {"test_speed": 2024.116, "test_speed_se": 689.4497327443837, "test_speed_short": 261.58000000000004, "test_speed_short_se": 87.9043788152255}}, "num_model_parameters": 32296476672, "max_sequence_length": 8192, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "instruction_tuned", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "swerec", "task": "sentiment-classification", "dataset_languages": ["sv"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.8138143612347908, "macro_f1": 0.820883859748073}, {"mcc": 0.8012126713583518, "macro_f1": 0.7929685192588164}, {"mcc": 0.8233142198934998, "macro_f1": 0.8218037721911191}, {"mcc": 0.7908264902389106, "macro_f1": 0.7874562719848163}, {"mcc": 0.8134457144914368, "macro_f1": 0.8274313834594419}, {"mcc": 0.8378367941094927, "macro_f1": 0.8401439748611425}, {"mcc": 0.8414993988557815, "macro_f1": 0.8307435710381901}, {"mcc": 0.7977595068772101, "macro_f1": 0.8200921794989312}, {"mcc": 0.8196037991707468, "macro_f1": 0.8208409596504973}, {"mcc": 0.8099546191644751, "macro_f1": 0.818016486920059}], "total": {"test_mcc": 81.49267575394697, "test_mcc_se": 1.01529048910683, "test_macro_f1": 81.80380978611088, "test_macro_f1_se": 0.9986427975137405}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "angry-tweets", "task": "sentiment-classification", "dataset_languages": ["da"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.49351383346843086, "macro_f1": 0.6456088272718484}, {"mcc": 0.5352533496237146, "macro_f1": 0.6838729298209337}, {"mcc": 0.5378679650756092, "macro_f1": 0.6848054413594488}, {"mcc": 0.5352199777900186, "macro_f1": 0.6744684129775029}, {"mcc": 0.5754073040673559, "macro_f1": 0.7167360710806183}, {"mcc": 0.4910004814255549, "macro_f1": 0.611344949647702}, {"mcc": 0.5062739010425277, "macro_f1": 0.648628852229812}, {"mcc": 0.44804535001151946, "macro_f1": 0.6018933495345163}, {"mcc": 0.5476134309029995, "macro_f1": 0.6928144322180483}, {"mcc": 0.5078657221050284, "macro_f1": 0.6542972594443183}], "total": {"test_mcc": 51.78061315512758, "test_mcc_se": 2.227428271337631, "test_macro_f1": 66.1447052558475, "test_macro_f1_se": 2.242562992465332}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "norec", "task": "sentiment-classification", "dataset_languages": ["nb", "nn", "no"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.4175330306763829, "macro_f1": 0.40774876818566136}, {"mcc": 0.37963470110459924, "macro_f1": 0.3756009518448999}, {"mcc": 0.3943573214939963, "macro_f1": 0.4107418522067146}, {"mcc": 0.41830931188061027, "macro_f1": 0.41428698007645376}, {"mcc": 0.3694772940538996, "macro_f1": 0.3695942556388469}, {"mcc": 0.40829248788063305, "macro_f1": 0.4064778675821909}, {"mcc": 0.41001673581789905, "macro_f1": 0.3994933797805918}, {"mcc": 0.4099585458819813, "macro_f1": 0.40459116697875297}, {"mcc": 0.42116186968336533, "macro_f1": 0.4086997319034853}, {"mcc": 0.4018979999026272, "macro_f1": 0.4038574396441406}], "total": {"test_mcc": 40.30639298375994, "test_mcc_se": 1.0628836465477276, "test_macro_f1": 40.01092393841738, "test_macro_f1_se": 0.9361415261257252}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "hotter-and-colder-sentiment", "task": "sentiment-classification", "dataset_languages": ["is"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.36860961781586077, "macro_f1": 0.5634251036723626}, {"mcc": 0.4150623032370426, "macro_f1": 0.58589341015479}, {"mcc": 0.28691649623993765, "macro_f1": 0.47669185390436003}, {"mcc": 0.41681190240665766, "macro_f1": 0.5961473320403433}, {"mcc": 0.38914866096315, "macro_f1": 0.5543789533042299}, {"mcc": 0.44688865864422433, "macro_f1": 0.607336327225839}, {"mcc": 0.40691415610337245, "macro_f1": 0.5759501957738051}, {"mcc": 0.3341062885405451, "macro_f1": 0.5124456717934339}, {"mcc": 0.4189713543918953, "macro_f1": 0.5966379504203777}, {"mcc": 0.38326683130280353, "macro_f1": 0.5585225259703109}], "total": {"test_mcc": 38.666962696454895, "test_mcc_se": 2.910946837007435, "test_macro_f1": 56.27429324259852, "test_macro_f1_se": 2.5292109924955684}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "sb10k", "task": "sentiment-classification", "dataset_languages": ["de"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.3490098432523716, "macro_f1": 0.4883045972305404}, {"mcc": 0.5775203022965735, "macro_f1": 0.7053293738225245}, {"mcc": 0.5360724914032367, "macro_f1": 0.6630816422250263}, {"mcc": 0.5398621718686833, "macro_f1": 0.6871493430406703}, {"mcc": 0.540836471111797, "macro_f1": 0.6555216944561207}, {"mcc": 0.6003521055639178, "macro_f1": 0.7160184561136643}, {"mcc": 0.5230242468209058, "macro_f1": 0.6429841637756547}, {"mcc": 0.5200029116066951, "macro_f1": 0.656524443792971}, {"mcc": 0.5546454044795841, "macro_f1": 0.6606669398045252}, {"mcc": 0.5852508494118273, "macro_f1": 0.7078918382798126}], "total": {"test_mcc": 53.26576797815592, "test_mcc_se": 4.3314878593689885, "test_macro_f1": 65.83472492541509, "test_macro_f1_se": 4.02779574835879}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "dutch-social", "task": "sentiment-classification", "dataset_languages": ["nl"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.06697601399118686, "macro_f1": 0.1981897749555235}, {"mcc": 0.13992443210066363, "macro_f1": 0.22877492877492878}, {"mcc": 0.050793169012262045, "macro_f1": 0.19469996871131426}, {"mcc": 0.10066648332033858, "macro_f1": 0.207042499100072}, {"mcc": 0.06017718301923774, "macro_f1": 0.1755241410908556}, {"mcc": 0.05448936194505428, "macro_f1": 0.1835406017140968}, {"mcc": 0.09651144173091389, "macro_f1": 0.22585423197492163}, {"mcc": 0.1055549670980885, "macro_f1": 0.2140661799462089}, {"mcc": 0.059873228184869544, "macro_f1": 0.2080766571432299}, {"mcc": 0.09796435076842569, "macro_f1": 0.2150107718349491}], "total": {"test_mcc": 8.329306311710408, "test_mcc_se": 1.8046063974272357, "test_macro_f1": 20.507797552461003, "test_macro_f1_se": 1.0691408379918899}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "sst5", "task": "sentiment-classification", "dataset_languages": ["en"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.6847682209517968, "macro_f1": 0.6776160585161123}, {"mcc": 0.6814592183999655, "macro_f1": 0.6403193941927396}, {"mcc": 0.6994286986604374, "macro_f1": 0.6951700333268986}, {"mcc": 0.6823876916981937, "macro_f1": 0.6534164682880917}, {"mcc": 0.5695706345806417, "macro_f1": 0.6595284142918855}, {"mcc": 0.676343135940726, "macro_f1": 0.6890480730695502}, {"mcc": 0.6934115830983743, "macro_f1": 0.7087148232107761}, {"mcc": 0.6883471551672252, "macro_f1": 0.6735403104871511}, {"mcc": 0.686570942580337, "macro_f1": 0.6303205966221448}, {"mcc": 0.7022157869670581, "macro_f1": 0.7163003152493664}], "total": {"test_mcc": 67.64503068044756, "test_mcc_se": 2.3805299733604115, "test_macro_f1": 67.43974487254715, "test_macro_f1_se": 1.7719840588772056}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "fosent", "task": "sentiment-classification", "dataset_languages": ["fo"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.5841117964600929, "macro_f1": 0.721995804361434}, {"mcc": 0.5543114176673339, "macro_f1": 0.6366196412948382}, {"mcc": 0.5952787831278507, "macro_f1": 0.678036152907088}, {"mcc": 0.48388808417699064, "macro_f1": 0.6500473709647766}, {"mcc": 0.24382058860111985, "macro_f1": 0.46678678678678676}, {"mcc": 0.5985472182127968, "macro_f1": 0.7280345951308224}, {"mcc": 0.5029416751678346, "macro_f1": 0.6779599603079354}, {"mcc": 0.613969711697406, "macro_f1": 0.7033260142321388}, {"mcc": 0.5804109469591603, "macro_f1": 0.6856719571339395}, {"mcc": 0.4874460354988527, "macro_f1": 0.6206582224294364}], "total": {"test_mcc": 52.447262575694396, "test_mcc_se": 6.800123161140401, "test_macro_f1": 65.69136505549196, "test_macro_f1_se": 4.66965645396835}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "allocine", "task": "sentiment-classification", "dataset_languages": ["fr"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.948757097535919, "macro_f1": 0.9740933668909009}, {"mcc": 0.9513852390910416, "macro_f1": 0.9755799755799756}, {"mcc": 0.9533677342493229, "macro_f1": 0.9764112369127285}, {"mcc": 0.9570394534687304, "macro_f1": 0.9785047807970444}, {"mcc": 0.9579580970434123, "macro_f1": 0.9789506644246295}, {"mcc": 0.9599084761119315, "macro_f1": 0.9799521294057958}, {"mcc": 0.9397724993682326, "macro_f1": 0.9692366309742855}, {"mcc": 0.9354952877612495, "macro_f1": 0.9672802806318748}, {"mcc": 0.9511619042484105, "macro_f1": 0.9754871685860405}, {"mcc": 0.9656523751325686, "macro_f1": 0.9828240575336564}], "total": {"test_mcc": 95.20498164010819, "test_mcc_se": 0.5639845962825254, "test_macro_f1": 97.58320291736933, "test_macro_f1_se": 0.2942943913082178}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "suc3", "task": "named-entity-recognition", "dataset_languages": ["sv"], "model": "google/gemma-2-27b", "results": {"raw": [{"micro_f1_no_misc": 0.284326710816777, "micro_f1": 0.22676745220097821}, {"micro_f1_no_misc": 0.3453631795340338, "micro_f1": 0.372857642532354}, {"micro_f1_no_misc": 0.36628849270664504, "micro_f1": 0.342053629823414}, {"micro_f1_no_misc": 0.33373253493013977, "micro_f1": 0.336935791481246}, {"micro_f1_no_misc": 0.3680468423253868, "micro_f1": 0.37055677886127714}, {"micro_f1_no_misc": 0.3593251253989968, "micro_f1": 0.3369464230411892}, {"micro_f1_no_misc": 0.3169042769857434, "micro_f1": 0.32383720930232557}, {"micro_f1_no_misc": 0.3271812080536913, "micro_f1": 0.2641044592264104}, {"micro_f1_no_misc": 0.33906840117498954, "micro_f1": 0.34749034749034746}, {"micro_f1_no_misc": 0.3389391979301423, "micro_f1": 0.3406900839291265}], "total": {"test_micro_f1_no_misc": 33.79175969856546, "test_micro_f1_no_misc_se": 1.5564003942930398, "test_micro_f1": 32.622398178886684, "test_micro_f1_se": 2.8506638152940433}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "dansk", "task": "named-entity-recognition", "dataset_languages": ["da"], "model": "google/gemma-2-27b", "results": {"raw": [{"micro_f1_no_misc": 0.4742268041237113, "micro_f1": 0.39062499999999994}, {"micro_f1_no_misc": 0.4597855227882038, "micro_f1": 0.34368530020703936}, {"micro_f1_no_misc": 0.4041095890410959, "micro_f1": 0.35093896713615025}, {"micro_f1_no_misc": 0.45920090039392236, "micro_f1": 0.37381489841986454}, {"micro_f1_no_misc": 0.3707936507936508, "micro_f1": 0.25949542556140837}, {"micro_f1_no_misc": 0.3955773955773956, "micro_f1": 0.33659158521036975}, {"micro_f1_no_misc": 0.4197247706422018, "micro_f1": 0.3368134362233318}, {"micro_f1_no_misc": 0.4317460317460317, "micro_f1": 0.372568093385214}, {"micro_f1_no_misc": 0.4351676154332701, "micro_f1": 0.3563068920676203}, {"micro_f1_no_misc": 0.3994691439946914, "micro_f1": 0.3351206434316354}], "total": {"test_micro_f1_no_misc": 42.49801424534175, "test_micro_f1_no_misc_se": 2.052753964133163, "test_micro_f1": 34.55960241642634, "test_micro_f1_se": 2.203931381069997}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "norne-nb", "task": "named-entity-recognition", "dataset_languages": ["nb", "no"], "model": "google/gemma-2-27b", "results": {"raw": [{"micro_f1_no_misc": 0.4455549710678591, "micro_f1": 0.4677996422182468}, {"micro_f1_no_misc": 0.46033350176857, "micro_f1": 0.45370776656428263}, {"micro_f1_no_misc": 0.4069934078532531, "micro_f1": 0.4264382960035134}, {"micro_f1_no_misc": 0.4340376927470017, "micro_f1": 0.451071761416589}, {"micro_f1_no_misc": 0.41527313266443705, "micro_f1": 0.4163069544364509}, {"micro_f1_no_misc": 0.4716699801192843, "micro_f1": 0.45825789579561454}, {"micro_f1_no_misc": 0.4217252396166134, "micro_f1": 0.4100747737111374}, {"micro_f1_no_misc": 0.41927960611557397, "micro_f1": 0.4298327137546468}, {"micro_f1_no_misc": 0.41759958344181203, "micro_f1": 0.41676704408576243}, {"micro_f1_no_misc": 0.4614201486798257, "micro_f1": 0.4871695178849145}], "total": {"test_micro_f1_no_misc": 43.53887264074231, "test_micro_f1_no_misc_se": 1.415633798852599, "test_micro_f1": 44.17426365871158, "test_micro_f1_se": 1.5862950242566052}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "norne-nn", "task": "named-entity-recognition", "dataset_languages": ["nn"], "model": "google/gemma-2-27b", "results": {"raw": [{"micro_f1_no_misc": 0.3764058985253686, "micro_f1": 0.4080130825838103}, {"micro_f1_no_misc": 0.38985970957420624, "micro_f1": 0.37489102005231034}, {"micro_f1_no_misc": 0.3930323846908735, "micro_f1": 0.40169559951554296}, {"micro_f1_no_misc": 0.3952702702702703, "micro_f1": 0.40991088893718763}, {"micro_f1_no_misc": 0.34334763948497854, "micro_f1": 0.37298387096774194}, {"micro_f1_no_misc": 0.3325170068027211, "micro_f1": 0.3175675675675676}, {"micro_f1_no_misc": 0.42382534409112477, "micro_f1": 0.43309712380158394}, {"micro_f1_no_misc": 0.39346895074946464, "micro_f1": 0.38915041470522305}, {"micro_f1_no_misc": 0.4127571065403282, "micro_f1": 0.41625788367203925}, {"micro_f1_no_misc": 0.3754712239256094, "micro_f1": 0.3932734569080833}], "total": {"test_micro_f1_no_misc": 38.359555346549456, "test_micro_f1_no_misc_se": 1.7503380443561687, "test_micro_f1": 39.16840908711091, "test_micro_f1_se": 1.976471350357059}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "mim-gold-ner", "task": "named-entity-recognition", "dataset_languages": ["is"], "model": "google/gemma-2-27b", "results": {"raw": [{"micro_f1_no_misc": 0.4144247281053234, "micro_f1": 0.42549371633752237}, {"micro_f1_no_misc": 0.34805653710247353, "micro_f1": 0.301036354663596}, {"micro_f1_no_misc": 0.36292270531400966, "micro_f1": 0.33026073215696605}, {"micro_f1_no_misc": 0.38498402555910544, "micro_f1": 0.40601210228381807}, {"micro_f1_no_misc": 0.36363636363636365, "micro_f1": 0.4072783584978707}, {"micro_f1_no_misc": 0.35941530524505594, "micro_f1": 0.40613328091212897}, {"micro_f1_no_misc": 0.3675115207373272, "micro_f1": 0.43255904743314466}, {"micro_f1_no_misc": 0.3860890067786619, "micro_f1": 0.39656853187755897}, {"micro_f1_no_misc": 0.3685817374410214, "micro_f1": 0.3814986123959297}, {"micro_f1_no_misc": 0.3331222292590247, "micro_f1": 0.3524886877828055}], "total": {"test_micro_f1_no_misc": 36.88744159178366, "test_micro_f1_no_misc_se": 1.3854015366298174, "test_micro_f1": 38.393294243413415, "test_micro_f1_se": 2.655783883443117}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "fone", "task": "named-entity-recognition", "dataset_languages": ["fo"], "model": "google/gemma-2-27b", "results": {"raw": [{"micro_f1_no_misc": 0.5010222748305176, "micro_f1": 0.4954601733388362}, {"micro_f1_no_misc": 0.4314063848144953, "micro_f1": 0.42606934350053616}, {"micro_f1_no_misc": 0.4181500872600349, "micro_f1": 0.40888005301524194}, {"micro_f1_no_misc": 0.3276525563496427, "micro_f1": 0.33597185576077393}, {"micro_f1_no_misc": 0.45329436122274996, "micro_f1": 0.4569062010297739}, {"micro_f1_no_misc": 0.44874464176362516, "micro_f1": 0.44862385321100917}, {"micro_f1_no_misc": 0.4509399636143117, "micro_f1": 0.4677055566946894}, {"micro_f1_no_misc": 0.5110132158590308, "micro_f1": 0.5034718395238621}, {"micro_f1_no_misc": 0.4652312036710201, "micro_f1": 0.4700308505949758}, {"micro_f1_no_misc": 0.4764523069567296, "micro_f1": 0.472689559806407}], "total": {"test_micro_f1_no_misc": 44.83906996342157, "test_micro_f1_no_misc_se": 3.1721368207629905, "test_micro_f1": 44.85809286476107, "test_micro_f1_se": 3.0259137823708215}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "germeval", "task": "named-entity-recognition", "dataset_languages": ["de"], "model": "google/gemma-2-27b", "results": {"raw": [{"micro_f1_no_misc": 0.4056037358238826, "micro_f1": 0.39630562552476906}, {"micro_f1_no_misc": 0.4287573194534808, "micro_f1": 0.39976169198689304}, {"micro_f1_no_misc": 0.41032342371773933, "micro_f1": 0.40405177265053466}, {"micro_f1_no_misc": 0.3467166979362101, "micro_f1": 0.3175803402646503}, {"micro_f1_no_misc": 0.34660766961651923, "micro_f1": 0.35294117647058826}, {"micro_f1_no_misc": 0.39653448850383205, "micro_f1": 0.3837916063675832}, {"micro_f1_no_misc": 0.38844184345281646, "micro_f1": 0.3878490334458423}, {"micro_f1_no_misc": 0.39837097371343944, "micro_f1": 0.397006182883176}, {"micro_f1_no_misc": 0.39973127309371853, "micro_f1": 0.3845924156464617}, {"micro_f1_no_misc": 0.44412607449856734, "micro_f1": 0.4278874925194494}], "total": {"test_micro_f1_no_misc": 39.65213499810206, "test_micro_f1_no_misc_se": 1.919203409980505, "test_micro_f1": 38.51767337759948, "test_micro_f1_se": 1.879374386628491}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "conll-nl", "task": "named-entity-recognition", "dataset_languages": ["nl"], "model": "google/gemma-2-27b", "results": {"raw": [{"micro_f1_no_misc": 0.41469489414694893, "micro_f1": 0.4397221087045362}, {"micro_f1_no_misc": 0.3879742304939156, "micro_f1": 0.45222929936305734}, {"micro_f1_no_misc": 0.42637644046094747, "micro_f1": 0.42590178183398525}, {"micro_f1_no_misc": 0.4225941422594142, "micro_f1": 0.4422158548233047}, {"micro_f1_no_misc": 0.4071122011036174, "micro_f1": 0.4375}, {"micro_f1_no_misc": 0.4619289340101523, "micro_f1": 0.4533236469306211}, {"micro_f1_no_misc": 0.4233378561736771, "micro_f1": 0.4857715430861723}, {"micro_f1_no_misc": 0.40958904109589045, "micro_f1": 0.4185201997276441}, {"micro_f1_no_misc": 0.4250720461095101, "micro_f1": 0.4357197844495766}, {"micro_f1_no_misc": 0.4272266473569877, "micro_f1": 0.42272508425613864}], "total": {"test_micro_f1_no_misc": 42.059064332110616, "test_micro_f1_no_misc_se": 1.1722381026996957, "test_micro_f1": 44.13629303175036, "test_micro_f1_se": 1.20320791782663}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "conll-en", "task": "named-entity-recognition", "dataset_languages": ["en"], "model": "google/gemma-2-27b", "results": {"raw": [{"micro_f1_no_misc": 0.3576803895528995, "micro_f1": 0.3472423324996709}, {"micro_f1_no_misc": 0.32369430510475067, "micro_f1": 0.3477321814254859}, {"micro_f1_no_misc": 0.4403100775193798, "micro_f1": 0.44891500904159126}, {"micro_f1_no_misc": 0.413852073535699, "micro_f1": 0.3989180289309655}, {"micro_f1_no_misc": 0.3497638724911452, "micro_f1": 0.36840144081480564}, {"micro_f1_no_misc": 0.3083504991285058, "micro_f1": 0.3288854003139718}, {"micro_f1_no_misc": 0.32996811902231665, "micro_f1": 0.39785055257021185}, {"micro_f1_no_misc": 0.3752975773701163, "micro_f1": 0.38042959427207634}, {"micro_f1_no_misc": 0.34683809239049185, "micro_f1": 0.3315137797896419}, {"micro_f1_no_misc": 0.3905469277515192, "micro_f1": 0.43146303093864713}], "total": {"test_micro_f1_no_misc": 36.36301933866824, "test_micro_f1_no_misc_se": 2.576833550878028, "test_micro_f1": 37.81351350597068, "test_micro_f1_se": 2.5515224435670394}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "eltec", "task": "named-entity-recognition", "dataset_languages": ["fr"], "model": "google/gemma-2-27b", "results": {"raw": [{"micro_f1_no_misc": 0.20842255531763026, "micro_f1": 0.21713578719455615}, {"micro_f1_no_misc": 0.2519946808510638, "micro_f1": 0.27928717499269645}, {"micro_f1_no_misc": 0.2576644850155012, "micro_f1": 0.27299703264094954}, {"micro_f1_no_misc": 0.27566718995290423, "micro_f1": 0.2884874629280129}, {"micro_f1_no_misc": 0.22749529190207157, "micro_f1": 0.27534883720930226}, {"micro_f1_no_misc": 0.2702341137123746, "micro_f1": 0.3033123460169724}, {"micro_f1_no_misc": 0.2524831784684396, "micro_f1": 0.2496984318455971}, {"micro_f1_no_misc": 0.25858884373845586, "micro_f1": 0.25552825552825553}, {"micro_f1_no_misc": 0.2320866829779797, "micro_f1": 0.24445851804939836}, {"micro_f1_no_misc": 0.25416036308623297, "micro_f1": 0.2718562874251497}], "total": {"test_micro_f1_no_misc": 24.887973850226537, "test_micro_f1_no_misc_se": 1.2688253053258116, "test_micro_f1": 26.581101338308898, "test_micro_f1_se": 1.5282819408667598}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "scala-sv", "task": "linguistic-acceptability", "dataset_languages": ["sv"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.6136574977248771, "macro_f1": 0.8031852316069111}, {"mcc": 0.5178365960384506, "macro_f1": 0.7316475253140111}, {"mcc": 0.6180916456535562, "macro_f1": 0.8051014375729961}, {"mcc": 0.5255111694393064, "macro_f1": 0.7493721534073979}, {"mcc": 0.5805649058577371, "macro_f1": 0.7900382615595305}, {"mcc": 0.5765842742780068, "macro_f1": 0.7839895508644039}, {"mcc": 0.5933378934913761, "macro_f1": 0.7831529820414356}, {"mcc": 0.5970962349725194, "macro_f1": 0.7915476699393011}, {"mcc": 0.6135376105448795, "macro_f1": 0.8067130052739614}, {"mcc": 0.5965325136062954, "macro_f1": 0.7883756346037548}], "total": {"test_mcc": 58.327503416070044, "test_mcc_se": 2.1855075056681676, "test_macro_f1": 78.33123452183703, "test_macro_f1_se": 1.5150653095639468}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "scala-da", "task": "linguistic-acceptability", "dataset_languages": ["da"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.5262468242692778, "macro_f1": 0.7540429195105454}, {"mcc": 0.5352694492346387, "macro_f1": 0.7563700362620196}, {"mcc": 0.5127422094919126, "macro_f1": 0.7563406270064992}, {"mcc": 0.39126685808362366, "macro_f1": 0.6094857867096316}, {"mcc": 0.5760878314255683, "macro_f1": 0.7875550589095457}, {"mcc": 0.520900382140464, "macro_f1": 0.7448846821228338}, {"mcc": 0.5729116402750438, "macro_f1": 0.784893662479487}, {"mcc": 0.5203015486023449, "macro_f1": 0.7600547672855666}, {"mcc": 0.5747811393651758, "macro_f1": 0.7844884902045006}, {"mcc": 0.5580321429923912, "macro_f1": 0.7680042622200818}], "total": {"test_mcc": 52.88540025880442, "test_mcc_se": 3.363680044925256, "test_macro_f1": 75.0612029271071, "test_macro_f1_se": 3.2077568101165936}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "scala-nb", "task": "linguistic-acceptability", "dataset_languages": ["nb", "no"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.5435146634553638, "macro_f1": 0.7571100587799802}, {"mcc": 0.5619955980485901, "macro_f1": 0.7807428474826672}, {"mcc": 0.6258765307111844, "macro_f1": 0.8112208005578401}, {"mcc": 0.5790304261091516, "macro_f1": 0.784594337848344}, {"mcc": 0.5315772635745035, "macro_f1": 0.7599721396233492}, {"mcc": 0.6365504554696011, "macro_f1": 0.8102157677862699}, {"mcc": 0.6041348193360764, "macro_f1": 0.7892407576865881}, {"mcc": 0.5986060008019667, "macro_f1": 0.7853500457361757}, {"mcc": 0.5345783909258085, "macro_f1": 0.7405317619061613}, {"mcc": 0.5538180506540739, "macro_f1": 0.776827321446668}], "total": {"test_mcc": 57.69682199086319, "test_mcc_se": 2.345550628526927, "test_macro_f1": 77.95805838854044, "test_macro_f1_se": 1.3881632370372834}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "scala-nn", "task": "linguistic-acceptability", "dataset_languages": ["nn"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.24498409371932717, "macro_f1": 0.5275276425856441}, {"mcc": 0.4548104643202496, "macro_f1": 0.7198653198653199}, {"mcc": 0.486683709293487, "macro_f1": 0.7429983777077965}, {"mcc": 0.4358841456408208, "macro_f1": 0.7145821699765658}, {"mcc": 0.41480558141757284, "macro_f1": 0.6996271204319363}, {"mcc": 0.4531220851942693, "macro_f1": 0.7265108194579916}, {"mcc": 0.5389909393628765, "macro_f1": 0.7692299612206565}, {"mcc": 0.46887521920528186, "macro_f1": 0.7336504252510927}, {"mcc": 0.48163235414991024, "macro_f1": 0.740574149735812}, {"mcc": 0.44413818349687295, "macro_f1": 0.717901132683487}], "total": {"test_mcc": 44.239267758006676, "test_mcc_se": 4.782289465285913, "test_macro_f1": 70.924671189163, "test_macro_f1_se": 4.129079163710705}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "scala-is", "task": "linguistic-acceptability", "dataset_languages": ["is"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.17739097206216448, "macro_f1": 0.5882681220210775}, {"mcc": 0.1532696754500139, "macro_f1": 0.5704681380792769}, {"mcc": 0.12926486693008982, "macro_f1": 0.46394652711400786}, {"mcc": 0.17832640384686216, "macro_f1": 0.5149449732318322}, {"mcc": 0.158712915695457, "macro_f1": 0.5626552774811844}, {"mcc": 0.17876311162450118, "macro_f1": 0.5840249967057871}, {"mcc": 0.15617707465861805, "macro_f1": 0.5722507580914661}, {"mcc": 0.08945203998375058, "macro_f1": 0.4655052469485459}, {"mcc": 0.17366702790451874, "macro_f1": 0.5631753204044398}, {"mcc": 0.15396973982283707, "macro_f1": 0.5764369122874468}], "total": {"test_mcc": 15.489938279788129, "test_mcc_se": 1.7207090627653108, "test_macro_f1": 54.616762723650645, "test_macro_f1_se": 2.936033296893749}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "scala-fo", "task": "linguistic-acceptability", "dataset_languages": ["fo"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.11647719416539959, "macro_f1": 0.5161606986472671}, {"mcc": 0.17132433023738755, "macro_f1": 0.5808891348459694}, {"mcc": 0.1067968181664586, "macro_f1": 0.42546619817287423}, {"mcc": 0.09407200416394607, "macro_f1": 0.542283679148703}, {"mcc": 0.1300627676841231, "macro_f1": 0.46264663816885354}, {"mcc": 0.16615812863926666, "macro_f1": 0.5829183164798812}, {"mcc": 0.1346427759447004, "macro_f1": 0.5672555304805301}, {"mcc": 0.13489831320819273, "macro_f1": 0.5213632975459491}, {"mcc": 0.13926655738816382, "macro_f1": 0.5408636664494266}, {"mcc": 0.08325692151520378, "macro_f1": 0.5377151665258868}], "total": {"test_mcc": 12.76955811112842, "test_mcc_se": 1.7640507093537459, "test_macro_f1": 52.77562326465342, "test_macro_f1_se": 3.118667603039316}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "scala-de", "task": "linguistic-acceptability", "dataset_languages": ["de"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.43236853676670767, "macro_f1": 0.7151945192072424}, {"mcc": 0.20998930102894098, "macro_f1": 0.4720646170911373}, {"mcc": 0.4854475772807506, "macro_f1": 0.741468546181359}, {"mcc": 0.38516442718061994, "macro_f1": 0.6246014288880932}, {"mcc": 0.431144340560667, "macro_f1": 0.6871055499563644}, {"mcc": 0.4767270367788726, "macro_f1": 0.7372924111634702}, {"mcc": 0.2709263898792134, "macro_f1": 0.630262975458705}, {"mcc": 0.4417693106339714, "macro_f1": 0.7183201907207042}, {"mcc": 0.3712300899208149, "macro_f1": 0.6369697317494628}, {"mcc": 0.45522452942013697, "macro_f1": 0.7119567482714787}], "total": {"test_mcc": 39.59991539450696, "test_mcc_se": 5.606964353612752, "test_macro_f1": 66.75236718688016, "test_macro_f1_se": 5.057092449043712}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "scala-nl", "task": "linguistic-acceptability", "dataset_languages": ["nl"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.45991122111295024, "macro_f1": 0.6979226642607657}, {"mcc": 0.4873638225039595, "macro_f1": 0.7436420143843725}, {"mcc": 0.5230008756204279, "macro_f1": 0.7507409188945955}, {"mcc": 0.5431055080185248, "macro_f1": 0.7715215611572129}, {"mcc": 0.49233548708128894, "macro_f1": 0.7285578461791278}, {"mcc": 0.553578940741535, "macro_f1": 0.7713485167427511}, {"mcc": 0.5043121016459232, "macro_f1": 0.7443585260844436}, {"mcc": 0.46497375067267316, "macro_f1": 0.7086109524716036}, {"mcc": 0.5563991160408179, "macro_f1": 0.7762215825611135}, {"mcc": 0.4335228723533392, "macro_f1": 0.6919834536810144}], "total": {"test_mcc": 50.1850369579144, "test_mcc_se": 2.604712951799199, "test_macro_f1": 73.84908036417, "test_macro_f1_se": 1.9165596612834936}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "scala-en", "task": "linguistic-acceptability", "dataset_languages": ["en"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.5060029986380232, "macro_f1": 0.7529296875}, {"mcc": 0.4900501203315635, "macro_f1": 0.7235941352810153}, {"mcc": 0.4652918607746765, "macro_f1": 0.7297719032539354}, {"mcc": 0.45546616628229625, "macro_f1": 0.7231671016381165}, {"mcc": 0.4723388926491829, "macro_f1": 0.7354259800565603}, {"mcc": 0.40224694040716985, "macro_f1": 0.6345176003763474}, {"mcc": 0.438346917509429, "macro_f1": 0.7164820488028112}, {"mcc": 0.4968990079628877, "macro_f1": 0.7460662812630781}, {"mcc": 0.46736490347575216, "macro_f1": 0.728813559322034}, {"mcc": 0.4759118319001267, "macro_f1": 0.7376302662664793}], "total": {"test_mcc": 46.69919639931107, "test_mcc_se": 1.8713700483037528, "test_macro_f1": 72.28398563760378, "test_macro_f1_se": 2.0397908462678993}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "scala-fr", "task": "linguistic-acceptability", "dataset_languages": ["fr"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.42845677671857746, "macro_f1": 0.6998867914131177}, {"mcc": 0.4056217823976324, "macro_f1": 0.70135932997922}, {"mcc": 0.48828811318874366, "macro_f1": 0.7360570911958759}, {"mcc": 0.35062651912583714, "macro_f1": 0.6619766258025302}, {"mcc": 0.47180145568786924, "macro_f1": 0.7185415305399812}, {"mcc": 0.4799935862642327, "macro_f1": 0.7399960863485154}, {"mcc": 0.4090382385953634, "macro_f1": 0.6781521670156736}, {"mcc": 0.38766276791777354, "macro_f1": 0.6519653915993175}, {"mcc": 0.39772241209417075, "macro_f1": 0.6920322240157603}, {"mcc": 0.38770580609427185, "macro_f1": 0.6715032756479018}], "total": {"test_mcc": 42.069174580844724, "test_mcc_se": 2.8280426093703035, "test_macro_f1": 69.51470513557894, "test_macro_f1_se": 1.8612451599806032}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "google/gemma-2-27b", "results": {"raw": [{"f1": 68.09878927115764, "em": 62.42424242424242}, {"f1": 69.109189450756, "em": 64.13949962092494}, {"f1": 69.5273133487726, "em": 64.56876456876456}, {"f1": 69.23122499311052, "em": 64.45966514459666}, {"f1": 69.25524103241912, "em": 64.76484194294525}, {"f1": 69.47018093915263, "em": 63.89312977099237}, {"f1": 66.92167022938584, "em": 60.06240249609984}, {"f1": 69.17105575688136, "em": 63.679245283018865}, {"f1": 68.24085925273275, "em": 63.46749226006192}, {"f1": 70.01347346313514, "em": 63.657407407407405}], "total": {"test_f1": 68.90389977375037, "test_f1_se": 0.5582705956158307, "test_em": 63.511669091905425, "test_em_se": 0.8582692272872189}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "norquad", "task": "reading-comprehension", "dataset_languages": ["nb", "nn", "no"], "model": "google/gemma-2-27b", "results": {"raw": [{"f1": 85.98186210749849, "em": 67.3202614379085}, {"f1": 68.20177986080824, "em": 41.300813008130085}, {"f1": 82.09035682795185, "em": 62.11490424646128}, {"f1": 77.67502657114697, "em": 53.80212591986918}, {"f1": 87.08274323828637, "em": 70.30603804797353}, {"f1": 68.40407263212758, "em": 42.63374485596708}, {"f1": 70.48341464295049, "em": 46.890756302521005}, {"f1": 82.0701281114435, "em": 60.810810810810814}, {"f1": 64.97890459270427, "em": 39.285714285714285}, {"f1": 66.41080993297722, "em": 38.57615894039735}], "total": {"test_f1": 75.3379098517895, "test_f1_se": 5.296377639209701, "test_em": 52.304132785575305, "test_em_se": 7.512600666100969}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "scandiqa-sv", "task": "reading-comprehension", "dataset_languages": ["sv"], "model": "google/gemma-2-27b", "results": {"raw": [{"f1": 64.88383838383821, "em": 53.10606060606061}, {"f1": 67.8902474781705, "em": 60.1213040181956}, {"f1": 68.57114207861498, "em": 63.63636363636363}, {"f1": 69.08542102538874, "em": 64.07914764079148}, {"f1": 68.41612625661784, "em": 62.22050886661527}, {"f1": 66.9303889494729, "em": 61.14503816793893}, {"f1": 69.33123240386702, "em": 63.10452418096724}, {"f1": 69.18730945853581, "em": 64.46540880503144}, {"f1": 69.552037356399, "em": 63.85448916408669}, {"f1": 67.47330001959624, "em": 61.72839506172839}], "total": {"test_f1": 68.13210434105011, "test_f1_se": 0.8823067832058559, "test_em": 61.746124014777934, "test_em_se": 2.0719873598582264}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "nqii", "task": "reading-comprehension", "dataset_languages": ["is"], "model": "google/gemma-2-27b", "results": {"raw": [{"f1": 63.983021915225905, "em": 37.53846153846154}, {"f1": 62.836838403785094, "em": 30.685358255451714}, {"f1": 56.49293466962715, "em": 35.82554517133956}, {"f1": 58.56643915350779, "em": 35.426008968609864}, {"f1": 63.30382563245684, "em": 34.1692789968652}, {"f1": 64.94636285757409, "em": 40.523882896764256}, {"f1": 64.68622269438778, "em": 39.23076923076923}, {"f1": 57.86047491297646, "em": 34.66257668711656}, {"f1": 64.07728492637743, "em": 35.993975903614455}, {"f1": 62.61624305753893, "em": 40.06024096385542}], "total": {"test_f1": 61.936964822345736, "test_f1_se": 1.9174792693297833, "test_em": 36.41160986128479, "test_em_se": 1.8686609569574126}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "foqa", "task": "reading-comprehension", "dataset_languages": ["fo"], "model": "google/gemma-2-27b", "results": {"raw": [{"f1": 73.07336724251073, "em": 54.411764705882355}, {"f1": 72.17487045394343, "em": 54.2997542997543}, {"f1": 76.6638115068961, "em": 56.54320987654321}, {"f1": 71.16980212160787, "em": 52.9126213592233}, {"f1": 78.12199424530854, "em": 59.9009900990099}, {"f1": 72.23257471306164, "em": 55.06172839506173}, {"f1": 77.23554493410937, "em": 58.9622641509434}, {"f1": 77.41921158798256, "em": 56.23409669211196}, {"f1": 79.11254464153163, "em": 59.10224438902743}, {"f1": 76.59743195700563, "em": 57.002457002457}], "total": {"test_f1": 75.38011534039575, "test_f1_se": 1.7940826862766188, "test_em": 56.44311309700146, "test_em_se": 1.4437564867284385}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "germanquad", "task": "reading-comprehension", "dataset_languages": ["de"], "model": "google/gemma-2-27b", "results": {"raw": [{"f1": 72.22778588685048, "em": 44.31818181818182}, {"f1": 67.52773530287753, "em": 37.5284306292646}, {"f1": 72.59380064645688, "em": 39.54933954933955}, {"f1": 75.03174659937707, "em": 46.118721461187214}, {"f1": 75.31951464824651, "em": 45.48959136468774}, {"f1": 73.80203208392517, "em": 43.74045801526717}, {"f1": 63.9697605127213, "em": 36.11544461778471}, {"f1": 69.48044570578159, "em": 41.11635220125786}, {"f1": 70.29568221547653, "em": 42.027863777089784}, {"f1": 59.0735468847628, "em": 33.101851851851855}], "total": {"test_f1": 69.93220504864759, "test_f1_se": 3.213554349414047, "test_em": 40.91062352859123, "test_em_se": 2.6639996220589723}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "squad", "task": "reading-comprehension", "dataset_languages": ["en"], "model": "google/gemma-2-27b", "results": {"raw": [{"f1": 89.9202423859671, "em": 80.68181818181819}, {"f1": 86.93809225383241, "em": 74.67778620166793}, {"f1": 90.7311717914839, "em": 80.88578088578089}, {"f1": 88.78449611246033, "em": 80.2130898021309}, {"f1": 90.01001368285799, "em": 81.26445643793369}, {"f1": 83.85102746233716, "em": 72.36641221374046}, {"f1": 87.81389382405384, "em": 77.61310452418097}, {"f1": 88.8429383405433, "em": 76.17924528301887}, {"f1": 88.50798805898529, "em": 78.25077399380805}, {"f1": 85.31496736784254, "em": 73.14814814814815}], "total": {"test_f1": 88.07148312803639, "test_f1_se": 1.3437936672239528, "test_em": 77.5280615672228, "test_em_se": 2.05244091372156}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "squad-nl", "task": "reading-comprehension", "dataset_languages": ["nl"], "model": "google/gemma-2-27b", "results": {"raw": [{"f1": 85.1454556045912, "em": 71.13636363636364}, {"f1": 82.99251046752899, "em": 70.50796057619408}, {"f1": 78.88541859308569, "em": 67.05516705516706}, {"f1": 80.50481115153292, "em": 69.71080669710807}, {"f1": 82.73136845796115, "em": 70.2390131071704}, {"f1": 84.82344059202606, "em": 69.23664122137404}, {"f1": 80.83112361177045, "em": 68.17472698907956}, {"f1": 82.69801236697433, "em": 71.14779874213836}, {"f1": 81.14098865304652, "em": 70.58823529411765}, {"f1": 84.49667651166438, "em": 71.14197530864197}], "total": {"test_f1": 82.42498060101818, "test_f1_se": 1.2792759473476358, "test_em": 69.89386886273549, "test_em_se": 0.8551286726004047}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "fquad", "task": "reading-comprehension", "dataset_languages": ["fr"], "model": "google/gemma-2-27b", "results": {"raw": [{"f1": 78.11570698399535, "em": 50.15151515151515}, {"f1": 77.05716772882714, "em": 50.18953752843063}, {"f1": 77.6530145329423, "em": 48.64024864024864}, {"f1": 79.05153153540572, "em": 51.67427701674277}, {"f1": 68.03661142060851, "em": 42.17424826522745}, {"f1": 78.33013755432368, "em": 48.3206106870229}, {"f1": 75.49365033632812, "em": 47.113884555382214}, {"f1": 75.35999937135503, "em": 48.742138364779876}, {"f1": 69.5372887429811, "em": 45.20123839009288}, {"f1": 79.56457193913437, "em": 49.151234567901234}], "total": {"test_f1": 75.81996801459012, "test_f1_se": 2.4558547601112557, "test_em": 48.135893316734375, "test_em_se": 1.6997368415817493}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "nordjylland-news", "task": "summarization", "dataset_languages": ["da"], "model": "google/gemma-2-27b", "results": {"raw": [{"bertscore": 0.6914109211356845, "rouge_l": 0.2638140762531713}, {"bertscore": 0.688497734372504, "rouge_l": 0.25546088480672113}, {"bertscore": 0.6811785290919943, "rouge_l": 0.2529756709841002}, {"bertscore": 0.6841835587110836, "rouge_l": 0.2510651634588471}, {"bertscore": 0.6902040303975809, "rouge_l": 0.25697383085523184}, {"bertscore": 0.6872723048145417, "rouge_l": 0.25496074656532786}, {"bertscore": 0.6828474023059243, "rouge_l": 0.24733813768648735}, {"bertscore": 0.6859026234888006, "rouge_l": 0.2525427197945078}, {"bertscore": 0.6813356762286276, "rouge_l": 0.2487123505971125}, {"bertscore": 0.688440425117733, "rouge_l": 0.2593189649311395}], "total": {"test_bertscore": 68.61273205664475, "test_bertscore_se": 0.22523732423322101, "test_rouge_l": 25.43162545932647, "test_rouge_l_se": 0.30618550558263213}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "mlsum", "task": "summarization", "dataset_languages": ["de"], "model": "google/gemma-2-27b", "results": {"raw": [{"bertscore": 0.6893060938746203, "rouge_l": 0.2711396615229166}, {"bertscore": 0.6987101674312726, "rouge_l": 0.29864359655547024}, {"bertscore": 0.6857971623976482, "rouge_l": 0.2693921593304639}, {"bertscore": 0.6742670608364278, "rouge_l": 0.24665573748171576}, {"bertscore": 0.7245472529320978, "rouge_l": 0.3546937972480557}, {"bertscore": 0.6879107743734494, "rouge_l": 0.2652974020836201}, {"bertscore": 0.7259156559157418, "rouge_l": 0.3614271934900139}, {"bertscore": 0.6799370251392247, "rouge_l": 0.25288654402998834}, {"bertscore": 0.7022695730556734, "rouge_l": 0.2896177865993942}, {"bertscore": 0.674856228477438, "rouge_l": 0.2709354386649512}], "total": {"test_bertscore": 69.43516994433594, "test_bertscore_se": 1.1542232096437626, "test_rouge_l": 28.8068931700659, "test_rouge_l_se": 2.4736155984125063}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "rrn", "task": "summarization", "dataset_languages": ["is"], "model": "google/gemma-2-27b", "results": {"raw": [{"bertscore": 0.6879815658321604, "rouge_l": 0.23819826949986955}, {"bertscore": 0.6899768073053565, "rouge_l": 0.239979076032466}, {"bertscore": 0.6875659174402244, "rouge_l": 0.2285667739137753}, {"bertscore": 0.687174214865081, "rouge_l": 0.22712948951821327}, {"bertscore": 0.6911939769051969, "rouge_l": 0.2426565095137797}, {"bertscore": 0.6879938618512824, "rouge_l": 0.23067714288067886}, {"bertscore": 0.6854349803470541, "rouge_l": 0.2329552423075109}, {"bertscore": 0.684594660269795, "rouge_l": 0.2251918123047782}, {"bertscore": 0.6810886208841112, "rouge_l": 0.22294874237233372}, {"bertscore": 0.6850511623779312, "rouge_l": 0.2275067829577094}], "total": {"test_bertscore": 68.68055768078193, "test_bertscore_se": 0.17934206272379682, "test_rouge_l": 23.158098413011146, "test_rouge_l_se": 0.41347622830495273}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "no-sammendrag", "task": "summarization", "dataset_languages": ["nb", "nn", "no"], "model": "google/gemma-2-27b", "results": {"raw": [{"bertscore": 0.6505982079688692, "rouge_l": 0.20965750977188125}, {"bertscore": 0.671793688336038, "rouge_l": 0.22774707409671674}, {"bertscore": 0.6744387940852903, "rouge_l": 0.2273938526838701}, {"bertscore": 0.6749890015198616, "rouge_l": 0.23425874310005945}, {"bertscore": 0.6771143249497982, "rouge_l": 0.2344971003119719}, {"bertscore": 0.6728272701657261, "rouge_l": 0.22694886492202704}, {"bertscore": 0.6671524847188266, "rouge_l": 0.221550258073834}, {"bertscore": 0.6792961711471435, "rouge_l": 0.23737360093607884}, {"bertscore": 0.6765280160470866, "rouge_l": 0.23477018255008705}, {"bertscore": 0.6610018952778773, "rouge_l": 0.2076813961600807}], "total": {"test_bertscore": 67.05739854216517, "test_bertscore_se": 0.5457036380960631, "test_rouge_l": 22.618785826066073, "test_rouge_l_se": 0.645006925519106}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "wiki-lingua-nl", "task": "summarization", "dataset_languages": ["nl"], "model": "google/gemma-2-27b", "results": {"raw": [{"bertscore": 0.6981750343111344, "rouge_l": 0.2504015100416663}, {"bertscore": 0.692262413300341, "rouge_l": 0.24467657083277172}, {"bertscore": 0.6215145843307255, "rouge_l": 0.12870241141786792}, {"bertscore": 0.7074905960907927, "rouge_l": 0.2717815926547331}, {"bertscore": 0.6806105926953023, "rouge_l": 0.24512770610965046}, {"bertscore": 0.7109394700528355, "rouge_l": 0.2762599979446468}, {"bertscore": 0.7020284024983994, "rouge_l": 0.2781048416740881}, {"bertscore": 0.6991536672139773, "rouge_l": 0.25948292272304424}, {"bertscore": 0.7071058459987398, "rouge_l": 0.2725615488209076}, {"bertscore": 0.7142025157081662, "rouge_l": 0.2819512245931849}], "total": {"test_bertscore": 69.33483122200414, "test_bertscore_se": 1.6771051476592396, "test_rouge_l": 25.09050326812561, "test_rouge_l_se": 2.7972198647032056}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "swedn", "task": "summarization", "dataset_languages": ["sv"], "model": "google/gemma-2-27b", "results": {"raw": [{"bertscore": 0.6527858569315867, "rouge_l": 0.20092490045226724}, {"bertscore": 0.6587524818314705, "rouge_l": 0.20216088450747433}, {"bertscore": 0.6636457208369393, "rouge_l": 0.21600591979434547}, {"bertscore": 0.6591321577288909, "rouge_l": 0.20560505902338883}, {"bertscore": 0.6524768076633336, "rouge_l": 0.19161013729902876}, {"bertscore": 0.6571995092090219, "rouge_l": 0.20130341464161372}, {"bertscore": 0.6548364073678385, "rouge_l": 0.19863925182173786}, {"bertscore": 0.652790978536359, "rouge_l": 0.20082461491833725}, {"bertscore": 0.6573834995506331, "rouge_l": 0.20200673305717937}, {"bertscore": 0.6645323445700342, "rouge_l": 0.2121638591517221}], "total": {"test_bertscore": 65.73535764226108, "test_bertscore_se": 0.2678436299021872, "test_rouge_l": 20.31244774667095, "test_rouge_l_se": 0.42447272394429403}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "cnn-dailymail", "task": "summarization", "dataset_languages": ["en"], "model": "google/gemma-2-27b", "results": {"raw": [{"bertscore": 0.7076651815732475, "rouge_l": 0.25131815772701704}, {"bertscore": 0.7121976205235114, "rouge_l": 0.2547724087655059}, {"bertscore": 0.7110024012217764, "rouge_l": 0.25841021456878677}, {"bertscore": 0.7027608577045612, "rouge_l": 0.21107716981015137}, {"bertscore": 0.7023837671149522, "rouge_l": 0.23467462260368446}, {"bertscore": 0.7124608486628858, "rouge_l": 0.24976820838913502}, {"bertscore": 0.7151108389371075, "rouge_l": 0.27518220382460473}, {"bertscore": 0.701162491881405, "rouge_l": 0.24807799064871225}, {"bertscore": 0.6980374225822743, "rouge_l": 0.262131719171728}, {"bertscore": 0.7117219398787711, "rouge_l": 0.257003322547007}], "total": {"test_bertscore": 70.74503370080492, "test_bertscore_se": 0.3655377685115501, "test_rouge_l": 25.024160180563328, "test_rouge_l_se": 1.0692796912312599}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "orange-sum", "task": "summarization", "dataset_languages": ["fr"], "model": "google/gemma-2-27b", "results": {"raw": [{"bertscore": 0.6845296713872813, "rouge_l": 0.24784712512271873}, {"bertscore": 0.6868047617608681, "rouge_l": 0.25365746313455084}, {"bertscore": 0.6823254424380139, "rouge_l": 0.25129306783090477}, {"bertscore": 0.6532364241138566, "rouge_l": 0.20721772388925908}, {"bertscore": 0.6569756612880155, "rouge_l": 0.20609467390701003}, {"bertscore": 0.6871373634785414, "rouge_l": 0.2528891005550984}, {"bertscore": 0.6891959132626653, "rouge_l": 0.26447866424601285}, {"bertscore": 0.6481773045379668, "rouge_l": 0.19630234530184348}, {"bertscore": 0.6859735267935321, "rouge_l": 0.24997153028719343}, {"bertscore": 0.6805291574564762, "rouge_l": 0.24677944683043698}], "total": {"test_bertscore": 67.54885226517217, "test_bertscore_se": 0.9906694422759784, "test_rouge_l": 23.765311411050284, "test_rouge_l_se": 1.5132839368523217}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "danske-talemaader", "task": "knowledge", "dataset_languages": ["da"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.781414474249808, "accuracy": 0.8353960396039604}, {"mcc": 0.7942551181609534, "accuracy": 0.8452970297029703}, {"mcc": 0.8038248967571499, "accuracy": 0.8514851485148515}, {"mcc": 0.7738602020622104, "accuracy": 0.8292079207920792}, {"mcc": 0.8108198861067493, "accuracy": 0.8576732673267327}, {"mcc": 0.7893736440249467, "accuracy": 0.8415841584158416}, {"mcc": 0.7837340388996874, "accuracy": 0.8378712871287128}, {"mcc": 0.7871696999509374, "accuracy": 0.8403465346534653}, {"mcc": 0.805336642881032, "accuracy": 0.8539603960396039}, {"mcc": 0.8095328951760975, "accuracy": 0.8576732673267327}], "total": {"test_mcc": 79.39321498269571, "test_mcc_se": 0.7966112283293134, "test_accuracy": 84.50495049504948, "test_accuracy_se": 0.6091786151089317}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "danish-citizen-tests", "task": "knowledge", "dataset_languages": ["da"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.8192107475540604, "accuracy": 0.87890625}, {"mcc": 0.8144129506017691, "accuracy": 0.876953125}, {"mcc": 0.8228247730848067, "accuracy": 0.8828125}, {"mcc": 0.8343269395852184, "accuracy": 0.888671875}, {"mcc": 0.8180920063094488, "accuracy": 0.87890625}, {"mcc": 0.8052415899255901, "accuracy": 0.8671875}, {"mcc": 0.8063694532472612, "accuracy": 0.87109375}, {"mcc": 0.7942740873503787, "accuracy": 0.86328125}, {"mcc": 0.7793503431742165, "accuracy": 0.853515625}, {"mcc": 0.8091746176650105, "accuracy": 0.873046875}], "total": {"test_mcc": 81.03277508497759, "test_mcc_se": 0.9575744083162993, "test_accuracy": 87.34375, "test_accuracy_se": 0.6323813260154474}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "mmlu-no", "task": "knowledge", "dataset_languages": ["nb", "nn", "no"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.5659383613296001, "accuracy": 0.671875}, {"mcc": 0.5484771383963872, "accuracy": 0.658203125}, {"mcc": 0.5638361069225698, "accuracy": 0.671875}, {"mcc": 0.5584545657992388, "accuracy": 0.66796875}, {"mcc": 0.5443116963059905, "accuracy": 0.65771484375}, {"mcc": 0.5797504250771127, "accuracy": 0.68359375}, {"mcc": 0.5639712898557426, "accuracy": 0.6708984375}, {"mcc": 0.5930273677008188, "accuracy": 0.6923828125}, {"mcc": 0.5704275023882228, "accuracy": 0.67626953125}, {"mcc": 0.5682019661818944, "accuracy": 0.6748046875}], "total": {"test_mcc": 56.56396419957579, "test_mcc_se": 0.8730925102494023, "test_accuracy": 67.255859375, "test_accuracy_se": 0.6483371690703664}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "mmlu-sv", "task": "knowledge", "dataset_languages": ["sv"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.5285417500124778, "accuracy": 0.64404296875}, {"mcc": 0.5626634567913943, "accuracy": 0.67041015625}, {"mcc": 0.5503148026809981, "accuracy": 0.6591796875}, {"mcc": 0.5569348130461601, "accuracy": 0.6640625}, {"mcc": 0.5656536198664651, "accuracy": 0.669921875}, {"mcc": 0.5462227929422251, "accuracy": 0.65625}, {"mcc": 0.55107903241737, "accuracy": 0.662109375}, {"mcc": 0.5464034637873433, "accuracy": 0.65673828125}, {"mcc": 0.5543102908012764, "accuracy": 0.6630859375}, {"mcc": 0.5684536371652512, "accuracy": 0.6748046875}], "total": {"test_mcc": 55.3057765951096, "test_mcc_se": 0.7180810592907657, "test_accuracy": 66.2060546875, "test_accuracy_se": 0.5442751515291717}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "mmlu-de", "task": "knowledge", "dataset_languages": ["de"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.5588533453964007, "accuracy": 0.66455078125}, {"mcc": 0.600550810173139, "accuracy": 0.70068359375}, {"mcc": 0.5796471829731543, "accuracy": 0.68017578125}, {"mcc": 0.5778078566200551, "accuracy": 0.68115234375}, {"mcc": 0.5924400917600045, "accuracy": 0.693359375}, {"mcc": 0.570834918006485, "accuracy": 0.67529296875}, {"mcc": 0.5936073112752748, "accuracy": 0.6943359375}, {"mcc": 0.5827137408758843, "accuracy": 0.68505859375}, {"mcc": 0.5890882837484847, "accuracy": 0.6904296875}, {"mcc": 0.6018190081956372, "accuracy": 0.7001953125}], "total": {"test_mcc": 58.47362549024518, "test_mcc_se": 0.8366926560184359, "test_accuracy": 68.65234375, "test_accuracy_se": 0.7129008513345669}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "mmlu-nl", "task": "knowledge", "dataset_languages": ["nl"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.5660122332283122, "accuracy": 0.67431640625}, {"mcc": 0.5843245239705064, "accuracy": 0.68701171875}, {"mcc": 0.5726560295118419, "accuracy": 0.67626953125}, {"mcc": 0.5847469096436196, "accuracy": 0.68505859375}, {"mcc": 0.5386357922581168, "accuracy": 0.65283203125}, {"mcc": 0.5626442190895713, "accuracy": 0.6708984375}, {"mcc": 0.5663299346375439, "accuracy": 0.6708984375}, {"mcc": 0.5644331379834817, "accuracy": 0.671875}, {"mcc": 0.5643703962544698, "accuracy": 0.67236328125}, {"mcc": 0.5770988774681572, "accuracy": 0.68017578125}], "total": {"test_mcc": 56.81252054045621, "test_mcc_se": 0.8198847697207603, "test_accuracy": 67.4169921875, "test_accuracy_se": 0.5877155494720214}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "mmlu", "task": "knowledge", "dataset_languages": ["en"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.6561390409134715, "accuracy": 0.740234375}, {"mcc": 0.6475695046528867, "accuracy": 0.7353515625}, {"mcc": 0.6639921700623168, "accuracy": 0.74755859375}, {"mcc": 0.6612451179790326, "accuracy": 0.7451171875}, {"mcc": 0.6638037992627182, "accuracy": 0.74658203125}, {"mcc": 0.6192030066238033, "accuracy": 0.71337890625}, {"mcc": 0.6612648101569266, "accuracy": 0.7451171875}, {"mcc": 0.6572175364228144, "accuracy": 0.7412109375}, {"mcc": 0.6478046780473367, "accuracy": 0.734375}, {"mcc": 0.6647366491372323, "accuracy": 0.748046875}], "total": {"test_mcc": 65.42976313258538, "test_mcc_se": 0.8571148210313408, "test_accuracy": 73.9697265625, "test_accuracy_se": 0.6468049286317405}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "mmlu-fr", "task": "knowledge", "dataset_languages": ["fr"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.5700150399249407, "accuracy": 0.67431640625}, {"mcc": 0.569864052807564, "accuracy": 0.671875}, {"mcc": 0.602175156042804, "accuracy": 0.6962890625}, {"mcc": 0.5715740017200664, "accuracy": 0.67578125}, {"mcc": 0.5699230261360598, "accuracy": 0.67578125}, {"mcc": 0.5863931089104006, "accuracy": 0.68798828125}, {"mcc": 0.5958250824228578, "accuracy": 0.69384765625}, {"mcc": 0.5719173509391513, "accuracy": 0.6767578125}, {"mcc": 0.6139256441882776, "accuracy": 0.708984375}, {"mcc": 0.5873452413123375, "accuracy": 0.6845703125}], "total": {"test_mcc": 58.38957704404459, "test_mcc_se": 0.9857505737760273, "test_accuracy": 68.4619140625, "test_accuracy_se": 0.7486148872665571}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "arc-is", "task": "knowledge", "dataset_languages": ["is"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.6053424771311569, "accuracy": 0.703125}, {"mcc": 0.6469051514818237, "accuracy": 0.734375}, {"mcc": 0.6206659852505318, "accuracy": 0.71484375}, {"mcc": 0.6216355160829823, "accuracy": 0.7158203125}, {"mcc": 0.6174992537665578, "accuracy": 0.712890625}, {"mcc": 0.6420274395185901, "accuracy": 0.7314453125}, {"mcc": 0.6202687616009965, "accuracy": 0.7138671875}, {"mcc": 0.5942166563408766, "accuracy": 0.6943359375}, {"mcc": 0.6282680069169243, "accuracy": 0.720703125}, {"mcc": 0.5963676214355011, "accuracy": 0.6962890625}], "total": {"test_mcc": 61.93196869525941, "test_mcc_se": 1.0775810854525238, "test_accuracy": 71.376953125, "test_accuracy_se": 0.8232446871460997}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "hellaswag-da", "task": "common-sense-reasoning", "dataset_languages": ["da"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.42701821311199323, "accuracy": 0.53125}, {"mcc": 0.32639365838506096, "accuracy": 0.462890625}, {"mcc": 0.47465163702595337, "accuracy": 0.56787109375}, {"mcc": 0.3751927332743013, "accuracy": 0.498046875}, {"mcc": 0.3704295858291233, "accuracy": 0.4697265625}, {"mcc": 0.48296198540579977, "accuracy": 0.58544921875}, {"mcc": 0.27521517948641055, "accuracy": 0.40234375}, {"mcc": 0.41343994838000914, "accuracy": 0.53271484375}, {"mcc": 0.43744496812561146, "accuracy": 0.55126953125}, {"mcc": 0.5451679731004523, "accuracy": 0.64697265625}], "total": {"test_mcc": 41.27915882124715, "test_mcc_se": 4.9188992253862205, "test_accuracy": 52.48535156250001, "test_accuracy_se": 4.325914168964832}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "hellaswag-no", "task": "common-sense-reasoning", "dataset_languages": ["nb", "nn", "no"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.30904318374955336, "accuracy": 0.41943359375}, {"mcc": 0.45852261781084996, "accuracy": 0.56884765625}, {"mcc": 0.5616562101812028, "accuracy": 0.65478515625}, {"mcc": 0.4387188874232957, "accuracy": 0.5458984375}, {"mcc": 0.524360729728598, "accuracy": 0.61474609375}, {"mcc": 0.5185176180094158, "accuracy": 0.626953125}, {"mcc": 0.574902414878558, "accuracy": 0.67138671875}, {"mcc": 0.5083326932881538, "accuracy": 0.60888671875}, {"mcc": 0.49279979100873716, "accuracy": 0.60107421875}, {"mcc": 0.5088912523628029, "accuracy": 0.6162109375}], "total": {"test_mcc": 48.95745398441168, "test_mcc_se": 4.6840806104159425, "test_accuracy": 59.28222656249999, "test_accuracy_se": 4.400712564837844}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "hellaswag-sv", "task": "common-sense-reasoning", "dataset_languages": ["sv"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.36196631564699533, "accuracy": 0.47119140625}, {"mcc": 0.4053663032140043, "accuracy": 0.53515625}, {"mcc": 0.4784893318546974, "accuracy": 0.56689453125}, {"mcc": 0.39795433173884637, "accuracy": 0.50146484375}, {"mcc": 0.3256604088832404, "accuracy": 0.455078125}, {"mcc": 0.325120980769123, "accuracy": 0.4326171875}, {"mcc": 0.4475007952232275, "accuracy": 0.56396484375}, {"mcc": 0.4068113233776207, "accuracy": 0.51513671875}, {"mcc": 0.3715263652311982, "accuracy": 0.486328125}, {"mcc": 0.45566426484795985, "accuracy": 0.5732421875}], "total": {"test_mcc": 39.760604207869136, "test_mcc_se": 3.2619611227972682, "test_accuracy": 51.0107421875, "test_accuracy_se": 3.0610391820298073}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "winogrande-is", "task": "common-sense-reasoning", "dataset_languages": ["is"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.275839613108884, "accuracy": 0.6439732142857143}, {"mcc": 0.2921599146164813, "accuracy": 0.6484375}, {"mcc": 0.3052448487465901, "accuracy": 0.6640625}, {"mcc": 0.25219179734248176, "accuracy": 0.6339285714285714}, {"mcc": 0.28693488443228327, "accuracy": 0.6618303571428571}, {"mcc": 0.30962821394458845, "accuracy": 0.6595982142857143}, {"mcc": 0.2536100300601099, "accuracy": 0.6417410714285714}, {"mcc": 0.22389351174450906, "accuracy": 0.6160714285714286}, {"mcc": 0.32932226447851565, "accuracy": 0.6819196428571429}, {"mcc": 0.2394710565942927, "accuracy": 0.6227678571428571}], "total": {"test_mcc": 27.682961350687364, "test_mcc_se": 2.0989019051630966, "test_accuracy": 64.74330357142857, "test_accuracy_se": 1.2466191257925043}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "hellaswag-de", "task": "common-sense-reasoning", "dataset_languages": ["de"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.45867029203909654, "accuracy": 0.56396484375}, {"mcc": 0.5535788225700011, "accuracy": 0.65185546875}, {"mcc": 0.4529875962351594, "accuracy": 0.5517578125}, {"mcc": 0.4267375694628184, "accuracy": 0.51708984375}, {"mcc": 0.4433318623413815, "accuracy": 0.5478515625}, {"mcc": 0.3956485752443047, "accuracy": 0.51806640625}, {"mcc": 0.502171268705557, "accuracy": 0.61083984375}, {"mcc": 0.513773751925767, "accuracy": 0.6083984375}, {"mcc": 0.5328525733129476, "accuracy": 0.62744140625}, {"mcc": 0.32253061334095406, "accuracy": 0.4619140625}], "total": {"test_mcc": 46.02282925177988, "test_mcc_se": 4.2945474459272415, "test_accuracy": 56.591796875, "test_accuracy_se": 3.638453305279367}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "hellaswag-nl", "task": "common-sense-reasoning", "dataset_languages": ["nl"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.4393036573446841, "accuracy": 0.53466796875}, {"mcc": 0.5204896176718701, "accuracy": 0.62353515625}, {"mcc": 0.4008583910677845, "accuracy": 0.50732421875}, {"mcc": 0.3842126129828885, "accuracy": 0.49072265625}, {"mcc": 0.4966878340673179, "accuracy": 0.5927734375}, {"mcc": 0.3969950232025268, "accuracy": 0.5078125}, {"mcc": 0.3639956156353648, "accuracy": 0.478515625}, {"mcc": 0.3931696476109205, "accuracy": 0.51025390625}, {"mcc": 0.5623280821977786, "accuracy": 0.65869140625}, {"mcc": 0.438576506547495, "accuracy": 0.55322265625}], "total": {"test_mcc": 43.96616988328631, "test_mcc_se": 4.086394490173901, "test_accuracy": 54.5751953125, "test_accuracy_se": 3.748584158468636}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "hellaswag", "task": "common-sense-reasoning", "dataset_languages": ["en"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.47107408508672144, "accuracy": 0.57568359375}, {"mcc": 0.6614842627892367, "accuracy": 0.740234375}, {"mcc": 0.4897191913765277, "accuracy": 0.58349609375}, {"mcc": 0.5606053160319674, "accuracy": 0.64697265625}, {"mcc": 0.6158892133395679, "accuracy": 0.69921875}, {"mcc": 0.6330357969872611, "accuracy": 0.7138671875}, {"mcc": 0.6057813058804496, "accuracy": 0.6884765625}, {"mcc": 0.6206432990627246, "accuracy": 0.69921875}, {"mcc": 0.6361143113433113, "accuracy": 0.7158203125}, {"mcc": 0.5023041598922575, "accuracy": 0.6083984375}], "total": {"test_mcc": 57.96650941790025, "test_mcc_se": 4.265873496473553, "test_accuracy": 66.7138671875, "test_accuracy_se": 3.674220390985477}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "hellaswag-fr", "task": "common-sense-reasoning", "dataset_languages": ["fr"], "model": "google/gemma-2-27b", "results": {"raw": [{"mcc": 0.5438764443992644, "accuracy": 0.6357421875}, {"mcc": 0.41880305339458435, "accuracy": 0.5400390625}, {"mcc": 0.46790091592443334, "accuracy": 0.5673828125}, {"mcc": 0.47301092507873466, "accuracy": 0.583984375}, {"mcc": 0.4757649554214015, "accuracy": 0.5751953125}, {"mcc": 0.5409915284771891, "accuracy": 0.6435546875}, {"mcc": 0.4798799619541158, "accuracy": 0.58740234375}, {"mcc": 0.4793117734536004, "accuracy": 0.587890625}, {"mcc": 0.5832733862065216, "accuracy": 0.67724609375}, {"mcc": 0.5608520015198476, "accuracy": 0.6591796875}], "total": {"test_mcc": 50.23664945829693, "test_mcc_se": 3.1979134592762817, "test_accuracy": 60.57617187499999, "test_accuracy_se": 2.7832337991730722}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "google/gemma-2-27b", "results": {"raw": [{"test_speed": 291.81, "test_speed_short": 33.36}, {"test_speed": 561.18, "test_speed_short": 270.15000000000003}, {"test_speed": 829.23, "test_speed_short": 118.32000000000001}, {"test_speed": 1118.3799999999999, "test_speed_short": 147.96}, {"test_speed": 1389.96, "test_speed_short": 178.01999999999998}, {"test_speed": 1658.74, "test_speed_short": 232.56}, {"test_speed": 1924.72, "test_speed_short": 261.76}, {"test_speed": 2193.51, "test_speed_short": 285.41999999999996}, {"test_speed": 2467.21, "test_speed_short": 274.56}, {"test_speed": 2754.9300000000003, "test_speed_short": 351.9}], "total": {"test_speed": 1518.967, "test_speed_se": 512.026202967615, "test_speed_short": 215.401, "test_speed_short_se": 58.794460811760146}}, "num_model_parameters": 27227128320, "max_sequence_length": 4096, "vocabulary_size": 256000, "merge": false, "generative": true, "generative_type": "base", "few_shot": true, "validation_split": false, "scandeval_version": "15.1.0"}
