
{"dataset": "swerec", "task": "sentiment-classification", "dataset_languages": ["sv"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"mcc": 0.4813134866270237, "macro_f1": 0.5634731936165807}, {"mcc": 0.5592933867450048, "macro_f1": 0.6268946481906637}, {"mcc": 0.6573988378184388, "macro_f1": 0.7115088954655707}, {"mcc": 0.59009725234873, "macro_f1": 0.6574454688916583}, {"mcc": 0.649733042368775, "macro_f1": 0.7082546392606807}, {"mcc": 0.6439234064474625, "macro_f1": 0.6854759054811845}, {"mcc": 0.5477462126079645, "macro_f1": 0.6156046613537448}, {"mcc": 0.4120910571294445, "macro_f1": 0.4804763988433088}, {"mcc": 0.6187956308834788, "macro_f1": 0.6768951454340616}, {"mcc": 0.5968131581442241, "macro_f1": 0.6643832518897047}], "total": {"test_mcc": 57.57205471120545, "test_mcc_se": 4.8911760373227136, "test_macro_f1": 63.90412208427158, "test_macro_f1_se": 4.438929404154443}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "angry-tweets", "task": "sentiment-classification", "dataset_languages": ["da"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"mcc": 0.23812712547081108, "macro_f1": 0.3569458114218347}, {"mcc": 0.25523205009452216, "macro_f1": 0.3990433118561676}, {"mcc": 0.24028024413548016, "macro_f1": 0.3610009625878292}, {"mcc": 0.3336271699993825, "macro_f1": 0.4915461267938317}, {"mcc": 0.3086276555272795, "macro_f1": 0.44616140527770504}, {"mcc": 0.2655268224108754, "macro_f1": 0.3960948896486461}, {"mcc": 0.12571738701777102, "macro_f1": 0.2477711902360531}, {"mcc": 0.1119422445583073, "macro_f1": 0.21295964122424604}, {"mcc": 0.3200377929329005, "macro_f1": 0.42541031909260485}, {"mcc": 0.28337399363424504, "macro_f1": 0.42747663840557465}], "total": {"test_mcc": 24.82492485781575, "test_mcc_se": 4.685323743787802, "test_macro_f1": 37.64410296544493, "test_macro_f1_se": 5.383975170302959}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "norec", "task": "sentiment-classification", "dataset_languages": ["nb", "nn", "no"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"mcc": 0.10359891648566283, "macro_f1": 0.24056642383563887}, {"mcc": 0.0967198784735981, "macro_f1": 0.23749995072124386}, {"mcc": 0.1622900352589694, "macro_f1": 0.26529923451635357}, {"mcc": 0.16371995265367434, "macro_f1": 0.2577855862372294}, {"mcc": 0.0795909879925074, "macro_f1": 0.16826475885649086}, {"mcc": 0.05057113972913788, "macro_f1": 0.20409618694504916}, {"mcc": 0.09092057880997964, "macro_f1": 0.18817883578945896}, {"mcc": 0.11493492068113317, "macro_f1": 0.19809152345541195}, {"mcc": 0.20459640280007776, "macro_f1": 0.2990868649885068}, {"mcc": 0.10435331468650623, "macro_f1": 0.2693243295853737}], "total": {"test_mcc": 11.712961275712468, "test_mcc_se": 2.8593573895102886, "test_macro_f1": 23.28193694930757, "test_macro_f1_se": 2.586679498557799}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "hotter-and-colder-sentiment", "task": "sentiment-classification", "dataset_languages": ["is"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"mcc": 0.029098991306551657, "macro_f1": 0.1705635931276118}, {"mcc": 0.012633665604936785, "macro_f1": 0.1562190598409036}, {"mcc": 0.00682290755797274, "macro_f1": 0.160192090841366}, {"mcc": 0.01564394077317023, "macro_f1": 0.1715498197883967}, {"mcc": 0.007675004731716842, "macro_f1": 0.14881962893367026}, {"mcc": 0.02850047763475771, "macro_f1": 0.17380959130799215}, {"mcc": 0.0, "macro_f1": 0.16315707377944158}, {"mcc": -0.02441634614456138, "macro_f1": 0.15775358889414734}, {"mcc": 0.060881419515347875, "macro_f1": 0.2113472654819841}, {"mcc": 0.08816837864318255, "macro_f1": 0.18977298089275832}], "total": {"test_mcc": 2.25008439623075, "test_mcc_se": 1.9797527704815157, "test_macro_f1": 17.031846928882718, "test_macro_f1_se": 1.1406801875707115}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "sb10k", "task": "sentiment-classification", "dataset_languages": ["de"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"mcc": 0.2872731938568314, "macro_f1": 0.504181546632145}, {"mcc": 0.3491069581213146, "macro_f1": 0.5298473541015288}, {"mcc": 0.4459403685881379, "macro_f1": 0.6028979069224828}, {"mcc": 0.407652329083421, "macro_f1": 0.5895069342805178}, {"mcc": 0.4254983110876629, "macro_f1": 0.5764333871001387}, {"mcc": 0.4630840458767808, "macro_f1": 0.6308129203191034}, {"mcc": 0.36824615239264663, "macro_f1": 0.5154313886601759}, {"mcc": 0.4044865503537014, "macro_f1": 0.5796442552981788}, {"mcc": 0.3877272741946255, "macro_f1": 0.5092737103702016}, {"mcc": 0.44831910304045186, "macro_f1": 0.591080824623528}], "total": {"test_mcc": 39.87334286595574, "test_mcc_se": 3.3052481573725743, "test_macro_f1": 56.29110228308001, "test_macro_f1_se": 2.7600521721587805}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "dutch-social", "task": "sentiment-classification", "dataset_languages": ["nl"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"mcc": 0.10538508112640195, "macro_f1": 0.24178329954503242}, {"mcc": 0.04741537716287074, "macro_f1": 0.21205707491082046}, {"mcc": 0.05255732567509136, "macro_f1": 0.21493120284065692}, {"mcc": 0.10412819675928915, "macro_f1": 0.23182272379944746}, {"mcc": 0.04463884825661399, "macro_f1": 0.19941029464838989}, {"mcc": 0.04031801631018269, "macro_f1": 0.20544903978295195}, {"mcc": 0.0440525581994442, "macro_f1": 0.09440513686645059}, {"mcc": 0.04786637997337019, "macro_f1": 0.2154114199730736}, {"mcc": 0.028847594642617033, "macro_f1": 0.21008962559503694}, {"mcc": 0.056805868636755415, "macro_f1": 0.22294793248262015}], "total": {"test_mcc": 5.720152467426367, "test_mcc_se": 1.6199664169203523, "test_macro_f1": 20.4830775044448, "test_macro_f1_se": 2.5252592461952963}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "sst5", "task": "sentiment-classification", "dataset_languages": ["en"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"mcc": 0.45506086392191447, "macro_f1": 0.5786204247708128}, {"mcc": 0.3689090656005358, "macro_f1": 0.4731323292873346}, {"mcc": 0.38567153953106886, "macro_f1": 0.4735863004222434}, {"mcc": 0.47252561396452464, "macro_f1": 0.602618296690283}, {"mcc": 0.3708136801633499, "macro_f1": 0.4917279285367537}, {"mcc": 0.34891092580893535, "macro_f1": 0.4503307859278706}, {"mcc": 0.2786373012906798, "macro_f1": 0.3833991005588198}, {"mcc": 0.3884035445132253, "macro_f1": 0.4944447567616909}, {"mcc": 0.41917831295585034, "macro_f1": 0.5388131106688178}, {"mcc": 0.49858883282930555, "macro_f1": 0.6167194395596917}], "total": {"test_mcc": 39.866996805793896, "test_mcc_se": 4.019928059485347, "test_macro_f1": 51.033924731843186, "test_macro_f1_se": 4.543096722503432}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "fosent", "task": "sentiment-classification", "dataset_languages": ["fo"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"mcc": -0.002802607923855782, "macro_f1": 0.2655149051490515}, {"mcc": 0.0588756250602438, "macro_f1": 0.16658422890717509}, {"mcc": 0.0, "macro_f1": 0.14404432132963987}, {"mcc": -0.10149360045768623, "macro_f1": 0.16235793680529534}, {"mcc": 0.03261974053985181, "macro_f1": 0.21132208157524615}, {"mcc": 0.0, "macro_f1": 0.19851116625310172}, {"mcc": 0.01816940367860105, "macro_f1": 0.26994316839662624}, {"mcc": 0.058083720770610685, "macro_f1": 0.15575641802171178}, {"mcc": 0.12021425263681522, "macro_f1": 0.2904111286674722}, {"mcc": 0.02022044481806214, "macro_f1": 0.14938353636028054}], "total": {"test_mcc": 2.0388697912264266, "test_mcc_se": 3.5332216526709286, "test_macro_f1": 20.138288914656005, "test_macro_f1_se": 3.435676298727535}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "suc3", "task": "named-entity-recognition", "dataset_languages": ["sv"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}], "total": {"test_micro_f1_no_misc": 0.0, "test_micro_f1_no_misc_se": 0.0, "test_micro_f1": 0.0, "test_micro_f1_se": 0.0}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "dansk", "task": "named-entity-recognition", "dataset_languages": ["da"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}], "total": {"test_micro_f1_no_misc": 0.0, "test_micro_f1_no_misc_se": 0.0, "test_micro_f1": 0.0, "test_micro_f1_se": 0.0}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "norne-nb", "task": "named-entity-recognition", "dataset_languages": ["nb", "no"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}], "total": {"test_micro_f1_no_misc": 0.0, "test_micro_f1_no_misc_se": 0.0, "test_micro_f1": 0.0, "test_micro_f1_se": 0.0}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "norne-nn", "task": "named-entity-recognition", "dataset_languages": ["nn"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}], "total": {"test_micro_f1_no_misc": 0.0, "test_micro_f1_no_misc_se": 0.0, "test_micro_f1": 0.0, "test_micro_f1_se": 0.0}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "mim-gold-ner", "task": "named-entity-recognition", "dataset_languages": ["is"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}], "total": {"test_micro_f1_no_misc": 0.0, "test_micro_f1_no_misc_se": 0.0, "test_micro_f1": 0.0, "test_micro_f1_se": 0.0}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "fone", "task": "named-entity-recognition", "dataset_languages": ["fo"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}], "total": {"test_micro_f1_no_misc": 0.0, "test_micro_f1_no_misc_se": 0.0, "test_micro_f1": 0.0, "test_micro_f1_se": 0.0}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "germeval", "task": "named-entity-recognition", "dataset_languages": ["de"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}], "total": {"test_micro_f1_no_misc": 0.0, "test_micro_f1_no_misc_se": 0.0, "test_micro_f1": 0.0, "test_micro_f1_se": 0.0}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "conll-nl", "task": "named-entity-recognition", "dataset_languages": ["nl"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}], "total": {"test_micro_f1_no_misc": 0.0, "test_micro_f1_no_misc_se": 0.0, "test_micro_f1": 0.0, "test_micro_f1_se": 0.0}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "conll-en", "task": "named-entity-recognition", "dataset_languages": ["en"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}], "total": {"test_micro_f1_no_misc": 0.0, "test_micro_f1_no_misc_se": 0.0, "test_micro_f1": 0.0, "test_micro_f1_se": 0.0}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "scala-sv", "task": "linguistic-acceptability", "dataset_languages": ["sv"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"mcc": -0.014806971186299252, "macro_f1": 0.34060444760324166}, {"mcc": -0.008415085373645674, "macro_f1": 0.33170112508436583}, {"mcc": -0.021674929109631865, "macro_f1": 0.33743125202199936}, {"mcc": 0.06531863982881707, "macro_f1": 0.3568664613440733}, {"mcc": 0.0, "macro_f1": 0.3298429319371728}, {"mcc": 0.011202509304783243, "macro_f1": 0.34203732160759287}, {"mcc": 0.022059356846603494, "macro_f1": 0.3339812041322862}, {"mcc": 0.01665789093387171, "macro_f1": 0.3771988921326447}, {"mcc": 0.0, "macro_f1": 0.34169077467052394}, {"mcc": 0.0, "macro_f1": 0.3359273670557717}], "total": {"test_mcc": 0.7034141124449872, "test_mcc_se": 1.5211857136425178, "test_macro_f1": 34.272817775896726, "test_macro_f1_se": 0.8844474212937208}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "scala-da", "task": "linguistic-acceptability", "dataset_languages": ["da"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"mcc": 0.05741088375607121, "macro_f1": 0.3404698871320612}, {"mcc": 0.0, "macro_f1": 0.3412672885172081}, {"mcc": 0.04196062344538055, "macro_f1": 0.34128059568168656}, {"mcc": -0.005567367547750033, "macro_f1": 0.34270543111038143}, {"mcc": 0.0, "macro_f1": 0.33571196886149857}, {"mcc": 0.022124078691435844, "macro_f1": 0.3346354166666667}, {"mcc": 0.013329598393380084, "macro_f1": 0.3385521600713616}, {"mcc": 0.013580381590432523, "macro_f1": 0.3391978907716602}, {"mcc": 0.07658812344213561, "macro_f1": 0.41606528364959194}, {"mcc": 0.0, "macro_f1": 0.3397807865892972}], "total": {"test_mcc": 2.1942632177108576, "test_mcc_se": 1.7273506272923265, "test_macro_f1": 34.69666709051413, "test_macro_f1_se": 1.5127625537209284}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "scala-nb", "task": "linguistic-acceptability", "dataset_languages": ["nb", "no"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"mcc": -0.01052567583807492, "macro_f1": 0.3919761358603133}, {"mcc": 0.017617040140494557, "macro_f1": 0.39477188055295104}, {"mcc": 0.009647634407446102, "macro_f1": 0.36240805923048913}, {"mcc": 0.037083267263235055, "macro_f1": 0.3542024261041281}, {"mcc": 0.0015393796624939738, "macro_f1": 0.3475013275119476}, {"mcc": 0.018773175720152926, "macro_f1": 0.40540270611448087}, {"mcc": 0.0, "macro_f1": 0.33571196886149857}, {"mcc": 0.0, "macro_f1": 0.33506493506493507}, {"mcc": 0.0, "macro_f1": 0.3322464949462015}, {"mcc": -0.00018907866885927682, "macro_f1": 0.36266263230169815}], "total": {"test_mcc": 0.739457426868884, "test_mcc_se": 0.8511997648140236, "test_macro_f1": 36.21948566548643, "test_macro_f1_se": 1.6566127864975952}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "scala-nn", "task": "linguistic-acceptability", "dataset_languages": ["nn"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"mcc": 0.030822917481992975, "macro_f1": 0.33757551349174386}, {"mcc": 0.01967052880122283, "macro_f1": 0.4992255300714784}, {"mcc": -0.01857435560458122, "macro_f1": 0.33999190578736205}, {"mcc": 0.014189012771235656, "macro_f1": 0.41958173639311835}, {"mcc": -0.003382592398515504, "macro_f1": 0.48550273256155607}, {"mcc": -0.01814439175406758, "macro_f1": 0.43760131211728515}, {"mcc": -0.0012315089301549206, "macro_f1": 0.33359647851785973}, {"mcc": 0.026640647490867732, "macro_f1": 0.3431052058068825}, {"mcc": 0.0, "macro_f1": 0.33202870189171557}, {"mcc": -0.026116018213685657, "macro_f1": 0.3343296865632691}], "total": {"test_mcc": 0.23874239644314316, "test_mcc_se": 1.2327530566669822, "test_macro_f1": 38.62538803202271, "test_macro_f1_se": 4.190566208546213}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "scala-is", "task": "linguistic-acceptability", "dataset_languages": ["is"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"mcc": -0.018425369541007902, "macro_f1": 0.3254089005472306}, {"mcc": 0.0, "macro_f1": 0.3361426256077796}, {"mcc": 0.020144372827074757, "macro_f1": 0.4987205891530925}, {"mcc": -0.010959586432085383, "macro_f1": 0.3299817673058265}, {"mcc": 0.0, "macro_f1": 0.3263157894736842}, {"mcc": 0.0, "macro_f1": 0.34169077467052394}, {"mcc": -0.022494479464640612, "macro_f1": 0.3291844087782509}, {"mcc": 0.015810469987955995, "macro_f1": 0.4455996078460064}, {"mcc": 0.00029605864466963026, "macro_f1": 0.35277732424957575}, {"mcc": 0.04531504160353856, "macro_f1": 0.3755757746673989}], "total": {"test_mcc": 0.29686507625505043, "test_mcc_se": 1.2373358682139073, "test_macro_f1": 36.61397562299369, "test_macro_f1_se": 3.671173403493814}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "scala-fo", "task": "linguistic-acceptability", "dataset_languages": ["fo"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"mcc": -0.022368186096494266, "macro_f1": 0.3440667067477633}, {"mcc": -0.08326690163284292, "macro_f1": 0.4242998876613519}, {"mcc": 0.022581330471749483, "macro_f1": 0.45979113207643807}, {"mcc": 0.04696794394390119, "macro_f1": 0.4223329124041463}, {"mcc": -0.009425435173792088, "macro_f1": 0.34468295090056744}, {"mcc": 0.032563023158462226, "macro_f1": 0.42659033816856223}, {"mcc": -0.01623412777957737, "macro_f1": 0.38081648349424174}, {"mcc": -0.014009670009611718, "macro_f1": 0.3363309148013902}, {"mcc": 0.03418540898991262, "macro_f1": 0.36150764170651023}, {"mcc": 0.02871528024361972, "macro_f1": 0.3864770247143875}], "total": {"test_mcc": 0.19708666115326878, "test_mcc_se": 2.419362257890879, "test_macro_f1": 38.86895992675359, "test_macro_f1_se": 2.643120448723144}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "scala-de", "task": "linguistic-acceptability", "dataset_languages": ["de"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"mcc": -0.053287610059268235, "macro_f1": 0.47196061997398703}, {"mcc": 0.024914760648710736, "macro_f1": 0.35794854733555553}, {"mcc": -0.025340849097868573, "macro_f1": 0.4095687620682257}, {"mcc": 0.024900552975118295, "macro_f1": 0.5034040251431555}, {"mcc": -0.05240530964296769, "macro_f1": 0.46686057503526707}, {"mcc": -0.014350987706474987, "macro_f1": 0.45790762221295755}, {"mcc": 0.014817049195098382, "macro_f1": 0.4925333103017409}, {"mcc": -0.017808778309742552, "macro_f1": 0.45058967750260626}, {"mcc": -0.03398346740339989, "macro_f1": 0.3393043882110368}, {"mcc": -0.006780431670559629, "macro_f1": 0.46980910944498855}], "total": {"test_mcc": -1.3932507107135415, "test_mcc_se": 1.785785829358741, "test_macro_f1": 44.198866372295214, "test_macro_f1_se": 3.4325040002705594}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "scala-nl", "task": "linguistic-acceptability", "dataset_languages": ["nl"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"mcc": 0.06595342522147357, "macro_f1": 0.5310171799225909}, {"mcc": 0.0053282510082109085, "macro_f1": 0.42613636363636365}, {"mcc": 0.05288530249768424, "macro_f1": 0.5253083895663748}, {"mcc": 0.06469323918188158, "macro_f1": 0.3834808112199777}, {"mcc": 0.052040192004572654, "macro_f1": 0.5214078702993195}, {"mcc": 0.14444206180436003, "macro_f1": 0.4542810636766291}, {"mcc": 0.04972068815038472, "macro_f1": 0.500278453841208}, {"mcc": 0.01960502605252769, "macro_f1": 0.4964364553768527}, {"mcc": 0.06116997177612784, "macro_f1": 0.529105380147709}, {"mcc": 0.16891133156120156, "macro_f1": 0.5801437939442152}], "total": {"test_mcc": 6.847494892584247, "test_mcc_se": 3.1450469635632237, "test_macro_f1": 49.47595761631241, "test_macro_f1_se": 3.589884634773346}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "scala-en", "task": "linguistic-acceptability", "dataset_languages": ["en"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"mcc": 0.05832151809934792, "macro_f1": 0.4387397989910878}, {"mcc": 0.015497823883739137, "macro_f1": 0.35741865620715424}, {"mcc": 0.04405814771040968, "macro_f1": 0.34969323252834944}, {"mcc": 0.07257334265170111, "macro_f1": 0.4396794447772979}, {"mcc": 0.023311027648991442, "macro_f1": 0.3699311367908228}, {"mcc": 0.034667928910417235, "macro_f1": 0.37318840579710144}, {"mcc": 0.09163282964691316, "macro_f1": 0.41761067751321707}, {"mcc": 0.09337100915219623, "macro_f1": 0.4885114885114885}, {"mcc": 0.05090301834836759, "macro_f1": 0.43474218694128586}, {"mcc": 0.014247622386061415, "macro_f1": 0.3783401905327541}], "total": {"test_mcc": 4.985842684381449, "test_mcc_se": 1.806448039278459, "test_macro_f1": 40.47855218590559, "test_macro_f1_se": 2.820232312013049}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"f1": 1.5502505312588637, "em": 0.0}, {"f1": 2.046070133368788, "em": 0.0}, {"f1": 1.5982709573664386, "em": 0.0}, {"f1": 2.178801592902475, "em": 0.0}, {"f1": 1.9621128592384485, "em": 0.0}, {"f1": 1.73931480422917, "em": 0.0}, {"f1": 1.3898529253756278, "em": 0.0}, {"f1": 1.9808935769062705, "em": 0.0}, {"f1": 2.2004559897744693, "em": 0.0}, {"f1": 2.1793302731051143, "em": 0.0}], "total": {"test_f1": 1.8825353643525666, "test_f1_se": 0.18178083189460156, "test_em": 0.0, "test_em_se": 0.0}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "norquad", "task": "reading-comprehension", "dataset_languages": ["nb", "nn", "no"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"f1": 1.605813423438775, "em": 0.0}, {"f1": 1.4185214215823538, "em": 0.0}, {"f1": 1.5231577895619324, "em": 0.0}, {"f1": 1.5743491490260173, "em": 0.0}, {"f1": 1.71698704934241, "em": 0.0}, {"f1": 1.634312410923862, "em": 0.0}, {"f1": 1.6827184738206573, "em": 0.0}, {"f1": 1.4661753434779505, "em": 0.0}, {"f1": 1.559813850969091, "em": 0.0}, {"f1": 1.5263952957245401, "em": 0.0}], "total": {"test_f1": 1.570824420786759, "test_f1_se": 0.05757720043284864, "test_em": 0.0, "test_em_se": 0.0}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "scandiqa-sv", "task": "reading-comprehension", "dataset_languages": ["sv"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"f1": 1.735433428115214, "em": 0.0}, {"f1": 1.7313087408469392, "em": 0.0}, {"f1": 1.7582067398843464, "em": 0.0}, {"f1": 1.6883749949351832, "em": 0.0}, {"f1": 1.497639354814082, "em": 0.0}, {"f1": 1.8723303642172435, "em": 0.0}, {"f1": 1.7614693334918223, "em": 0.0}, {"f1": 1.6973154137213606, "em": 0.0}, {"f1": 1.691682216868088, "em": 0.0}, {"f1": 1.622840038807345, "em": 0.0}], "total": {"test_f1": 1.7056600625701623, "test_f1_se": 0.06055053136118036, "test_em": 0.0, "test_em_se": 0.0}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "nqii", "task": "reading-comprehension", "dataset_languages": ["is"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"f1": 1.5102978292542653, "em": 0.0}, {"f1": 2.0061528330483505, "em": 0.0}, {"f1": 1.6306433184675786, "em": 0.0}, {"f1": 1.9732360594615952, "em": 0.0}, {"f1": 2.1884237383896052, "em": 0.0}, {"f1": 2.1037060047324676, "em": 0.0}, {"f1": 1.650768934111527, "em": 0.0}, {"f1": 1.5975291906997489, "em": 0.0}, {"f1": 2.4857930640549086, "em": 0.0}, {"f1": 1.5922695846502883, "em": 0.0}], "total": {"test_f1": 1.8738820556870337, "test_f1_se": 0.20141026972784895, "test_em": 0.0, "test_em_se": 0.0}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "foqa", "task": "reading-comprehension", "dataset_languages": ["fo"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"f1": 0.5409548663961085, "em": 0.0}, {"f1": 0.6257167633255202, "em": 0.0}, {"f1": 0.7142302153259638, "em": 0.0}, {"f1": 0.5951863184438089, "em": 0.0}, {"f1": 0.8684274343164782, "em": 0.0}, {"f1": 0.7849637792261329, "em": 0.0}, {"f1": 0.5807252869783985, "em": 0.0}, {"f1": 0.9375225379322568, "em": 0.0}, {"f1": 0.6107351214878279, "em": 0.0}, {"f1": 0.6190838507501031, "em": 0.0}], "total": {"test_f1": 0.6877546174182598, "test_f1_se": 0.0829625207456311, "test_em": 0.0, "test_em_se": 0.0}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "germanquad", "task": "reading-comprehension", "dataset_languages": ["de"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"f1": 1.5046481460223315, "em": 0.0}, {"f1": 1.7110651145441664, "em": 0.0}, {"f1": 1.8885030954418274, "em": 0.0}, {"f1": 1.6632367125684617, "em": 0.0}, {"f1": 1.6152185848986043, "em": 0.0}, {"f1": 1.9687902996524378, "em": 0.0}, {"f1": 1.877584987349343, "em": 0.0}, {"f1": 1.8486126177634243, "em": 0.0}, {"f1": 1.6798445354592213, "em": 0.0}, {"f1": 1.6666567162582373, "em": 0.0}], "total": {"test_f1": 1.7424160809958056, "test_f1_se": 0.09047069028490627, "test_em": 0.0, "test_em_se": 0.0}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "squad", "task": "reading-comprehension", "dataset_languages": ["en"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"f1": 3.604128463992603, "em": 0.0}, {"f1": 3.5512022120883486, "em": 0.0}, {"f1": 3.6517891584442626, "em": 0.0}, {"f1": 3.545121014305522, "em": 0.0}, {"f1": 3.388475026770318, "em": 0.0}, {"f1": 3.6063346414652724, "em": 0.0}, {"f1": 3.659286333684283, "em": 0.0}, {"f1": 3.7205916757408484, "em": 0.0}, {"f1": 3.5473197543583876, "em": 0.0}, {"f1": 3.4506822176816967, "em": 0.0}], "total": {"test_f1": 3.572493049853154, "test_f1_se": 0.06144006457726249, "test_em": 0.0, "test_em_se": 0.0}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "squad-nl", "task": "reading-comprehension", "dataset_languages": ["nl"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"f1": 1.340765340762473, "em": 0.0}, {"f1": 1.6020057872745905, "em": 0.0}, {"f1": 1.5521958534319866, "em": 0.0}, {"f1": 1.511598606892263, "em": 0.0}, {"f1": 1.3200205203281994, "em": 0.0}, {"f1": 1.4321265555931082, "em": 0.0}, {"f1": 1.4746859266215033, "em": 0.0}, {"f1": 1.3783857160152388, "em": 0.0}, {"f1": 1.4641280077085337, "em": 0.0}, {"f1": 1.5371128624065125, "em": 0.0}], "total": {"test_f1": 1.4613025177034407, "test_f1_se": 0.057973627890104275, "test_em": 0.0, "test_em_se": 0.0}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "nordjylland-news", "task": "summarization", "dataset_languages": ["da"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"bertscore": 0.5898588908457896, "rouge_l": 0.05438398437433205}, {"bertscore": 0.5906578055873979, "rouge_l": 0.05572563715156652}, {"bertscore": 0.5862041846412467, "rouge_l": 0.0514839467108813}, {"bertscore": 0.5921220628515584, "rouge_l": 0.05690426553198632}, {"bertscore": 0.5872197127755499, "rouge_l": 0.05452011264834245}, {"bertscore": 0.5880837963486556, "rouge_l": 0.05180546311423778}, {"bertscore": 0.5887568752368679, "rouge_l": 0.05582233399503332}, {"bertscore": 0.5919173180882353, "rouge_l": 0.054418635689313405}, {"bertscore": 0.5913237093191128, "rouge_l": 0.054475483734772734}, {"bertscore": 0.5879044745670399, "rouge_l": 0.05456539454582533}], "total": {"test_bertscore": 58.94048830261453, "test_bertscore_se": 0.1283414714175907, "test_rouge_l": 5.441052574962912, "test_rouge_l_se": 0.103988130413211}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "mlsum", "task": "summarization", "dataset_languages": ["de"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"bertscore": 0.5578755795722827, "rouge_l": 0.015052090106836181}, {"bertscore": 0.5695874905941309, "rouge_l": 0.028522039398735252}, {"bertscore": 0.5710512845980702, "rouge_l": 0.03513647577921069}, {"bertscore": 0.5763919216551585, "rouge_l": 0.041863834593680885}, {"bertscore": 0.5745508731197333, "rouge_l": 0.03985561544681196}, {"bertscore": 0.5761459702334832, "rouge_l": 0.038175350299522945}, {"bertscore": 0.5773043312656227, "rouge_l": 0.03722297443402223}, {"bertscore": 0.5687406603246927, "rouge_l": 0.027307961027053827}, {"bertscore": 0.5580933577148244, "rouge_l": 0.024386471583389597}, {"bertscore": 0.5721893319278024, "rouge_l": 0.036321527610620685}], "total": {"test_bertscore": 57.01930801005801, "test_bertscore_se": 0.4376576034663557, "test_rouge_l": 3.238443402798842, "test_rouge_l_se": 0.5188817716330288}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "rrn", "task": "summarization", "dataset_languages": ["is"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"bertscore": 0.5577034972375259, "rouge_l": 0.03765690534033056}, {"bertscore": 0.5640399896656163, "rouge_l": 0.03663946617379286}, {"bertscore": 0.5545200056221802, "rouge_l": 0.035281383358684526}, {"bertscore": 0.5594368967867922, "rouge_l": 0.03588422393888539}, {"bertscore": 0.5598496955062728, "rouge_l": 0.041076222579407407}, {"bertscore": 0.5588935016421601, "rouge_l": 0.03887981294558567}, {"bertscore": 0.5630846917629242, "rouge_l": 0.039139240388173804}, {"bertscore": 0.566536380094476, "rouge_l": 0.03816543163921075}, {"bertscore": 0.5623358655138873, "rouge_l": 0.038305002536728576}, {"bertscore": 0.5625396455288865, "rouge_l": 0.0403626162716363}], "total": {"test_bertscore": 56.08940169360721, "test_bertscore_se": 0.21603787016240922, "test_rouge_l": 3.8139030517243584, "test_rouge_l_se": 0.11480888722404331}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "no-sammendrag", "task": "summarization", "dataset_languages": ["nb", "nn", "no"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"bertscore": 0.582174952243804, "rouge_l": 0.051210309605931356}, {"bertscore": 0.5770457653416088, "rouge_l": 0.0448118890385393}, {"bertscore": 0.581224986439338, "rouge_l": 0.03712953332493273}, {"bertscore": 0.5770445327361813, "rouge_l": 0.041436578956051434}, {"bertscore": 0.5810878283664351, "rouge_l": 0.04629512059607796}, {"bertscore": 0.5807913265016396, "rouge_l": 0.044238998300038204}, {"bertscore": 0.5828344336187001, "rouge_l": 0.04434619126071501}, {"bertscore": 0.5745681023836369, "rouge_l": 0.03590667252107037}, {"bertscore": 0.5823723556677578, "rouge_l": 0.043266282473702275}, {"bertscore": 0.5822237843385665, "rouge_l": 0.046517956522246065}], "total": {"test_bertscore": 58.01368067637668, "test_bertscore_se": 0.17703828835236315, "test_rouge_l": 4.351595325993047, "test_rouge_l_se": 0.27864808286057635}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "wiki-lingua-nl", "task": "summarization", "dataset_languages": ["nl"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"bertscore": 0.5864755472721299, "rouge_l": 0.014105561033893172}, {"bertscore": 0.5861929040402174, "rouge_l": 0.013983374973465702}, {"bertscore": 0.5812947176600574, "rouge_l": 0.013755155275268958}, {"bertscore": 0.579856445590849, "rouge_l": 0.012427209474737192}, {"bertscore": 0.5758669627539348, "rouge_l": 0.008997969361180259}, {"bertscore": 0.5795654948451556, "rouge_l": 0.013390452096812482}, {"bertscore": 0.5808691820711829, "rouge_l": 0.013522152092238057}, {"bertscore": 0.5865284483734285, "rouge_l": 0.015621420325927667}, {"bertscore": 0.5798720114544267, "rouge_l": 0.013151262449514898}, {"bertscore": 0.5830002060247352, "rouge_l": 0.0159120989402984}], "total": {"test_bertscore": 58.19521920086117, "test_bertscore_se": 0.22014101590325316, "test_rouge_l": 1.3486665602333678, "test_rouge_l_se": 0.11784582452287742}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "swedn", "task": "summarization", "dataset_languages": ["sv"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"bertscore": 0.5833955277048517, "rouge_l": 0.03570625595744155}, {"bertscore": 0.5945195999520365, "rouge_l": 0.036099446887390634}, {"bertscore": 0.5798646577168256, "rouge_l": 0.03384854792430621}, {"bertscore": 0.5964866730500944, "rouge_l": 0.03598499527611784}, {"bertscore": 0.5835941187979188, "rouge_l": 0.02903310385835596}, {"bertscore": 0.5862926683330443, "rouge_l": 0.03239084927121966}, {"bertscore": 0.5813256945402827, "rouge_l": 0.03071532864687994}, {"bertscore": 0.5760267592559103, "rouge_l": 0.037081717021047914}, {"bertscore": 0.5788158599752933, "rouge_l": 0.03339984524384889}, {"bertscore": 0.5790309103758773, "rouge_l": 0.03485586601895059}], "total": {"test_bertscore": 58.39352469702135, "test_bertscore_se": 0.419212087975546, "test_rouge_l": 3.3911595610555914, "test_rouge_l_se": 0.15982266557429428}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "cnn-dailymail", "task": "summarization", "dataset_languages": ["en"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"bertscore": 0.6096985066251364, "rouge_l": 0.13949138436584432}, {"bertscore": 0.6098412343126256, "rouge_l": 0.14007165911172131}, {"bertscore": 0.602669381420128, "rouge_l": 0.1335315373217739}, {"bertscore": 0.6044664368673693, "rouge_l": 0.13647650151438118}, {"bertscore": 0.606574712524889, "rouge_l": 0.13857284218759702}, {"bertscore": 0.6164050110382959, "rouge_l": 0.14335716269029236}, {"bertscore": 0.6095744005287997, "rouge_l": 0.13806369451473136}, {"bertscore": 0.6025101835257374, "rouge_l": 0.1349745440700169}, {"bertscore": 0.6023133554554079, "rouge_l": 0.13124290478490286}, {"bertscore": 0.608463390293764, "rouge_l": 0.1382737406970783}], "total": {"test_bertscore": 60.72516612592154, "test_bertscore_se": 0.2767401980467195, "test_rouge_l": 13.740559712583394, "test_rouge_l_se": 0.21588599795555546}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "danske-talemaader", "task": "knowledge", "dataset_languages": ["da"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"mcc": 0.07690047782692709, "accuracy": 0.22772277227722773}, {"mcc": 0.0, "accuracy": 0.2524752475247525}, {"mcc": 0.0, "accuracy": 0.23267326732673269}, {"mcc": 0.0, "accuracy": 0.1905940594059406}, {"mcc": 0.008264702249740144, "accuracy": 0.20668316831683167}, {"mcc": 0.0, "accuracy": 0.22153465346534654}, {"mcc": -0.0303812047997962, "accuracy": 0.2264851485148515}, {"mcc": 0.058929368178865016, "accuracy": 0.2438118811881188}, {"mcc": 0.013984535106542123, "accuracy": 0.2400990099009901}, {"mcc": 0.0, "accuracy": 0.24133663366336633}], "total": {"test_mcc": 1.2769787856227819, "test_mcc_se": 1.9537364195955593, "test_accuracy": 22.834158415841582, "test_accuracy_se": 1.1487825571885772}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "danish-citizen-tests", "task": "knowledge", "dataset_languages": ["da"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"mcc": 0.03584566226984348, "accuracy": 0.3671875}, {"mcc": 0.013704538476315054, "accuracy": 0.369140625}, {"mcc": 0.07963728139135025, "accuracy": 0.34375}, {"mcc": 0.06374838898214794, "accuracy": 0.3671875}, {"mcc": 0.07300966360359909, "accuracy": 0.36328125}, {"mcc": 0.08775105098571206, "accuracy": 0.396484375}, {"mcc": 0.12183295212701066, "accuracy": 0.388671875}, {"mcc": 0.08939011944553438, "accuracy": 0.40234375}, {"mcc": 0.10337379249192252, "accuracy": 0.404296875}, {"mcc": -0.014334019491405114, "accuracy": 0.33984375}], "total": {"test_mcc": 6.539594302820303, "test_mcc_se": 2.6025728802992707, "test_accuracy": 37.421875, "test_accuracy_se": 1.4246020856472716}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "mmlu-no", "task": "knowledge", "dataset_languages": ["nb", "nn", "no"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"mcc": -0.005276853952590563, "accuracy": 0.22412109375}, {"mcc": 0.009965979923662216, "accuracy": 0.21484375}, {"mcc": 0.0380285123244092, "accuracy": 0.2421875}, {"mcc": 0.038445920896745506, "accuracy": 0.220703125}, {"mcc": 0.07598031658176414, "accuracy": 0.287109375}, {"mcc": 0.0, "accuracy": 0.2294921875}, {"mcc": 0.04520846317698952, "accuracy": 0.26025390625}, {"mcc": 0.02893556249493886, "accuracy": 0.248046875}, {"mcc": -0.002297221389106833, "accuracy": 0.24560546875}, {"mcc": -0.02051893455393019, "accuracy": 0.23388671875}], "total": {"test_mcc": 2.0847174550288186, "test_mcc_se": 1.8229943490826528, "test_accuracy": 24.0625, "test_accuracy_se": 1.3280032368611587}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "mmlu-sv", "task": "knowledge", "dataset_languages": ["sv"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"mcc": 0.03766017537906788, "accuracy": 0.2255859375}, {"mcc": -0.010715431539391028, "accuracy": 0.2138671875}, {"mcc": 0.06695849982116407, "accuracy": 0.2587890625}, {"mcc": -0.02475150861812731, "accuracy": 0.20703125}, {"mcc": 0.0544688316736427, "accuracy": 0.23974609375}, {"mcc": 0.04086567825460553, "accuracy": 0.26123046875}, {"mcc": -0.023248222484147655, "accuracy": 0.22265625}, {"mcc": -0.017062208477758342, "accuracy": 0.2177734375}, {"mcc": 0.05687436464706254, "accuracy": 0.24755859375}, {"mcc": 0.04234366498174047, "accuracy": 0.23974609375}], "total": {"test_mcc": 2.2339384363785886, "test_mcc_se": 2.2758931728245586, "test_accuracy": 23.33984375, "test_accuracy_se": 1.1682926291695765}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "mmlu-de", "task": "knowledge", "dataset_languages": ["de"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"mcc": -0.0026904245861185047, "accuracy": 0.21484375}, {"mcc": -0.0032731166595736613, "accuracy": 0.2216796875}, {"mcc": 0.044838444455057545, "accuracy": 0.26123046875}, {"mcc": -0.00478654769100396, "accuracy": 0.2236328125}, {"mcc": 0.02580158101966951, "accuracy": 0.2529296875}, {"mcc": 0.06235160489147709, "accuracy": 0.26806640625}, {"mcc": 0.019413662747966187, "accuracy": 0.2333984375}, {"mcc": 0.01789441522475801, "accuracy": 0.23779296875}, {"mcc": 0.001756281810200102, "accuracy": 0.228515625}, {"mcc": -0.006027795908337192, "accuracy": 0.22802734375}], "total": {"test_mcc": 1.5527810530409514, "test_mcc_se": 1.4506623809613712, "test_accuracy": 23.701171875, "test_accuracy_se": 1.1080458418769223}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "mmlu-nl", "task": "knowledge", "dataset_languages": ["nl"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"mcc": 0.03117723369856956, "accuracy": 0.2822265625}, {"mcc": 0.01736158050861499, "accuracy": 0.25732421875}, {"mcc": 0.023770088398542832, "accuracy": 0.259765625}, {"mcc": 0.011522292447628587, "accuracy": 0.23583984375}, {"mcc": 0.09111608414144062, "accuracy": 0.31396484375}, {"mcc": -0.005789590816049464, "accuracy": 0.24267578125}, {"mcc": 0.023224451770232923, "accuracy": 0.2587890625}, {"mcc": 0.023285595683727754, "accuracy": 0.25634765625}, {"mcc": 0.030684370680600166, "accuracy": 0.26123046875}, {"mcc": 0.03794796950665684, "accuracy": 0.26171875}], "total": {"test_mcc": 2.843000760199648, "test_mcc_se": 1.558087635392063, "test_accuracy": 26.298828125, "test_accuracy_se": 1.3432725224641515}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "mmlu", "task": "knowledge", "dataset_languages": ["en"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"mcc": 0.12281470352354627, "accuracy": 0.31005859375}, {"mcc": 0.119144219108271, "accuracy": 0.2841796875}, {"mcc": 0.19291084764825636, "accuracy": 0.36376953125}, {"mcc": 0.13801666865239173, "accuracy": 0.3349609375}, {"mcc": 0.13908321564382126, "accuracy": 0.31298828125}, {"mcc": 0.09978740175355748, "accuracy": 0.26220703125}, {"mcc": 0.11402131571737961, "accuracy": 0.3173828125}, {"mcc": 0.17011505418235112, "accuracy": 0.36767578125}, {"mcc": 0.1260687252805125, "accuracy": 0.296875}, {"mcc": 0.08499583064691395, "accuracy": 0.24462890625}], "total": {"test_mcc": 13.069579821570015, "test_mcc_se": 1.969488351981093, "test_accuracy": 30.947265624999996, "test_accuracy_se": 2.470417984891177}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "arc-is", "task": "knowledge", "dataset_languages": ["is"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"mcc": 0.025228160477673737, "accuracy": 0.248046875}, {"mcc": 0.014790209430103392, "accuracy": 0.2060546875}, {"mcc": -0.015302778586322719, "accuracy": 0.2216796875}, {"mcc": 0.010777486719562454, "accuracy": 0.2373046875}, {"mcc": 0.013185679706000007, "accuracy": 0.25390625}, {"mcc": -0.008978065448051855, "accuracy": 0.2255859375}, {"mcc": 0.023455693804196345, "accuracy": 0.248046875}, {"mcc": 0.040746095671232704, "accuracy": 0.263671875}, {"mcc": 0.04373819114162993, "accuracy": 0.244140625}, {"mcc": 0.0, "accuracy": 0.244140625}], "total": {"test_mcc": 1.47640672916024, "test_mcc_se": 1.2053378265135373, "test_accuracy": 23.92578125, "test_accuracy_se": 1.0564977712660373}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "hellaswag-da", "task": "common-sense-reasoning", "dataset_languages": ["da"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"mcc": 0.030168672905764922, "accuracy": 0.27001953125}, {"mcc": 0.027496697527556624, "accuracy": 0.2548828125}, {"mcc": -0.0021490428648291895, "accuracy": 0.25048828125}, {"mcc": -0.006985338968449832, "accuracy": 0.2724609375}, {"mcc": -0.022582533378689942, "accuracy": 0.2333984375}, {"mcc": 0.0038106616497704674, "accuracy": 0.25048828125}, {"mcc": 0.02415908335602093, "accuracy": 0.2705078125}, {"mcc": -0.008893309069633142, "accuracy": 0.25244140625}, {"mcc": 0.04202191373131928, "accuracy": 0.27880859375}, {"mcc": 0.027193956352447087, "accuracy": 0.2646484375}], "total": {"test_mcc": 1.142407612412772, "test_mcc_se": 1.32404928879209, "test_accuracy": 25.9814453125, "test_accuracy_se": 0.8550966958932352}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "hellaswag-no", "task": "common-sense-reasoning", "dataset_languages": ["nb", "nn", "no"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"mcc": -0.009268180737098505, "accuracy": 0.25341796875}, {"mcc": 0.0013412077138068843, "accuracy": 0.27001953125}, {"mcc": -0.028123702040924858, "accuracy": 0.23095703125}, {"mcc": 0.025018932110590607, "accuracy": 0.2685546875}, {"mcc": 0.007924324991116715, "accuracy": 0.25537109375}, {"mcc": 0.03635231284648873, "accuracy": 0.26904296875}, {"mcc": -0.004840925566471121, "accuracy": 0.24462890625}, {"mcc": 0.009340012223657808, "accuracy": 0.25244140625}, {"mcc": 0.009977264559516147, "accuracy": 0.263671875}, {"mcc": -0.021724910859758955, "accuracy": 0.2490234375}], "total": {"test_mcc": 0.25996335240923457, "test_mcc_se": 1.2228822706672884, "test_accuracy": 25.571289062500004, "test_accuracy_se": 0.7750114986974939}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "hellaswag-sv", "task": "common-sense-reasoning", "dataset_languages": ["sv"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"mcc": 0.02990439234223641, "accuracy": 0.26220703125}, {"mcc": 0.0, "accuracy": 0.2568359375}, {"mcc": 0.015699440171461337, "accuracy": 0.25244140625}, {"mcc": 0.0, "accuracy": 0.23193359375}, {"mcc": 0.020535434669251007, "accuracy": 0.2685546875}, {"mcc": 0.022623603664708242, "accuracy": 0.2568359375}, {"mcc": 0.0, "accuracy": 0.2568359375}, {"mcc": 0.021743882375987907, "accuracy": 0.26123046875}, {"mcc": 0.021510857411687404, "accuracy": 0.24951171875}, {"mcc": 0.0, "accuracy": 0.25537109375}], "total": {"test_mcc": 1.3201761063533233, "test_mcc_se": 0.7353774073711252, "test_accuracy": 25.517578125, "test_accuracy_se": 0.6034611373056936}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "winogrande-is", "task": "common-sense-reasoning", "dataset_languages": ["is"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"mcc": 0.0, "accuracy": 0.4486607142857143}, {"mcc": 0.0, "accuracy": 0.5625}, {"mcc": 0.029279206545156995, "accuracy": 0.43526785714285715}, {"mcc": 0.03798794431377352, "accuracy": 0.5647321428571429}, {"mcc": -0.002981695045507849, "accuracy": 0.5792410714285714}, {"mcc": 0.0, "accuracy": 0.5479910714285714}, {"mcc": -0.0282287764075783, "accuracy": 0.5825892857142857}, {"mcc": 0.0038130934624385697, "accuracy": 0.5401785714285714}, {"mcc": 0.019815883954579627, "accuracy": 0.5703125}, {"mcc": -0.0297475410936319, "accuracy": 0.5569196428571429}], "total": {"test_mcc": 0.29938115729230663, "test_mcc_se": 1.3561359203370675, "test_accuracy": 53.88392857142858, "test_accuracy_se": 3.26920474317192}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "hellaswag-de", "task": "common-sense-reasoning", "dataset_languages": ["de"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"mcc": 0.022859910997990663, "accuracy": 0.25927734375}, {"mcc": 0.0023094672160916064, "accuracy": 0.25244140625}, {"mcc": 0.005354052609605999, "accuracy": 0.24365234375}, {"mcc": 0.013560913273891972, "accuracy": 0.26416015625}, {"mcc": -0.023604147535440625, "accuracy": 0.23681640625}, {"mcc": 0.011995563698377358, "accuracy": 0.25732421875}, {"mcc": 0.013321418781622036, "accuracy": 0.26220703125}, {"mcc": -0.0006629502460630046, "accuracy": 0.2490234375}, {"mcc": 0.010241596187290158, "accuracy": 0.267578125}, {"mcc": -0.005248499603828685, "accuracy": 0.2548828125}], "total": {"test_mcc": 0.5012732537953748, "test_mcc_se": 0.8003225636906841, "test_accuracy": 25.473632812500004, "test_accuracy_se": 0.5911685655791193}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "hellaswag-nl", "task": "common-sense-reasoning", "dataset_languages": ["nl"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"mcc": -0.0309753543687089, "accuracy": 0.22998046875}, {"mcc": 0.015114101631925912, "accuracy": 0.24169921875}, {"mcc": -0.019727794231107137, "accuracy": 0.234375}, {"mcc": -0.005185851788645021, "accuracy": 0.23828125}, {"mcc": -4.6047511890948806e-05, "accuracy": 0.23779296875}, {"mcc": 0.0016994865300464838, "accuracy": 0.2607421875}, {"mcc": -0.008528650214723429, "accuracy": 0.24609375}, {"mcc": 0.0030871827675173135, "accuracy": 0.2490234375}, {"mcc": -0.0018649546782305898, "accuracy": 0.25048828125}, {"mcc": -0.008491052200312162, "accuracy": 0.2294921875}], "total": {"test_mcc": -0.5491893406412848, "test_mcc_se": 0.790266989992451, "test_accuracy": 24.1796875, "test_accuracy_se": 0.6127657843475057}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "hellaswag", "task": "common-sense-reasoning", "dataset_languages": ["en"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"mcc": 0.009793410464866769, "accuracy": 0.26025390625}, {"mcc": 0.020508874409509673, "accuracy": 0.25732421875}, {"mcc": -0.013623982689234105, "accuracy": 0.2470703125}, {"mcc": -0.017577753066675892, "accuracy": 0.23388671875}, {"mcc": -0.010722683488976335, "accuracy": 0.2314453125}, {"mcc": -0.0028585354864648312, "accuracy": 0.23681640625}, {"mcc": 0.008224727216784529, "accuracy": 0.25537109375}, {"mcc": 0.064400452089446, "accuracy": 0.27978515625}, {"mcc": -0.01185740310102724, "accuracy": 0.2412109375}, {"mcc": 0.001043315748047135, "accuracy": 0.24072265625}], "total": {"test_mcc": 0.47330422096275704, "test_mcc_se": 1.4993959133424615, "test_accuracy": 24.8388671875, "test_accuracy_se": 0.9235943834005809}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B", "results": {"raw": [{"test_speed": 976.96, "test_speed_short": 70.0}, {"test_speed": 1236.57, "test_speed_short": 128.85}, {"test_speed": 1751.3000000000002, "test_speed_short": 250.56}, {"test_speed": 2425.03, "test_speed_short": 308.88}, {"test_speed": 2864.16, "test_speed_short": 373.66999999999996}, {"test_speed": 3464.8300000000004, "test_speed_short": 489.63}, {"test_speed": 4040.9300000000003, "test_speed_short": 873.6}, {"test_speed": 4605.81, "test_speed_short": 979.0899999999999}, {"test_speed": 5186.820000000001, "test_speed_short": 1079.52}, {"test_speed": 5671.09, "test_speed_short": 727.6}], "total": {"test_speed": 3222.35, "test_speed_se": 1020.777816854826, "test_speed_short": 528.14, "test_speed_short_se": 225.02540246524464}}, "num_model_parameters": 8030261248, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "swerec", "task": "sentiment-classification", "dataset_languages": ["sv"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"mcc": 0.23877790877940874, "macro_f1": 0.25914666715069173}, {"mcc": 0.5046145243726445, "macro_f1": 0.5755673487492629}, {"mcc": 0.26299165567780064, "macro_f1": 0.2908043367347}, {"mcc": 0.4556652104265958, "macro_f1": 0.5240107298548976}, {"mcc": 0.359103061289883, "macro_f1": 0.3935317839610127}, {"mcc": 0.3300135892845705, "macro_f1": 0.3804935241969549}, {"mcc": 0.32389381226455866, "macro_f1": 0.3677338592809791}, {"mcc": 0.06309113709751615, "macro_f1": 0.09346532448182922}, {"mcc": 0.10807729945833948, "macro_f1": 0.12088408264500332}, {"mcc": 0.15922900879347038, "macro_f1": 0.16789609798559857}], "total": {"test_mcc": 28.054572074447883, "test_mcc_se": 8.890456186251761, "test_macro_f1": 31.735337550409305, "test_macro_f1_se": 10.055097839837012}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "angry-tweets", "task": "sentiment-classification", "dataset_languages": ["da"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"mcc": 0.1700744536954035, "macro_f1": 0.27329371320514745}, {"mcc": 0.1309042409126271, "macro_f1": 0.22334535693314317}, {"mcc": 0.11795041871050471, "macro_f1": 0.21910269863961343}, {"mcc": 0.1528109046574582, "macro_f1": 0.2459021226200331}, {"mcc": 0.1814573629286154, "macro_f1": 0.2751409109480189}, {"mcc": 0.05233317891988462, "macro_f1": 0.18546167045760828}, {"mcc": 0.10504336685924368, "macro_f1": 0.21316012663774667}, {"mcc": 0.020324346616304816, "macro_f1": 0.17263645794574456}, {"mcc": 0.08723454429255607, "macro_f1": 0.2024749323622449}, {"mcc": 0.11480065245315345, "macro_f1": 0.22298252853871903}], "total": {"test_mcc": 11.329334700457515, "test_mcc_se": 3.1245914355107764, "test_macro_f1": 22.335005182880195, "test_macro_f1_se": 2.0917318020410507}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "norec", "task": "sentiment-classification", "dataset_languages": ["nb", "nn", "no"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"mcc": 0.3410993990897119, "macro_f1": 0.5002179524589796}, {"mcc": 0.2806502521679787, "macro_f1": 0.3536620890146686}, {"mcc": 0.35224016126669466, "macro_f1": 0.39524285592445124}, {"mcc": 0.2775556277344269, "macro_f1": 0.31073804438564107}, {"mcc": 0.17338797912707238, "macro_f1": 0.26796720805196017}, {"mcc": 0.2989897491922411, "macro_f1": 0.32683273737595936}, {"mcc": 0.2855023320717858, "macro_f1": 0.35056994275290143}, {"mcc": 0.2437176935438028, "macro_f1": 0.2707289901416843}, {"mcc": 0.3308326324061789, "macro_f1": 0.34504547079732556}, {"mcc": 0.29906539310220404, "macro_f1": 0.48189541552236775}], "total": {"test_mcc": 28.830412197020973, "test_mcc_se": 3.2199095414843115, "test_macro_f1": 36.02900706425939, "test_macro_f1_se": 4.892575671603166}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "hotter-and-colder-sentiment", "task": "sentiment-classification", "dataset_languages": ["is"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"mcc": 0.0, "macro_f1": 0.16828778755596727}, {"mcc": 0.073739671906865, "macro_f1": 0.17519230769230767}, {"mcc": -0.09260404420751986, "macro_f1": 0.16818541336476892}, {"mcc": 0.03970159007869508, "macro_f1": 0.16097569504284776}, {"mcc": 0.007675004731716842, "macro_f1": 0.14881962893367026}, {"mcc": 0.013975400990198134, "macro_f1": 0.1625389404047192}, {"mcc": 0.0, "macro_f1": 0.16315707377944158}, {"mcc": 0.0, "macro_f1": 0.1581600504095778}, {"mcc": 0.04654474784184943, "macro_f1": 0.17452480601177237}, {"mcc": 0.05358009792847276, "macro_f1": 0.17056963274327552}], "total": {"test_mcc": 1.4261246927027742, "test_mcc_se": 2.8363163851341517, "test_macro_f1": 16.50411335938348, "test_macro_f1_se": 0.49790811526942425}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "sb10k", "task": "sentiment-classification", "dataset_languages": ["de"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"mcc": 0.16801575742037644, "macro_f1": 0.24359898158063178}, {"mcc": 0.05075717253762571, "macro_f1": 0.18005282182385282}, {"mcc": 0.18497281144957717, "macro_f1": 0.2584165791727843}, {"mcc": 0.06123469484978262, "macro_f1": 0.17914335787314675}, {"mcc": 0.0, "macro_f1": 0.1632251720747296}, {"mcc": 0.0987400793699512, "macro_f1": 0.19856705112867448}, {"mcc": 0.03765572286609861, "macro_f1": 0.16398640178323057}, {"mcc": 0.07771061991789219, "macro_f1": 0.1860985656313913}, {"mcc": 0.16485659175292844, "macro_f1": 0.23948265011966755}, {"mcc": 0.18588570561344525, "macro_f1": 0.26129260730223924}], "total": {"test_mcc": 10.298291557776777, "test_mcc_se": 4.216561590102217, "test_macro_f1": 20.738641884903487, "test_macro_f1_se": 2.4235936369183335}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "dutch-social", "task": "sentiment-classification", "dataset_languages": ["nl"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"mcc": 0.10361222072782603, "macro_f1": 0.3458786617487248}, {"mcc": 0.07025444907885493, "macro_f1": 0.34482022108012494}, {"mcc": 0.1425585186999451, "macro_f1": 0.3834160691303548}, {"mcc": 0.11019572725065248, "macro_f1": 0.36503090130724253}, {"mcc": 0.06396587497776096, "macro_f1": 0.2792319350773583}, {"mcc": 0.14972932591591315, "macro_f1": 0.3951525578459039}, {"mcc": 0.010226511167471815, "macro_f1": 0.2848138482191059}, {"mcc": 0.07903503513358523, "macro_f1": 0.337857483510627}, {"mcc": 0.1079417876730492, "macro_f1": 0.3508861070714495}, {"mcc": 0.11884662375155464, "macro_f1": 0.3923958406360705}], "total": {"test_mcc": 9.563660743766135, "test_mcc_se": 2.5564604117683527, "test_macro_f1": 34.794836256269626, "test_macro_f1_se": 2.4951881269524994}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "sst5", "task": "sentiment-classification", "dataset_languages": ["en"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"mcc": 0.28485532706773004, "macro_f1": 0.34337871897769934}, {"mcc": 0.27637562886001427, "macro_f1": 0.325130458260801}, {"mcc": 0.2320302711360067, "macro_f1": 0.2563723184886249}, {"mcc": 0.2778104409102421, "macro_f1": 0.32300914799105546}, {"mcc": 0.25919281335152955, "macro_f1": 0.29803771694326114}, {"mcc": 0.14472280741528398, "macro_f1": 0.17196620348195624}, {"mcc": 0.2328242805396294, "macro_f1": 0.26525652048910314}, {"mcc": 0.22391963453360733, "macro_f1": 0.25636635540164343}, {"mcc": 0.44097101003666167, "macro_f1": 0.541317923824812}, {"mcc": 0.20242618709339782, "macro_f1": 0.2259280209021491}], "total": {"test_mcc": 25.75128400944103, "test_mcc_se": 4.770615304685774, "test_macro_f1": 30.067633847611052, "test_macro_f1_se": 6.1347262717670565}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "fosent", "task": "sentiment-classification", "dataset_languages": ["fo"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"mcc": 0.1299993599774951, "macro_f1": 0.39961705586342466}, {"mcc": 0.13191378435453613, "macro_f1": 0.3310153594213558}, {"mcc": 0.1571830278782468, "macro_f1": 0.34695620773623465}, {"mcc": 0.07206073735472157, "macro_f1": 0.3178076498473927}, {"mcc": -0.07664138198175402, "macro_f1": 0.14856289660321015}, {"mcc": -0.012524590513929113, "macro_f1": 0.294318545636277}, {"mcc": 0.22099554321653586, "macro_f1": 0.46542592295890145}, {"mcc": -0.02556557786815349, "macro_f1": 0.22713349445237332}, {"mcc": 0.16745982375806534, "macro_f1": 0.4200939324701701}, {"mcc": 0.09884442031960576, "macro_f1": 0.3149912568195116}], "total": {"test_mcc": 8.6372514649537, "test_mcc_se": 5.949805514420426, "test_macro_f1": 32.65922321808851, "test_macro_f1_se": 5.718964791078944}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "suc3", "task": "named-entity-recognition", "dataset_languages": ["sv"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}], "total": {"test_micro_f1_no_misc": 0.0, "test_micro_f1_no_misc_se": 0.0, "test_micro_f1": 0.0, "test_micro_f1_se": 0.0}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "dansk", "task": "named-entity-recognition", "dataset_languages": ["da"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}], "total": {"test_micro_f1_no_misc": 0.0, "test_micro_f1_no_misc_se": 0.0, "test_micro_f1": 0.0, "test_micro_f1_se": 0.0}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "norne-nb", "task": "named-entity-recognition", "dataset_languages": ["nb", "no"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}], "total": {"test_micro_f1_no_misc": 0.0, "test_micro_f1_no_misc_se": 0.0, "test_micro_f1": 0.0, "test_micro_f1_se": 0.0}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "norne-nn", "task": "named-entity-recognition", "dataset_languages": ["nn"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}], "total": {"test_micro_f1_no_misc": 0.0, "test_micro_f1_no_misc_se": 0.0, "test_micro_f1": 0.0, "test_micro_f1_se": 0.0}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "mim-gold-ner", "task": "named-entity-recognition", "dataset_languages": ["is"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}], "total": {"test_micro_f1_no_misc": 0.0, "test_micro_f1_no_misc_se": 0.0, "test_micro_f1": 0.0, "test_micro_f1_se": 0.0}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "fone", "task": "named-entity-recognition", "dataset_languages": ["fo"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}], "total": {"test_micro_f1_no_misc": 0.0, "test_micro_f1_no_misc_se": 0.0, "test_micro_f1": 0.0, "test_micro_f1_se": 0.0}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "germeval", "task": "named-entity-recognition", "dataset_languages": ["de"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}], "total": {"test_micro_f1_no_misc": 0.0, "test_micro_f1_no_misc_se": 0.0, "test_micro_f1": 0.0, "test_micro_f1_se": 0.0}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "conll-nl", "task": "named-entity-recognition", "dataset_languages": ["nl"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}], "total": {"test_micro_f1_no_misc": 0.0, "test_micro_f1_no_misc_se": 0.0, "test_micro_f1": 0.0, "test_micro_f1_se": 0.0}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "conll-en", "task": "named-entity-recognition", "dataset_languages": ["en"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}], "total": {"test_micro_f1_no_misc": 0.0, "test_micro_f1_no_misc_se": 0.0, "test_micro_f1": 0.0, "test_micro_f1_se": 0.0}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "scala-sv", "task": "linguistic-acceptability", "dataset_languages": ["sv"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"mcc": 0.2481942752232845, "macro_f1": 0.5229346077046008}, {"mcc": 0.030392018011316836, "macro_f1": 0.32909073624135243}, {"mcc": 0.0, "macro_f1": 0.33764553686934023}, {"mcc": 0.0, "macro_f1": 0.330718954248366}, {"mcc": 0.0822664345804076, "macro_f1": 0.3540439048283437}, {"mcc": 0.0, "macro_f1": 0.33914165859954826}, {"mcc": 0.0, "macro_f1": 0.33289902280130296}, {"mcc": 0.11994481826478963, "macro_f1": 0.3728103562286011}, {"mcc": 0.0, "macro_f1": 0.34169077467052394}, {"mcc": 0.0, "macro_f1": 0.3359273670557717}], "total": {"test_mcc": 4.8079754607979845, "test_mcc_se": 5.084342703261643, "test_macro_f1": 35.969029192477514, "test_macro_f1_se": 3.645988964417282}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "scala-da", "task": "linguistic-acceptability", "dataset_languages": ["da"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"mcc": 0.09710122757787243, "macro_f1": 0.34921624545977453}, {"mcc": 0.0, "macro_f1": 0.3412672885172081}, {"mcc": 0.0, "macro_f1": 0.3331162487788994}, {"mcc": 0.0, "macro_f1": 0.3393548387096774}, {"mcc": 0.0, "macro_f1": 0.33571196886149857}, {"mcc": 0.0, "macro_f1": 0.3335502766026684}, {"mcc": 0.0, "macro_f1": 0.33657272432782637}, {"mcc": 0.05051349469654601, "macro_f1": 0.3376610593710892}, {"mcc": 0.0, "macro_f1": 0.3322464949462015}, {"mcc": 0.03219506426896406, "macro_f1": 0.3420080321285141}], "total": {"test_mcc": 1.798097865433825, "test_mcc_se": 2.043547690047805, "test_macro_f1": 33.80705177703357, "test_macro_f1_se": 0.3190451412997841}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "scala-nb", "task": "linguistic-acceptability", "dataset_languages": ["nb", "no"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"mcc": 0.09735838280824931, "macro_f1": 0.37316655919614566}, {"mcc": 0.0, "macro_f1": 0.33137446947437155}, {"mcc": 0.0, "macro_f1": 0.3346328784925276}, {"mcc": 0.0, "macro_f1": 0.33137446947437155}, {"mcc": 0.0, "macro_f1": 0.3399935546245569}, {"mcc": 0.02175978876390385, "macro_f1": 0.33091151495713395}, {"mcc": 0.030931197279674, "macro_f1": 0.3330826121948198}, {"mcc": 0.07291348113445395, "macro_f1": 0.343333922687051}, {"mcc": 0.0, "macro_f1": 0.3344166395840104}, {"mcc": 0.0, "macro_f1": 0.3335502766026684}], "total": {"test_mcc": 2.2296284998628115, "test_mcc_se": 2.191737573752793, "test_macro_f1": 33.85836897287656, "test_macro_f1_se": 0.7926378263786499}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "scala-nn", "task": "linguistic-acceptability", "dataset_languages": ["nn"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"mcc": 0.038601747521714164, "macro_f1": 0.33833567801629294}, {"mcc": 0.0, "macro_f1": 0.3435897435897436}, {"mcc": 0.0, "macro_f1": 0.3342002600780234}, {"mcc": 0.0, "macro_f1": 0.34169077467052394}, {"mcc": 0.0, "macro_f1": 0.3324641460234681}, {"mcc": 0.0, "macro_f1": 0.3412672885172081}, {"mcc": 0.0, "macro_f1": 0.3372168284789644}, {"mcc": 0.0, "macro_f1": 0.3339837398373984}, {"mcc": 0.0, "macro_f1": 0.3346328784925276}, {"mcc": 0.0, "macro_f1": 0.33484897694056515}], "total": {"test_mcc": 0.38601747521714164, "test_mcc_se": 0.7565942514255977, "test_macro_f1": 33.72230314644716, "test_macro_f1_se": 0.23800706792314386}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "scala-is", "task": "linguistic-acceptability", "dataset_languages": ["is"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"mcc": -0.0019459060359787736, "macro_f1": 0.32505281811177983}, {"mcc": 0.0, "macro_f1": 0.3361426256077796}, {"mcc": 0.0, "macro_f1": 0.3335502766026684}, {"mcc": 0.0, "macro_f1": 0.3285245901639344}, {"mcc": 0.0, "macro_f1": 0.3263157894736842}, {"mcc": 0.0, "macro_f1": 0.34169077467052394}, {"mcc": 0.0, "macro_f1": 0.32940406024885394}, {"mcc": -0.03247960209962905, "macro_f1": 0.32431540745628507}, {"mcc": 0.0, "macro_f1": 0.33376707872478856}, {"mcc": 0.0, "macro_f1": 0.3382875605815832}], "total": {"test_mcc": -0.34425508135607824, "test_mcc_se": 0.6334973962783876, "test_macro_f1": 33.17050981641881, "test_macro_f1_se": 0.3659285282088782}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "scala-fo", "task": "linguistic-acceptability", "dataset_languages": ["fo"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"mcc": 0.017226126296245762, "macro_f1": 0.3521037646314457}, {"mcc": 0.0, "macro_f1": 0.3324641460234681}, {"mcc": 0.0, "macro_f1": 0.3427471116816431}, {"mcc": 0.0, "macro_f1": 0.3342002600780234}, {"mcc": 0.0, "macro_f1": 0.326758711374096}, {"mcc": 0.0, "macro_f1": 0.33376707872478856}, {"mcc": 0.0, "macro_f1": 0.31505016722408025}, {"mcc": 0.0, "macro_f1": 0.3333333333333333}, {"mcc": 0.0, "macro_f1": 0.32587228439763}, {"mcc": 0.0, "macro_f1": 0.34568690095846644}], "total": {"test_mcc": 0.1722612629624576, "test_mcc_se": 0.33763207540641693, "test_macro_f1": 33.41983758426974, "test_macro_f1_se": 0.6601377162826277}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "scala-de", "task": "linguistic-acceptability", "dataset_languages": ["de"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"mcc": 0.30732485047980007, "macro_f1": 0.6106694874924735}, {"mcc": 0.1668449538839742, "macro_f1": 0.45018979097313977}, {"mcc": 0.04889463375533628, "macro_f1": 0.3360633030999087}, {"mcc": 0.1029374920618065, "macro_f1": 0.3687372797506447}, {"mcc": 0.10427824664878789, "macro_f1": 0.3889713966092541}, {"mcc": 0.17056129449126567, "macro_f1": 0.40758582012456096}, {"mcc": 0.10801962251560585, "macro_f1": 0.36734390144764406}, {"mcc": 0.4270163946853737, "macro_f1": 0.7052097751619695}, {"mcc": 0.190336506561563, "macro_f1": 0.4423076640009656}, {"mcc": 0.07707228015888334, "macro_f1": 0.34713704535794526}], "total": {"test_mcc": 17.032862752423963, "test_mcc_se": 7.195530837886944, "test_macro_f1": 44.242154640185056, "test_macro_f1_se": 7.533277947972805}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "scala-nl", "task": "linguistic-acceptability", "dataset_languages": ["nl"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"mcc": 0.47971992241745054, "macro_f1": 0.7357017584814496}, {"mcc": 0.10551856430389076, "macro_f1": 0.3714338096231803}, {"mcc": 0.1447396165892548, "macro_f1": 0.3752842634401352}, {"mcc": 0.0, "macro_f1": 0.34169077467052394}, {"mcc": 0.11891532983425022, "macro_f1": 0.3620938252588789}, {"mcc": 0.27034853134790693, "macro_f1": 0.4841601156591669}, {"mcc": 0.06380675638982074, "macro_f1": 0.35245097776515416}, {"mcc": 0.47642582719944315, "macro_f1": 0.7320738514383855}, {"mcc": 0.2764256436641854, "macro_f1": 0.5112072555173008}, {"mcc": 0.4589633425368346, "macro_f1": 0.6700147844314583}], "total": {"test_mcc": 23.948635342830368, "test_mcc_se": 11.202682386706753, "test_macro_f1": 49.36111416285634, "test_macro_f1_se": 10.041392545770599}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "scala-en", "task": "linguistic-acceptability", "dataset_languages": ["en"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"mcc": 0.382139457334511, "macro_f1": 0.587529473408436}, {"mcc": 0.43190827753295563, "macro_f1": 0.7158203125}, {"mcc": 0.44295468008261063, "macro_f1": 0.700153228966214}, {"mcc": 0.46827202960692205, "macro_f1": 0.6935332154500773}, {"mcc": 0.4202919580306567, "macro_f1": 0.699712785399969}, {"mcc": 0.4514189926808843, "macro_f1": 0.7163758291000777}, {"mcc": 0.4917584451382612, "macro_f1": 0.726852038108112}, {"mcc": 0.3003601608913389, "macro_f1": 0.5250501617111254}, {"mcc": 0.4192723913294047, "macro_f1": 0.6462232252250624}, {"mcc": 0.40558520147540317, "macro_f1": 0.6774018448167866}], "total": {"test_mcc": 42.13961594102948, "test_mcc_se": 3.2713256000217292, "test_macro_f1": 66.8865211468586, "test_macro_f1_se": 4.040503018472226}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"f1": 0.8491983983152348, "em": 0.0}, {"f1": 0.9897438539033611, "em": 0.0}, {"f1": 0.7577860946348615, "em": 0.0}, {"f1": 1.3200607422078303, "em": 0.0}, {"f1": 1.628875853244368, "em": 0.0}, {"f1": 2.193262085536399, "em": 0.0}, {"f1": 2.120940008725419, "em": 0.0}, {"f1": 1.425706419229209, "em": 0.0}, {"f1": 1.063836542695496, "em": 0.0}, {"f1": 0.8518265835782529, "em": 0.0}], "total": {"test_f1": 1.3201236582070432, "test_f1_se": 0.32287382792344, "test_em": 0.0, "test_em_se": 0.0}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "norquad", "task": "reading-comprehension", "dataset_languages": ["nb", "nn", "no"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"f1": 1.1037455934941607, "em": 0.0}, {"f1": 1.3377894965168606, "em": 0.0}, {"f1": 1.4588441971622, "em": 0.0}, {"f1": 1.4951591118055645, "em": 0.0}, {"f1": 1.6003039270842625, "em": 0.0}, {"f1": 1.2343883378167788, "em": 0.0}, {"f1": 1.1501932205435765, "em": 0.0}, {"f1": 1.2987662363225874, "em": 0.0}, {"f1": 1.2293864934327319, "em": 0.0}, {"f1": 1.3580046461898672, "em": 0.0}], "total": {"test_f1": 1.326658126036859, "test_f1_se": 0.09722108814907379, "test_em": 0.0, "test_em_se": 0.0}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "scandiqa-sv", "task": "reading-comprehension", "dataset_languages": ["sv"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"f1": 0.7017960371170129, "em": 0.0}, {"f1": 0.8625694688292658, "em": 0.0}, {"f1": 0.7457908832273531, "em": 0.0}, {"f1": 0.9427885090841284, "em": 0.0}, {"f1": 0.6632848072394298, "em": 0.0}, {"f1": 0.7455106696795012, "em": 0.0}, {"f1": 0.9738359798394138, "em": 0.0}, {"f1": 0.7409171728716278, "em": 0.0}, {"f1": 0.7973177939398394, "em": 0.0}, {"f1": 0.9099915588233929, "em": 0.0}], "total": {"test_f1": 0.8083802880650965, "test_f1_se": 0.06651788380771904, "test_em": 0.0, "test_em_se": 0.0}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "nqii", "task": "reading-comprehension", "dataset_languages": ["is"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"f1": 1.4825374496908754, "em": 0.0}, {"f1": 2.0101579420659696, "em": 0.0}, {"f1": 1.7331821707724466, "em": 0.0}, {"f1": 1.5844117531426691, "em": 0.0}, {"f1": 2.012917439694122, "em": 0.0}, {"f1": 1.7597165264810124, "em": 0.0}, {"f1": 1.3896102375737798, "em": 0.0}, {"f1": 1.5960941571986167, "em": 0.0}, {"f1": 1.6239537487471862, "em": 0.0}, {"f1": 1.6957600935698471, "em": 0.0}], "total": {"test_f1": 1.6888341518936527, "test_f1_se": 0.12586216521551039, "test_em": 0.0, "test_em_se": 0.0}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "foqa", "task": "reading-comprehension", "dataset_languages": ["fo"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"f1": 0.42188517933189884, "em": 0.0}, {"f1": 0.5007505183810051, "em": 0.0}, {"f1": 0.5711713968583743, "em": 0.0}, {"f1": 0.4810725020104865, "em": 0.0}, {"f1": 0.591889349553353, "em": 0.0}, {"f1": 0.47997932546018535, "em": 0.0}, {"f1": 0.5727905331736421, "em": 0.0}, {"f1": 0.5888343274429001, "em": 0.0}, {"f1": 0.6275588303957317, "em": 0.0}, {"f1": 0.4667450967978934, "em": 0.0}], "total": {"test_f1": 0.5302677059405471, "test_f1_se": 0.04222797617648501, "test_em": 0.0, "test_em_se": 0.0}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "germanquad", "task": "reading-comprehension", "dataset_languages": ["de"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"f1": 1.3859599016489708, "em": 0.0}, {"f1": 1.3613855608148886, "em": 0.0}, {"f1": 1.208349040488043, "em": 0.0}, {"f1": 1.1442636940490563, "em": 0.0}, {"f1": 1.2819902310648723, "em": 0.0}, {"f1": 1.4113029482028978, "em": 0.0}, {"f1": 1.368668694329429, "em": 0.0}, {"f1": 1.3692794351949933, "em": 0.0}, {"f1": 0.9279379296277961, "em": 0.0}, {"f1": 1.214912492060407, "em": 0.0}], "total": {"test_f1": 1.2674049927481352, "test_f1_se": 0.09264543987216915, "test_em": 0.0, "test_em_se": 0.0}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "squad", "task": "reading-comprehension", "dataset_languages": ["en"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"f1": 3.873255445537096, "em": 0.0}, {"f1": 3.7155070480862706, "em": 0.0}, {"f1": 3.486959397439937, "em": 0.0}, {"f1": 3.5363393752993124, "em": 0.0}, {"f1": 3.6873287795217764, "em": 0.0}, {"f1": 3.8979500124747464, "em": 0.0}, {"f1": 3.8166621197266997, "em": 0.0}, {"f1": 3.9215135371996896, "em": 0.0}, {"f1": 3.728795589241402, "em": 0.0}, {"f1": 3.588062766919705, "em": 0.0}], "total": {"test_f1": 3.7252374071446632, "test_f1_se": 0.0950014837104555, "test_em": 0.0, "test_em_se": 0.0}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "squad-nl", "task": "reading-comprehension", "dataset_languages": ["nl"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"f1": 0.9772216831036526, "em": 0.0}, {"f1": 1.0136139703846774, "em": 0.0}, {"f1": 1.106833093916754, "em": 0.0}, {"f1": 1.0461764891041379, "em": 0.0}, {"f1": 0.8813561955272039, "em": 0.0}, {"f1": 0.8657879267084339, "em": 0.0}, {"f1": 1.1270922192870974, "em": 0.0}, {"f1": 1.0582033793984797, "em": 0.0}, {"f1": 1.1905723395396655, "em": 0.0}, {"f1": 1.057883808537907, "em": 0.0}], "total": {"test_f1": 1.0324741105508008, "test_f1_se": 0.06367964017745271, "test_em": 0.0, "test_em_se": 0.0}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "nordjylland-news", "task": "summarization", "dataset_languages": ["da"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"bertscore": 0.5816895953030325, "rouge_l": 0.05731940756439586}, {"bertscore": 0.5804834582522744, "rouge_l": 0.05933915719506472}, {"bertscore": 0.583202930400148, "rouge_l": 0.060266806930430315}, {"bertscore": 0.5858130882261321, "rouge_l": 0.06129657397485092}, {"bertscore": 0.5798951386241242, "rouge_l": 0.06080967596768344}, {"bertscore": 0.5853235752001638, "rouge_l": 0.05722969944219124}, {"bertscore": 0.5805299132916844, "rouge_l": 0.05534131914191713}, {"bertscore": 0.5790680409991182, "rouge_l": 0.057085407446100765}, {"bertscore": 0.5766826526523801, "rouge_l": 0.05582081787352648}, {"bertscore": 0.5792971552873496, "rouge_l": 0.058703906804009415}], "total": {"test_bertscore": 58.119855482364066, "test_bertscore_se": 0.17745364894703783, "test_rouge_l": 5.832127723401702, "test_rouge_l_se": 0.1288031315704493}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "mlsum", "task": "summarization", "dataset_languages": ["de"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"bertscore": 0.5694812050205655, "rouge_l": 0.04263569610689526}, {"bertscore": 0.5672651277564, "rouge_l": 0.042674623064165335}, {"bertscore": 0.5668926706130151, "rouge_l": 0.04116999813942006}, {"bertscore": 0.5639690666575916, "rouge_l": 0.042027284600348595}, {"bertscore": 0.5641411362303188, "rouge_l": 0.03934565191417613}, {"bertscore": 0.5675899155176012, "rouge_l": 0.04227081779027844}, {"bertscore": 0.5705757329269545, "rouge_l": 0.04526312736457825}, {"bertscore": 0.5666805594228208, "rouge_l": 0.04199680044359157}, {"bertscore": 0.5688724261854077, "rouge_l": 0.0410722007555796}, {"bertscore": 0.5681209945323644, "rouge_l": 0.041519832772377016}], "total": {"test_bertscore": 56.73588834863039, "test_bertscore_se": 0.13116940843563504, "test_rouge_l": 4.1997603295141035, "test_rouge_l_se": 0.09333091200789354}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "rrn", "task": "summarization", "dataset_languages": ["is"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"bertscore": 0.553875445126323, "rouge_l": 0.04454746687999027}, {"bertscore": 0.5637376142549329, "rouge_l": 0.04263256204847765}, {"bertscore": 0.5572156692796852, "rouge_l": 0.04418154289020469}, {"bertscore": 0.5576896304846741, "rouge_l": 0.0408860973823409}, {"bertscore": 0.5587350134446751, "rouge_l": 0.0434412357579622}, {"bertscore": 0.5586579511873424, "rouge_l": 0.04354956276396537}, {"bertscore": 0.5585373176145367, "rouge_l": 0.04195606406830531}, {"bertscore": 0.5543833870615344, "rouge_l": 0.043023832662009445}, {"bertscore": 0.5471773941826541, "rouge_l": 0.044067517764385646}, {"bertscore": 0.5578473319183104, "rouge_l": 0.04226433597737617}], "total": {"test_bertscore": 55.678567545546684, "test_bertscore_se": 0.26730171941183506, "test_rouge_l": 4.305502181950176, "test_rouge_l_se": 0.07046281763837174}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "no-sammendrag", "task": "summarization", "dataset_languages": ["nb", "nn", "no"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"bertscore": 0.5682338354818057, "rouge_l": 0.052379799708834586}, {"bertscore": 0.559017129795393, "rouge_l": 0.05160268095698709}, {"bertscore": 0.5637906964839203, "rouge_l": 0.050357183782335155}, {"bertscore": 0.5570808687480167, "rouge_l": 0.0466358462550026}, {"bertscore": 0.5569272827269742, "rouge_l": 0.04968400052986698}, {"bertscore": 0.5642848864808911, "rouge_l": 0.04995074215759576}, {"bertscore": 0.5631362475542119, "rouge_l": 0.05290479141019368}, {"bertscore": 0.5608942040416878, "rouge_l": 0.05311626973715448}, {"bertscore": 0.5682386438857066, "rouge_l": 0.05123233928149426}, {"bertscore": 0.5571094687038567, "rouge_l": 0.05041969867368515}], "total": {"test_bertscore": 56.18713263902464, "test_bertscore_se": 0.2704058826907531, "test_rouge_l": 5.082833524931497, "test_rouge_l_se": 0.1182547591414176}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "wiki-lingua-nl", "task": "summarization", "dataset_languages": ["nl"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"bertscore": 0.5838610886858078, "rouge_l": 0.015058574823845405}, {"bertscore": 0.5838823170633987, "rouge_l": 0.015605878545081369}, {"bertscore": 0.5874426891823532, "rouge_l": 0.01614009206749839}, {"bertscore": 0.5832096115627792, "rouge_l": 0.015538873735085282}, {"bertscore": 0.5810744954796974, "rouge_l": 0.014955068507547815}, {"bertscore": 0.5827631950232899, "rouge_l": 0.015421608195704904}, {"bertscore": 0.58348547690548, "rouge_l": 0.015323666702924663}, {"bertscore": 0.5832014053012244, "rouge_l": 0.015864632452027624}, {"bertscore": 0.5824885927431751, "rouge_l": 0.01683123777605676}, {"bertscore": 0.5830065884802025, "rouge_l": 0.015081193882192413}], "total": {"test_bertscore": 58.34415460427408, "test_bertscore_se": 0.10048009491125591, "test_rouge_l": 1.5582082668796464, "test_rouge_l_se": 0.035573134717735086}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "swedn", "task": "summarization", "dataset_languages": ["sv"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"bertscore": 0.5757994355953997, "rouge_l": 0.04087038936500416}, {"bertscore": 0.5789438730134862, "rouge_l": 0.04019441585096088}, {"bertscore": 0.580316023377236, "rouge_l": 0.04111730215625953}, {"bertscore": 0.5826376095646992, "rouge_l": 0.041876796718635825}, {"bertscore": 0.581525563946343, "rouge_l": 0.039081505867150104}, {"bertscore": 0.5821297649963526, "rouge_l": 0.039445628815190675}, {"bertscore": 0.5789281710749492, "rouge_l": 0.040102069976893}, {"bertscore": 0.5821774244104745, "rouge_l": 0.04047421857476155}, {"bertscore": 0.5772568544489332, "rouge_l": 0.03882727482002932}, {"bertscore": 0.5807166815066012, "rouge_l": 0.04097424298563297}], "total": {"test_bertscore": 58.00431401934475, "test_bertscore_se": 0.14102031369511997, "test_rouge_l": 4.02963845130518, "test_rouge_l_se": 0.05987314905393956}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "cnn-dailymail", "task": "summarization", "dataset_languages": ["en"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"bertscore": 0.5946965095790802, "rouge_l": 0.12720475638338513}, {"bertscore": 0.6018542064994108, "rouge_l": 0.1330723606669763}, {"bertscore": 0.6022871321474668, "rouge_l": 0.1344960730327836}, {"bertscore": 0.6012455052696168, "rouge_l": 0.13309228391910982}, {"bertscore": 0.6014841765572783, "rouge_l": 0.1343863575612629}, {"bertscore": 0.601155082200421, "rouge_l": 0.12996754155909745}, {"bertscore": 0.6025394113676157, "rouge_l": 0.13300850458801505}, {"bertscore": 0.5973822920932434, "rouge_l": 0.12787449267474565}, {"bertscore": 0.6002165637037251, "rouge_l": 0.13087462520413307}, {"bertscore": 0.5999156663892791, "rouge_l": 0.12927446002200904}], "total": {"test_bertscore": 60.02776545807137, "test_bertscore_se": 0.1524780349128995, "test_rouge_l": 13.13251455611518, "test_rouge_l_se": 0.16472685081668248}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "danske-talemaader", "task": "knowledge", "dataset_languages": ["da"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"mcc": 0.0, "accuracy": 0.2400990099009901}, {"mcc": 0.0, "accuracy": 0.22277227722772278}, {"mcc": 0.0, "accuracy": 0.2660891089108911}, {"mcc": 0.026833047927527703, "accuracy": 0.2660891089108911}, {"mcc": -0.03480781798056701, "accuracy": 0.24133663366336633}, {"mcc": 0.0011643089243357917, "accuracy": 0.2747524752475248}, {"mcc": 0.018693864305610933, "accuracy": 0.2636138613861386}, {"mcc": 0.0, "accuracy": 0.2524752475247525}, {"mcc": -0.002470474546884212, "accuracy": 0.23143564356435645}, {"mcc": 0.0, "accuracy": 0.2599009900990099}], "total": {"test_mcc": 0.09412928630023208, "test_mcc_se": 0.9864372223544262, "test_accuracy": 25.185643564356436, "test_accuracy_se": 1.0624457099138733}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "danish-citizen-tests", "task": "knowledge", "dataset_languages": ["da"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"mcc": 0.0, "accuracy": 0.3515625}, {"mcc": 0.0, "accuracy": 0.35546875}, {"mcc": 0.0, "accuracy": 0.396484375}, {"mcc": 0.0, "accuracy": 0.3515625}, {"mcc": 0.0, "accuracy": 0.365234375}, {"mcc": 0.0, "accuracy": 0.33203125}, {"mcc": 0.0, "accuracy": 0.373046875}, {"mcc": 0.0, "accuracy": 0.369140625}, {"mcc": 0.0, "accuracy": 0.3671875}, {"mcc": 0.0, "accuracy": 0.3671875}], "total": {"test_mcc": 0.0, "test_mcc_se": 0.0, "test_accuracy": 36.2890625, "test_accuracy_se": 1.0488410487642617}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "mmlu-no", "task": "knowledge", "dataset_languages": ["nb", "nn", "no"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"mcc": 0.0, "accuracy": 0.25537109375}, {"mcc": 0.013842626794232227, "accuracy": 0.2392578125}, {"mcc": 0.0, "accuracy": 0.25048828125}, {"mcc": -0.032003773423861286, "accuracy": 0.2353515625}, {"mcc": 0.01849732605796372, "accuracy": 0.23828125}, {"mcc": -0.016817516817382906, "accuracy": 0.2314453125}, {"mcc": 0.023407490594876477, "accuracy": 0.24267578125}, {"mcc": 0.0, "accuracy": 0.2392578125}, {"mcc": -0.0029153691926746958, "accuracy": 0.23046875}, {"mcc": -0.00013757905678938437, "accuracy": 0.23779296875}], "total": {"test_mcc": 0.0387320495636415, "test_mcc_se": 1.011246994651585, "test_accuracy": 24.00390625, "test_accuracy_se": 0.48363494231985105}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "mmlu-sv", "task": "knowledge", "dataset_languages": ["sv"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"mcc": 0.0, "accuracy": 0.2734375}, {"mcc": 0.0, "accuracy": 0.2666015625}, {"mcc": 0.0, "accuracy": 0.2724609375}, {"mcc": 0.0, "accuracy": 0.2568359375}, {"mcc": 0.0, "accuracy": 0.26171875}, {"mcc": -0.00965439535960212, "accuracy": 0.27099609375}, {"mcc": -0.021171829741259918, "accuracy": 0.26904296875}, {"mcc": 0.0, "accuracy": 0.26416015625}, {"mcc": 0.0, "accuracy": 0.2587890625}, {"mcc": 0.0, "accuracy": 0.23681640625}], "total": {"test_mcc": -0.30826225100862037, "test_mcc_se": 0.4365264690159072, "test_accuracy": 26.308593749999996, "test_accuracy_se": 0.6714089398153525}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "mmlu-de", "task": "knowledge", "dataset_languages": ["de"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"mcc": -0.028920569320158455, "accuracy": 0.2509765625}, {"mcc": 0.0, "accuracy": 0.2529296875}, {"mcc": 0.0, "accuracy": 0.26171875}, {"mcc": 0.0, "accuracy": 0.236328125}, {"mcc": 0.0, "accuracy": 0.251953125}, {"mcc": 0.003636439931600497, "accuracy": 0.24365234375}, {"mcc": 0.0, "accuracy": 0.25}, {"mcc": 0.0, "accuracy": 0.251953125}, {"mcc": 0.0, "accuracy": 0.2509765625}, {"mcc": 0.0, "accuracy": 0.25732421875}], "total": {"test_mcc": -0.2528412938855796, "test_mcc_se": 0.5791107400509198, "test_accuracy": 25.078125, "test_accuracy_se": 0.4285201749479908}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "mmlu-nl", "task": "knowledge", "dataset_languages": ["nl"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"mcc": 0.0261245561651016, "accuracy": 0.2548828125}, {"mcc": 0.02065106395441965, "accuracy": 0.26513671875}, {"mcc": 0.0, "accuracy": 0.23388671875}, {"mcc": 0.002875777973835551, "accuracy": 0.24755859375}, {"mcc": 0.03154721484448486, "accuracy": 0.2578125}, {"mcc": -0.008158633360217216, "accuracy": 0.25341796875}, {"mcc": 0.002838621249808474, "accuracy": 0.24169921875}, {"mcc": 0.028659746954255796, "accuracy": 0.2587890625}, {"mcc": 0.041313122837301326, "accuracy": 0.265625}, {"mcc": 0.05139420137300791, "accuracy": 0.28564453125}], "total": {"test_mcc": 1.9724567199199794, "test_mcc_se": 1.2163253279654163, "test_accuracy": 25.644531250000004, "test_accuracy_se": 0.8854248908165845}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "mmlu", "task": "knowledge", "dataset_languages": ["en"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"mcc": 0.06870745448968361, "accuracy": 0.2607421875}, {"mcc": 0.035612839141022985, "accuracy": 0.24267578125}, {"mcc": 0.07033587400052584, "accuracy": 0.2568359375}, {"mcc": 0.0768495955070116, "accuracy": 0.28271484375}, {"mcc": 0.08582082884357918, "accuracy": 0.2607421875}, {"mcc": 0.024669826625746693, "accuracy": 0.23583984375}, {"mcc": 0.0997713083374311, "accuracy": 0.31494140625}, {"mcc": 0.04139194274727455, "accuracy": 0.2451171875}, {"mcc": 0.14314876748556632, "accuracy": 0.32373046875}, {"mcc": 0.07148222180033878, "accuracy": 0.28564453125}], "total": {"test_mcc": 7.177906589781806, "test_mcc_se": 2.123317318621701, "test_accuracy": 27.089843749999996, "test_accuracy_se": 1.8688565760019098}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "arc-is", "task": "knowledge", "dataset_languages": ["is"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"mcc": 0.0, "accuracy": 0.255859375}, {"mcc": 0.0, "accuracy": 0.28125}, {"mcc": 0.0, "accuracy": 0.25}, {"mcc": -0.0007984077025434723, "accuracy": 0.248046875}, {"mcc": -0.03942014955316048, "accuracy": 0.248046875}, {"mcc": -0.011810708333081312, "accuracy": 0.248046875}, {"mcc": -0.0007487752556054882, "accuracy": 0.2431640625}, {"mcc": 0.0, "accuracy": 0.271484375}, {"mcc": 0.0, "accuracy": 0.2578125}, {"mcc": -0.015479447843495732, "accuracy": 0.2294921875}], "total": {"test_mcc": -0.6825748868788647, "test_mcc_se": 0.7916782594147252, "test_accuracy": 25.332031249999996, "test_accuracy_se": 0.9010336132828924}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "hellaswag-da", "task": "common-sense-reasoning", "dataset_languages": ["da"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"mcc": 0.0, "accuracy": 0.25439453125}, {"mcc": 0.0, "accuracy": 0.25}, {"mcc": 0.0, "accuracy": 0.25244140625}, {"mcc": 0.025596205590691506, "accuracy": 0.24755859375}, {"mcc": 0.0, "accuracy": 0.26123046875}, {"mcc": 0.004070918095159538, "accuracy": 0.25048828125}, {"mcc": 0.0, "accuracy": 0.22998046875}, {"mcc": 0.0, "accuracy": 0.2431640625}, {"mcc": 1.7628533657700063e-05, "accuracy": 0.24853515625}, {"mcc": 0.019429823118315814, "accuracy": 0.25341796875}], "total": {"test_mcc": 0.49114575337824556, "test_mcc_se": 0.5872853638284116, "test_accuracy": 24.912109375, "test_accuracy_se": 0.5107754273029408}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "hellaswag-no", "task": "common-sense-reasoning", "dataset_languages": ["nb", "nn", "no"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"mcc": 0.0, "accuracy": 0.25390625}, {"mcc": -0.0080206794620333, "accuracy": 0.2294921875}, {"mcc": 0.0, "accuracy": 0.24755859375}, {"mcc": -0.013207979030752975, "accuracy": 0.2412109375}, {"mcc": -0.014230010826461382, "accuracy": 0.2392578125}, {"mcc": 0.03351786858157472, "accuracy": 0.2578125}, {"mcc": -0.03333961672171007, "accuracy": 0.25146484375}, {"mcc": 0.0, "accuracy": 0.265625}, {"mcc": 0.0006110578849595644, "accuracy": 0.26171875}, {"mcc": -0.012941498460837274, "accuracy": 0.2529296875}], "total": {"test_mcc": -0.4761085803526072, "test_mcc_se": 1.0569934244356942, "test_accuracy": 25.009765625000004, "test_accuracy_se": 0.6813393381629291}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "hellaswag-sv", "task": "common-sense-reasoning", "dataset_languages": ["sv"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"mcc": 0.0, "accuracy": 0.25}, {"mcc": 0.0, "accuracy": 0.248046875}, {"mcc": 0.0, "accuracy": 0.24267578125}, {"mcc": 0.0, "accuracy": 0.24609375}, {"mcc": 0.0, "accuracy": 0.2412109375}, {"mcc": 0.0, "accuracy": 0.2568359375}, {"mcc": -0.004704810884087632, "accuracy": 0.265625}, {"mcc": 0.0, "accuracy": 0.251953125}, {"mcc": 0.0, "accuracy": 0.2568359375}, {"mcc": 0.0, "accuracy": 0.23291015625}], "total": {"test_mcc": -0.047048108840876325, "test_mcc_se": 0.09221429332811758, "test_accuracy": 24.921875, "test_accuracy_se": 0.576023639123236}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "winogrande-is", "task": "common-sense-reasoning", "dataset_languages": ["is"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"mcc": 0.0, "accuracy": 0.4486607142857143}, {"mcc": 0.0, "accuracy": 0.4375}, {"mcc": 0.0, "accuracy": 0.4341517857142857}, {"mcc": 0.0, "accuracy": 0.43638392857142855}, {"mcc": 0.0, "accuracy": 0.41964285714285715}, {"mcc": 0.007777790295778762, "accuracy": 0.4921875}, {"mcc": 0.0, "accuracy": 0.41629464285714285}, {"mcc": 0.0, "accuracy": 0.45982142857142855}, {"mcc": 0.0, "accuracy": 0.4095982142857143}, {"mcc": 0.0, "accuracy": 0.4419642857142857}], "total": {"test_mcc": 0.07777790295778762, "test_mcc_se": 0.15244468979726372, "test_accuracy": 43.96205357142857, "test_accuracy_se": 1.4823619390697995}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "hellaswag-de", "task": "common-sense-reasoning", "dataset_languages": ["de"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"mcc": 0.0, "accuracy": 0.23828125}, {"mcc": 0.0, "accuracy": 0.24169921875}, {"mcc": 0.0, "accuracy": 0.25439453125}, {"mcc": 0.0, "accuracy": 0.2431640625}, {"mcc": 0.0, "accuracy": 0.24267578125}, {"mcc": -0.0015325705611138327, "accuracy": 0.24951171875}, {"mcc": 0.02183317367884901, "accuracy": 0.2568359375}, {"mcc": 0.0, "accuracy": 0.26513671875}, {"mcc": 0.0, "accuracy": 0.22900390625}, {"mcc": -0.018317019448388937, "accuracy": 0.244140625}], "total": {"test_mcc": 0.019835836693462387, "test_mcc_se": 0.5895065391802645, "test_accuracy": 24.6484375, "test_accuracy_se": 0.6379889314941007}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "hellaswag-nl", "task": "common-sense-reasoning", "dataset_languages": ["nl"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"mcc": 0.031590682643274234, "accuracy": 0.25390625}, {"mcc": 0.047083838595279245, "accuracy": 0.296875}, {"mcc": 0.0047893004008055864, "accuracy": 0.2626953125}, {"mcc": 0.03834764678544039, "accuracy": 0.27001953125}, {"mcc": 0.028683516141889492, "accuracy": 0.248046875}, {"mcc": 0.05012551536341178, "accuracy": 0.27099609375}, {"mcc": 0.060435502874470366, "accuracy": 0.29052734375}, {"mcc": 0.0005941347509882581, "accuracy": 0.26611328125}, {"mcc": 0.026564422487644193, "accuracy": 0.27197265625}, {"mcc": 0.019420934850740902, "accuracy": 0.2724609375}], "total": {"test_mcc": 3.076354948939444, "test_mcc_se": 1.1895932322753706, "test_accuracy": 27.036132812500004, "test_accuracy_se": 0.9152930886649495}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "hellaswag", "task": "common-sense-reasoning", "dataset_languages": ["en"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"mcc": 0.04321546186182561, "accuracy": 0.2568359375}, {"mcc": -0.024813853861053704, "accuracy": 0.2685546875}, {"mcc": 0.0, "accuracy": 0.2451171875}, {"mcc": 0.02133399782144414, "accuracy": 0.26318359375}, {"mcc": 0.0, "accuracy": 0.28271484375}, {"mcc": 0.0, "accuracy": 0.26708984375}, {"mcc": 0.019969997625068677, "accuracy": 0.26904296875}, {"mcc": 0.0, "accuracy": 0.2568359375}, {"mcc": 0.0399805467501881, "accuracy": 0.24853515625}, {"mcc": 0.04961456435358367, "accuracy": 0.28369140625}], "total": {"test_mcc": 1.4930071455105647, "test_mcc_se": 1.4852700347267602, "test_accuracy": 26.416015625, "test_accuracy_se": 0.7981638074956292}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "results": {"raw": [{"test_speed": 420.32, "test_speed_short": 48.48}, {"test_speed": 809.34, "test_speed_short": 91.05000000000001}, {"test_speed": 1215.36, "test_speed_short": 173.13}, {"test_speed": 1615.75, "test_speed_short": 217.08}, {"test_speed": 1990.17, "test_speed_short": 258.0}, {"test_speed": 2403.91, "test_speed_short": 339.71999999999997}, {"test_speed": 2788.8799999999997, "test_speed_short": 376.32}, {"test_speed": 3169.65, "test_speed_short": 420.32}, {"test_speed": 3565.15, "test_speed_short": 461.76}, {"test_speed": 3960.65, "test_speed_short": 508.3}], "total": {"test_speed": 2193.9180000000006, "test_speed_se": 737.6541832362703, "test_speed_short": 289.41600000000005, "test_speed_short_se": 97.33843090691956}}, "num_model_parameters": 14770033664, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "swerec", "task": "sentiment-classification", "dataset_languages": ["sv"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"mcc": 0.7452011288932064, "macro_f1": 0.7693276472642027}, {"mcc": 0.7317467101511187, "macro_f1": 0.7609520585267081}, {"mcc": 0.7635911034913638, "macro_f1": 0.790506696718778}, {"mcc": 0.7556892526630786, "macro_f1": 0.7794043942175325}, {"mcc": 0.7476756966428645, "macro_f1": 0.78223431542992}, {"mcc": 0.7596826257310773, "macro_f1": 0.7879626115189998}, {"mcc": 0.7774874684145907, "macro_f1": 0.7868094986630455}, {"mcc": 0.7306546237072503, "macro_f1": 0.7698579152972794}, {"mcc": 0.7947953251116557, "macro_f1": 0.8114440505015463}, {"mcc": 0.7174560014550105, "macro_f1": 0.7512275730454521}], "total": {"test_mcc": 75.23979936261217, "test_mcc_se": 1.4329443744033663, "test_macro_f1": 77.89726761183464, "test_macro_f1_se": 1.0557191545758522}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "angry-tweets", "task": "sentiment-classification", "dataset_languages": ["da"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"mcc": 0.5431222438251889, "macro_f1": 0.6886555385340949}, {"mcc": 0.5723301656538573, "macro_f1": 0.717650506014575}, {"mcc": 0.5690589546447363, "macro_f1": 0.7116493698706198}, {"mcc": 0.5709641416652599, "macro_f1": 0.714036213586413}, {"mcc": 0.5726926270599341, "macro_f1": 0.7199094395267228}, {"mcc": 0.5560995862948621, "macro_f1": 0.693078673407483}, {"mcc": 0.4464572749131401, "macro_f1": 0.6074624259897656}, {"mcc": 0.47698471518301017, "macro_f1": 0.6102852839410237}, {"mcc": 0.5676384853525716, "macro_f1": 0.707828199598174}, {"mcc": 0.45533283821339243, "macro_f1": 0.6064492505436795}], "total": {"test_mcc": 53.30681032805952, "test_mcc_se": 3.2245334838236497, "test_macro_f1": 67.7700490101255, "test_macro_f1_se": 3.0413018046815474}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "norec", "task": "sentiment-classification", "dataset_languages": ["nb", "nn", "no"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"mcc": 0.3225972243690069, "macro_f1": 0.3396173989113356}, {"mcc": 0.3557057662963673, "macro_f1": 0.3650783572918686}, {"mcc": 0.38877785614978766, "macro_f1": 0.39817051162303724}, {"mcc": 0.337520050990349, "macro_f1": 0.35376293151954025}, {"mcc": 0.3767746850890196, "macro_f1": 0.38282707949012273}, {"mcc": 0.3627780081300955, "macro_f1": 0.36429132545608045}, {"mcc": 0.3339697338735504, "macro_f1": 0.34107465857971847}, {"mcc": 0.36831279350389295, "macro_f1": 0.3740962743225809}, {"mcc": 0.39788516492848586, "macro_f1": 0.3952524619595572}, {"mcc": 0.3460676404036667, "macro_f1": 0.35491667158957396}], "total": {"test_mcc": 35.90388923734222, "test_mcc_se": 1.5191742047018821, "test_macro_f1": 36.69087670743415, "test_macro_f1_se": 1.2777234717829047}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "hotter-and-colder-sentiment", "task": "sentiment-classification", "dataset_languages": ["is"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"mcc": 0.5293223794549218, "macro_f1": 0.6782913009970944}, {"mcc": 0.5432932697535204, "macro_f1": 0.6877733109617168}, {"mcc": 0.5233182274538088, "macro_f1": 0.6750521162719885}, {"mcc": 0.4620294630321174, "macro_f1": 0.6275902173581209}, {"mcc": 0.5124822029756176, "macro_f1": 0.6735260281596567}, {"mcc": 0.518480120427234, "macro_f1": 0.6718037990320521}, {"mcc": 0.5135914528671447, "macro_f1": 0.6691550638005433}, {"mcc": 0.4385471567374013, "macro_f1": 0.581052227916388}, {"mcc": 0.4986352864693179, "macro_f1": 0.6623005264289555}, {"mcc": 0.46379409631921265, "macro_f1": 0.6214369202947269}], "total": {"test_mcc": 50.03493655490298, "test_mcc_se": 2.1159952460338656, "test_macro_f1": 65.47981511221244, "test_macro_f1_se": 2.0910289238611197}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "sb10k", "task": "sentiment-classification", "dataset_languages": ["de"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"mcc": 0.5634519353322689, "macro_f1": 0.6977042357217004}, {"mcc": 0.6081553013071291, "macro_f1": 0.7311691191898655}, {"mcc": 0.5641040305503017, "macro_f1": 0.7023306448427221}, {"mcc": 0.5057665730438125, "macro_f1": 0.6746330853612313}, {"mcc": 0.5559583481578131, "macro_f1": 0.7024484499640686}, {"mcc": 0.614501987884445, "macro_f1": 0.7416026747107387}, {"mcc": 0.5703826793719001, "macro_f1": 0.7039011406190004}, {"mcc": 0.5274324055414693, "macro_f1": 0.6793560097249873}, {"mcc": 0.6213471144205533, "macro_f1": 0.7436657409662356}, {"mcc": 0.6133101850737869, "macro_f1": 0.7386593308541589}], "total": {"test_mcc": 57.444105606834796, "test_mcc_se": 2.4414228103260767, "test_macro_f1": 71.1547043195471, "test_macro_f1_se": 1.5831395334489118}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "dutch-social", "task": "sentiment-classification", "dataset_languages": ["nl"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"mcc": 0.1293915513638039, "macro_f1": 0.1949915426736619}, {"mcc": 0.0876348047763749, "macro_f1": 0.15146690644945168}, {"mcc": 0.07428931831746204, "macro_f1": 0.16592192384096877}, {"mcc": 0.09409575516302025, "macro_f1": 0.17249609019293757}, {"mcc": 0.11658624941023601, "macro_f1": 0.20095767119412122}, {"mcc": 0.09342491926887644, "macro_f1": 0.18430954223576357}, {"mcc": 0.10568367436804967, "macro_f1": 0.1883718450499272}, {"mcc": 0.09325301908491819, "macro_f1": 0.16857486636290117}, {"mcc": 0.08332577140777532, "macro_f1": 0.17042233735756132}, {"mcc": 0.14743199508165847, "macro_f1": 0.2253053459486477}], "total": {"test_mcc": 10.25117058242175, "test_mcc_se": 1.3972109225057083, "test_macro_f1": 18.22818071305942, "test_macro_f1_se": 1.3138404728646604}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "sst5", "task": "sentiment-classification", "dataset_languages": ["en"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"mcc": 0.681725158910689, "macro_f1": 0.6995570955654989}, {"mcc": 0.6838284997539841, "macro_f1": 0.7176484957895872}, {"mcc": 0.6608278514018309, "macro_f1": 0.7127990582408344}, {"mcc": 0.7030159196423935, "macro_f1": 0.6946316737402468}, {"mcc": 0.6588108879368754, "macro_f1": 0.6565609083509442}, {"mcc": 0.6843764309698588, "macro_f1": 0.7350676230901437}, {"mcc": 0.683684125814639, "macro_f1": 0.7012107246452962}, {"mcc": 0.6816481727441435, "macro_f1": 0.6942691581257646}, {"mcc": 0.7148160115949559, "macro_f1": 0.6871908769874926}, {"mcc": 0.7087073031935408, "macro_f1": 0.7419263505277721}], "total": {"test_mcc": 68.61440361962912, "test_mcc_se": 1.142517385225068, "test_macro_f1": 70.40861965063581, "test_macro_f1_se": 1.5196460505788398}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "fosent", "task": "sentiment-classification", "dataset_languages": ["fo"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"mcc": 0.3834889788481912, "macro_f1": 0.40861375541835576}, {"mcc": 0.3370927849654417, "macro_f1": 0.40921458853695486}, {"mcc": 0.4151757420569233, "macro_f1": 0.4401962894499169}, {"mcc": 0.2526162866527519, "macro_f1": 0.314204025350848}, {"mcc": 0.19028663274049754, "macro_f1": 0.25156739811912227}, {"mcc": 0.23213281268877678, "macro_f1": 0.2989533830655326}, {"mcc": 0.36554535860878146, "macro_f1": 0.41560421735604214}, {"mcc": 0.4684783043708422, "macro_f1": 0.4654377880184331}, {"mcc": 0.3329871076049936, "macro_f1": 0.3546395580931161}, {"mcc": 0.33382901746732385, "macro_f1": 0.3828520499108734}], "total": {"test_mcc": 33.11633026004523, "test_mcc_se": 5.295713045957502, "test_macro_f1": 37.41283053319195, "test_macro_f1_se": 4.211959698273861}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "suc3", "task": "named-entity-recognition", "dataset_languages": ["sv"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"micro_f1_no_misc": 0.001276324186343331, "micro_f1": 0.0011396011396011397}, {"micro_f1_no_misc": 0.01335113484646195, "micro_f1": 0.011940298507462687}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.007518796992481203, "micro_f1": 0.006607929515418502}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}], "total": {"test_micro_f1_no_misc": 0.22146256025286484, "test_micro_f1_no_misc_se": 0.2828023708898285, "test_micro_f1": 0.19687829162482332, "test_micro_f1_se": 0.25199795026452254}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "dansk", "task": "named-entity-recognition", "dataset_languages": ["da"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"micro_f1_no_misc": 0.26386806596701645, "micro_f1": 0.21402660217654176}, {"micro_f1_no_misc": 0.014941302027748132, "micro_f1": 0.01354784081287045}, {"micro_f1_no_misc": 0.006389776357827476, "micro_f1": 0.006855184233076264}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0019102196752626551, "micro_f1": 0.0015117157974300832}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0035555555555555553, "micro_f1": 0.0029006526468455403}, {"micro_f1_no_misc": 0.003872216844143272, "micro_f1": 0.003108003108003108}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}], "total": {"test_micro_f1_no_misc": 2.9453713642755357, "test_micro_f1_no_misc_se": 5.112996037127366, "test_micro_f1": 2.4194999877476717, "test_micro_f1_se": 4.1425817051205245}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "norne-nb", "task": "named-entity-recognition", "dataset_languages": ["nb", "no"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"micro_f1_no_misc": 0.14948636202621324, "micro_f1": 0.13946117274167988}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.00423908435777872, "micro_f1": 0.0037707390648567115}, {"micro_f1_no_misc": 0.06995884773662552, "micro_f1": 0.06311926605504588}, {"micro_f1_no_misc": 0.06515463917525773, "micro_f1": 0.06610969851071558}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.1566218809980806, "micro_f1": 0.14310285517715857}, {"micro_f1_no_misc": 0.0026052974381241857, "micro_f1": 0.0023594180102241443}, {"micro_f1_no_misc": 0.045454545454545456, "micro_f1": 0.04414856341976174}, {"micro_f1_no_misc": 0.005183585313174946, "micro_f1": 0.004665629860031104}], "total": {"test_micro_f1_no_misc": 4.987042424998004, "test_micro_f1_no_misc_se": 3.76804606171015, "test_micro_f1": 4.667373428394737, "test_micro_f1_se": 3.4871512410635845}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "norne-nn", "task": "named-entity-recognition", "dataset_languages": ["nn"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"micro_f1_no_misc": 0.043696275071633234, "micro_f1": 0.04215162000629128}, {"micro_f1_no_misc": 0.01446516939474686, "micro_f1": 0.013440860215053764}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.16550412175015852, "micro_f1": 0.1592871066555277}, {"micro_f1_no_misc": 0.09463512162612463, "micro_f1": 0.0925764192139738}, {"micro_f1_no_misc": 0.10218487394957984, "micro_f1": 0.09851355290002914}, {"micro_f1_no_misc": 0.011909192407889839, "micro_f1": 0.014545454545454545}, {"micro_f1_no_misc": 0.034324942791762014, "micro_f1": 0.03449419568822554}, {"micro_f1_no_misc": 0.015742397137745973, "micro_f1": 0.018233259981138006}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}], "total": {"test_micro_f1_no_misc": 4.824620941296409, "test_micro_f1_no_misc_se": 3.4072859210367907, "test_micro_f1": 4.732424692056938, "test_micro_f1_se": 3.2629496807597533}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "mim-gold-ner", "task": "named-entity-recognition", "dataset_languages": ["is"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"micro_f1_no_misc": 0.17691723614274865, "micro_f1": 0.1682005712472231}, {"micro_f1_no_misc": 0.015238095238095238, "micro_f1": 0.013572854291417165}, {"micro_f1_no_misc": 0.0028571428571428576, "micro_f1": 0.0024330900243309003}, {"micro_f1_no_misc": 0.02776572668112798, "micro_f1": 0.02569750367107195}, {"micro_f1_no_misc": 0.044311887515977845, "micro_f1": 0.03946441155743481}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.15028901734104047, "micro_f1": 0.15271565495207667}, {"micro_f1_no_misc": 0.006927263730826323, "micro_f1": 0.010551948051948052}, {"micro_f1_no_misc": 0.08851774530271399, "micro_f1": 0.07884813164209806}, {"micro_f1_no_misc": 0.32005447735784814, "micro_f1": 0.27563185503099663}], "total": {"test_micro_f1_no_misc": 8.328785921675214, "test_micro_f1_no_misc_se": 6.4614273841988, "test_micro_f1": 7.671160204685974, "test_micro_f1_se": 5.750671000350302}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "fone", "task": "named-entity-recognition", "dataset_languages": ["fo"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"micro_f1_no_misc": 0.017976668579078217, "micro_f1": 0.01738487146291844}, {"micro_f1_no_misc": 0.08707529451937852, "micro_f1": 0.08419667817793126}, {"micro_f1_no_misc": 0.48198814409484725, "micro_f1": 0.4372642951788808}, {"micro_f1_no_misc": 0.5188422917897224, "micro_f1": 0.4903005755702409}, {"micro_f1_no_misc": 0.0003717472118959108, "micro_f1": 0.00036245016310257344}, {"micro_f1_no_misc": 0.6658390233809229, "micro_f1": 0.6492821564605918}, {"micro_f1_no_misc": 0.21824303642485463, "micro_f1": 0.21272885789014823}, {"micro_f1_no_misc": 0.22243285939968407, "micro_f1": 0.21977038914566865}, {"micro_f1_no_misc": 0.4057349591317165, "micro_f1": 0.3990323402088108}, {"micro_f1_no_misc": 0.09135285913528593, "micro_f1": 0.08993288590604026}], "total": {"test_micro_f1_no_misc": 27.098568836673863, "test_micro_f1_no_misc_se": 14.439953681971593, "test_micro_f1": 26.00255500164334, "test_micro_f1_se": 13.786539926721744}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "germeval", "task": "named-entity-recognition", "dataset_languages": ["de"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"micro_f1_no_misc": 0.31173780487804875, "micro_f1": 0.27067221891731114}, {"micro_f1_no_misc": 0.011875309252845127, "micro_f1": 0.012313104661389622}, {"micro_f1_no_misc": 0.08872109485606418, "micro_f1": 0.09773828756058157}, {"micro_f1_no_misc": 0.5425925925925926, "micro_f1": 0.4174174174174174}, {"micro_f1_no_misc": 0.25245098039215685, "micro_f1": 0.23126483553747035}, {"micro_f1_no_misc": 0.003131524008350731, "micro_f1": 0.0027894002789400274}, {"micro_f1_no_misc": 0.24276457883369332, "micro_f1": 0.2333456017666544}, {"micro_f1_no_misc": 0.22340898976412996, "micro_f1": 0.19505198996055934}, {"micro_f1_no_misc": 0.27880741337630943, "micro_f1": 0.25518453427065024}, {"micro_f1_no_misc": 0.16521340064249657, "micro_f1": 0.1527219583485568}], "total": {"test_micro_f1_no_misc": 21.207036885966872, "test_micro_f1_no_misc_se": 9.853812917284928, "test_micro_f1": 18.68499348719531, "test_micro_f1_se": 7.799222617698559}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "conll-nl", "task": "named-entity-recognition", "dataset_languages": ["nl"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"micro_f1_no_misc": 0.014736842105263156, "micro_f1": 0.012012012012012012}, {"micro_f1_no_misc": 0.08139534883720931, "micro_f1": 0.05673758865248227}, {"micro_f1_no_misc": 0.01693121693121693, "micro_f1": 0.012470771628994546}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.013086150490730643, "micro_f1": 0.009111617312072893}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.006734006734006735, "micro_f1": 0.004780876494023904}, {"micro_f1_no_misc": 0.19392752203721839, "micro_f1": 0.16473791695030632}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.021531100478468897, "micro_f1": 0.018487394957983197}], "total": {"test_micro_f1_no_misc": 3.483421876141141, "test_micro_f1_no_misc_se": 3.769166110129337, "test_micro_f1": 2.7833817800787513, "test_micro_f1_se": 3.1575669492511262}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "conll-en", "task": "named-entity-recognition", "dataset_languages": ["en"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.004110152075626799, "micro_f1": 0.0036127167630057803}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0012104095218882388, "micro_f1": 0.0010570824524312895}, {"micro_f1_no_misc": 0.007723577235772358, "micro_f1": 0.007180039490217195}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}], "total": {"test_micro_f1_no_misc": 0.13044138833287394, "test_micro_f1_no_misc_se": 0.16135763514164098, "test_micro_f1": 0.11849838705654266, "test_micro_f1_se": 0.148524157948745}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "scala-sv", "task": "linguistic-acceptability", "dataset_languages": ["sv"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"mcc": 0.4137986103982084, "macro_f1": 0.6157233386197714}, {"mcc": 0.2585286138722492, "macro_f1": 0.4696708651208947}, {"mcc": 0.41503843300540477, "macro_f1": 0.6120230977921611}, {"mcc": 0.37173383854295544, "macro_f1": 0.567491385696481}, {"mcc": 0.4093714668919048, "macro_f1": 0.6057703690037294}, {"mcc": 0.46646366143378337, "macro_f1": 0.6610257684802783}, {"mcc": 0.17818545558570503, "macro_f1": 0.40726962721847637}, {"mcc": 0.3544054393742578, "macro_f1": 0.5622354478027896}, {"mcc": 0.5141695766944222, "macro_f1": 0.6948719697406869}, {"mcc": 0.5424006361475704, "macro_f1": 0.729399720612635}], "total": {"test_mcc": 39.24095731946462, "test_mcc_se": 6.844488241705739, "test_macro_f1": 59.25481590087903, "test_macro_f1_se": 6.057092417208138}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "scala-da", "task": "linguistic-acceptability", "dataset_languages": ["da"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"mcc": 0.4681306953186072, "macro_f1": 0.6814500444315597}, {"mcc": 0.5184827520839671, "macro_f1": 0.7268834254526662}, {"mcc": 0.49766448927583484, "macro_f1": 0.7137469139908796}, {"mcc": 0.4683875849498317, "macro_f1": 0.6859269290809455}, {"mcc": 0.5060563529805143, "macro_f1": 0.7197188557637366}, {"mcc": 0.4455696656020552, "macro_f1": 0.6504626947467314}, {"mcc": 0.5702540764899319, "macro_f1": 0.7745744565301721}, {"mcc": 0.3436456298197423, "macro_f1": 0.5622868315643281}, {"mcc": 0.5260210871443004, "macro_f1": 0.7253076447476017}, {"mcc": 0.46122129366962156, "macro_f1": 0.6724555415244453}], "total": {"test_mcc": 48.05433627334407, "test_mcc_se": 3.761010631626206, "test_macro_f1": 69.12813337833066, "test_macro_f1_se": 3.546216977859651}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "scala-nb", "task": "linguistic-acceptability", "dataset_languages": ["nb", "no"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"mcc": 0.4960697742980028, "macro_f1": 0.7426627654418496}, {"mcc": 0.5171860582642519, "macro_f1": 0.7344225830229718}, {"mcc": 0.5588332643790115, "macro_f1": 0.779175572519084}, {"mcc": 0.5218777592581803, "macro_f1": 0.7186502744177772}, {"mcc": 0.47485858480549425, "macro_f1": 0.7034185247175858}, {"mcc": 0.5371506204184272, "macro_f1": 0.7656089355094368}, {"mcc": 0.503413895694159, "macro_f1": 0.7230397463798907}, {"mcc": 0.5492272691208515, "macro_f1": 0.7624164382767071}, {"mcc": 0.5076382521698755, "macro_f1": 0.7531042796865581}, {"mcc": 0.5460258006765797, "macro_f1": 0.7729153805048026}], "total": {"test_mcc": 52.122812790848336, "test_mcc_se": 1.6487339817011175, "test_macro_f1": 74.55414500476664, "test_macro_f1_se": 1.5681321199088538}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "scala-nn", "task": "linguistic-acceptability", "dataset_languages": ["nn"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"mcc": 0.3405708295803104, "macro_f1": 0.6266065461832786}, {"mcc": 0.31390417752681354, "macro_f1": 0.62679254266979}, {"mcc": 0.4132286309727339, "macro_f1": 0.7046023132960103}, {"mcc": 0.4017674987421968, "macro_f1": 0.689546127444798}, {"mcc": 0.3964505964107317, "macro_f1": 0.6917961183165808}, {"mcc": 0.3217769870419202, "macro_f1": 0.6565334556504787}, {"mcc": 0.44911105069864343, "macro_f1": 0.705090540758418}, {"mcc": 0.4214208261886389, "macro_f1": 0.6999379310528544}, {"mcc": 0.3479368394513742, "macro_f1": 0.6591833886258469}, {"mcc": 0.389724379262503, "macro_f1": 0.6884847759593016}], "total": {"test_mcc": 37.95891815875866, "test_mcc_se": 2.8308842695770253, "test_macro_f1": 67.48573739957357, "test_macro_f1_se": 1.8863167971143864}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "scala-is", "task": "linguistic-acceptability", "dataset_languages": ["is"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"mcc": 0.1758600290184196, "macro_f1": 0.5065870591867163}, {"mcc": 0.09075594915817226, "macro_f1": 0.40531556688044523}, {"mcc": 0.13919643852920338, "macro_f1": 0.5044333536614294}, {"mcc": 0.14040589105306978, "macro_f1": 0.46318415883633274}, {"mcc": 0.11693668486195583, "macro_f1": 0.4564670985871516}, {"mcc": 0.1561736778986728, "macro_f1": 0.43617758760800707}, {"mcc": 0.1293495172171019, "macro_f1": 0.5134841628959276}, {"mcc": 0.09209055029970445, "macro_f1": 0.4183266571616086}, {"mcc": 0.11688757097278059, "macro_f1": 0.5047776621835609}, {"mcc": 0.14569992293335407, "macro_f1": 0.5132612246342734}], "total": {"test_mcc": 13.033562319424346, "test_mcc_se": 1.6716466200995401, "test_macro_f1": 47.22014531635453, "test_macro_f1_se": 2.586693245007774}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "scala-fo", "task": "linguistic-acceptability", "dataset_languages": ["fo"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"mcc": 0.15549144314267593, "macro_f1": 0.4958376332989336}, {"mcc": 0.20945320050090444, "macro_f1": 0.6036572254285542}, {"mcc": 0.09992661199718773, "macro_f1": 0.4231634574841883}, {"mcc": 0.16268615702840103, "macro_f1": 0.5680019486055292}, {"mcc": 0.16894253365155262, "macro_f1": 0.4681811609709266}, {"mcc": 0.2700759620476077, "macro_f1": 0.6344923419915403}, {"mcc": 0.20182059966863075, "macro_f1": 0.595627539632576}, {"mcc": 0.04591896500499579, "macro_f1": 0.375554496773341}, {"mcc": 0.15442684665245368, "macro_f1": 0.47993905535804976}, {"mcc": 0.14317845233337434, "macro_f1": 0.5371326606344845}], "total": {"test_mcc": 16.11920772027784, "test_mcc_se": 3.77250750527051, "test_macro_f1": 51.81587520178124, "test_macro_f1_se": 5.206459054920721}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "scala-de", "task": "linguistic-acceptability", "dataset_languages": ["de"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"mcc": 0.414082702153617, "macro_f1": 0.6849808021460055}, {"mcc": 0.2344648783736487, "macro_f1": 0.5874017172168828}, {"mcc": 0.4432522990707149, "macro_f1": 0.7216199536224032}, {"mcc": 0.35141816087991556, "macro_f1": 0.6626583747339934}, {"mcc": 0.4630930000417455, "macro_f1": 0.728620109031771}, {"mcc": 0.4526278925481227, "macro_f1": 0.7028961543447789}, {"mcc": 0.3421664856708963, "macro_f1": 0.6649000682686299}, {"mcc": 0.3681120545984324, "macro_f1": 0.6809395644727184}, {"mcc": 0.3046999066107562, "macro_f1": 0.639045850548265}, {"mcc": 0.41307203759655775, "macro_f1": 0.7065345026671463}], "total": {"test_mcc": 37.86989417544406, "test_mcc_se": 4.510308268777127, "test_macro_f1": 67.79597097052594, "test_macro_f1_se": 2.62058644180321}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "scala-nl", "task": "linguistic-acceptability", "dataset_languages": ["nl"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"mcc": 0.6376172502744445, "macro_f1": 0.8183203907851473}, {"mcc": 0.5088016776586124, "macro_f1": 0.7357963770497432}, {"mcc": 0.5724780445291722, "macro_f1": 0.7805768457777769}, {"mcc": 0.5964428338970055, "macro_f1": 0.7876463560392826}, {"mcc": 0.5581671755895634, "macro_f1": 0.7749296978534053}, {"mcc": 0.624268374254267, "macro_f1": 0.8083297998724134}, {"mcc": 0.5794190709871615, "macro_f1": 0.7879285955994628}, {"mcc": 0.5919785072984116, "macro_f1": 0.7913456746070847}, {"mcc": 0.5611371787537556, "macro_f1": 0.7583587524575908}, {"mcc": 0.6087348106588499, "macro_f1": 0.8041046409363499}], "total": {"test_mcc": 58.39044923901243, "test_mcc_se": 2.2936674571468254, "test_macro_f1": 78.47337130978256, "test_macro_f1_se": 1.5078250462336291}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "scala-en", "task": "linguistic-acceptability", "dataset_languages": ["en"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"mcc": 0.5155384745352172, "macro_f1": 0.7460415451811915}, {"mcc": 0.5306758397510598, "macro_f1": 0.7512944543842113}, {"mcc": 0.5031096361266452, "macro_f1": 0.7376085738643283}, {"mcc": 0.487878037745937, "macro_f1": 0.6901170498351121}, {"mcc": 0.4799625339789055, "macro_f1": 0.7040360878332298}, {"mcc": 0.47469386672995073, "macro_f1": 0.6917066615719376}, {"mcc": 0.5151398051067533, "macro_f1": 0.7154483858410137}, {"mcc": 0.4879610479577064, "macro_f1": 0.7115348370927319}, {"mcc": 0.4856073745269031, "macro_f1": 0.7201784306005782}, {"mcc": 0.4622656197296102, "macro_f1": 0.6869128526724719}], "total": {"test_mcc": 49.428322361886885, "test_mcc_se": 1.3163214714731843, "test_macro_f1": 71.54878878876806, "test_macro_f1_se": 1.446140132147797}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"f1": 42.89575086954821, "em": 25.984848484848484}, {"f1": 40.569019250462134, "em": 23.12357846853677}, {"f1": 39.27271825249863, "em": 30.76923076923077}, {"f1": 49.58280964181686, "em": 33.181126331811264}, {"f1": 17.909650695072045, "em": 14.957594448727834}, {"f1": 43.74040635360633, "em": 29.16030534351145}, {"f1": 48.58844300374155, "em": 29.641185647425896}, {"f1": 45.62721305916285, "em": 31.5251572327044}, {"f1": 34.740262081973256, "em": 25.61919504643963}, {"f1": 16.063064854315794, "em": 10.95679012345679}], "total": {"test_f1": 37.898933806219766, "test_f1_se": 7.348087090594845, "test_em": 25.49190118966933, "test_em_se": 4.536587589341532}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "norquad", "task": "reading-comprehension", "dataset_languages": ["nb", "nn", "no"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"f1": 2.2008078593537856, "em": 0.0}, {"f1": 25.21678798575944, "em": 8.861788617886178}, {"f1": 2.1042172249848194, "em": 0.0}, {"f1": 22.155396146036793, "em": 9.076042518397383}, {"f1": 5.501152025187001, "em": 1.3234077750206783}, {"f1": 28.130634869915383, "em": 11.358024691358025}, {"f1": 2.5540496112244506, "em": 0.08403361344537816}, {"f1": 3.3621667211305266, "em": 0.5067567567567568}, {"f1": 2.0952450802798745, "em": 0.0}, {"f1": 21.891762226058916, "em": 9.105960264900663}], "total": {"test_f1": 11.5212219749931, "test_f1_se": 6.950237363111707, "test_em": 4.031601423776506, "test_em_se": 3.0103445846566417}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "scandiqa-sv", "task": "reading-comprehension", "dataset_languages": ["sv"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"f1": 1.5602033375550326, "em": 0.0}, {"f1": 41.43655211889229, "em": 31.53904473085671}, {"f1": 55.49976812221864, "em": 44.36674436674436}, {"f1": 57.32636340028397, "em": 44.52054794520548}, {"f1": 53.71990975823864, "em": 42.09714726291442}, {"f1": 38.60113214920997, "em": 31.297709923664122}, {"f1": 42.170689330754605, "em": 32.68330733229329}, {"f1": 52.23899238903028, "em": 43.003144654088054}, {"f1": 39.51316757364438, "em": 31.73374613003096}, {"f1": 1.118502913653054, "em": 0.0}], "total": {"test_f1": 38.31852810934809, "test_f1_se": 12.816926845424208, "test_em": 30.12413923457974, "test_em_se": 10.426301458562046}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "nqii", "task": "reading-comprehension", "dataset_languages": ["is"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"f1": 38.98901544575218, "em": 11.384615384615385}, {"f1": 9.150292019924585, "em": 1.4018691588785046}, {"f1": 49.98668045784892, "em": 21.339563862928348}, {"f1": 37.19118056746448, "em": 14.798206278026905}, {"f1": 12.920244726888821, "em": 2.0376175548589344}, {"f1": 54.60536315059387, "em": 21.879815100154083}, {"f1": 52.362197329254535, "em": 20.615384615384617}, {"f1": 34.84469436836315, "em": 11.809815950920246}, {"f1": 34.060214311585916, "em": 9.337349397590362}, {"f1": 19.991742607338107, "em": 7.680722891566265}], "total": {"test_f1": 34.410162498501464, "test_f1_se": 9.928795222786292, "test_em": 12.228496019492365, "test_em_se": 4.638671627282399}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "foqa", "task": "reading-comprehension", "dataset_languages": ["fo"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"f1": 2.548399433711103, "em": 1.2254901960784315}, {"f1": 0.6882689433373642, "em": 0.0}, {"f1": 59.442780264190866, "em": 33.58024691358025}, {"f1": 61.376538401267595, "em": 40.53398058252427}, {"f1": 7.435045801475861, "em": 2.9702970297029703}, {"f1": 1.9992712286921543, "em": 0.7407407407407407}, {"f1": 0.7613876987735784, "em": 0.2358490566037736}, {"f1": 3.611554866007792, "em": 2.544529262086514}, {"f1": 26.933688994532943, "em": 18.204488778054863}, {"f1": 5.166897671397172, "em": 2.9484029484029484}], "total": {"test_f1": 16.99638333033864, "test_f1_se": 14.9608221384095, "test_em": 10.298402550777475, "test_em_se": 9.394300757095426}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "germanquad", "task": "reading-comprehension", "dataset_languages": ["de"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"f1": 16.159176091515647, "em": 5.681818181818182}, {"f1": 24.5669489564856, "em": 7.202426080363912}, {"f1": 9.52340399379786, "em": 2.4087024087024087}, {"f1": 25.854614592151904, "em": 7.610350076103501}, {"f1": 24.508904949325643, "em": 9.406322282189668}, {"f1": 15.002956497920321, "em": 4.961832061068702}, {"f1": 27.171341199449852, "em": 10.29641185647426}, {"f1": 7.4447618600373495, "em": 2.830188679245283}, {"f1": 7.838093702439815, "em": 2.2445820433436534}, {"f1": 38.96421916524725, "em": 16.435185185185187}], "total": {"test_f1": 19.703442100837123, "test_f1_se": 6.336597251257852, "test_em": 6.907781885449476, "test_em_se": 2.717277433366508}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "squad", "task": "reading-comprehension", "dataset_languages": ["en"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"f1": 17.95368842253298, "em": 11.515151515151516}, {"f1": 27.716968083814837, "em": 18.423047763457163}, {"f1": 5.100354968108061, "em": 1.2432012432012431}, {"f1": 63.3938595110184, "em": 48.32572298325723}, {"f1": 4.706455896196987, "em": 1.079414032382421}, {"f1": 15.441981691546237, "em": 10.305343511450381}, {"f1": 35.31459223251463, "em": 25.663026521060843}, {"f1": 20.439924508671705, "em": 12.578616352201259}, {"f1": 17.574030351351382, "em": 11.455108359133128}, {"f1": 20.800929319718797, "em": 15.200617283950617}], "total": {"test_f1": 22.8442784985474, "test_f1_se": 10.501795022723512, "test_em": 15.57892495652458, "test_em_se": 8.43876921711843}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "squad-nl", "task": "reading-comprehension", "dataset_languages": ["nl"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"f1": 52.10034973012577, "em": 25.0}, {"f1": 49.83615363384794, "em": 29.264594389689158}, {"f1": 58.919565330128, "em": 34.032634032634036}, {"f1": 21.695242648610325, "em": 13.318112633181126}, {"f1": 12.647602814904802, "em": 3.1611410948342327}, {"f1": 9.79624942782031, "em": 4.885496183206107}, {"f1": 29.253181730174557, "em": 11.154446177847113}, {"f1": 62.818887240846685, "em": 41.90251572327044}, {"f1": 43.70149396365186, "em": 23.761609907120743}, {"f1": 40.932454161425966, "em": 21.373456790123456}], "total": {"test_f1": 38.17011806815363, "test_f1_se": 11.71250880864665, "test_em": 20.785400693190645, "test_em_se": 7.8187981822618005}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "nordjylland-news", "task": "summarization", "dataset_languages": ["da"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"bertscore": 0.5594175746809924, "rouge_l": 0.040725026306754425}, {"bertscore": 0.5642519854591228, "rouge_l": 0.046815859053296785}, {"bertscore": 0.5705113770090975, "rouge_l": 0.05677517127675061}, {"bertscore": 0.567498038726626, "rouge_l": 0.0504035691623399}, {"bertscore": 0.5775612724100938, "rouge_l": 0.06882749139575006}, {"bertscore": 0.5749058215005789, "rouge_l": 0.0656219838807547}, {"bertscore": 0.5552231682959246, "rouge_l": 0.03336490282464363}, {"bertscore": 0.573210915812524, "rouge_l": 0.06489840514976239}, {"bertscore": 0.5630018198717153, "rouge_l": 0.043454097173878006}, {"bertscore": 0.5563923032023013, "rouge_l": 0.03568397394539009}], "total": {"test_bertscore": 56.619742769689765, "test_bertscore_se": 0.4849660152182266, "test_rouge_l": 5.065704801693206, "test_rouge_l_se": 0.7955979310780776}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "mlsum", "task": "summarization", "dataset_languages": ["de"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"bertscore": 0.5701531070371857, "rouge_l": 0.03893733853478193}, {"bertscore": 0.5705435342970304, "rouge_l": 0.041392518824634905}, {"bertscore": 0.5702602405217476, "rouge_l": 0.04106654488340862}, {"bertscore": 0.568198663459043, "rouge_l": 0.0397757876846276}, {"bertscore": 0.5683873056404991, "rouge_l": 0.03837772686845296}, {"bertscore": 0.5685718000604538, "rouge_l": 0.039049070356357504}, {"bertscore": 0.5703185902821133, "rouge_l": 0.04132225421696572}, {"bertscore": 0.5707955245161429, "rouge_l": 0.04079767249154721}, {"bertscore": 0.5716312838776503, "rouge_l": 0.04207640302105163}, {"bertscore": 0.5702498221944552, "rouge_l": 0.03865072428874238}], "total": {"test_bertscore": 56.991098718863206, "test_bertscore_se": 0.07051937383894677, "test_rouge_l": 4.014460411705705, "test_rouge_l_se": 0.08289796792046392}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "rrn", "task": "summarization", "dataset_languages": ["is"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"bertscore": 0.5373790576704778, "rouge_l": 0.02468691157579926}, {"bertscore": 0.5455515736830421, "rouge_l": 0.02933671673873294}, {"bertscore": 0.5464763533673249, "rouge_l": 0.029703029178358577}, {"bertscore": 0.5396301093569491, "rouge_l": 0.028298963776352517}, {"bertscore": 0.5442904981318861, "rouge_l": 0.028226917070711486}, {"bertscore": 0.5373940026329365, "rouge_l": 0.023695663151063986}, {"bertscore": 0.5420944373763632, "rouge_l": 0.02538133991938625}, {"bertscore": 0.5339986095204949, "rouge_l": 0.021732666744733516}, {"bertscore": 0.5351957010279875, "rouge_l": 0.02368552638896837}, {"bertscore": 0.5360148539184593, "rouge_l": 0.02430165082500447}], "total": {"test_bertscore": 53.980251966859214, "test_bertscore_se": 0.2802068340805361, "test_rouge_l": 2.5904938536911137, "test_rouge_l_se": 0.17145283143825452}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "no-sammendrag", "task": "summarization", "dataset_languages": ["nb", "nn", "no"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"bertscore": 0.577727608018904, "rouge_l": 0.06393419091749108}, {"bertscore": 0.569801684396225, "rouge_l": 0.051866352898770146}, {"bertscore": 0.5721060716168722, "rouge_l": 0.05301004152795684}, {"bertscore": 0.5671588399272878, "rouge_l": 0.04652937815753111}, {"bertscore": 0.5666363202035427, "rouge_l": 0.04857328103210471}, {"bertscore": 0.5689610459521646, "rouge_l": 0.05296086171230463}, {"bertscore": 0.5741922191082267, "rouge_l": 0.06193444420136944}, {"bertscore": 0.5645949893223587, "rouge_l": 0.04483568988156551}, {"bertscore": 0.5736409885575995, "rouge_l": 0.05862843871066058}, {"bertscore": 0.5660677904379554, "rouge_l": 0.04505485179885872}], "total": {"test_bertscore": 57.008875575411366, "test_bertscore_se": 0.2614034638525689, "test_rouge_l": 5.273275308386127, "test_rouge_l_se": 0.42485702205739906}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "wiki-lingua-nl", "task": "summarization", "dataset_languages": ["nl"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"bertscore": 0.5823102879949147, "rouge_l": 0.007567995177994785}, {"bertscore": 0.5823447895236313, "rouge_l": 0.009346346986975196}, {"bertscore": 0.5830159095639829, "rouge_l": 0.0059862907261835395}, {"bertscore": 0.5866115279932274, "rouge_l": 0.014244230680491687}, {"bertscore": 0.5848648520914139, "rouge_l": 0.011681500891056067}, {"bertscore": 0.5824080285528908, "rouge_l": 0.011320977368898796}, {"bertscore": 0.5869647553190589, "rouge_l": 0.012222311806099482}, {"bertscore": 0.583420032824506, "rouge_l": 0.007436246814238741}, {"bertscore": 0.5843102326616645, "rouge_l": 0.012246093480520802}, {"bertscore": 0.5828610578173539, "rouge_l": 0.012300070996574073}], "total": {"test_bertscore": 58.39111474342644, "test_bertscore_se": 0.10756479066914504, "test_rouge_l": 1.0435206492903315, "test_rouge_l_se": 0.16655568034235327}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "swedn", "task": "summarization", "dataset_languages": ["sv"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"bertscore": 0.6080983312713215, "rouge_l": 0.13017636915527603}, {"bertscore": 0.607712061231723, "rouge_l": 0.13233425458091258}, {"bertscore": 0.6050079388951417, "rouge_l": 0.11605272610940962}, {"bertscore": 0.6054973892605631, "rouge_l": 0.1175128242065597}, {"bertscore": 0.5828579788212664, "rouge_l": 0.04621894590455783}, {"bertscore": 0.5975171681056963, "rouge_l": 0.09807942496775357}, {"bertscore": 0.5911007279646583, "rouge_l": 0.08104879285585374}, {"bertscore": 0.6076354327378795, "rouge_l": 0.13150308484092688}, {"bertscore": 0.596133566548815, "rouge_l": 0.09389762946545288}, {"bertscore": 0.6145028929458931, "rouge_l": 0.13382005627900131}], "total": {"test_bertscore": 60.16063487782958, "test_bertscore_se": 0.5906615602647416, "test_rouge_l": 10.806441083657042, "test_rouge_l_se": 1.7650737058228418}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "cnn-dailymail", "task": "summarization", "dataset_languages": ["en"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"bertscore": 0.5859016389003955, "rouge_l": 0.11117578106068843}, {"bertscore": 0.5827146692899987, "rouge_l": 0.10862261777281765}, {"bertscore": 0.5899303875048645, "rouge_l": 0.11409981092029059}, {"bertscore": 0.5946561063174158, "rouge_l": 0.12012093878888597}, {"bertscore": 0.5662757670506835, "rouge_l": 0.09008169931019924}, {"bertscore": 0.5724603014532477, "rouge_l": 0.09904491513020126}, {"bertscore": 0.5775650506402599, "rouge_l": 0.10277963895791392}, {"bertscore": 0.5747224354417995, "rouge_l": 0.09998977853292648}, {"bertscore": 0.5755566100706346, "rouge_l": 0.10095139242651319}, {"bertscore": 0.5737606248731026, "rouge_l": 0.09953590006696482}], "total": {"test_bertscore": 57.93543591542403, "test_bertscore_se": 0.5423458105493247, "test_rouge_l": 10.464024729674016, "test_rouge_l_se": 0.5449432242520671}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "danske-talemaader", "task": "knowledge", "dataset_languages": ["da"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"mcc": 0.7339215574765668, "accuracy": 0.7995049504950495}, {"mcc": 0.7740612239111455, "accuracy": 0.8267326732673267}, {"mcc": 0.784216097821844, "accuracy": 0.8353960396039604}, {"mcc": 0.7227005696820578, "accuracy": 0.7871287128712872}, {"mcc": 0.7885526629560087, "accuracy": 0.8403465346534653}, {"mcc": 0.7681291438274639, "accuracy": 0.8254950495049505}, {"mcc": 0.7831367113918346, "accuracy": 0.8353960396039604}, {"mcc": 0.7661335815608664, "accuracy": 0.8217821782178217}, {"mcc": 0.729500750648613, "accuracy": 0.7945544554455446}, {"mcc": 0.7687601523993578, "accuracy": 0.8242574257425742}], "total": {"test_mcc": 76.1911245167576, "test_mcc_se": 1.5012672789984924, "test_accuracy": 81.9059405940594, "test_accuracy_se": 1.154771235823982}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "danish-citizen-tests", "task": "knowledge", "dataset_languages": ["da"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"mcc": 0.854285622496446, "accuracy": 0.90234375}, {"mcc": 0.8084351890628634, "accuracy": 0.873046875}, {"mcc": 0.8493531806116648, "accuracy": 0.8984375}, {"mcc": 0.8842108592172409, "accuracy": 0.921875}, {"mcc": 0.8542068244052119, "accuracy": 0.900390625}, {"mcc": 0.8459508827325825, "accuracy": 0.896484375}, {"mcc": 0.8518132568515167, "accuracy": 0.90234375}, {"mcc": 0.8470966475304144, "accuracy": 0.89453125}, {"mcc": 0.8274803250627434, "accuracy": 0.884765625}, {"mcc": 0.8266759243317157, "accuracy": 0.884765625}], "total": {"test_mcc": 84.495087123024, "test_mcc_se": 1.266661979750147, "test_accuracy": 89.58984375, "test_accuracy_se": 0.8151699337818098}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "mmlu-no", "task": "knowledge", "dataset_languages": ["nb", "nn", "no"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"mcc": 0.5322032510552124, "accuracy": 0.6416015625}, {"mcc": 0.5595434544626157, "accuracy": 0.66455078125}, {"mcc": 0.5299866640936405, "accuracy": 0.63818359375}, {"mcc": 0.5450149007766889, "accuracy": 0.6533203125}, {"mcc": 0.530586616526943, "accuracy": 0.64599609375}, {"mcc": 0.5275193106820515, "accuracy": 0.6416015625}, {"mcc": 0.5193485309381034, "accuracy": 0.630859375}, {"mcc": 0.5458242071237613, "accuracy": 0.6552734375}, {"mcc": 0.5737609709949656, "accuracy": 0.67626953125}, {"mcc": 0.523159670858271, "accuracy": 0.62939453125}], "total": {"test_mcc": 53.869475775122524, "test_mcc_se": 1.0694129258590124, "test_accuracy": 64.7705078125, "test_accuracy_se": 0.9183123641207599}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "mmlu-sv", "task": "knowledge", "dataset_languages": ["sv"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"mcc": 0.5633206468688217, "accuracy": 0.65869140625}, {"mcc": 0.5156284845658857, "accuracy": 0.61865234375}, {"mcc": 0.5672321174383615, "accuracy": 0.65283203125}, {"mcc": 0.5286594364949825, "accuracy": 0.6337890625}, {"mcc": 0.5695816973938898, "accuracy": 0.66943359375}, {"mcc": 0.6052236351818437, "accuracy": 0.70361328125}, {"mcc": 0.5190496059033934, "accuracy": 0.619140625}, {"mcc": 0.5838779802666422, "accuracy": 0.68115234375}, {"mcc": 0.5743892179193412, "accuracy": 0.673828125}, {"mcc": 0.5830512467291487, "accuracy": 0.673828125}], "total": {"test_mcc": 56.1001406876231, "test_mcc_se": 1.8624988453402702, "test_accuracy": 65.849609375, "test_accuracy_se": 1.7183501349884809}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "mmlu-de", "task": "knowledge", "dataset_languages": ["de"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"mcc": 0.54707664084976, "accuracy": 0.6455078125}, {"mcc": 0.5337344821411869, "accuracy": 0.63916015625}, {"mcc": 0.4581732049640348, "accuracy": 0.54248046875}, {"mcc": 0.40661519953474035, "accuracy": 0.5341796875}, {"mcc": 0.5163225976777801, "accuracy": 0.62646484375}, {"mcc": 0.5280958035720177, "accuracy": 0.6328125}, {"mcc": 0.5059547180056123, "accuracy": 0.611328125}, {"mcc": 0.576636540156112, "accuracy": 0.6708984375}, {"mcc": 0.44637121592707857, "accuracy": 0.55419921875}, {"mcc": 0.4384594486612506, "accuracy": 0.54541015625}], "total": {"test_mcc": 49.57439851489574, "test_mcc_se": 3.4118137470312457, "test_accuracy": 60.0244140625, "test_accuracy_se": 3.1505585681507213}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "mmlu-nl", "task": "knowledge", "dataset_languages": ["nl"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"mcc": 0.5214718480015963, "accuracy": 0.611328125}, {"mcc": 0.572248365818687, "accuracy": 0.66845703125}, {"mcc": 0.5534542363868212, "accuracy": 0.65771484375}, {"mcc": 0.5744510787091662, "accuracy": 0.6689453125}, {"mcc": 0.5559146253594954, "accuracy": 0.65673828125}, {"mcc": 0.5597637249219622, "accuracy": 0.66162109375}, {"mcc": 0.5763035791631835, "accuracy": 0.66845703125}, {"mcc": 0.4992890163734092, "accuracy": 0.591796875}, {"mcc": 0.4414680704566834, "accuracy": 0.55224609375}, {"mcc": 0.4491288714452363, "accuracy": 0.5556640625}], "total": {"test_mcc": 53.034934166362405, "test_mcc_se": 3.160749966424751, "test_accuracy": 62.9296875, "test_accuracy_se": 2.945964042357232}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "mmlu", "task": "knowledge", "dataset_languages": ["en"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"mcc": 0.6847339341040247, "accuracy": 0.75634765625}, {"mcc": 0.6224434027690409, "accuracy": 0.708984375}, {"mcc": 0.6934149859120098, "accuracy": 0.76611328125}, {"mcc": 0.637139639421107, "accuracy": 0.71923828125}, {"mcc": 0.694616562277943, "accuracy": 0.7705078125}, {"mcc": 0.6919062416631478, "accuracy": 0.76513671875}, {"mcc": 0.5490456182106029, "accuracy": 0.64013671875}, {"mcc": 0.6824860510712208, "accuracy": 0.75927734375}, {"mcc": 0.6074604016273955, "accuracy": 0.6923828125}, {"mcc": 0.6022206835213567, "accuracy": 0.6865234375}], "total": {"test_mcc": 64.65467520577849, "test_mcc_se": 3.1321317075627757, "test_accuracy": 72.646484375, "test_accuracy_se": 2.7365367322983043}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "arc-is", "task": "knowledge", "dataset_languages": ["is"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"mcc": 0.6185938006889105, "accuracy": 0.70703125}, {"mcc": 0.6325863471364538, "accuracy": 0.71484375}, {"mcc": 0.5970553126415509, "accuracy": 0.681640625}, {"mcc": 0.6075036627588309, "accuracy": 0.6962890625}, {"mcc": 0.6169382952223941, "accuracy": 0.7099609375}, {"mcc": 0.6189825554381512, "accuracy": 0.712890625}, {"mcc": 0.5880505385535036, "accuracy": 0.6826171875}, {"mcc": 0.5962893149283315, "accuracy": 0.6884765625}, {"mcc": 0.5953121954025696, "accuracy": 0.6884765625}, {"mcc": 0.5866792843600748, "accuracy": 0.6806640625}], "total": {"test_mcc": 60.5799130713077, "test_mcc_se": 0.9556315974193069, "test_accuracy": 69.62890625, "test_accuracy_se": 0.849310940960826}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "hellaswag-da", "task": "common-sense-reasoning", "dataset_languages": ["da"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"mcc": 0.685984769673776, "accuracy": 0.75927734375}, {"mcc": 0.6859484747062305, "accuracy": 0.7578125}, {"mcc": 0.683831646883513, "accuracy": 0.755859375}, {"mcc": 0.698643902680106, "accuracy": 0.76708984375}, {"mcc": 0.653407089534732, "accuracy": 0.7333984375}, {"mcc": 0.7265588686702606, "accuracy": 0.79052734375}, {"mcc": 0.5594869927602871, "accuracy": 0.6533203125}, {"mcc": 0.6903470898550873, "accuracy": 0.765625}, {"mcc": 0.6703018930909037, "accuracy": 0.748046875}, {"mcc": 0.7011919642127381, "accuracy": 0.7734375}], "total": {"test_mcc": 67.55702692067634, "test_mcc_se": 2.7937746650172253, "test_accuracy": 75.0439453125, "test_accuracy_se": 2.3130103194849543}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "hellaswag-no", "task": "common-sense-reasoning", "dataset_languages": ["nb", "nn", "no"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"mcc": 0.670728987011555, "accuracy": 0.74755859375}, {"mcc": 0.7050651295364616, "accuracy": 0.7744140625}, {"mcc": 0.7046600596194351, "accuracy": 0.775390625}, {"mcc": 0.6481945298460119, "accuracy": 0.73095703125}, {"mcc": 0.7111426799010541, "accuracy": 0.779296875}, {"mcc": 0.6793038247305945, "accuracy": 0.7587890625}, {"mcc": 0.6771388707198346, "accuracy": 0.75341796875}, {"mcc": 0.7062676611036033, "accuracy": 0.77685546875}, {"mcc": 0.6779207725680527, "accuracy": 0.75341796875}, {"mcc": 0.6955052500117263, "accuracy": 0.767578125}], "total": {"test_mcc": 68.75927765048327, "test_mcc_se": 1.251722480615865, "test_accuracy": 76.1767578125, "test_accuracy_se": 0.9720609115715912}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "hellaswag-sv", "task": "common-sense-reasoning", "dataset_languages": ["sv"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"mcc": 0.7031904849395731, "accuracy": 0.76904296875}, {"mcc": 0.6613808386791628, "accuracy": 0.7333984375}, {"mcc": 0.7172796674269145, "accuracy": 0.78076171875}, {"mcc": 0.6621667639807484, "accuracy": 0.73779296875}, {"mcc": 0.7338486788750976, "accuracy": 0.7978515625}, {"mcc": 0.724031044778427, "accuracy": 0.79052734375}, {"mcc": 0.7189192633356185, "accuracy": 0.7841796875}, {"mcc": 0.3885118017190759, "accuracy": 0.4775390625}, {"mcc": 0.6496388321448958, "accuracy": 0.72412109375}, {"mcc": 0.6586930439635716, "accuracy": 0.732421875}], "total": {"test_mcc": 66.17660419843085, "test_mcc_se": 6.265251254631051, "test_accuracy": 73.2763671875, "test_accuracy_se": 5.809775283226248}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "winogrande-is", "task": "common-sense-reasoning", "dataset_languages": ["is"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"mcc": 0.18847614183218842, "accuracy": 0.6004464285714286}, {"mcc": 0.20197344654669483, "accuracy": 0.5111607142857143}, {"mcc": 0.15910672913880258, "accuracy": 0.6015625}, {"mcc": 0.21514776762916127, "accuracy": 0.5792410714285714}, {"mcc": 0.24702660069709217, "accuracy": 0.5747767857142857}, {"mcc": 0.07081118138637606, "accuracy": 0.48325892857142855}, {"mcc": 0.1545499309476249, "accuracy": 0.5558035714285714}, {"mcc": 0.20743243837405506, "accuracy": 0.5915178571428571}, {"mcc": 0.1594702566411581, "accuracy": 0.5892857142857143}, {"mcc": 0.17799217494829084, "accuracy": 0.5055803571428571}], "total": {"test_mcc": 17.819866681414442, "test_mcc_se": 2.953372603249693, "test_accuracy": 55.92633928571428, "test_accuracy_se": 2.697808274849646}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "hellaswag-de", "task": "common-sense-reasoning", "dataset_languages": ["de"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"mcc": 0.6610545837772798, "accuracy": 0.7392578125}, {"mcc": 0.6503266069239213, "accuracy": 0.72412109375}, {"mcc": 0.7521686992356762, "accuracy": 0.810546875}, {"mcc": 0.6603118111963692, "accuracy": 0.73828125}, {"mcc": 0.7213397700758951, "accuracy": 0.78857421875}, {"mcc": 0.512761403974056, "accuracy": 0.60546875}, {"mcc": 0.6923714357040472, "accuracy": 0.7626953125}, {"mcc": 0.6180234226649677, "accuracy": 0.701171875}, {"mcc": 0.7219615785506391, "accuracy": 0.78466796875}, {"mcc": 0.6151948987131285, "accuracy": 0.6943359375}], "total": {"test_mcc": 66.0551421081598, "test_mcc_se": 4.268485278939294, "test_accuracy": 73.4912109375, "test_accuracy_se": 3.6688541874848055}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "hellaswag-nl", "task": "common-sense-reasoning", "dataset_languages": ["nl"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"mcc": 0.6774439556763263, "accuracy": 0.75390625}, {"mcc": 0.6707222389369312, "accuracy": 0.748046875}, {"mcc": 0.710926773093992, "accuracy": 0.77685546875}, {"mcc": 0.6038076036080513, "accuracy": 0.68603515625}, {"mcc": 0.6837037788125447, "accuracy": 0.7607421875}, {"mcc": 0.598148395762674, "accuracy": 0.69189453125}, {"mcc": 0.689549574311427, "accuracy": 0.76171875}, {"mcc": 0.6842023419265669, "accuracy": 0.76171875}, {"mcc": 0.6462441752047051, "accuracy": 0.728515625}, {"mcc": 0.6415624382772773, "accuracy": 0.72314453125}], "total": {"test_mcc": 66.06311275610494, "test_mcc_se": 2.313757984972327, "test_accuracy": 73.92578125, "test_accuracy_se": 1.9189477587364248}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "hellaswag", "task": "common-sense-reasoning", "dataset_languages": ["en"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"mcc": 0.7232242841023314, "accuracy": 0.7822265625}, {"mcc": 0.795814675363805, "accuracy": 0.84228515625}, {"mcc": 0.7359828829809185, "accuracy": 0.79296875}, {"mcc": 0.8023145834388745, "accuracy": 0.84716796875}, {"mcc": 0.8489745055087946, "accuracy": 0.88623046875}, {"mcc": 0.8439517078623312, "accuracy": 0.8818359375}, {"mcc": 0.8204684208558977, "accuracy": 0.86376953125}, {"mcc": 0.8450848150741014, "accuracy": 0.88330078125}, {"mcc": 0.8139024563815853, "accuracy": 0.85693359375}, {"mcc": 0.7868674259323867, "accuracy": 0.8359375}], "total": {"test_mcc": 80.16585757501026, "test_mcc_se": 2.7044892336302877, "test_accuracy": 84.7265625, "test_accuracy_se": 2.2356144391279287}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "results": {"raw": [{"test_speed": 238.56, "test_speed_short": 27.36}, {"test_speed": 461.07, "test_speed_short": 51.3}, {"test_speed": 683.6400000000001, "test_speed_short": 97.73}, {"test_speed": 910.44, "test_speed_short": 120.60000000000001}, {"test_speed": 1147.77, "test_speed_short": 147.49}, {"test_speed": 1380.8799999999999, "test_speed_short": 193.23000000000002}, {"test_speed": 1585.93, "test_speed_short": 421.12}, {"test_speed": 1806.42, "test_speed_short": 236.43}, {"test_speed": 2044.44, "test_speed_short": 260.52}, {"test_speed": 2285.2599999999998, "test_speed_short": 283.05}], "total": {"test_speed": 1254.441, "test_speed_se": 425.1297655341941, "test_speed_short": 183.88299999999998, "test_speed_short_se": 74.39585035558068}}, "num_model_parameters": 70553706496, "max_sequence_length": 131072, "vocabulary_size": 128256, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "swerec", "task": "sentiment-classification", "dataset_languages": ["sv"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"mcc": 0.77922491693918, "macro_f1": 0.7662167390446378}, {"mcc": 0.7665268397306418, "macro_f1": 0.749928872285868}, {"mcc": 0.7693779905845116, "macro_f1": 0.6922220639571277}, {"mcc": 0.7726215322977414, "macro_f1": 0.7837425628917094}, {"mcc": 0.7400001698844768, "macro_f1": 0.7503103216397254}, {"mcc": 0.7552801835868578, "macro_f1": 0.7195292249737034}, {"mcc": 0.7757966709328356, "macro_f1": 0.7399155374697844}, {"mcc": 0.7199698434490237, "macro_f1": 0.7478426346974856}, {"mcc": 0.7427348586497912, "macro_f1": 0.6725520806223161}, {"mcc": 0.735201331184462, "macro_f1": 0.7538007209988863}], "total": {"test_mcc": 75.56734337239521, "test_mcc_se": 1.2498203987267822, "test_macro_f1": 73.76060758581244, "test_macro_f1_se": 2.091818391264664}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "angry-tweets", "task": "sentiment-classification", "dataset_languages": ["da"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"mcc": 0.557314237479433, "macro_f1": 0.7102218985569634}, {"mcc": 0.5270656781277497, "macro_f1": 0.6850286237956119}, {"mcc": 0.5656352206693218, "macro_f1": 0.7023539223773837}, {"mcc": 0.5455976067467143, "macro_f1": 0.6923767910924826}, {"mcc": 0.524178192427203, "macro_f1": 0.682536365199074}, {"mcc": 0.5651884092374293, "macro_f1": 0.7049406174016681}, {"mcc": 0.4938444427694864, "macro_f1": 0.6646864980338087}, {"mcc": 0.4956707227374006, "macro_f1": 0.6596500693415522}, {"mcc": 0.5255081465469348, "macro_f1": 0.6393000012212368}, {"mcc": 0.47323861842634235, "macro_f1": 0.6425693456426911}], "total": {"test_mcc": 52.732412751680144, "test_mcc_se": 1.981238211679197, "test_macro_f1": 67.83664132662473, "test_macro_f1_se": 1.586871394127413}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "norec", "task": "sentiment-classification", "dataset_languages": ["nb", "nn", "no"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"mcc": 0.34359828187249297, "macro_f1": 0.3583996459445216}, {"mcc": 0.3425797582810196, "macro_f1": 0.3569065244749336}, {"mcc": 0.39334831784198593, "macro_f1": 0.40434154017119733}, {"mcc": 0.3717914323321932, "macro_f1": 0.38689503732981995}, {"mcc": 0.30676153552589347, "macro_f1": 0.32134463671983693}, {"mcc": 0.3542322318394876, "macro_f1": 0.36984992440627296}, {"mcc": 0.3265621260444123, "macro_f1": 0.3425344274665158}, {"mcc": 0.3037383621498202, "macro_f1": 0.31384886655225214}, {"mcc": 0.3659473151135595, "macro_f1": 0.38578755481043664}, {"mcc": 0.3405965010295743, "macro_f1": 0.35391856314805753}], "total": {"test_mcc": 34.49155862030439, "test_mcc_se": 1.742228805121102, "test_macro_f1": 35.93826721023845, "test_macro_f1_se": 1.7803840798478372}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "hotter-and-colder-sentiment", "task": "sentiment-classification", "dataset_languages": ["is"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"mcc": 0.39423344716032743, "macro_f1": 0.44987895639663567}, {"mcc": 0.3591900176141498, "macro_f1": 0.4375364209610784}, {"mcc": 0.33702925157384894, "macro_f1": 0.41071958866102715}, {"mcc": 0.36403371079121627, "macro_f1": 0.4565520403499172}, {"mcc": 0.3572391756474705, "macro_f1": 0.4364290166828167}, {"mcc": 0.38593719047642583, "macro_f1": 0.4536731092303467}, {"mcc": 0.3437266337174814, "macro_f1": 0.4250167216460863}, {"mcc": 0.4153956258280103, "macro_f1": 0.5392114177625752}, {"mcc": 0.38899856773672564, "macro_f1": 0.4477670706149433}, {"mcc": 0.3672579512559397, "macro_f1": 0.43927506612712897}], "total": {"test_mcc": 37.13041571801596, "test_mcc_se": 1.510616868222346, "test_macro_f1": 44.96059408432555, "test_macro_f1_se": 2.131128602150518}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "sb10k", "task": "sentiment-classification", "dataset_languages": ["de"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"mcc": 0.5770448227080287, "macro_f1": 0.7004855162300281}, {"mcc": 0.609766969264999, "macro_f1": 0.7249462494027711}, {"mcc": 0.6466366904795795, "macro_f1": 0.7511055550524529}, {"mcc": 0.5379941781301614, "macro_f1": 0.6894696722413745}, {"mcc": 0.6032380259122117, "macro_f1": 0.7243404244856437}, {"mcc": 0.6838477556561929, "macro_f1": 0.7777307222854403}, {"mcc": 0.596055905591499, "macro_f1": 0.7286965712535968}, {"mcc": 0.5605982481956523, "macro_f1": 0.6954491376261869}, {"mcc": 0.6252827633690969, "macro_f1": 0.7034894908451946}, {"mcc": 0.6200182847532388, "macro_f1": 0.7449437737673031}], "total": {"test_mcc": 60.604836440606604, "test_mcc_se": 2.607212950284473, "test_macro_f1": 72.40657113189992, "test_macro_f1_se": 1.7365000707891654}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "dutch-social", "task": "sentiment-classification", "dataset_languages": ["nl"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"mcc": 0.11115418157682087, "macro_f1": 0.1677003931156815}, {"mcc": 0.09201981599043765, "macro_f1": 0.15057673509286412}, {"mcc": 0.05992210105408479, "macro_f1": 0.17319865624199693}, {"mcc": 0.09949212140788824, "macro_f1": 0.18022173837815156}, {"mcc": 0.04812720390901005, "macro_f1": 0.14827162660115809}, {"mcc": 0.07533355820016815, "macro_f1": 0.17254220245247165}, {"mcc": 0.07289888332851384, "macro_f1": 0.164455984077899}, {"mcc": 0.10560843990138005, "macro_f1": 0.16763309806705617}, {"mcc": 0.09037099509793313, "macro_f1": 0.19782307352098716}, {"mcc": 0.1170310238107538, "macro_f1": 0.19326028648062546}], "total": {"test_mcc": 8.719583242769904, "test_mcc_se": 1.4017078874540005, "test_macro_f1": 17.156837940288916, "test_macro_f1_se": 0.9914225802210933}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "sst5", "task": "sentiment-classification", "dataset_languages": ["en"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"mcc": 0.663603185189594, "macro_f1": 0.6697601025229134}, {"mcc": 0.65298816170301, "macro_f1": 0.6556497083080681}, {"mcc": 0.6867535243399225, "macro_f1": 0.6415377141106094}, {"mcc": 0.6845508046328078, "macro_f1": 0.6341629732157582}, {"mcc": 0.648276376749775, "macro_f1": 0.6114064776292628}, {"mcc": 0.6743607087420054, "macro_f1": 0.6289548164064082}, {"mcc": 0.6773360929423916, "macro_f1": 0.669034903133373}, {"mcc": 0.6845712601736906, "macro_f1": 0.655592780555892}, {"mcc": 0.6917913525967245, "macro_f1": 0.6295412781977983}, {"mcc": 0.6832003260330016, "macro_f1": 0.6752281484298129}], "total": {"test_mcc": 67.47431793102923, "test_mcc_se": 0.9248611223096109, "test_macro_f1": 64.70868902509898, "test_macro_f1_se": 1.313334185781317}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "fosent", "task": "sentiment-classification", "dataset_languages": ["fo"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"mcc": 0.17819089302382127, "macro_f1": 0.26902489521789624}, {"mcc": 0.29056942986140327, "macro_f1": 0.36286536972512584}, {"mcc": 0.2815248087956514, "macro_f1": 0.3467878463868841}, {"mcc": 0.24648163667718362, "macro_f1": 0.31593827870423613}, {"mcc": 0.07219638224126225, "macro_f1": 0.1620515748406128}, {"mcc": 0.2498943084220013, "macro_f1": 0.3486012837736976}, {"mcc": 0.14501199109309537, "macro_f1": 0.23712487692110107}, {"mcc": 0.3596921328038024, "macro_f1": 0.4044521859442365}, {"mcc": 0.26268841126783943, "macro_f1": 0.3501831501831501}, {"mcc": 0.2851502477492636, "macro_f1": 0.3940914940914941}], "total": {"test_mcc": 23.714002419353243, "test_mcc_se": 5.153485238142029, "test_macro_f1": 31.911209557884344, "test_macro_f1_se": 4.680752488435181}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "suc3", "task": "named-entity-recognition", "dataset_languages": ["sv"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}], "total": {"test_micro_f1_no_misc": 0.0, "test_micro_f1_no_misc_se": 0.0, "test_micro_f1": 0.0, "test_micro_f1_se": 0.0}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "dansk", "task": "named-entity-recognition", "dataset_languages": ["da"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}], "total": {"test_micro_f1_no_misc": 0.0, "test_micro_f1_no_misc_se": 0.0, "test_micro_f1": 0.0, "test_micro_f1_se": 0.0}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "norne-nb", "task": "named-entity-recognition", "dataset_languages": ["nb", "no"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}], "total": {"test_micro_f1_no_misc": 0.0, "test_micro_f1_no_misc_se": 0.0, "test_micro_f1": 0.0, "test_micro_f1_se": 0.0}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "norne-nn", "task": "named-entity-recognition", "dataset_languages": ["nn"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}], "total": {"test_micro_f1_no_misc": 0.0, "test_micro_f1_no_misc_se": 0.0, "test_micro_f1": 0.0, "test_micro_f1_se": 0.0}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "mim-gold-ner", "task": "named-entity-recognition", "dataset_languages": ["is"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}], "total": {"test_micro_f1_no_misc": 0.0, "test_micro_f1_no_misc_se": 0.0, "test_micro_f1": 0.0, "test_micro_f1_se": 0.0}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "fone", "task": "named-entity-recognition", "dataset_languages": ["fo"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}], "total": {"test_micro_f1_no_misc": 0.0, "test_micro_f1_no_misc_se": 0.0, "test_micro_f1": 0.0, "test_micro_f1_se": 0.0}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "germeval", "task": "named-entity-recognition", "dataset_languages": ["de"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}], "total": {"test_micro_f1_no_misc": 0.0, "test_micro_f1_no_misc_se": 0.0, "test_micro_f1": 0.0, "test_micro_f1_se": 0.0}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "conll-nl", "task": "named-entity-recognition", "dataset_languages": ["nl"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}], "total": {"test_micro_f1_no_misc": 0.0, "test_micro_f1_no_misc_se": 0.0, "test_micro_f1": 0.0, "test_micro_f1_se": 0.0}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "conll-en", "task": "named-entity-recognition", "dataset_languages": ["en"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}], "total": {"test_micro_f1_no_misc": 0.0, "test_micro_f1_no_misc_se": 0.0, "test_micro_f1": 0.0, "test_micro_f1_se": 0.0}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "scala-sv", "task": "linguistic-acceptability", "dataset_languages": ["sv"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"mcc": 0.36434742077468374, "macro_f1": 0.5827251430098469}, {"mcc": 0.0, "macro_f1": 0.3269799539927703}, {"mcc": 0.5101066879730792, "macro_f1": 0.7449802477077025}, {"mcc": 0.15002175664016, "macro_f1": 0.3810001040117962}, {"mcc": 0.521668932058994, "macro_f1": 0.7421111383375534}, {"mcc": 0.34230171061679215, "macro_f1": 0.5628979498581785}, {"mcc": 0.3071625113568182, "macro_f1": 0.5217516565831173}, {"mcc": 0.4769564532272868, "macro_f1": 0.6925443411555616}, {"mcc": 0.49077262940815275, "macro_f1": 0.7033105639916786}, {"mcc": 0.18463318611885507, "macro_f1": 0.4044981809821344}], "total": {"test_mcc": 33.47971288174822, "test_mcc_se": 10.945344217376759, "test_macro_f1": 56.627992796303396, "test_macro_f1_se": 9.637532355476953}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "scala-da", "task": "linguistic-acceptability", "dataset_languages": ["da"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"mcc": 0.17388784201100413, "macro_f1": 0.4181912386604668}, {"mcc": 0.10553144593184957, "macro_f1": 0.3644214162348877}, {"mcc": 0.32459626719612494, "macro_f1": 0.5526003501896403}, {"mcc": 0.16774110755625044, "macro_f1": 0.40306759141869886}, {"mcc": 0.48702008149822507, "macro_f1": 0.7384955404280737}, {"mcc": 0.08000434707099151, "macro_f1": 0.3475155828097004}, {"mcc": 0.3274269381073653, "macro_f1": 0.5645490264846589}, {"mcc": 0.12260887759691123, "macro_f1": 0.3672292254423228}, {"mcc": 0.27418121526251854, "macro_f1": 0.5033683755939454}, {"mcc": 0.415842555720053, "macro_f1": 0.6534662652509511}], "total": {"test_mcc": 24.788406779512936, "test_mcc_se": 8.626462883325706, "test_macro_f1": 49.129046125133456, "test_macro_f1_se": 8.314751078957524}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "scala-nb", "task": "linguistic-acceptability", "dataset_languages": ["nb", "no"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"mcc": 0.45202650018770996, "macro_f1": 0.7141936424648735}, {"mcc": 0.48218670225783095, "macro_f1": 0.7165180872274695}, {"mcc": 0.47223538468664555, "macro_f1": 0.7085955599943283}, {"mcc": 0.55124415099094, "macro_f1": 0.7734980292056413}, {"mcc": 0.33664595697522515, "macro_f1": 0.5747840104949806}, {"mcc": 0.4970796193480153, "macro_f1": 0.7429797504730269}, {"mcc": 0.5244922199542048, "macro_f1": 0.7588000833768487}, {"mcc": 0.5357832182591626, "macro_f1": 0.7657427933671027}, {"mcc": 0.45800957382250823, "macro_f1": 0.7192982456140351}, {"mcc": 0.43237413022691507, "macro_f1": 0.6817038011067862}], "total": {"test_mcc": 47.42077456709158, "test_mcc_se": 3.824820967906632, "test_macro_f1": 71.56114003325092, "test_macro_f1_se": 3.5500736994265343}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "scala-nn", "task": "linguistic-acceptability", "dataset_languages": ["nn"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"mcc": 0.287518291013497, "macro_f1": 0.6417578321026597}, {"mcc": 0.32896335677247723, "macro_f1": 0.6277901693515077}, {"mcc": 0.25197177821271133, "macro_f1": 0.499404691504041}, {"mcc": 0.32744773969528773, "macro_f1": 0.660879258478782}, {"mcc": 0.17426791369936748, "macro_f1": 0.4240490988962252}, {"mcc": 0.20882862358248486, "macro_f1": 0.45454958454290634}, {"mcc": 0.40409077872774113, "macro_f1": 0.6914870924707184}, {"mcc": 0.3454184674287175, "macro_f1": 0.6619549233216291}, {"mcc": 0.3241240895198434, "macro_f1": 0.6262376095100882}, {"mcc": 0.27714724659412987, "macro_f1": 0.5262938360403552}], "total": {"test_mcc": 29.29778285246257, "test_mcc_se": 4.218144527333881, "test_macro_f1": 58.14404096218912, "test_macro_f1_se": 5.964363719532706}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "scala-is", "task": "linguistic-acceptability", "dataset_languages": ["is"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"mcc": 0.13652052103532974, "macro_f1": 0.568237323544334}, {"mcc": -0.005748030916068729, "macro_f1": 0.34028599184881214}, {"mcc": 0.07223269312095065, "macro_f1": 0.3616037324147454}, {"mcc": 0.11104761209025511, "macro_f1": 0.4962804885024299}, {"mcc": 0.04285683494141652, "macro_f1": 0.3992445427919424}, {"mcc": 0.12084266518562635, "macro_f1": 0.5603006713463268}, {"mcc": 0.14230500012538394, "macro_f1": 0.5140256254824854}, {"mcc": 0.14010581349336354, "macro_f1": 0.5698121296565662}, {"mcc": 0.10620604094250241, "macro_f1": 0.5181547872577773}, {"mcc": 0.1332727303386544, "macro_f1": 0.5412941774157829}], "total": {"test_mcc": 9.996418803574139, "test_mcc_se": 3.0401413064199487, "test_macro_f1": 48.69239470261203, "test_macro_f1_se": 5.407907889472158}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "scala-fo", "task": "linguistic-acceptability", "dataset_languages": ["fo"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"mcc": 0.08506458585249999, "macro_f1": 0.5018141038327066}, {"mcc": 0.10495618031658467, "macro_f1": 0.4383818522766581}, {"mcc": 0.04391418157031284, "macro_f1": 0.4305875772721508}, {"mcc": 0.07029831387808042, "macro_f1": 0.5351491569390402}, {"mcc": 0.1037922253647245, "macro_f1": 0.5517060818060862}, {"mcc": 0.11732258095700261, "macro_f1": 0.46613239762456904}, {"mcc": 0.09827611420377046, "macro_f1": 0.49257916364515353}, {"mcc": 0.09236236077106189, "macro_f1": 0.48055313997305915}, {"mcc": 0.12515336138798555, "macro_f1": 0.45368606701940034}, {"mcc": 0.07167907470547419, "macro_f1": 0.5341792432587082}], "total": {"test_mcc": 9.128189790074972, "test_mcc_se": 1.5077584375295106, "test_macro_f1": 48.847687836475316, "test_macro_f1_se": 2.6209006406802775}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "scala-de", "task": "linguistic-acceptability", "dataset_languages": ["de"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"mcc": 0.49316601461138354, "macro_f1": 0.7320039973653064}, {"mcc": 0.4431428776794457, "macro_f1": 0.6860880070293932}, {"mcc": 0.44224472006956694, "macro_f1": 0.6604545854899462}, {"mcc": 0.4725384845058354, "macro_f1": 0.7244338168299893}, {"mcc": 0.4611701183423902, "macro_f1": 0.6751532523584591}, {"mcc": 0.4720573963492319, "macro_f1": 0.6819202109137166}, {"mcc": 0.4986355433591883, "macro_f1": 0.7229805171489423}, {"mcc": 0.48164614186495636, "macro_f1": 0.7318727015997641}, {"mcc": 0.43282516045195896, "macro_f1": 0.7091959278958389}, {"mcc": 0.4329017773320317, "macro_f1": 0.677948717948718}], "total": {"test_mcc": 46.30328234565989, "test_mcc_se": 1.5105183520525316, "test_macro_f1": 70.02051734580074, "test_macro_f1_se": 1.6576222507955625}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "scala-nl", "task": "linguistic-acceptability", "dataset_languages": ["nl"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"mcc": 0.6459209025375507, "macro_f1": 0.8227467642314804}, {"mcc": 0.6210074073210806, "macro_f1": 0.8105006105006105}, {"mcc": 0.6243216888012443, "macro_f1": 0.810888666457491}, {"mcc": 0.5836964112201212, "macro_f1": 0.774166627401923}, {"mcc": 0.5346916507605582, "macro_f1": 0.7348352430892731}, {"mcc": 0.6141658719586957, "macro_f1": 0.8070272739525226}, {"mcc": 0.6118331710914693, "macro_f1": 0.804773207538605}, {"mcc": 0.6041319158289747, "macro_f1": 0.7939013805255615}, {"mcc": 0.6029352586648828, "macro_f1": 0.7951856236062724}, {"mcc": 0.5829159709725471, "macro_f1": 0.7876787781492908}], "total": {"test_mcc": 60.25620249157125, "test_mcc_se": 1.878282901600322, "test_macro_f1": 79.4170417545303, "test_macro_f1_se": 1.5466981244839113}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "scala-en", "task": "linguistic-acceptability", "dataset_languages": ["en"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"mcc": 0.5356141682405909, "macro_f1": 0.762691035043806}, {"mcc": 0.40511004892095165, "macro_f1": 0.6914148508547238}, {"mcc": 0.5086918494736995, "macro_f1": 0.7543054338848445}, {"mcc": 0.4853893859877155, "macro_f1": 0.6994733168348246}, {"mcc": 0.47496953536960124, "macro_f1": 0.7280415762941979}, {"mcc": 0.49982334181284754, "macro_f1": 0.7487578973195117}, {"mcc": 0.5036634288049754, "macro_f1": 0.7307307307307307}, {"mcc": 0.46461534569737517, "macro_f1": 0.7319041308068122}, {"mcc": 0.5061744107733405, "macro_f1": 0.747198625945839}, {"mcc": 0.4706584032016349, "macro_f1": 0.7332379245922005}], "total": {"test_mcc": 48.547099182827324, "test_mcc_se": 2.1905353568349497, "test_macro_f1": 73.27755522307491, "test_macro_f1_se": 1.4117509629640221}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"f1": 1.4311971730654225, "em": 0.0}, {"f1": 1.2795563334079558, "em": 0.0}, {"f1": 1.4991715238215015, "em": 0.0}, {"f1": 1.6412245725120442, "em": 0.0}, {"f1": 1.2083668764043518, "em": 0.0}, {"f1": 1.6459687184718734, "em": 0.0}, {"f1": 1.4409710152297563, "em": 0.0}, {"f1": 1.76700515706261, "em": 0.0}, {"f1": 1.2641796222334554, "em": 0.0}, {"f1": 2.428950309206724, "em": 0.0}], "total": {"test_f1": 1.5606591301415695, "test_f1_se": 0.22022003616201927, "test_em": 0.0, "test_em_se": 0.0}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "norquad", "task": "reading-comprehension", "dataset_languages": ["nb", "nn", "no"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"f1": 1.7817699234487625, "em": 0.0}, {"f1": 1.5595133490274808, "em": 0.0}, {"f1": 1.5264062450794447, "em": 0.0}, {"f1": 1.6107977249993675, "em": 0.0}, {"f1": 1.6614849593907375, "em": 0.0}, {"f1": 1.828880118371678, "em": 0.0}, {"f1": 1.9326335455786954, "em": 0.0}, {"f1": 1.5946631821198713, "em": 0.0}, {"f1": 1.7651264541806453, "em": 0.0}, {"f1": 1.800883361000607, "em": 0.0}], "total": {"test_f1": 1.706215886319729, "test_f1_se": 0.08311850797846995, "test_em": 0.0, "test_em_se": 0.0}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "scandiqa-sv", "task": "reading-comprehension", "dataset_languages": ["sv"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"f1": 1.7899053605288113, "em": 0.0}, {"f1": 2.4217733934213737, "em": 0.0}, {"f1": 2.361019762276809, "em": 0.0}, {"f1": 1.7274270128660048, "em": 0.0}, {"f1": 1.6728096113427786, "em": 0.0}, {"f1": 1.727066420608921, "em": 0.0}, {"f1": 2.3889042159626888, "em": 0.0}, {"f1": 2.3463083178807818, "em": 0.0}, {"f1": 2.74838331063048, "em": 0.0}, {"f1": 1.972529566920664, "em": 0.0}], "total": {"test_f1": 2.115612697243931, "test_f1_se": 0.2361558562560024, "test_em": 0.0, "test_em_se": 0.0}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "nqii", "task": "reading-comprehension", "dataset_languages": ["is"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"f1": 1.8988147940363411, "em": 0.0}, {"f1": 2.0335986467907814, "em": 0.0}, {"f1": 2.3492631083001863, "em": 0.0}, {"f1": 1.747625598948773, "em": 0.0}, {"f1": 2.1460736222544416, "em": 0.0}, {"f1": 1.9161326341180538, "em": 0.0}, {"f1": 2.394908410276013, "em": 0.0}, {"f1": 1.766636149572763, "em": 0.0}, {"f1": 2.046673074901299, "em": 0.0}, {"f1": 1.8172627316384382, "em": 0.0}], "total": {"test_f1": 2.0116988770837088, "test_f1_se": 0.14164382647742418, "test_em": 0.0, "test_em_se": 0.0}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "foqa", "task": "reading-comprehension", "dataset_languages": ["fo"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"f1": 0.5214166813312094, "em": 0.0}, {"f1": 0.5811425638744239, "em": 0.0}, {"f1": 0.7283903071221832, "em": 0.0}, {"f1": 0.7826593881117153, "em": 0.0}, {"f1": 0.8922829942007222, "em": 0.0}, {"f1": 0.7230109786949048, "em": 0.0}, {"f1": 0.43676344499150466, "em": 0.0}, {"f1": 0.4971381879730579, "em": 0.0}, {"f1": 0.6523173201283499, "em": 0.0}, {"f1": 0.794585221638546, "em": 0.0}], "total": {"test_f1": 0.6609707088066618, "test_f1_se": 0.09183772505847888, "test_em": 0.0, "test_em_se": 0.0}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "germanquad", "task": "reading-comprehension", "dataset_languages": ["de"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"f1": 1.622699550658741, "em": 0.0}, {"f1": 1.4346526086107096, "em": 0.0}, {"f1": 1.6020889250334647, "em": 0.0}, {"f1": 1.4009285586408513, "em": 0.0}, {"f1": 1.58056013046596, "em": 0.0}, {"f1": 1.4189070352932867, "em": 0.0}, {"f1": 1.5305995020138037, "em": 0.0}, {"f1": 1.44642474566846, "em": 0.0}, {"f1": 1.099949893372941, "em": 0.0}, {"f1": 1.761690194838168, "em": 0.0}], "total": {"test_f1": 1.4898501144596386, "test_f1_se": 0.11008754788155212, "test_em": 0.0, "test_em_se": 0.0}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "squad", "task": "reading-comprehension", "dataset_languages": ["en"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"f1": 3.986202518601913, "em": 0.0}, {"f1": 3.809320250279248, "em": 0.0}, {"f1": 3.8399002763248298, "em": 0.0}, {"f1": 3.5297008144588937, "em": 0.0}, {"f1": 3.662329727351491, "em": 0.0}, {"f1": 3.6773911288864136, "em": 0.0}, {"f1": 3.4407097732204046, "em": 0.0}, {"f1": 3.5336110091501047, "em": 0.0}, {"f1": 3.4479849704448555, "em": 0.0}, {"f1": 3.5335841761100957, "em": 0.0}], "total": {"test_f1": 3.646073464482825, "test_f1_se": 0.11342602703972818, "test_em": 0.0, "test_em_se": 0.0}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "squad-nl", "task": "reading-comprehension", "dataset_languages": ["nl"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"f1": 2.0377324260995975, "em": 0.0}, {"f1": 2.474825701744817, "em": 0.0}, {"f1": 2.3941485543839796, "em": 0.0}, {"f1": 2.0491632931817527, "em": 0.0}, {"f1": 1.9040709688412467, "em": 0.0}, {"f1": 1.9009975246482522, "em": 0.0}, {"f1": 2.209848090286777, "em": 0.0}, {"f1": 2.2397737720600315, "em": 0.0}, {"f1": 2.5708938453666907, "em": 0.0}, {"f1": 1.711225678965345, "em": 0.0}], "total": {"test_f1": 2.149267985557849, "test_f1_se": 0.17182616030620793, "test_em": 0.0, "test_em_se": 0.0}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "nordjylland-news", "task": "summarization", "dataset_languages": ["da"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"bertscore": 0.5858757945243269, "rouge_l": 0.05064867439940787}, {"bertscore": 0.584768045853707, "rouge_l": 0.05009668316851422}, {"bertscore": 0.5729548443050589, "rouge_l": 0.0412586762960782}, {"bertscore": 0.5784307626017835, "rouge_l": 0.041904012984260296}, {"bertscore": 0.5793899410055019, "rouge_l": 0.04908526098611942}, {"bertscore": 0.5654437936609611, "rouge_l": 0.045394952734738314}, {"bertscore": 0.5793456596293254, "rouge_l": 0.04571804619773301}, {"bertscore": 0.5796015269297641, "rouge_l": 0.046941929296540494}, {"bertscore": 0.5804264752659947, "rouge_l": 0.047436058598175665}, {"bertscore": 0.5812526156951208, "rouge_l": 0.04922018362442555}], "total": {"test_bertscore": 57.87489459471544, "test_bertscore_se": 0.362760513313132, "test_rouge_l": 4.677044782859931, "test_rouge_l_se": 0.20136856958135135}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "mlsum", "task": "summarization", "dataset_languages": ["de"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"bertscore": 0.5729263668326894, "rouge_l": 0.04256795836418967}, {"bertscore": 0.5749077124492032, "rouge_l": 0.04267115410561534}, {"bertscore": 0.5744730218139011, "rouge_l": 0.04229730932448466}, {"bertscore": 0.5748787833581446, "rouge_l": 0.04245247825372865}, {"bertscore": 0.5662654992047464, "rouge_l": 0.039877317886833796}, {"bertscore": 0.5737727001687745, "rouge_l": 0.04077854902300547}, {"bertscore": 0.573247692300356, "rouge_l": 0.04359504059702228}, {"bertscore": 0.5750195215223357, "rouge_l": 0.04038576758673471}, {"bertscore": 0.5664412049809471, "rouge_l": 0.036666263269326496}, {"bertscore": 0.568227717114496, "rouge_l": 0.03685552101234882}], "total": {"test_bertscore": 57.201602197455934, "test_bertscore_se": 0.22208062082070668, "test_rouge_l": 4.081473594232898, "test_rouge_l_se": 0.15052164698879178}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "rrn", "task": "summarization", "dataset_languages": ["is"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"bertscore": 0.5510508501320146, "rouge_l": 0.05282902128595771}, {"bertscore": 0.5488942279771436, "rouge_l": 0.0508494852950372}, {"bertscore": 0.5585091911489144, "rouge_l": 0.0515265845538581}, {"bertscore": 0.5632348577200901, "rouge_l": 0.044106506304634654}, {"bertscore": 0.5517393781337887, "rouge_l": 0.05002656973689554}, {"bertscore": 0.5580023391812574, "rouge_l": 0.048905139538692265}, {"bertscore": 0.5534137281647418, "rouge_l": 0.04628272761922338}, {"bertscore": 0.550446504203137, "rouge_l": 0.051282587701240515}, {"bertscore": 0.5649222001957241, "rouge_l": 0.04385597368868622}, {"bertscore": 0.546747429179959, "rouge_l": 0.05258882479388921}], "total": {"test_bertscore": 55.46960706036771, "test_bertscore_se": 0.38181230956653445, "test_rouge_l": 4.922534205181147, "test_rouge_l_se": 0.20753121696720123}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "no-sammendrag", "task": "summarization", "dataset_languages": ["nb", "nn", "no"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"bertscore": 0.5750313120952342, "rouge_l": 0.047016792725004905}, {"bertscore": 0.5708837926504202, "rouge_l": 0.04700060365347371}, {"bertscore": 0.5707626311341301, "rouge_l": 0.041127759303065836}, {"bertscore": 0.5719215189601528, "rouge_l": 0.04304189909329313}, {"bertscore": 0.5660772090486716, "rouge_l": 0.04448580573650784}, {"bertscore": 0.5733884485234739, "rouge_l": 0.046438220413625005}, {"bertscore": 0.5694859265349805, "rouge_l": 0.039694547877534264}, {"bertscore": 0.5685622659948422, "rouge_l": 0.04853574970121159}, {"bertscore": 0.5784851429052651, "rouge_l": 0.04654935746869782}, {"bertscore": 0.5780703380150953, "rouge_l": 0.03924040194693505}], "total": {"test_bertscore": 57.22668585862266, "test_bertscore_se": 0.24904924615262347, "test_rouge_l": 4.431311379193492, "test_rouge_l_se": 0.20736088668798094}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "wiki-lingua-nl", "task": "summarization", "dataset_languages": ["nl"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"bertscore": 0.5744116398564074, "rouge_l": 0.01681088994668916}, {"bertscore": 0.5745254685607506, "rouge_l": 0.020009375242546965}, {"bertscore": 0.5708378750277916, "rouge_l": 0.021922572024958155}, {"bertscore": 0.576279443121166, "rouge_l": 0.02363153794202065}, {"bertscore": 0.5699556660110829, "rouge_l": 0.017248535014909177}, {"bertscore": 0.5729440371214878, "rouge_l": 0.01814013344956815}, {"bertscore": 0.5734726817609044, "rouge_l": 0.019720929224697856}, {"bertscore": 0.5736828744993545, "rouge_l": 0.020130046948702296}, {"bertscore": 0.5726736493234057, "rouge_l": 0.02093195860249692}, {"bertscore": 0.5734742539207218, "rouge_l": 0.021136135268822204}], "total": {"test_bertscore": 57.32257589203073, "test_bertscore_se": 0.11227436016792919, "test_rouge_l": 1.996821136654115, "test_rouge_l_se": 0.13120222157523}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "swedn", "task": "summarization", "dataset_languages": ["sv"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"bertscore": 0.5688922801637091, "rouge_l": 0.028889469643743386}, {"bertscore": 0.5688593621161999, "rouge_l": 0.026128478960979685}, {"bertscore": 0.5688629703654442, "rouge_l": 0.036551186911880806}, {"bertscore": 0.5626452054857509, "rouge_l": 0.030011759870926705}, {"bertscore": 0.5783710006508045, "rouge_l": 0.030725073039936247}, {"bertscore": 0.5772172185534146, "rouge_l": 0.032383025606798635}, {"bertscore": 0.5777866567077581, "rouge_l": 0.03506838461256918}, {"bertscore": 0.5609237704338739, "rouge_l": 0.029044605390101418}, {"bertscore": 0.5765672811103286, "rouge_l": 0.03439281268918991}, {"bertscore": 0.5581671837280737, "rouge_l": 0.034694505071012266}], "total": {"test_bertscore": 56.982929293153575, "test_bertscore_se": 0.4647120601409885, "test_rouge_l": 3.1788930179713826, "test_rouge_l_se": 0.20791860267331763}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "cnn-dailymail", "task": "summarization", "dataset_languages": ["en"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"bertscore": 0.6060609215346631, "rouge_l": 0.13534356122687308}, {"bertscore": 0.6052511112939101, "rouge_l": 0.1359629636260784}, {"bertscore": 0.6100396363181062, "rouge_l": 0.1398364768660123}, {"bertscore": 0.6048478266457096, "rouge_l": 0.13306086663470937}, {"bertscore": 0.6000163537682965, "rouge_l": 0.12820025317831052}, {"bertscore": 0.6003884271194693, "rouge_l": 0.12766391853847778}, {"bertscore": 0.6008676686906256, "rouge_l": 0.13032640046462815}, {"bertscore": 0.6068474822386634, "rouge_l": 0.13565286096627224}, {"bertscore": 0.6001327027624939, "rouge_l": 0.1284344580179944}, {"bertscore": 0.5891177840821911, "rouge_l": 0.11083628418673605}], "total": {"test_bertscore": 60.235699144541286, "test_bertscore_se": 0.357417410825781, "test_rouge_l": 13.053180437060924, "test_rouge_l_se": 0.4966469099298961}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "danske-talemaader", "task": "knowledge", "dataset_languages": ["da"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"mcc": -0.00046352280064596496, "accuracy": 0.2685643564356436}, {"mcc": 0.09211136458750362, "accuracy": 0.3056930693069307}, {"mcc": 0.002081282419204388, "accuracy": 0.22896039603960397}, {"mcc": 0.09289959466289792, "accuracy": 0.2908415841584158}, {"mcc": -0.0012989347748862661, "accuracy": 0.2636138613861386}, {"mcc": -0.018597927137101573, "accuracy": 0.26485148514851486}, {"mcc": 0.1059136482531468, "accuracy": 0.3155940594059406}, {"mcc": 0.038213339386503994, "accuracy": 0.2871287128712871}, {"mcc": 0.13349149957779197, "accuracy": 0.3465346534653465}, {"mcc": 0.0, "accuracy": 0.24133663366336633}], "total": {"test_mcc": 4.443503441744149, "test_mcc_se": 3.470680616368439, "test_accuracy": 28.131188118811878, "test_accuracy_se": 2.1897617706263963}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "danish-citizen-tests", "task": "knowledge", "dataset_languages": ["da"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"mcc": -0.00045119304051956114, "accuracy": 0.3515625}, {"mcc": 0.0, "accuracy": 0.3671875}, {"mcc": 0.0, "accuracy": 0.3203125}, {"mcc": -0.022256690213700154, "accuracy": 0.3359375}, {"mcc": 0.025267800492149755, "accuracy": 0.001953125}, {"mcc": 0.0, "accuracy": 0.0}, {"mcc": -0.001969122773361612, "accuracy": 0.314453125}, {"mcc": 0.0, "accuracy": 0.291015625}, {"mcc": 0.19631211661998882, "accuracy": 0.435546875}, {"mcc": 0.0, "accuracy": 0.341796875}], "total": {"test_mcc": 1.9690291108455722, "test_mcc_se": 3.9090563363883417, "test_accuracy": 27.597656250000004, "test_accuracy_se": 9.293587628556429}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "mmlu-no", "task": "knowledge", "dataset_languages": ["nb", "nn", "no"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"mcc": 0.0891068493870715, "accuracy": 0.3212890625}, {"mcc": 0.0, "accuracy": 0.2109375}, {"mcc": 0.022387774381817816, "accuracy": 0.24365234375}, {"mcc": 0.07244956007058256, "accuracy": 0.259765625}, {"mcc": 0.01812823596099682, "accuracy": 0.28662109375}, {"mcc": 0.042287738191619295, "accuracy": 0.24609375}, {"mcc": 0.0252254356155525, "accuracy": 0.28173828125}, {"mcc": 0.00433459083923994, "accuracy": 0.2421875}, {"mcc": 0.03841590557664948, "accuracy": 0.2666015625}, {"mcc": 0.0, "accuracy": 0.2353515625}], "total": {"test_mcc": 3.123360900235299, "test_mcc_se": 1.8688733683554215, "test_accuracy": 25.942382812499996, "test_accuracy_se": 1.934847487628193}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "mmlu-sv", "task": "knowledge", "dataset_languages": ["sv"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"mcc": 0.047753558004267824, "accuracy": 0.26904296875}, {"mcc": 0.029552953704911313, "accuracy": 0.216796875}, {"mcc": 0.03313700492865062, "accuracy": 0.283203125}, {"mcc": 0.02494701409345586, "accuracy": 0.27783203125}, {"mcc": 0.048640852170360634, "accuracy": 0.2939453125}, {"mcc": 0.027762616160175196, "accuracy": 0.25537109375}, {"mcc": 0.013577059403190636, "accuracy": 0.26025390625}, {"mcc": 0.008381224525558733, "accuracy": 0.263671875}, {"mcc": 0.06963734699185925, "accuracy": 0.2998046875}, {"mcc": 0.0, "accuracy": 0.2314453125}], "total": {"test_mcc": 3.0338962998243004, "test_mcc_se": 1.291933279836593, "test_accuracy": 26.513671875, "test_accuracy_se": 1.617291574205832}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "mmlu-de", "task": "knowledge", "dataset_languages": ["de"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"mcc": 0.09883542186386682, "accuracy": 0.294921875}, {"mcc": 0.03023251397732839, "accuracy": 0.2236328125}, {"mcc": 0.3192924631116804, "accuracy": 0.44921875}, {"mcc": 0.08368387687988331, "accuracy": 0.2744140625}, {"mcc": 0.10253343134566831, "accuracy": 0.32421875}, {"mcc": 0.003531507261275105, "accuracy": 0.2685546875}, {"mcc": 0.009698756619981166, "accuracy": 0.2763671875}, {"mcc": 0.011232174309651573, "accuracy": 0.27099609375}, {"mcc": 0.019074403653681446, "accuracy": 0.28369140625}, {"mcc": 0.0, "accuracy": 0.22802734375}], "total": {"test_mcc": 6.781145490230164, "test_mcc_se": 6.014884022776113, "test_accuracy": 28.940429687500004, "test_accuracy_se": 3.924192506158685}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "mmlu-nl", "task": "knowledge", "dataset_languages": ["nl"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"mcc": 0.03561212940869092, "accuracy": 0.24853515625}, {"mcc": -0.018383995645940846, "accuracy": 0.2421875}, {"mcc": 0.02738980755120191, "accuracy": 0.283203125}, {"mcc": 0.07474311710030021, "accuracy": 0.2529296875}, {"mcc": -0.012949948908482774, "accuracy": 0.2490234375}, {"mcc": 0.021246850858505858, "accuracy": 0.25634765625}, {"mcc": 0.011439635141337966, "accuracy": 0.26806640625}, {"mcc": 0.011087847521241527, "accuracy": 0.2578125}, {"mcc": 0.045862345688768874, "accuracy": 0.2666015625}, {"mcc": 0.0, "accuracy": 0.2470703125}], "total": {"test_mcc": 1.9604778871562363, "test_mcc_se": 1.7378846992481916, "test_accuracy": 25.717773437500004, "test_accuracy_se": 0.7652327825661568}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "mmlu", "task": "knowledge", "dataset_languages": ["en"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"mcc": 0.04861190204238326, "accuracy": 0.30078125}, {"mcc": 0.049056992340985525, "accuracy": 0.25830078125}, {"mcc": 0.0409695716373977, "accuracy": 0.24755859375}, {"mcc": 0.0037729325261866404, "accuracy": 0.25048828125}, {"mcc": -0.019502416542182145, "accuracy": 0.26123046875}, {"mcc": -0.015827006539836888, "accuracy": 0.25830078125}, {"mcc": 0.04306180707384121, "accuracy": 0.28125}, {"mcc": 0.0011450361151127658, "accuracy": 0.25439453125}, {"mcc": -0.001039752255041198, "accuracy": 0.2578125}, {"mcc": 0.0, "accuracy": 0.21435546875}], "total": {"test_mcc": 1.5024906639884688, "test_mcc_se": 1.6901668871493196, "test_accuracy": 25.844726562499996, "test_accuracy_se": 1.380180051117955}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "arc-is", "task": "knowledge", "dataset_languages": ["is"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"mcc": 0.09927607744731576, "accuracy": 0.3095703125}, {"mcc": 0.04664637393706717, "accuracy": 0.2763671875}, {"mcc": 0.021407743212551483, "accuracy": 0.263671875}, {"mcc": 0.025299578452590777, "accuracy": 0.251953125}, {"mcc": 0.0, "accuracy": 0.248046875}, {"mcc": -0.00802887857503988, "accuracy": 0.2705078125}, {"mcc": 0.026840223706476343, "accuracy": 0.26953125}, {"mcc": 0.02320121568500406, "accuracy": 0.259765625}, {"mcc": 0.01669208981065664, "accuracy": 0.2724609375}, {"mcc": 0.0, "accuracy": 0.244140625}], "total": {"test_mcc": 2.513344236766223, "test_mcc_se": 1.8952148706550864, "test_accuracy": 26.66015625, "test_accuracy_se": 1.153742089428686}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "hellaswag-da", "task": "common-sense-reasoning", "dataset_languages": ["da"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"mcc": -0.006224910157208287, "accuracy": 0.236328125}, {"mcc": 0.011258462448832995, "accuracy": 0.25}, {"mcc": 0.042593692221282405, "accuracy": 0.25830078125}, {"mcc": 0.04544555635981944, "accuracy": 0.2734375}, {"mcc": 0.019825931473748143, "accuracy": 0.24658203125}, {"mcc": 0.04505091488857091, "accuracy": 0.27587890625}, {"mcc": 0.021698837479366823, "accuracy": 0.2705078125}, {"mcc": -0.0012613269387189467, "accuracy": 0.24755859375}, {"mcc": 0.07719889611446275, "accuracy": 0.3056640625}, {"mcc": 0.0, "accuracy": 0.26220703125}], "total": {"test_mcc": 2.555860538901562, "test_mcc_se": 1.6485961651213143, "test_accuracy": 26.2646484375, "test_accuracy_se": 1.2329205775996577}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "hellaswag-no", "task": "common-sense-reasoning", "dataset_languages": ["nb", "nn", "no"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"mcc": 0.041141344851147274, "accuracy": 0.240234375}, {"mcc": 0.07652257938159993, "accuracy": 0.30615234375}, {"mcc": 0.15519014785550325, "accuracy": 0.35107421875}, {"mcc": 0.02539910293950097, "accuracy": 0.26611328125}, {"mcc": -0.010410844940147887, "accuracy": 0.2578125}, {"mcc": 0.015803731095274127, "accuracy": 0.2509765625}, {"mcc": 0.011756232047100636, "accuracy": 0.2529296875}, {"mcc": 0.031163970537893295, "accuracy": 0.2646484375}, {"mcc": 0.044352095323887265, "accuracy": 0.28076171875}, {"mcc": 0.0, "accuracy": 0.251953125}], "total": {"test_mcc": 3.9091835909175887, "test_mcc_se": 2.953063017361945, "test_accuracy": 27.2265625, "test_accuracy_se": 2.06620266535463}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "hellaswag-sv", "task": "common-sense-reasoning", "dataset_languages": ["sv"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"mcc": 0.07037003458011923, "accuracy": 0.2880859375}, {"mcc": 0.05784562095787926, "accuracy": 0.28466796875}, {"mcc": 0.07337451316447649, "accuracy": 0.2744140625}, {"mcc": 0.052086788975086284, "accuracy": 0.275390625}, {"mcc": 0.0697978788993383, "accuracy": 0.26953125}, {"mcc": 0.01853301349731732, "accuracy": 0.25048828125}, {"mcc": 0.028939689806786597, "accuracy": 0.263671875}, {"mcc": -0.01614315211592065, "accuracy": 0.24169921875}, {"mcc": 0.060064512236252886, "accuracy": 0.271484375}, {"mcc": 0.0, "accuracy": 0.25537109375}], "total": {"test_mcc": 4.148689000013357, "test_mcc_se": 1.9713307951699222, "test_accuracy": 26.748046874999996, "test_accuracy_se": 0.9159321842178898}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "winogrande-is", "task": "common-sense-reasoning", "dataset_languages": ["is"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"mcc": 0.0, "accuracy": 0.4486607142857143}, {"mcc": -0.04356671006129078, "accuracy": 0.5524553571428571}, {"mcc": 0.0, "accuracy": 0.4341517857142857}, {"mcc": -0.030030003152850194, "accuracy": 0.5223214285714286}, {"mcc": 0.0, "accuracy": 0.5803571428571429}, {"mcc": 0.0, "accuracy": 0.5479910714285714}, {"mcc": 0.0, "accuracy": 0.5837053571428571}, {"mcc": 0.0, "accuracy": 0.5401785714285714}, {"mcc": 0.0, "accuracy": 0.5904017857142857}, {"mcc": 0.0, "accuracy": 0.5580357142857143}], "total": {"test_mcc": -0.7359671321414097, "test_mcc_se": 0.9817866960455665, "test_accuracy": 53.58258928571429, "test_accuracy_se": 3.351066275425132}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "hellaswag-de", "task": "common-sense-reasoning", "dataset_languages": ["de"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"mcc": 0.09632764846684086, "accuracy": 0.27197265625}, {"mcc": 0.0999392701050278, "accuracy": 0.32080078125}, {"mcc": 0.11112277060428583, "accuracy": 0.28466796875}, {"mcc": 0.06357547631606951, "accuracy": 0.27587890625}, {"mcc": 0.032983145788409565, "accuracy": 0.25830078125}, {"mcc": 0.0, "accuracy": 0.2646484375}, {"mcc": 0.06694835154369849, "accuracy": 0.27783203125}, {"mcc": 0.07744932961300977, "accuracy": 0.27197265625}, {"mcc": 0.4018287681666155, "accuracy": 0.52978515625}, {"mcc": 0.03724613375429364, "accuracy": 0.26318359375}], "total": {"test_mcc": 9.874208943582511, "test_mcc_se": 6.9320786237961975, "test_accuracy": 30.190429687499996, "test_accuracy_se": 5.078422400896711}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "hellaswag-nl", "task": "common-sense-reasoning", "dataset_languages": ["nl"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"mcc": 0.03591578416626457, "accuracy": 0.2548828125}, {"mcc": 0.013372269248057537, "accuracy": 0.251953125}, {"mcc": 0.12763201971623714, "accuracy": 0.2939453125}, {"mcc": 0.12673505611422597, "accuracy": 0.2861328125}, {"mcc": 0.0, "accuracy": 0.2626953125}, {"mcc": 0.03174377341143525, "accuracy": 0.27099609375}, {"mcc": -0.01613047460042263, "accuracy": 0.24658203125}, {"mcc": 0.037372064931583765, "accuracy": 0.26953125}, {"mcc": 0.058962097366002775, "accuracy": 0.28662109375}, {"mcc": 0.0, "accuracy": 0.22998046875}], "total": {"test_mcc": 4.156025903533845, "test_mcc_se": 3.112019194167669, "test_accuracy": 26.533203125, "test_accuracy_se": 1.2483216339228584}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "hellaswag", "task": "common-sense-reasoning", "dataset_languages": ["en"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"mcc": 0.08356353523228523, "accuracy": 0.28857421875}, {"mcc": 0.013955674948677846, "accuracy": 0.24462890625}, {"mcc": 0.34482904963068495, "accuracy": 0.490234375}, {"mcc": 0.06389646003238289, "accuracy": 0.25732421875}, {"mcc": 0.03758692006065512, "accuracy": 0.248046875}, {"mcc": 0.006174328065355721, "accuracy": 0.24072265625}, {"mcc": 0.08459201043594734, "accuracy": 0.29736328125}, {"mcc": -0.000586032355590418, "accuracy": 0.2412109375}, {"mcc": 0.042962672143448136, "accuracy": 0.2734375}, {"mcc": 0.0, "accuracy": 0.25048828125}], "total": {"test_mcc": 6.76974618193847, "test_mcc_se": 6.360891540660529, "test_accuracy": 28.3203125, "test_accuracy_se": 4.675717731760738}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "results": {"raw": [{"test_speed": 297.49, "test_speed_short": 34.08}, {"test_speed": 569.64, "test_speed_short": 62.85000000000001}, {"test_speed": 848.2199999999999, "test_speed_short": 121.22}, {"test_speed": 1129.62, "test_speed_short": 152.64000000000001}, {"test_speed": 1425.06, "test_speed_short": 181.89000000000001}, {"test_speed": 1675.58, "test_speed_short": 238.83}, {"test_speed": 1954.18, "test_speed_short": 268.16}, {"test_speed": 2238.3900000000003, "test_speed_short": 296.07}, {"test_speed": 2511.38, "test_speed_short": 328.38}, {"test_speed": 2804.0, "test_speed_short": 360.40000000000003}], "total": {"test_speed": 1545.3560000000002, "test_speed_se": 521.2058946998465, "test_speed_short": 204.452, "test_speed_short_se": 69.29990766115507}}, "num_model_parameters": 32763876352, "max_sequence_length": 131072, "vocabulary_size": 152064, "generative": true, "few_shot": true, "validation_split": false, "scandeval_version": "14.3.0"}